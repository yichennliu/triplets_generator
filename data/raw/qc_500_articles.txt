quantum computing for all
programs for quantum computers are still largely written in assembly, where bits are initiated at 0 (or 1), then the circuit is configured manually using gates to account for the logical steps needed for the program to work.
the tool, qubit workbench, offers users one of the best experiences to learn and get a head start in a nascent and growing field.
it is the first circuit tool in the world to allow users to build programs upwards of 100 qubits in one workspace.
prospective software engineers and enthusiasts are able to build their intuition about quantum programming and familiarize themselves with the gate- level logic required to become better at writing software for these computers.
the tool is aimed for everyday users wanting to learn quantum programming as well as educators.
a visual way forward
workbench is available for individuals or enterprise customers.
companies looking to build their own software have the additional option of licensing out elyah's quantum simulator to save on time.
"the tool is fairly new, and we're still making large improvements month by month based on feedback," says sal, an engineer at the company.
"we aim to provide the best circuit interface product in the market before the years' end, and we're very close."
the tool allows for a lot more flexibility than existing ones.
you can create functions, download .tex versions of your circuits to convert to pdf and simulate with a click of a button, removing the need to simulate from your cli.
in addition to the extra features, what distinguishes workbench from many other platforms is the teams' unique attention to design and detail.
the tool is built to open the door for a broader more commercial era in quantum computing, and quantum software development.
what is elyah?
elyah was founded in 2018 just a year before google's quantum supremacy claim.
the company has focused its efforts in providing the best quality tools for end users to jump into quantum computing.
the challenge with the industry at the moment is figuring what the early use cases of quantum computers would be.
elyah has focused its efforts into building simple software solutions for future fault tolerant machines with a focus on search and optimization.
find out more here.
photo(s):
a team of harvard physicists led by mikhail d. lukin has achieved the first- ever quantum entanglement of photons and solid- state materials.
the work marks a key advance toward practical quantum networks, as the first experimental demonstration of a means by which solid- state quantum bits, or "qubits," can communicate with one another over long distances.
quantum networking applications such as long- distance communication and distributed computing would require the nodes that process and store quantum data in qubits to be connected to one another by entanglement, a state where two different atoms become indelibly linked such that one inherits the properties of the other.
"in quantum computing and quantum communication, a big question has been whether or how it would be possible to actually connect qubits, separated by long distances, to one another," says lukin, professor of physics at harvard and co- author of a paper describing the work in this week's issue of the journal nature.
"demonstration of quantum entanglement between a solid- state material and photons is an important advance toward linking qubits together into a quantum network."
quantum entanglement has previously been demonstrated only with photons and individual ions or atoms.
"our work takes this one step further, showing how one can engineer and control the interaction between individual photons and matter in a solid- state material," says first author emre togan, a graduate student in physics at harvard.
"what's more, we show that the photons can be imprinted with the information stored in a qubit."
quantum entanglement, famously termed "spooky action at a distance" by a skeptical albert einstein, is a fundamental property of quantum mechanics.
it allows one to distribute quantum information over tens of thousands of kilometers, limited only by how fast and how far members of the entangled pair can propagate in space.
the new result builds upon earlier work by lukin's group to use single atom impurities in diamonds as qubits.
lukin and colleagues have previously shown that these impurities can be controlled by focusing laser light on a diamond lattice flaw where nitrogen replaces an atom of carbon.
that previous work showed that the so- called spin degrees of freedom of these impurities make excellent quantum memory.
lukin and his co- authors now say that these impurities are also remarkable because, when excited with a sequence of finely tuned microwave and laser pulses, they can emit photons one at a time, such that photons are entangled with quantum memory.
such a stream of single photons can be used for secure transmission of information.
"since photons are the fastest carriers of quantum information, and spin memory can robustly store quantum information for relatively long periods of time, entangled spin- photon pairs are ideal for the realization of quantum networks," lukin says.
"such a network, a quantum analog to the conventional internet, could allow for absolutely secure communication over long distances."
an international team of researchers have found a new way to speed up quantum computing that could pave the way for huge leaps forward in computer processing power.
scientists from the university of nottingham and university of stockholm have sped- up trapped ion quantum computing using a new experimental approach -  trapped rydberg ions; their results have just been published in nature.
in conventional digital computers, logic gates consist of operational bits that are silicon based electronic devices.
information is encoded in two classical states ("0" and "1") of a bit.
this means that capacities of a classical computer increase linearly with the number of bits.
to deal with emerging scientific and industrial problems, large computing facilities or supercomputers are built.
quantum entanglement enhancing capacity
a quantum computer is operated using quantum gates, i.e.
basic circuit operations on quantum bits (qubits) that are made of microscopic quantum particles, such as atoms and molecules.
a fundamentally new mechanism in a quantum computer is the utilisation of quantum entanglement, which can bind two or a group of qubits together such that their state can no longer be described by classical physics.
the capacity of a quantum computer increases exponentially with the number of qubits.
the efficient usage of quantum entanglement drastically enhances the capacity of a quantum computer to be able to deal with challenging problems in areas including cryptography, material, and medicine sciences.
among the different physical systems that can be used to make a quantum computer, trapped ions have led the field for years.
the main obstacle towards a large- scale trapped ion quantum computer is the slow- down of computing operations as the system is scaled- up.
this new research may have found the answer to this problem.
the experimental work was conducted by the group of markus hennrich at su using giant rydberg ions, 100,000,000 times larger than normal atoms or ions.
these huge ions are highly interactive, and exchange quantum information in less than a microsecond.
the interaction between them creates quantum entanglement.
chi zhang from the university of stockholm and colleagues used the entangling interaction to carry out a quantum computing operation (an entangling gate) around 100 times faster than is typical in trapped ion systems.
chi zhang explains, "usually quantum gates slow down in bigger systems.
this isn't the case for our quantum gate and rydberg ion gates in general!
our gate might allow quantum computers to be scaled up to sizes where they are truly useful!"
theoretical calculations supporting the experiment and investigating error sources have been conducted by weibin li (university of nottingham, uk) and igor lesanovsky (university of nottingham, uk, and university of tubingen, germany).
their theoretical work confirmed that there is indeed no slowdown expected once the ion crystals become larger, highlighting the prospect of a scalable quantum computer.
weibin li, assistant professor, school of physics and astronomy at the university of nottingham adds: "our theoretical analysis shows that a trapped rydberg ion quantum computer is not only fast, but also scalable, making large- scale quantum computation possible without worrying about environmental noise.
the joint theoretical and experimental work demonstrate that quantum computation based on trapped rydberg ions opens a new route to implement fast quantum gates and at the same time might overcome many obstacles found in other systems."
currently the team is working to entangle larger numbers of ions and achieve even faster quantum computing operations.
ionic rain irrigates forests of nanotubes, while ionic winds blow cool breezes over chips.
now, the ionic solid- state harbors the flag- ship of quantum computing- quantum bits.
q- bits can now be encoded on the spin of the electron that makes a quantum dot ionic, promising cheap, ultra- low- power and easy- to- fabricate quantum computers, according to researchers at the university of michigan (ann arbor), the university of california (san diego) and the naval research laboratory (washington, d.c.).
quantum computing could enable uncrackable encryption codes, accounting for the funding of such researcher by the national security agency (nsa).
quantum- computing milestones are no new thing these researchers who had already demonstrated the world's first quantum gate in a semiconductor.
"we had already demonstrated that a solid- state system could exhibit the qualities needed by quantum computers," said duncan steel, an ee professor at the university of michigan.
"but now we have an architecture that is scalable."
steel's group is one of about a half a dozen worldwide that are using a photonic material- gallium arsenide- to house quantum dots that are activated by a laser.
the indium arsenide quantum dots are 10- to 30- nanometers in diameter, surrounded by gallium arsenide.
for the current demonstration, steel's group had individual quantum dots grown at the naval research laboratory.
the original demonstration used lasers to toggle the indium arsenide atoms between two different quantum states of excitation, whereas the current demonstration encodes that quantum state onto a single electron inside the dot.
"each quantum dot has about a billion electrons, but it is electrically neutral.
we add one more electron to give a charge of minus one," said steel.
according to steel, encoding quantum bits on the spin of this single electron enables much longer quantum calculations to be performed- more than 100- fold longer than the coherence time of an atom.
unfortunately, all the designs of quantum computers, so far, have used the excitation of atoms to store quantum states.
by moving to electrons in semiconductors these researchers claim that quantum computers can perform much longer calculations.
what's more, all the manufacturing techniques for gallium- arsenide chips can be used to fabricating the quantum computer chips.
"the advantage of spin excitation is that its quantum memory is orders of magnitude longer- that's why so many groups are interested in it today," said steel.
about six groups worldwide are working on storing q- bits on the spin of electrons in gallium arsenide, but steel claims his group has consistently stayed on step ahead of the pack.
steel also claims that by encoding the q- bits onto a single electron, with a pulsed laser, his new design sets the standard for using ultra- low power to process information- over a millions times less than the nano- joules needed to switch a semiconductor memory bit.
"it only takes about ten to the minus 18 joules [18 billionths of a nano- joule] of power to switch from one quantum state to the other," said steel.
next, steel's group will fabricate quantum dots so close together that their wavelengths overlap with the adjacent quantum dots in the array.
the dual nature of matter enables the energy of an electron to be measured by its wavelength.
by making the quantum dots close enough together, the wavelength of the electrons, the spin of which holds the q- bit, can be made to overlap adjacent quantum dots in the array, thereby enabling quantum computations to be performed by virtue of "entanglement."
"right now we plan to demonstrate quantum entanglement by making our quantum dots about 20 nanometers in diameter and spacing them about 30 nanometers center to center," said steel.
usc scientists have demonstrated a theoretical method to enhance the performance of quantum computers, an important step to scale a technology with potential to solve some of society's biggest challenges.
the method addresses a weakness that bedevils performance of the next- generation computers by suppressing erroneous calculations while increasing fidelity of results, a critical step before the machines can outperform classic computers as intended.
called "dynamical decoupling," it worked on two quantum computers, proved easier and more reliable than other remedies and could be accessed via the cloud, which is a first for dynamical decoupling.
the technique administers staccato bursts of tiny, focused energy pulses to offset ambient disturbances that muck sensitive computations.
the researchers report they were able to sustain a quantum state up to three times longer than would otherwise occur in an uncontrolled state.
"this is a step forward," said daniel lidar, professor of electrical engineering, chemistry and physics at usc and director of the usc center for quantum information science and technology (cqist).
"without error suppression, there's no way quantum computing can overtake classical computing."
the results were published today in the journal physical review letters.
lidar is the viterbi professor of engineering at usc and corresponding author of the study; he led a team of researchers at cqist, which is a collaboration between the usc viterbi school of engineering and the usc dornsife school of letters, arts and sciences.
ibm and bay area startup rigetti computing provided cloud access to their quantum computers.
quantum computers are fast, but fragile
quantum computers have the potential to render obsolete today's super computers and propel breakthroughs in medicine, finance and defense capabilities.
they harness the speed and behavior of atoms, which function radically different than silicon computer chips, to perform seemingly impossible calculations.
quantum computing has the potential to optimize new drug therapies, models for climate change and designs for new machines.
they can achieve faster delivery of products, lower costs for manufactured goods and more efficient transportation.
they are powered by qubits, the subatomic workhorses and building blocks of quantum computing.
but qubits are as temperamental as high- performance race cars.
they are fast and hi- tech, but prone to error and need stability to sustain computations.
when they don't operate correctly, they produce poor results, which limits their capabilities relative to traditional computers.
scientists worldwide have yet to achieve a "quantum advantage- the point where a quantum computer outperforms a conventional computer on any task.
the problem is "noise," a catch- all descriptor for perturbations such as sound, temperature and vibration.
it can destabilize qubits, which creates "decoherence," an upset that disrupts the duration of the quantum state, which reduces time a quantum computer can perform a task while achieving accurate results.
"noise and decoherence have a large impact and ruin computations, and a quantum computer with too much noise is useless," lidar explained.
"but if you can knock down the problems associated with noise, then you start to approach the point where quantum computers become more useful than classic computers."
usc research spans multiple quantum computing platforms
usc is the only university in the world with a quantum computer; its 1098- qubit d- wave quantum annealer specializes in solving optimization problems.
part of the usc- lockheed martin center for quantum computing, it's located at usc's information sciences institute.
however, the latest research findings were achieved not on the d- wave machine, but on smaller scale, general- purpose quantum computers: ibm's 16- qubit qx5 and rigetti's 19- qubit acorn.
to achieve dynamical decoupling (dd), the researchers bathed the superconducting qubits with tightly focused, timed pulses of minute electromagnetic energy.
by manipulating the pulses, scientists were able to envelop the qubits in a microenvironment, sequestered- or decoupled- from surrounding ambient noise, thus perpetuating a quantum state.
"we tried a simple mechanism to reduce error in the machines that turned out to be effective," said bibek pokharel, an electrical engineering doctoral student at usc viterbi and first author of the study.
the time sequences for the experiments were exceedingly small with up to 200 pulses spanning up to 600 nanoseconds.
one- billionth of a second, or a nanosecond, is how long it takes for light to travel one foot.
for the ibm quantum computers, final fidelity improved threefold, from 28.9 percent to 88.4 percent.
for the rigetti quantum computer, final fidelity improvement was a more modest 17 percent, from 59.8 to 77.1, according to the study.
the scientists tested how long fidelity improvement could be sustained and found that more pulses always improved matters for the rigetti computer, while there was a limit of about 100 pulses for the ibm computer.
overall, the findings show the dd method works better than other quantum error correction methods that have been attempted so far, lidar said.
"to the best of our knowledge," the researchers wrote, "this amounts to the first unequivocal demonstration of successful decoherence mitigation in cloud- based superconducting qubit platforms ... we expect that the lessons drawn will have wide applicability."
high stakes in the race for quantum supremacy
the quest for quantum computing supremacy is a geopolitical priority for europe, china, canada, australia and the united states.
advantage gained by acquiring the first computer that renders all other computers obsolete would be enormous and bestow economic, military and public health advantages to the winner.
congress is considering two new bills to establish the united states as a leader in quantum computing.
in september, the house of representatives passed the national quantum initiative act to allocate $1.3 billion in five years to spur research and development.
it would create a national quantum coordination office in the white house to supervise research nationwide.
a separate bill, the quantum computing research act by sen. kamala harris, d- calif., directs the department of defense to lead a quantum computing effort.
"quantum computing is the next technological frontier that will change the world and we cannot afford to fall behind," harris said in prepared remarks.
"it could create jobs for the next generation, cure diseases and above all else make our nation stronger and safer.
...
without adequate research and coordination in quantum computing, we risk falling behind our global competition in the cyberspace race, which leaves us vulnerable to attacks from our adversaries," she said.
when a tiny, quantum- scale, hypothetical balloon is popped in a vacuum, do the particles inside spread out all over the place as predicted by classical mechanics"
the question is deceptively complex, since quantum particles do not look or act like air molecules in a real balloon.
matter at the infinitesimally small quantum scale is both a wave and a particle, and its location cannot be fixed precisely because measurement alters the system.
now, theoretical physicists at the university of southern california and the university of massachusetts boston have proven a long- standing hypothesis that quantum- scale chaos exists ... sort of.
writing in the april 17 edition of nature, senior author maxim olshanii reported that when an observer attempts to measure the energies of particles coming out of a quantum balloon, the interference caused by the attempt throws the system into a final, "relaxed" state analogous to the chaotic scattering of air molecules.
the result is the same for any starting arrangement of particles, olshanii added, since the act of measuring wipes out the differences between varying initial states.
"it's enough to know the properties of a single stationary state of definite energy of the system to predict the properties of the thermal equilibrium (the end state)," olshanii said.
the measurement - which must involve interaction between observer and observed, such as light traveling between the two - disrupts the "coherent" state of the system, olshanii said.
in mathematical terms, the resulting interference reveals the final state, which had been hidden in the equations describing the initial state of the system.
"the thermal equilibrium is already encoded in an initial state," olshanii said.
"you can see some signatures for the future equilibrium.
they were already there but more masked by quantum coherences."
the finding holds implications for the emerging fields of quantum computing and quantum information theory, said paolo zanardi, an associate professor of physics studying quantum information at usc.
in zanardi's world, researchers want to prevent coherent systems from falling into the chaos of thermal equilibrium.
"finding such 'unthermalizable' states of matter and manipulating them is exactly one of those things that quantum information/computation folks like me would love to do," zanardi wrote.
"such states would be immune from 'decoherence' (loss of quantum coherence induced by the coupling with environment) that's still the most serious, both conceptually and practically, obstacle between us and viable quantum information processing."
zanardi and a collaborator introduced the notion of "decoherence- free" quantum states in 1997.
researchers such as zanardi and daniel lidar, associate professor of chemistry, among others, have helped make usc a major center for the study of quantum computing.
posted april 18th,2008
quantum computing can solve challenges that modern computers can't- or it might take them a billion years to do so.
it can crack any encryption and make your data completely safe.
google reports that it has seen a quantum computer that performed at least 100 million times faster than any classical computer in its lab.
quantum blows away the processing of data and algorithms on conventional computers because of its ability to operate on electrical circuits that can be in more than one state at once.
a quantum computer operates on qubits (quantum bits) instead of on the standard bits that are used in conventional computing.
see: managing ai and ml in the enterprise 2020: tech leaders increase project development and implementation (techrepublic premium)
quantum results can quickly make an impact on life science and pharmaceutical companies, for financial institutions evaluating portfolio risks, and for other organizations that want to expedite time- to- results for processing that on conventional computing platforms would take days to complete.
who is using quantum computing?
few corporate ceos are comfortable trying to explain to their boards what quantum computing is and why it is important to invest in it.
"there are three major areas where we see immediate corporate engagement with quantum computing," said christopher savoie, ceo and co- founder of zapata quantum computing software company, a quantum computing solutions provider backed by honeywell.
"these areas are machine learning, optimization problems, and molecular simulation."
savoie said quantum computing can bring better results in machine learning than conventional computing because of its speed.
this rapid processing of data enables a machine learning application to consume large amounts of multi- dimensional data that can generate more sophisticated models of a particular problem or phenomenon under study.
see: forget quantum supremacy: this quantum- computing milestone could be just as important (techrepublic)
quantum computing is also well suited for solving problems in optimization.
"the mathematics of optimization in supply and distribution chains is highly complex," savoie said.
"you can optimize five nodes of a supply chain with conventional computing, but what about 15 nodes with over 85 million different routes?
add to this the optimization of work processes and people, and you have a very complex problem that can be overwhelming for a conventional computing approach."
a third application area is molecular simulation in chemistry and pharmaceuticals, which can be quite complex.
in each of these cases, models of circumstances, events, and problems can be rapidly developed and evaluated from a variety of dimensions that collate data from many diverse sources into a model.
see: inside ups: the logistics company's never- ending digital transformation (free pdf) (techrepublic)
"the current covid- 19 crisis is a prime example," savoie said.
"bill gates knew in 2015 that handling such a pandemic would present enormous challenges- but until recently, we didn't have the models to understand the complexities of those challenges."
quantum computing is still very new
for those engaging in quantum computing and analytics today, the relative newness of the technology presents its own share of glitches.
this makes it important to have quantum computing experts on board.
for this reason, most early adopter companies elect to go to the cloud for their quantum computing, partnering with a vendor that has the specialized expertise needed to run and maintain quantum analytics.
see: rural america is in the midst of a mental health crisis.
tech could help some patients see a way forward.
(cover story pdf) (techrepublic)
"these companies typically use a kubernetes cluster and management stack on premises," savoie said.
"they code a quantum circuit that contains information on how operations are to be performed on quantum qubits.
from there, the circuit and the prepared data are sent to the cloud, which performs the quantum operations on the data.
the data is processed in the cloud and sent back to the on- prem stack, and the process repeats itself until processing is complete."
savoie estimated that broad adoption of quantum computing for analytics will occur within a three- to five- year timeframe, with early innovators in sectors like oil and gas, and chemistry, that already understand the value of the technology and are adopting sooner.
"whether or not you adopt quantum analytics now, you should minimally have it on your it roadmap," savoie said.
"quantum computing is a bit like the covid- 19 crisis.
at first, there were only two deaths; then two weeks later, there were ten thousand.
quantum computing and analytics is a highly disruptive technology that can exponentially advance some companies over others."
also see
a new technique to study the properties of molecules and materials on a quantum simulator has been discovered.
the ground- breaking new technique, by physicist oleksandr kyriienko from the university of exeter, could pioneer a new pathway towards the next generation of quantum computing.
current quantum computing methods for studying the properties of molecules and materials on such a minute scale rely upon an ideal fault- tolerant quantum computer or variational techniques.
this new proposed approach, instead relies on the implementation of quantum evolution that would be readily available in many systems.
the approach is favourable for modern state- of- the- art quantum setups, notably including cold atom lattices, and can serve as a software for future applications in material science.
the study could pave the way to studying the properties of strongly correlated systems, including coveted fermi- hubbard model, which can potentially offer the explanation of high- temperature superconductivity.
the research is published in the new nature journal npj quantum information.
dr kyriienko, part of the physics department at the university of exeter and lead author said: "so far i have seen that the ability to run quantum dynamics can be used for finding the ground state properties.
"the question, however, remains -  can we use it for studying excited states?
can we devise other powerful algorithm based on the principles?
the experience tells this is possible, and will be a subject of future efforts."
the idea of quantum simulation was proposed by nobel prize winner richard feynman in 1982, where he suggested that quantum models can be most naturally simulated if we use a well- controlled and inherently quantum system.
developing on this idea, a separate branch of quantum information science has emerged, based on the notion of quantum computer -  a universal quantum device where digital sequences of operations (quantum gates) allow to solve certain problems with superior scaling of required operation as compared to conventional classical computers.
however, the original feynman's intention, which was later named analog quantum simulation, so far was mostly used for observing dynamical properties of quantum systems, while precluding finding the ground state associated to various computation tasks.
in the new study, oleksandr kyriienko has shown that it is possible to exploit sequential evolution of the system with wavefunction overlap measurements, such that effective study of ground state properties becomes possible with analog quantum simulators.
the main technique which allows to reach ground state is effective representation of non- unitary operator which "distils" the ground state by running the sum of unitary evolution operators for different evolution times.
importantly, the study suggests that dynamics of the quantum system is a valuable resource for computation, as the ability to propagate the system paired with overlap measurements can give access to the low- temperature spectrum of a quantum system which define its behaviour.
the findings establish the framework with dynamics- based quantum simulation using programmable quantum simulators, and serve as a quantum software to many well- controlled quantum lattice systems where large number of atoms (~100) precludes classical simulation.
this in turn can revolutionize our understanding of complex condensed matter systems and chemistry.
teleportation has long been a subject of popular fascination in modern science fiction.
it's inclusion in many books, films and television programs seems to suggest a subconscious association with teleportation as almost a kind of scientific magic.
an academic group of 20 european and chinese researchers has reportedly demonstrated chip- to- chip quantum teleportation- that is, they successfully passed information between computer chips via quantum entanglement.
their research paper describing the demonstration was published in nature physics.
the demonstration
the researchers generated pairs of entangled photons with encoded quantum information, and showed the utility of the entanglement link.
the group demonstrated that the two photons, when placed on two separate computer chips, maintained a single quantum state.
the teleportation refers to the bizarre fact that measuring one of the entangled photons immediately reveals the state of the other.
in their experiments, the researchers achieved a 91 percent fidelity rate for teleporting information.
the work is part of a relatively new field of research that subjects the physical world to true scientific sorcery: quantum engineering.
quantum engineers build and use computing hardware called silicon- photonic circuitry that harnesses quantum entanglement.
this chip- to- chip quantum teleportation would not be possible without the silicon- photonic hardware.
in this case, it was a reprogrammable linear- optic quantum circuit.
silicon- photonic circuitry belongs to the field of technological research known as integrated optics.
originally inspired by the electronic integrated circuit, integrated optics researchers design and create integrated optical devices, planar lightwave circuits and photonic integrated circuits.
these devices are constructed from different combinations of optical components such as lasers, photodetectors, optical filters, amplifiers and modulators.
bottom line
the foundations of classical physics were rocked for three decades in the early 20th century with the first quantum revolution.
the mechanics of atomic and subatomic physical laws were unmasked and readied for what engineers do best: they apply the laws of science to create innovative new technology.
through studying scientific discovery as it applies to the limitations of technological innovations, some of today's engineers are working out how quantum mechanics can be applied to networked and mobile computing hardware.
engineers have built quantum computers that successfully transfer information carried on qubits by applying the laws of quantum mechanics to the physical world.
quantum engineers are now fundamentally manipulating reality at a quantum level.
leti, an institute of cea, has created a quantum integrated circuit that demonstrates the possibility of integrating conventional electronic devices and elements with quantum dots on a cmos chip.
the chip, fabricated on 28nm fd- soi process, integrates analog and digital functions (multiplexer, buffer, signal amplifier, oscillator, level converter) that represent future instrumentation needs for the quantum accelerator envisioned in cea- leti's quantum initiative.
beyond the need to obtain reliable, entangled and coherent quantum bits, or qubits, on silicon, the goal of this work is ultimately to produce electronics capable of routing numerous signals to address a matrix of several hundred qubits.
these results also demonstrate cea- leti's know- how in cryogenic instrumentation in fd- soi technology and can also be used for other non- silicon quantum devices such as superconducting qubits.
the results were presented in a paper, 'a 110mk 295mw 28nm fd- soi cmos quantum integrated circuit with a 2.8ghz excitation and na current sensing of an on- chip double quantum dot', on feb. 18th at isscc 2020 in san francisco.
"to reach quantum supremacy, quantum computers need >50 logical qubits with <mv accurate biasing, ghz- range signal handling, and ms readout of thousands of physical qubits at sub- kelvin temperatures," the paper reported.
"silicon- based qubits are a promising approach to scale the qubit number owing to their low footprint (100 nm) and gaining from the cmos industrial background to reach maturity.
"moreover, the quantum silicon choice allows the ic community to integrate large- scale qubit- control electronics directly nearby the quantum silicon core, thus drastically reducing the wire- connection number and qubit- addressing fanout, meanwhile increasing the operation bandwidth for error correction and the spin- readout sensitivity."
loick le guevel, a lead author of the paper, explained that the quantum integrated circuit is a proof- of- concept circuit merging microelectronics benchmarks and quantum dots operating at sub- kelvin temperature within a limited power budget.
"it uses all elements required to properly design high- spec state- of- the- art circuits, such as passive elements, resistors and capacitors, transistors for digital operation up to 7ghz, and transistors for analog operation up to 3ghz," he said.
"on top of that, we were able to design a double quantum dot in the same semiconductor layer as transistors using a standard fabrication flow.
this realization emphasizes that fd- soi could one day allow circuit designers to use qubit arrays embedded in ip blocks with classic electronics to build custom- made, large- scale quantum silicon processors."
le guevel is a ph.d. student supervised by gael pillonnet and louis jansen in the quantum silicon grenoble research group led by maud vinet.
in addition to demonstrating a highly sensitive analog current read- out operating at 110mk, 40x lower than competing technologies, within the limited power budget, the researchers demonstrated the possibility of having ghz digital signal generation, and ghz signal analog manipulation.
a quantum dot structure, very similar to the first realization of a spin qubit on a silicon 300mm industrial wafer by the quantum silicon grenoble group, also was fabricated on the same semiconductor layer using industrial- grade design software and common foundry design rules.
despite the close proximity (<1 micrometer) of dissipating high- speed electronics and a sensitive quantum- dot device, quantum effects were preserved and open a path toward an industrially made quantum dot device that will benefit from a higher reproducibility/yield of the silicon cmos technology.
in the medium- term, the quantum community is focused on noisy intermediate- scale quantum technology (nisq) that has the potential to outperform classical supercalculators in some specific tasks, such as path optimization, quantum deep learning, neural networks, ai and recommendation systems.
this first co- integration together with the recently published performance on silicon- based qubits confirms that silicon is a serious contender, as it allows fast operations while maintain competitive fidelity with controlled process reproducibility on a scalable footprint.
an international group of researchers has achieved the world's first multi- qubit demonstration of a quantum chemistry calculation performed on a system of trapped ions, one of the leading hardware platforms in the race to develop a universal quantum computer.
the research, led by university of sydney physicist dr. cornelius hempel, explores a promising pathway for developing effective ways to model chemical bonds and reactions using quantum computers.
it is published today in the prestigious physical review x of the american physical society.
"even the largest supercomputers are struggling to model accurately anything but the most basic chemistry.
quantum computers simulating nature, however, unlock a whole new way of understanding matter.
they will provide us with a new tool to solve problems in materials science, medicine and industrial chemistry using simulations."
with quantum computing still in its infancy, it remains unclear exactly what problems these devices will be most effective at solving, but most experts agree that quantum chemistry is going to be one of the first 'killer apps' of this emergent technology.
quantum chemistry is the science of understanding the complicated bonds and reactions of molecules using quantum mechanics.
the 'moving parts' of anything but the most- simple chemical processes are beyond the capacity of the biggest and fastest supercomputers.
by modelling and understanding these processes using quantum computers, scientists expect to unlock lower- energy pathways for chemical reactions, allowing the design of new catalysts.
this will have huge implications for industries, such as the production of fertilisers.
other possible applications include the development of organic solar cells and better batteries through improved materials and using new insights to design personalised medicines.
working with colleagues at the institute for quantum optics and quantum information in innsbruck, austria, dr. hempel used just four qubits on a 20- qubit device to run algorithms to simulate the energy bonds of molecular hydrogen and lithium hydride.
these relatively simple molecules are chosen as they are well understood and can be simulated using classical computers.
this allows scientists to check the results provided by the quantum computers under development.
dr. hempel said: "this is an important stage of the development of this technology as it is allowing us to set benchmarks, look for errors and plan necessary improvements."
instead of aiming for the most accurate or largest simulation to date, dr. hempel's work focused on what can go wrong in a promising quantum- classical hybrid algorithm known as variational quantum eigensolver or vqe.
by looking at different ways to encode the chemistry problem, the researchers are after ways to suppress errors that arise in today's imperfect quantum computers and stand in the way of near- term usefulness of those machines.
error suppression is at the core of research pursued in the university of sydney's quantum control laboratory, led by professor michael biercuk, who recently launched australia's first private quantum start- up, q- ctrl.
dr. hempel, who did the experiments while at the university of innsbruck, now hopes to leverage sydney's expertise to improve what can be accomplished with these kinds of simulations.
the paper, published today in leading journal physical review x, was jointly written with innsbruck professor rainer blatt, a pioneer in quantum computing, and former harvard professor alan aspuru- guzik, who has since moved to the university of toronto.
professor blatt, from iqoqi in innsbruck, said: "quantum chemistry is an example where the advantages of a quantum computer will very soon become apparent in practical applications."
head of the university of sydney nano institute's quantum science domain, dr. ivan kassal, said: "this work is a remarkable implementation of one of the most promising approaches to quantum chemistry, proving its mettle on a real quantum- information processor."
he said that dr. hempel's decision to move to the university of sydney in 2016 was an excellent addition to the strong quantum team on campus.
"theoretical chemistry and materials science are strengths at this university and they will be augmented by these latest techniques in quantum computation," he said.
physicists at the california institute of technology have succeeded for the first time in the distribution of "entanglement" in a way that could lead to long- distance quantum communications, scalable quantum networks, and even a quantum internet.
in the april 5 online publication science express, caltech valentine professor of physics h. jeff kimble and his colleagues report that they have devised a crucial building- block of a "quantum repeater."
the team has demonstrated a way to create a segment of a channel that can distribute quantum entanglement over distances.
the division into segments and storage of entanglement in material systems is necessary for long- distance quantum communications to take place.
"this work provides a first primitive version of a quantum repeater segment," says julien laurat, a postdoctoral scholar in physics and one of the authors of the paper.
"it opens an avenue for further investigations into this promising and new quest of large- scale networks where the currency of the realm is no longer classical information but rather quantum information."
entanglement, one of the most striking features of quantum mechanics, leads to strong correlations between the various components of a physical system, regardless of the distance separating them.
entanglement's distribution enables quantum protocols, such as quantum cryptography where the security is guaranteed by the law of physics or quantum teleportation where a quantum state is faithfully transferred from one place to another.
"physicists for some time have understood that the entanglement of quantum states could be exploited for various advances that are impossible with devices that operate according to the laws of classical physics," says chin- wen chou, a former doctoral student of kimble's and the lead author of the paper.
however, entanglement as a resource is fragile, and achieving such protocols over very long- distance is a challenge for quantum physicists.
to achieve in a reasonable time long- distance quantum communications, namely the distribution of entanglement over such a distance, the channel has to be divided into many segments and entanglement generated and stored into material systems before connecting all them together.
the caltech group achievement is demonstrating an initial version of one of these segments.
the experiment involves two quantum nodes separated by 3 meters, each formed by two atomic ensembles separated by 1mm.
the ensembles are clouds of about 100,000 cooled cesium atoms.
with real- time control of the quantum states, entanglement is generated, stored into the atoms, which play the role of a quantum memory, and finally converted to photons on demand.
this entanglement is stored in a heralded way, a critical requirement for scalability.
in addition, the released entanglement is a so- called "polarization entanglement," an appropriate form for quantum communication applications.
"we demonstrated the capability to distribute entanglement between two locations in a form suitable both for quantum network architectures and for entanglement- based quantum communication schemes," says kimble.
"the import of our experiment goes well beyond quantum communication protocols," laurat explains.
"it incorporates many complex procedures, confirming the more and more efficient control we can have in our labs to address in a coherent way the quantum states of atoms and light, and their interface."
the new work reported by chou, laurat, kimble, and their coworkers is a significant leap towards quantum networking.
however, the researchers also emphasize that "the extension of their work to longer chains involving many segments becomes more complicated, and still out of reach of any current system.
a fully functional quantum repeater is still a challenging task, and its future achievement will be rich in fruitful discoveries."
this demonstration builds upon previous advances in the caltech quantum optics group in recent years, including the first demonstration of uncondi ... uantum teleportation and the initial demonstration of entanglement ... ote atomic ensembles, a crucial ingredient for the breakthrough reported here.
the title of the paper is "functional quantum nodes for entanglement distribution over scalable quantum networks."
it is available on the science express website.
the other authors are hui deng, a postdoctoral scholar in physics; kyung soo choi, a graduate student, and hugues de riedmatten and daniel felinto, both previous postdoctoral scholars at caltech.
many quantum technologies rely on quantum states that violate local realism, which means that they either violate locality (such as when entangled particles influence each other from far away) or realism (the assumption that quantum states have well- defined properties, independent of measurement), or possibly both.
violation of local realism is one of the many counterintuitive, yet experimentally supported, characteristics of the quantum world.
determining whether or not multiparticle quantum states violate local realism can be challenging.
now in a new paper, physicists have shown that a large family of multiparticle quantum states called hypergraph states violates local realism in many ways.
the results suggest that these states may serve as useful resources for quantum technologies, such as quantum computers and detecting gravitational waves.
the physicists, mariami gachechiladze, costantino budroni, and otfried guhne at the university of siegen in germany, have published their paper on the quantum hypergraph states in a recent issue of physical review letters.
the properties of multiparticle quantum systems are described by quantum states, some of which can be represented on a graph where each point corresponds to a particle and each edge to the interaction between particles.
while some quantum states can be represented by ordinary graphs, others are represented by hypergraphs.
on an ordinary graph, two points can be connected by an edge, while on a hypergraph, a hyperedge can connect more than two vertices.
whereas an ordinary edge is usually drawn as a straight line between two vertices, a hyperedge is depicted as a curve that wraps around three or more vertices.
in the new study, the physicists discovered that quantum hypergraph states have perfect correlations that are highly nonlocal.
as the scientists explain, this means that hypergraph states strongly violate local realism.
"we find a whole new class of elegantly described states that are highly entangled," gachechiladze told phys.org.
"this class is a generalization of a well- known and heavily used family of graph states."
the physicists also showed that the greater the number of particles in a quantum hypergraph state, the more strongly it violates local realism, with the strength increasing exponentially with the number of particles.
in addition, even if a quantum hypergraph state loses one of its particles, it continues to violate local realism.
this robustness to particle loss is in stark contrast to other types of quantum states, which no longer violate local realism if they lose a particle.
this property is particularly appealing for applications, since it might allow for more noise in experiments.
one such potential application is quantum computing, which may benefit because the exponential violation found here is expected to correspond to an exponential advantage for certain computation tasks.
another application is quantum metrology, where physicists take advantage of quantum properties to make extremely precise measurements that would not be possible using classical measurement techniques.
"high entanglement has been recognized to be the key for certain information- theoretic tasks," gachechiladze said.
"we find that some hypergraph states can be used in quantum metrology, namely, in measuring some parameters with a very high precision using quantum measurements.
such quantum- enhanced measurement strategies may play an important role in future precision experiments, such as in the search for gravitational waves."
the researchers will further explore these possibilities in the future.
"we plan to investigate more in the direction of entanglement properties," gachechiladze said.
"also we are working towards an experimental proposal to create hypergraph states using photons or trapped ions.
finally, we would like to employ this set of quantum states in applications such as measurement- based quantum computation and error- correcting codes."
a new google computing experiment published in nature has ushered in a new era of computing, making the quantum computer a reality.
the study is an experiment in theoretical computer science in which google scientists demonstrate the ability of a quantum processor to solve a calculation that a traditional computer would take thousands of years to perform.
to date, no one has managed to maintain quantum coherence for so long.
quantum computers work in a substantially different way from classical machines: a classic bit is a 1 or a 0, but a quantum bit, or qubit, can exist simultaneously in more states.
a team led by john martinis - an experimental physicist at the university of california, santa barbara, and google in mountain view, california - said that his quantum computer performed a specific calculation that goes beyond the practical capabilities of "classical" computers.
the same calculation would take the best classical supercomputer 10,000 years to complete.
a fundamental property that characterizes the qubit concerns the so- called interference and the possibility of the qubit to be entangled, leading to a deep correlation.
simply put, it means to be able to calculate at speeds never seen before.
physicists and engineers have long been studying how to manage qubit.
to exploit its characteristics and therefore maintain quantum coherence, efficient thermal management systems are needed.
moreover, they must be isolated in certain laboratory conditions; otherwise, they collapse.
so far, superconductors have been used, metals capable of working at temperatures well below zero.
both ibm and google work on josephson junctions composed of two strips of superconductors separated by an insulator.
there are other techniques, such as the topological quantum computing that microsoft is betting on.
the example computation implemented in google's experiment was the control of the outputs of a quantum random number generator.
although a limited example, it represents a huge scientific result.
the news was also leaked from the nasa website, which collaborates with google on quantum computing.
with this goal, google has reached the quantum supremacy that has been a fantasy for a long time.
"it looks like google has given us the first experimental evidence that quantum speed- up is achievable in a real- world system," says michelle simmons, a quantum physicist at the university of new south wales in sydney, australia.
google's algorithm was run on a 54- qubit quantum chip, each consisting of superconducting loops.
in general, this number is a small fraction of the million qubits that might be needed for a machine for computational purposes (see below figure).
the solution test essentially consisted of a comparison of data with different computational models performed by various classical computers, including the supercomputer summit at the oak ridge national laboratory in tennessee.
the google team has estimated that simulating the entire circuit would take 10,000 years, even on a computer with a million processing units.
the quantum computer, on the other hand, would take just 3 minutes and 20 seconds.
this is a step closer to a complete optimization of quantum computers to solve more sophisticated tasks.
physicists think that this will be essential to make large- scale quantum computers work.
google is working toward this, seeking to make further improvements to further demonstrate quantum super charity, even in a forthcoming commercial market.
the availability of a universal quantum computer can have a fundamental impact on a vast number of research fields and on our society.
quantum computing and artificial intelligence could achieve things we have never seen before.
never assume you can have the last laugh on ibm's predictions.
in fact, seldom assume ibm is off- base about ways in which technology might move forward.
not that ibm is the only wised- up voice to give pause.
but back in march ibm was spreading the word that it saw quantum computing going mainstream within five years.
ibm research posted a video with notes that said "quantum computing will no longer be exclusive to the scientific community, but instead will be used extensively by new categories of professionals and developers looking to this emerging method of computing to solve problems once considered unsolvable."
the five- year mark is still way off in the calendar but, ok, it is now fitting time to play fast- forward, and, like, really fast- forward.
companies focused on quantum computing want people to understand what is to come.
sean captain in fast company brought home the nature of what may change in terms of traditional computing as we know it.
"until recently, quantum computing has largely happened in the scientific underground- with large systems housed inside refrigerated cabinets at university and government labs, or in the skunkworks at cash- rich companies like google."
meantime, the idea is not lost on some tech giants to let the plain folks kick the tires of what is to come.
everyone gains including the tech giants who want to take quantum computing out of the labs.
fast company said on thursday that d- wave, ibm and rigetti were enabling the public to run "apps on cloud- based quantum computers."
(hpcwire: "rigetti computing launched its quantum cloud services (qcs) in early september.
longtime quantum player ibm launched its cloud offering, the ibm q experience, back in 2016.")
hpcwire said, "the idea all three companies share is to leverage cloud delivery of training and quantum compute time to accelerate development of a quantum computing ecosystem, particularly among developers and users unfamiliar with quantum computing."
the new online service, called d- wave leap, for example, offers access to educational materials, programming tools, user community, and 1 minute of realtime quantum computer operation, said captain.
hpcwire said its "brief session on leap suggested it's an easy- to- navigate platform rich in resources for relative novices and those more experienced with quantum computing concepts."
andrew tarantola in engadget similarly noted that d- wave systems was "opening access to its quantum computer farm to the public" via a web portal, the leap quantum application environment.
during a demo in san francisco, engadget was told by d- wave's alan baratz, evp of r&d and chief product officer, that leap was "an environment that will provide any and all developers immediate, free, real time access to a live quantum computer."
and now for some voices telling us to sober up.
fast company quoted chirag dekate, a gartner analyst.
"but if you speak with the realists in the space, what they will tell you is...it'll allow end users to explore and figure out that quantum computing is just not ready for primetime yet."
on the other hand, the alternative to doing nothing is starting to do something.
according to fast company, dektate agreed that "opening up these computers- whatever quantum technology they use- could have a huge benefit to advancing the nascent technology. "
thinking in a quantum mindset?
that alone may have merit.
neophytes are not the only ones who can get prepared.
the security sector will have its own homework for the occasion.
witness a news item from eweek on thursday that blackberry, growing revenue in security software development and licensing, was "moving into securing the quantum computing world."
it is "adding a quantum- resistant code- signing server to its lineup of cryptography tools," said the report, and this "will allow software to be digitally signed using a scheme that will be hard to break with a quantum computer, cto charles eagan told eweek and a group of it analysts."
so that is the aspect of quantum computing that will need industry's concerns, as it "gives bad actors the potential to crack traditional public key cryptosystems and then attack the underlying data they protect," eagan said.
techcrunch spelled it out clearly.
"quantum computing represents tremendous promise to completely alter technology as we've known it, allowing operations that weren't previously possible with traditional computing.
the downside of these powerful machines is that they could be strong enough to break conventional cryptography schemes."
a protocol for controlling quantum information pioneered by researchers at uc santa barbara, the kavli institute of nanoscience in delft, the netherlands, and the ames laboratory at iowa state university could open the door to larger- scale, more accurate quantum computations.
their findings, in a paper titled "decoherence- protected quantum gates for a hybrid solid- state spin register," are published in the current issue of the journal nature.
"although interactions between a quantum bit ('qubit') and its environment tend to corrupt the information it stores, it is possible to dynamically control qubits in a way that facilitates the execution of quantum information- processing algorithms while simultaneously protecting the qubits from environment- induced errors," said ucsb physicist david awschalom.
he and his group were responsible for developing the electron and nuclear spins used as the quantum bits -  the quantum version of the computer bit -  in their demonstration and for helping to analyze the results.
awschalom is director of ucsb's center for spintronics & quantum computation, professor of physics, electrical and computer engineering, and the peter j. clarke director of the california nanosystems institute.
dynamical protection of quantum information is essential for quantum computing as the qubits used for information processing and storage are highly susceptible to errors induced by interactions with atoms in the qubits' environment.
the scientists' previous research has shown that quantum information stored in qubits can be effectively protected through successive control operations (rotations) on a qubit that filter out these unwanted interactions.
however, these control operations also filter out the interactions between qubits that are essential for the realization of logic gates for quantum information processing.
thus, until recently, quantum information stored in protected qubit states could not be used for quantum computations.
the research team, which also included members from the university of southern california, showed that by precisely synchronizing the rotations of an electron spin with the rotation of a nearby nuclear spin, they could realize dynamical protection of both qubits from the environment while maintaining the interactions between the two spins that are necessary for quantum information processing.
as a proof of principle, the researchers demonstrated the high- fidelity execution of a quantum search algorithm using this two- qubit system.
quantum search algorithms, if executed on a larger number of qubits, could provide search results of certain databases considerably faster than search algorithms performed on a classical computer.
the results of this study point to greater possibilities for quantum computers that overcome, according to awschalom, the perception that spin qubits in semiconductors, such as those used in this work, suffer from too strong of environmental interactions to be useful qubits.
these solid state spin systems also offer the added benefit of operating at room temperature, in contrast to other candidate qubit systems which operate at only at a fraction of a degree above absolute zero.
"this demonstration of performing a quantum algorithm at the subatomic level with single spins suggests a pathway to build increasingly complex quantum machines, using qubit control protocols that circumvent the expected limitations from real materials," said awschalom.
quantum materials announced that it has achieved 99.5% quantum yield efficiency for the company's pure red (630 nm emission peak) cadmium- free quantum dots.
at this point quantum materials corp's pure red (630nm) cadmium- free quantum dots offer the highest photon conversion efficiency in the industry, according to the company, even when compared with quantum dots made with cadmium, a material known for its toxicity, by other quantum dot manufacturers.
quantum's pure red cd- free quantum dots were manufactured via the company's proprietary continuous- flow manufacturing process, assuring high uniformity through the high- volume production process.
"our primary focus is on driving cadmium- free quantum dots as a phosphor replacement in oled displays, in concordance with apple's recent announcement regarding their integration of quantum dot and oled technologies into qd- led for future iphone displays.
qd- led display form factors are more desirable for mobile devices and quantum dots as a phosphor replacement will radically improve color performance and energy efficiency over current oled technology," said toshi ando, quantum materials vp of marketing.
photo courtesy of quantum materials.
a team of physicists has uncovered a new state of matter- a breakthrough that offers promise for increasing storage capabilities in electronic devices and enhancing quantum computing.
"our research has succeeded in revealing experimental evidence for a new state of matter- topological superconductivity," says javad shabani, an assistant professor of physics at new york university.
"this new topological state can be manipulated in ways that could both speed calculation in quantum computing and boost storage."
the discovery, reported in a paper on arxiv, was conducted with igor zutic at the university of buffalo and alex matos- abiague at wayne state university.
the work centers on quantum computing- a method that can make calculations at significantly faster rates than can conventional computing.
this is because conventional computers process digital bits in the form of 0s and 1s while quantum computers deploy quantum bits (qubits) to tabulate any value between 0 and 1, exponentially lifting the capacity and speed of data processing.
in their research, shabani and his colleagues analyzed a transition of quantum state from its conventional state to a new topological state, measuring the energy barrier between these states.
they supplemented this by directly measuring signature characteristics of this transition in the order parameter that governs the new topological superconductivity phase.
here, they focused the inquiry on majorana particles, which are their own antiparticles- substances with the same mass, but with the opposite physical charge.
scientists see value in majorana particles because of their potential to store quantum information in a special computation space where quantum information is protected from the environment noise.
however, there is no natural host material for these particles, also known as majorana fermions.
as a result, researchers have sought to engineer platforms- i.e., new forms of matter- on which these calculations could be conducted.
"the new discovery of topological superconductivity in a two- dimensional platform paves the way for building scalable topological qubits to not only store quantum information, but also to manipulate the quantum states that are free of error," observes shabani.
more information: william mayer et al.
"phase signature of topological transition in josephson junctions," arxiv:1906.01179v1 [cond- mat.mes- hall] 2019. https://arxiv.org/pdf/1906.01179.pdf
scientists from argonne national laboratory and the university of chicago launched a new testbed for quantum communication experiments from argonne last week.
the quantum loop consists of a pair of connected 26- mile fiber- optic cables that wind circuitously between argonne to the illinois tollway near suburban bolingbrook and back.
at 52 total miles, it is among the longest ground- based quantum communication channels in the country.
the loop will serve as a testbed for researchers interested in leveraging the principles of quantum physics to send unhackable information across long distances.
researchers at argonne and uchicago plan to use the testbed to explore science underlying quantum engineering systems and to harness the properties of quantum entanglement- a phenomenon albert einstein famously characterized as "spooky action at a distance."
quantum entanglement links two (or more) particles so that they are in a shared state- such that whatever happens to one immediately affects the other, no matter how far they have traveled apart.
"inaugurating this quantum loop is a significant step for chicago and the nation in building a large- scale quantum network that can enable secure data transmissions over long distances," said principal investigator david awschalom, senior scientist in the materials science division at argonne, the liew family professor in molecular engineering at the university of chicago and director of the chicago quantum exchange.
"the loop will enable us to identify and address challenges in operating a quantum network and can be scaled to test and demonstrate communication across even greater distances to help lay the foundation for a quantum internet."
argonne scientists joe heremans, alan dibos and gary wolfowicz, who worked on the quantum loop project, demonstrated the operation of the testbed by generating and transmitting optical pulses through one and then both fiber loops.
they witnessed a delay of 200 microseconds for the transit time of the laser pulse along one fiber loop, which is consistent with the speed of light in the glass fiber.
they also began to use the loop for a series of experiments, including transmitting signals from photons emitted from ensembles of ions.
these ions can be used as a quantum memory for the network.
a functional quantum memory, which entails the storage and retrieval of quantum states, is a key technological advance needed for quantum communication and a quantum internet.
"we will need many of these quantum memories spaced out over about 100 kilometers to relay the quantum signal through a network.
the quantum loop enables us to test and refine this quantum memory technology before deploying it in large scale," said tian zhong, scientist in the nanoscience and technology division at argonne and assistant professor of molecular engineering at the university of chicago.
"research leading to science infrastructure such as the quantum loop will ensure that america remains a world leader in this pivotal, rapidly evolving field, which will open up important new avenues of investigation in areas like quantum data transfer and secure communications," said department of energy under secretary of science paul dabbar.
"we look forward to continued increased support and accomplishment for this and other areas of quantum information science."
"this quantum loop is a significant capability for the scientific communities in quantum physics, communications and computing," said paul kearns, argonne national laboratory director.
"these experiments demonstrate how argonne's world- leading scientists and engineers help ensure u.s. leadership in essential quantum information science."
in addition to the quantum loop, argonne plans to develop a two- way quantum link network with fermi national accelerator laboratory.
when the two projects are connected, the quantum link, also supported by the department of energy, is expected to be among the longest links in the world to send secure information using quantum physics.
researchers at the university of sydney and dartmouth college have developed a new way to design quantum memory, bringing quantum computers a step closer to reality.
the results will appear june 19 in the journal nature communications.
quantum computing may revolutionize information processing, by providing a means to solve problems too complex for traditional computers, with applications in code breaking, materials science and physics.
but figuring out how to engineer such a machine, including vital subsystems like quantum memory, remains elusive.
in the worldwide drive to build a useful quantum computer, the simple- sounding task of effectively preserving quantum information in a quantum memory is a major challenge.
the same physics that makes quantum computers potentially powerful also makes them likely to experience errors, even when quantum information is just being stored idly in memory.
keeping quantum information "alive" for long periods, while remaining accessible to the computer, is a key problem.
the sydney- dartmouth team's results demonstrate a path to what is considered a holy grail in the research community: storing quantum states with high fidelity for exceptionally long times, even hours according to their calculations.
today, most quantum states survive for tiny fractions of a second.
"our new approach allows us to simultaneously achieve very low error rates and very long storage times," said co- senior author dr. michael j. biercuk, director of the quantum control laboratory in the university of sydney's school of physics and arc centre for engineered quantum systems.
"but our work also addresses a vital practical issue - providing small access latencies, enabling on- demand retrieval with only a short time lag to extract stored information."
the team's new method is based on techniques to build in error resilience at the level of the quantum memory hardware, said dartmouth physics professor lorenza viola, a co- senior author who is leading the quantum control theory effort and the quantum information initiative at dartmouth.
"we've now developed the quantum 'firmware' appropriate to control a practically useful quantum memory," added biercuk.
"but vitally, we've shown that with our approach a user may guarantee that error never grows beyond a certain level even after very long times, so long as certain constraints are met.
the conditions we establish for the memory to function as advertised then inform system engineers how they can construct an efficient and effective quantum memory.
our method even incorporates a wide variety of realistic experimental imperfections."
scientists from the faculty of physics, university of warsaw, in collaboration with the university of oxford and nist, have shown that quantum interference enables processing of large sets of data faster and more accurately than with standard methods.
their studies may boost applications of quantum technologies in artificial intelligence, robotics and medical diagnostics, for example.
the results of this work have been published in science advances.
contemporary science, medicine, engineering and information technology demand efficient processing of data- still images, sound and radio signals, as well as information coming from different sensors and cameras.
since the 1970s, this has been achieved by means of the fast fourier transform algorithm (fft).
the fft makes it possible to efficiently compress and transmit data, store pictures, broadcast digital tv, and talk over a mobile phone.
without this algorithm, medical imaging systems based on magnetic resonance or ultrasound would not have been developed.
however, it is still too slow for many demanding applications.
to meet this goal, scientists have been trying for years to harness quantum mechanics.
this resulted in the development of a quantum counterpart of the fft, the quantum fourier transform (qft), which can be realized with a quantum computer.
as the quantum computer simultaneously processes all possible values (so- called "superpositions") of input data, the number of operations decreases considerably.
in spite of the rapid development of quantum computing, there is a relative stagnation in the field of quantum algorithms.
now scientists have shown that this result can be improved, and in a rather surprising way.
kravchuk transform
mathematics describes many transforms.
one of them is a kravchuk transform.
it is very similar to the fft, as it allows processing of discrete (e.g.
digital) data, but uses kravchuk functions to decompose the input sequence into the spectrum.
at the end of the 1990s, the kravchuk transform was "rediscovered" in computer science.
it turned out to be excellent for image and sound processing.
it allowed scientists to develop new and much more precise algorithms for the recognition of printed and handwritten text (including even the chinese language), gestures, sign language, people, and faces.
a dozen years ago, it was shown that this transform is ideal for processing low- quality, noisy and distorted data, and thus it could be used for computer vision in robotics and autonomous vehicles.
there is no fast algorithm to compute this transform, but it turns out that quantum mechanics allows one to circumvent this limitation.
"holy grail" of computer science
in their article published in science advances, scientists from the university of warsaw- dr. magdalena stobinska and dr. adam buraczewski, scientists from the university of oxford, and nist, have shown that the simplest quantum gate, which interferes between two quantum states, essentially computes the kravchuk transform.
such a gate could be a well- known optical device- a beam splitter, which divides photons between two outputs.
when two states of quantum light enter its input ports from two sides, they interfere.
for example, two identical photons, which simultaneously enter this device, bunch into pairs and come out together by the same exit port.
this is the well- known hong- ou- mandel effect, which can also be extended to states made of many particles.
by interfering "packets" consisting of many indistinguishable photons (indistinguishability is very important, as its absence destroys the quantum effect), which encode the information, one obtains a specialized quantum computer that computes the kravchuk transform.
the experiment was performed in a quantum optical laboratory at the department of physics at the university of oxford, where a special setup was built to produce multiphoton quantum states, so- called fock states.
this laboratory is equipped with tes (transmission edge sensors), developed by nist, which operate at near- absolute zero temperatures.
these detectors possess a unique feature: they can actually count photons.
this allows one to precisely read the quantum state leaving the beam splitter and thus, the result of the computation.
most importantly, such a computation of the quantum kravchuk transform always takes the same time, regardless of the size of the input data set.
it is the "holy grail" of computer science: an algorithm consisting of just one operation, implemented with a single gate.
of course, in order to obtain the result in practice, one needs to perform the experiment several hundred times to get the statistics.
this is how every quantum computer works.
however, it does not take long, because the laser produces dozens of millions of multiphoton "packets" per second.
the result obtained by scientists from poland, the united kingdom and the united states will find applications in the development of new quantum technologies and quantum algorithms.
its range of uses go beyond quantum photonics, since a similar quantum interference can be observed in many different quantum systems.
the university of warsaw applied for an international patent for this innovation.
the scientists hope that the kravchuk transform will soon find use in quantum computation, where it will become a component of new algorithms, especially in hybrid quantum- classical computers that merge quantum circuits with "normal" digital layouts.
researchers at the australian national university (anu) have taken a major leap forward to provide practical building blocks for a global quantum internet.
the team, led by associate professor matthew sellars, have shown that an erbium- doped crystal is uniquely suited to enable a global telecommunications network that harnesses the weird properties of quantum mechanics.
"the effort to build a quantum computer is often described as the 'space race of the 21st century', but today's computers didn't realise their full potential until we had the internet," said sellars, program manager in the centre for quantum computation and communication technology (cqc2t) at anu.
"we have shown that an erbium- doped crystal is the perfect material to form the building blocks of a quantum internet that will unlock the full potential of future quantum computers.
we had this idea 10 years ago, but many of our peers told us that such a simple idea couldn't work.
seeing this result, it feels great to know that our approach was the right one."
the work, published in nature physics, demonstrates how to dramatically improve the storage time of a telecom- compatible quantum memory, a crucial challenge that has eluded researchers worldwide.
"a telecom- compatible quantum memory is a vital component for a practical quantum internet," said dr rose ahlefeldt, decra fellow at anu and cqc2t.
"memories allow us to buffer and synchronise quantum information, operations necessary for long range quantum communication.
at the moment, researchers are using memories that don't work at the right wavelength, and have to employ a complicated conversion process to and from the communications wavelength.
this can be inefficient, and means they have to do three very difficult things instead of just one," she said.
erbium, a rare earth ion, has unique quantum properties such that it operates in the same band as existing fibre optic networks, eliminating the need for a conversion process.
"the unique advantage of our technology is that it operates in the same 1550 nanometre band as today's telecommunications infrastructure, making it compatible with the fibre optic cables found in existing networks," said first author and phd candidate milos ranci?.
"we've shown that erbium ions in a crystal can store quantum information for more than a second, which is 10,000 times longer than other attempts, and is long enough to one day send quantum information throughout a global network."
sellars said the new technology can also be operated as a quantum light source or used as an optical link for solid- state quantum computing devices, connecting them to the quantum internet.
"not only is our material compatible with existing fibre optics, but it's versatility means it will be able to connect with many types of quantum computers including cqc2t's silicon qubits, and superconducting qubits such as those being developed by google and ibm," said sellars.
"this result is so exciting to me because it allows us to take a lot of the in- principle work we've demonstrated and turn it into practical devices for a full- scale quantum internet."
a team of scientists, led by professor winfried hensinger at the university of sussex, have made a major breakthrough concerning one of the biggest problems facing quantum computing: how to reduce the disruptive effects of environmental "noise" on the highly sensitive function of a large- scale quantum computer.
in the real- world, technological developments need to operate in imperfect conditions; what can be successfully tested in a highly controlled laboratory may fail when presented with realistic environmental factors, such as the fluctuations in voltage from an electronic component or stray electromagnetic fields emitted by everyday electronic equipment.
the university of sussex's ion quantum technology group have managed to dramatically reduce the effects of such environmental "noise" affecting trapped ion quantum computers, reporting their findings in an article that has today, thursday 1 november 2018, been published in the prestigious journal physical review letters.
it means the team is one step closer to building a large- scale quantum computer with the capability to solve challenging real- world problems.
small- scale quantum computers currently in existence only contain a handful of quantum bits- components of quantum computers that store information and can exist in multiple states, also referred to as qubits.
as such, current quantum computers are small enough to be operated in a highly controlled environment inside a specialized laboratory.
however, such machines do not have the processing power required to solve complex problems because of the limited number of qubits.
when built, large- scale quantum computers will be able to solve certain problems that would take even the fastest super computers billions of years to calculate.
in order to create a quantum computer that can solve such problems, scientists will need to increase the number of qubits, which in turn will increase the size of the quantum computer.
the problem is that the more qubits that are added, the more difficult it becomes to isolate the computer from any realistic "noise" that would disrupt the computing processes.
hensinger's team of university of sussex physicists have made a quantum computing breakthrough that is capable of mitigating some of these problems.
they collaborated with theoretical scientist dr. florian mintert and colleagues from imperial college london, who proposed a theory of how one might be able to solve this problem by manipulating the strange quantum effects in use inside a quantum computer.
the theory allows- making use of the strange properties of quantum physics- the execution of quantum computations in such a way that changes in the initial operational parameters of the machine do not lead to a substantial change in the end result of the computation.
this in turn helps to insulate the quantum computer from the effects of environmental 'noise'.
dr. sebastian weidt, senior scientist in the sussex ion quantum technology group, explains the significance: "realising this technique may have a profound impact on the ability to develop commercial ion trap quantum computers beyond use in an academic laboratory."
the sussex team went to work to see whether they could actually implement this theory.
they used complicated radio- frequency and microwave signals capable of manipulating the quantum effects inherent in individual charged atoms (ions), to demonstrate this in practical experiments.
their implementation is based on microwave technology, such as that present in mobile phones.
following months of intensive work in the laboratory, the sussex scientists have managed to make this new method a reality, experimentally demonstrating its capabilities to substantially reduce the effect of "noise" on a trapped ion quantum computer.
prof hensinger, head of the ion quantum technology group at the university of sussex- which last year unveiled the first blueprint for a large- scale quantum computer- says: "with this advance we have made another practical step towards constructing quantum computers that can host millions of qubits.
such machines are capable of solving certain problems that even the fastest supercomputer may take billions of years to calculate and be of great benefit to humanity; they may be able to help us create new pharmaceuticals; find new cures for diseases, such as dementia; create powerful tools for the financial sector; be of benefit to agriculture, through more efficient fertilizer production, among many other applications.
we are only starting to understand the tremendous potential of these machines."
hensinger's group is now utilising this new technique as they put the final touches to a powerful quantum computer prototype that is currently in their laboratory at the university of sussex.
hensinger says: "it's now time to translate academic achievements into the construction of practical machines.
we're in a fantastic position to do this at sussex and my team is working round the clock to make large- scale quantum computing a future reality."
intel has officially begun testing its "spin qubit" chip for quantum computing, the smallest chip of its kind, the company said in a monday press release.
the chip was created in oregon at intel's d1d fab facility, "using the same silicon manufacturing techniques" the company uses to make tradition computer chips, according to the release.
using such a familiar process, and designing the chip for "dramatic" scalability, could mean that intel is positioning itself for future growth in the quantum computing space.
the spin qubit chip's qubits are roughly 50 nanometers across, and one can view them only with the help of an electron microscope.
"about 1,500 qubits could fit across the diameter of a single human hair," the release noted.
see: it leader's guide to the future of quantum computing (tech pro research)
the full chip itself is much smaller than even a pencil eraser.
intel mentioned in the release that it is "the tiniest quantum computing chip" the company has ever worked on.
qubits is short for "quantum bits," which can replace the traditional transistors commonly found in computer chips.
as noted by mit's technology review, standard computers store information as 1s or 0s, but qubits can enter a state of superposition where they exist in "multiple states of 1 and 0 at the same time."
as such, the use of superposition means that quantum computers can scale their power exponentially with the addition of a few qubits.
quantum computing chips require very particular environments, though.
intel's new spin qubit chip, for example, needs a temperature of "roughly 460 degrees below zero fahrenheit - 250 times colder than space," to operate effectively.
companies like microsoft, alphabet, and ibm are also working on similar chips, battling for quantum supremacy.
as their work continues to advance, it could create serious headway in the race toward commercially- viable quantum computing.
the big takeaways for tech leaders:
intel has revealed its new spin qubit chip for quantum computing, the tiniest chip it has ever produced for such work.
the battle for quantum supremacy among intel, ibm, alphabet, microsoft, and others could lead to commercially- viable quantum computing.
also see
quantum memories are devices that can store quantum information for a later time, which are usually implemented by storing and re- emitting photons with certain quantum states.
but often it's difficult to tell whether a memory is storing quantum or merely classical information.
in a new paper, physicists have developed a new test to verify the quantum nature of quantum memories.
the researchers, denis rosset, francesco buscemi, and yeong- cherng liang, have published a paper on the quantum memory test in a recent issue of physical review x.
"quantum memories are indispensable components of long- distance quantum communication networks and potentially even in a full- scale quantum computer," liang, a physicist at national cheng kung university in taiwan, told phys.org.
"for these components to serve their purpose, it's essential that they can preserve, at least, the quantum entanglement between certain inputs to the memory and whatever other parts that did not enter the memory.
our work strikes the right balance in certifying any device that possesses this ability while making the minimal assumptions."
as the scientists explain, the quantum entanglement between the system stored in the memory and any remote systems not in the memory must be maintained for the entire storage time.
if this entanglement is broken at any time, then the device no longer functions as a quantum memory but rather as an "entanglement- breaking channel" and as a result can transmit only classical information.
although currently there are tests that can verify the quantum nature of a quantum memory, these tests have certain limitations.
for one, they require the experimenter to trust that the measurement and state preparation devices used by the quantum memory are accurate.
for this reason, these tests are called device- dependent protocols.
however, a test that makes no assumptions cannot be "faithful," meaning it may overlook some genuine quantum memories.
this is because these methods test for the violation of a bell inequality as verification of entanglement, which is sufficient but not necessary, as some genuinely quantum channels do not violate bell inequalities and so would not pass this test.
although it would be ideal to design a test that is completely device- independent, the researchers explain that it is not possible to test a single memory in this manner, even in principle, due to the need to test the quantum memory at two different times.
however, their new test is measurement- device- independent, meaning it still requires the state preparation device to be trusted, but no assumptions need to be made regarding the measurement device.
the new test is also faithful, meaning it can correctly identify all quantum memories that function as non- entanglement- breaking quantum channels.
the new test uses a semiquantum framework that is very similar to that used in some tests of entanglement in quantum states, in which the entanglement refers to correlations in space, in contrast to the time- like entanglement in quantum memories.
conventional protocols for testing for space- like correlations often use two characters, alice as the sender and bob as the receiver of quantum states.
but since quantum memories involve time- like correlations, the protocol needs only a single character, whom the researchers call abby, to act as both the sender and receiver at different times.
in the test proposed in the new study, by comparing the relative frequencies of the signals that abby sends and receives, it is possible to estimate the time- like entanglement and therefore certify that a quantum memory can store quantum information.
the researchers showed that the new test is robust against noise and losses, and they expect that it should be possible to experimentally perform the test with current technology.
the test would then provide a very useful tool for the future development of quantum memories.
"in the development of novel quantum technologies, it's crucial that there exists a reliable way to benchmark the relevant components and make sure that they function as expected," liang said.
"our findings provide a way to certify one of the most important features of these components while making sure that we are not making more assumptions than necessary.
with these tests, we hope that it simplifies the quality control procedures of quantum devices while not falling into the trap of making unjustifiable assumptions."
scientists at pnnl are bridging the gap between today's fastest computers and tomorrow's even faster quantum computers.
the race toward the first practical quantum computer is in full stride.
companies, countries, collaborators, and competitors worldwide are vying for quantum supremacy.
google says it's already there.
but what does that mean?
how will the world know when it's been achieved?
using classical computers, computational scientists at pnnl have set a mark that a quantum system would need to surpass to establish quantum supremacy in the realm of chemistry.
that's because the fastest classical computers available today are getting better and better at simulating what a quantum computer will eventually be expected to do.
to prove itself in the real world, a quantum computer will need to be able to outdo what a fast supercomputer can do.
and that's where the pnnl- led team have set a benchmark for quantum computers to beat.
scalable qubit array.
image credit: pnnl
"classical simulation of quantum chemistry problems serves as a goalpost for quantum computers," said karol kowalski, a computational chemist at pnnl.
"when a quantum computer can beat what our best parallel computing systems can do, quantum computing developers will know they are where they need to be.
this is a benchmark to inspire innovation."
at 113 electrons, the recent benchmark simulation is the largest quantum system ever simulated at this precise level of accuracy using a classical computer.1 working with collaborators in hungary and the czech republic, the pnnl team set the benchmark by simulating the structure of an important chemical structure in nitrogenase, an enzyme that converts nitrogen in the atmosphere into useable fertilizer for plants.
the enzyme is the subject of intense study because it may hold to key to producing enough food to feed an ever- growing global population.
understanding how this enzyme is able to break the strong nitrogen triple bond, while expending very little energy, could be key to new catalyst design, eventually providing abundant fertilizer currently produced using a chemical process requiring large energy inputs.
shrinking the quantum chemistry problem
"complex quantum chemistry is exactly the kind of problem where having a quantum computer available could really make a difference," said sriram krishnamoorthy, a high- performance computing expert and quantum computing lead scientist at pnnl.
"we are working on creating the programs that will run on quantum computers.
krishnamoorthy, kowalski, and their pnnl colleagues are working collaboratively with partners at microsoft, through the northwest quantum nexus, to both simulate how a quantum computer will work and write programs that will work on any quantum computer that emerges from the intense global competition.
"conventional computers, including today's fastest supercomputers, are inadequate for simulating quantum systems required to describe challenging and important molecular systems and processes," said kowalski.
"better computational tools are needed to understand chemical systems and design new materials."
until a full- scale quantum computer is available, the pnnl team worked with microsoft experts to develop a bridge between current digital computers and what comes next.
the workflow takes advantage of what classical computers do well now, while using the current capabilities of quantum computing to describe chemical transformations relevant to industrial processes such as energy generation and energy storage.
the key, according to the research team, was to take the output of a classical computer and be able to convert that information into an input that can be interpreted by a quantum computer.
the researchers published that quantum computing method in mid- 2019.
since then, the pnnl team has taken another huge step in bridging classical and quantum computers.
they developed a computer algorithm that takes advantage of a mathematical trick called "downfolding.
"2 essentially, downfolding makes difficult and time- consuming calculations possible on current test- bed quantum computers.
"this is like shrinking a large box into a much smaller box," said kowalski.
"in this case, the box represents a huge numerical space.
we use a more compact description in a quantum computer, and what comes out accurately represents the energy of the much larger system.
it's a bridge between classical computing and what will be quantum computing in the coming years."
it may seem like a mathematical magic trick, but kowalski adds that the method uses properties of quantum mechanics and a series of rigorous mathematical theories that are reliable and reproducible.
opening new doors
the downfolding method not only opens up avenues to quantum computing, it also makes possible new, much more efficient and accurate ways of analyzing and validating the reams of data generated every day from the u.s. investment in u.s. department of energy (doe)- supported light sources used to study our world in subatomic detail.3
"we have shown how the quantum behavior of excited electronic states can be analyzed with hamiltonian downfolding," said kowalski.
"this provides a way to use theory to validate data interpretation."
these interim steps in the path to quantum computing are essential because they provide essential benchmarks that help show how close the world is to achieving quantum supremacy.
"we will be able to test the output of quantum computers against these calculations," said krishnamoorthy.
"if quantum computers can produce results close to these results, we will know they work."
1 https://arxiv.org/abs/2001.04890
2 https://aip.scitation.org/doi/full/10.1063/1.5094643
3 https://aip.scitation.org/doi/full/10.1063/1.5128103
researchers have developed a way for superconducting quantum chips to talk to each other over large distances through an optical fibre, allowing quantum entanglement or teleportation - both key steps towards building a truly global quantum internet via a quantum repeater.
devised by dr keyu xia and professor jason twamley from the arc centre of excellence for engineered quantum systems (equs) at macquarie university, and dr michael vanner at the university of queensland, their idea makes use of the tiny magnetic fields generated by these quantum chips to alter the properties of an optical cavity, via a magnetostrictive material.
a material that is "magnetostrictive" physically expands in the presence of a magnetic field.
by using this, the team were able to show how the magnetic fields from the quantum chips can effectively speak via the optical cavity and connected optical fibre through to a distant superconducting chip in another lab elsewhere.
"quantum cryptography, the secret transfer of information over optical fibres that is protected via the laws of quantum mechanics, is one application of quantum science and technology that already has commercial applications," said researcher professor twamley.
"this and other applications, such as quantum computing, quantum teleportation and quantum sensing, would benefit greatly from the ability to connect up quantum devices over long distance, for example, if they could plug in to a quantum internet.
"superconducting quantum chips are one of the most promising areas of development to become the hardware for tomorrow's quantum computers, and our superconducting/optical interface will help plug these chips together over large distances."
"our hybrid- quantum system approach allows us to take advantage of both the power of quantum computing with superconducting circuits and existing low- loss
high- speed optical telecommunications technology," said dr vanner.
"it's a very exciting direction, quantum technology certainly has a bright future," said dr xia.
for many years, quantum computers were not much more than an idea.
today, companies, governments and intelligence agencies are investing in the development of quantum technology.
robert konig, professor for the theory of complex quantum systems at the tum, in collaboration with david gosset from the institute for quantum computing at the university of waterloo and sergey bravyi from ibm, has now placed a cornerstone in this promising field.
why should quantum computers be faster?
conventional computers obey the laws of classical physics.
they rely on the binary numbers 0 and 1.
these numbers are stored and used for mathematical operations.
in conventional memory units, each bit -  the smallest unit of information -  is represented by a microscopic dot on a microchip.
each of these dots can hold a charge that determines whether the bit is set to 1 or 0.
in a quantum computer, however, a bit can be both 0 and 1 at the same time.
this is because the laws of quantum physics allow electrons to be in multiple places at one time.
quantum bits, or qubits, thus exist in multiple overlapping states.
this so- called superposition allows quantum computers to perform operations on many values in one fell swoop whereas a single conventional computer typically must execute these operations sequentially.
the promise of quantum computing lies in the ability to solve certain problems significantly faster.
from conjecture to proof
konig and his colleagues have now conclusively demonstrated the advantage of quantum computers.
to this end, they developed a quantum circuit that can solve a specific "difficult" algebraic problem.
the new circuit has a simple structure: it only performs a fixed number of operations on each qubit.
such a circuit is referred to as having a constant depth.
in their work, the researchers prove that the problem at hand cannot be solved using classical constant- depth circuits.
they furthermore answer the question of why the quantum algorithm beats any comparable classical circuit: the quantum algorithm exploits the non- locality of quantum physics.
prior to this work, the advantage of quantum computers had neither been proven nor experimentally demonstrated -  notwithstanding that evidence pointed in this direction.
one example is shor's quantum algorithm, which efficiently solves the problem of prime factorization.
however, it is merely a complexity- theoretic conjecture that this problem cannot be efficiently solved without quantum computers.
it is also conceivable that the right approach has simply not yet been found for classical computers.
a step on the road to quantum computing
robert konig considers the new results primarily as a contribution to complexity theory.
"our result shows that quantum information processing really does provide benefits -  without having to rely on unproven complexity- theoretic conjectures," he says.
beyond this, the work provides new milestones on the road to quantum computers.
because of its simple structure, the new quantum circuit is a candidate for a near- term experimental realization of quantum algorithms.
quantum computing has the potential to revolutionize technology, medicine, and science by providing faster and more efficient processors, sensors, and communication devices.
but transferring information and correcting errors within a quantum system remains a challenge to making effective quantum computers.
in a paper in the journal nature, researchers from purdue university and the university of rochester, including john nichol, an assistant professor of physics, and rochester ph.d. students yadav p. kandel and haifeng qiao, demonstrate their method of relaying information by transferring the state of electrons.
the research brings scientists one step closer to creating fully functional quantum computers and is the latest example of rochester's initiative to better understand quantum behavior and develop novel quantum systems.
the university recently received a $4 million grant from the department of energy to explore quantum materials.
quantum computers
a quantum computer operates on the principles of quantum mechanics, a unique set of rules that govern at the extremely small scale of atoms and subatomic particles.
when dealing with particles at these scales, many of the rules that govern classical physics no longer apply and quantum effects emerge; a quantum computer is able to perform complex calculations, factor extremely large numbers, and simulate the behaviors of atoms and particles at levels that classical computers cannot.
quantum computers have the potential to provide more insight into principles of physics and chemistry by simulating the behavior of matter at unusual conditions at the molecular level.
these simulations could be useful in developing new energy sources and studying the conditions of planets and galaxies or comparing compounds that could lead to new drug therapies.
"you and i are quantum systems.
the particles in our body obey quantum physics.
but, if you try to compute what happens with all of the atoms in our body, you cannot do it on a regular computer," nichol says.
"a quantum computer could easily do this."
quantum computers could also open doors for faster database searches and cryptography.
"it turns out that almost all of modern cryptography is based on the extreme difficulty for regular computers to factor large numbers," nichol says.
"quantum computers can easily factor large numbers and break encryption schemes, so you can imagine why lots of governments are interested in this."
bits vs. qubits
a regular computer consists of billions of transistors, called bits.
quantum computers, on the other hand, are based on quantum bits, also known as qubits, which can be made from a single electron.
unlike ordinary transistors, which can be either "0" or "1," qubits can be both "0" and "1" at the same time.
the ability for individual qubits to occupy these "superposition states," where they are simultaneously in multiple states, underlies the great potential of quantum computers.
just like ordinary computers, however, quantum computers need a way to transfer information between qubits, and this presents a major experimental challenge.
"a quantum computer needs to have many qubits, and they're really difficult to make and operate," nichol says.
"the state- of- the art right now is doing something with only a few qubits, so we're still a long ways away from realizing the full potential of quantum computers."
all computers, including both regular and quantum computers and devices like smart phones, also have to perform error correction.
a regular computer contains copies of bits so if one of the bits goes bad, "the rest are just going to take a majority vote" and fix the error.
however, quantum bits cannot be copied, nichol says, "so you have to be very clever about how you correct for errors.
what we're doing here is one step in that direction."
manipulating electrons
quantum error correction requires that individual qubits interact with many other qubits.
this can be difficult because an individual electron is like a bar magnet with a north pole and a south pole that can point either up or down.
the direction of the pole- whether the north pole is pointing up or down, for instance- is known as the electron's magnetic moment or quantum state.
if certain kinds of particles have the same magnetic moment, they cannot be in the same place at the same time.
that is, two electrons in the same quantum state cannot sit on top of each other.
"this is one of the main reasons something like a penny, which is made out of metal, doesn't collapse on itself," nichol says.
"the electrons are pushing themselves apart because they cannot be in the same place at the same time."
if two electrons are in opposite states, they can sit on top of each other.
a surprising consequence of this is that if the electrons are close enough, their states will swap back and forth in time.
"if you have one electron that's up and another electron that's down and you push them together for just the right amount of time, they will swap," nichol says.
"they did not switch places, but their states switched."
to force this phenomenon, nichol and his colleagues cooled down a semiconductor chip to extremely low temperatures.
using quantum dots- nanoscale semiconductors- they trapped four electrons in a row, then moved the electrons so they came in contact and their states switched.
"there's an easy way to switch the state between two neighboring electrons, but doing it over long distances- in our case, it's four electrons- requires a lot of control and technical skill," nichol says.
"our research shows this is now a viable approach to send information over long distances."
a first step
transmitting the state of an electron back and forth across an array of qubits, without moving the position of electrons, provides a striking example of the possibilities allowed by quantum physics for information science.
"this experiment demonstrates that information in quantum states can be transferred without actually transferring the individual electron spins down the chain," says michael manfra, a professor of physics and astronomy at purdue university.
"it is an important step for showing how information can be transmitted quantum- mechanically- in manners quite different than our classical intuition would lead us to believe."
nichol likens this to the steps that led from the first computing devices to today's computers.
that said, will we all someday have quantum computers to replace our desktop computers?
"if you had asked that question of ibm in the 1960s, they probably would've said no, there's no way that's going to happen," nichol says.
"that's my reaction now.
but, who knows?"
since the discovery of quantum mechanics, in the early 20th century, physicists have relied on optics to test its fundamentals.
even today, linear quantum optics- the physics of how single photons behave in mirrors, waveplates, and beamsplitters- leads the way in terms of observations of multi- party entanglement, tests of quantum nonlocality, and addressing fundamental questions about the nature of reality itself.
light notoriously avoids interaction.
one beam of light does not readily affect anything about a second beam- they simply add up, through interference, and go about their business.
to date, quantum mechanical tests have relied on our ability to produce states of light in which, when all the photons are measured, a subset of the measurement patterns can be sifted out- those in which a desired interaction has occurred.
physicists call this technique 'postselection'.
new work by a team at the university of bristol's centre for quantum photonics has uncovered fundamental limits on the quantum operations which can be carried out with postselection.
as physicists build bigger and bigger quantum states of light, fewer and fewer entangled states are reachable using postselection alone.
the bristol team found that as the complexity of the postselection scheme increases, the desired interacting state, which at first is easy to sift from the larger state, starts to behave indistinguishably from the noise, making postselection impossible.
each photon can carry a quantum bit, or 'qubit', of quantum information, for applications ranging from quantum computing to quantum communications.
an important class of entangled states are the 'graph states', so called because their entanglement can be visualised as connections between the qubit nodes of a graph.
applying their postselectability heuristics to graph states, the researchers catalogued which graphs of up to nine qubits are postselectable, finding these to be fewer than one fifth of the total.
this fraction is expected to drop severely for larger quantum systems, limiting the kinds of entanglement that can be reached with today's quantum photonic technology, and strengthening the call for new technologies to generate and entangle photons.
the work is published today in the journal quantum science and technology.
jeremy adcock, lead author of the new work, said: "even though our rules for postselection show that most states are off limits, they also tell us how to build experiments of maximum complexity."
dr. joshua silverstone, who led the project, and is a leverhulme early career fellow at bristol, added: "people have known about problems with postselection for many years, but it's remarkable that only now can we see through to its fundamental limits."
"postselection still has some fight left in it, but this work should really make people think about modern approaches to optical quantum tech."
more information: 'hard limits on the postselectability of optical graph states' by j. c. adcock, s. morley- short, j. w. silverstone, and m. g. thompson in quantum science and technology, quantum science and technology, 2018.
quantum machines, creator of the first complete hardware and software solution for the control and operation of quantum computers, announced today the launch of qua as a standard universal language for quantum computing.
qua allows researchers to intuitively program even the most complex quantum programs that are tightly integrated with classical processing and real- time decision- making.
the language is the first to address all requirements of an anticipated quantum computing software abstraction layer.
a primary challenge with quantum research and development today is that every quantum computer has its own unique hardware, coded in the researchers' language of choice.
the unique nature of every system results in teams spending inordinate amounts of time coding and programming new programs and algorithms, with any variation requiring either restarting the process or even re- routing the control hardware itself.
the resulting process is extremely time- consuming and difficult to scale to more complex systems and algorithms.
qua is a pulse- level programming language for quantum devices, and the first language designed specifically as a universal quantum computing software abstraction layer.
to achieve this, several different criteria had to be fulfilled to fit the distinct structure of quantum algorithms and quantum programming: semantical, technological, commercial and qubit agnostic.
from a semantic perspective, qua is the first language to combine universal quantum operations at the pulse- level, together with universal classical operations, namely, turing- complete classical processing, and comprehensive control- flow.
from a technological and commercial perspective, qua was developed to be extremely intuitive while relying on qm's proprietary compiler, xqp, to do the heavy lifting for optimizing the many entangled quantum and classical operations.
xqp compiles quantum programs to qm's pulse processor assembly language which can then run them with extremely low latencies and precision.
finally, qua is qubit agnostic and supports all quantum processors.
quantum computers are nothing like any other computer we use today.
however, for the inception of qua and its compiler xqp, inspiration was drawn from classical counterparts which have become standards or widely established across the computing industry, such as nvidia's cuda, intel's x86, and google's tensorflow.
these three languages are significantly different in nature, illustrating the complexity and distinct approach required for quantum computing.
used as part of quantum machines' existing quantum orchestration platform, qua is a universal language.
its beta version has already been adopted by leading teams in multinational corporates, startups, national- labs, and academic institutions that develop quantum computers based on the entire spectrum of quantum hardware available today including superconducting qubits, trapped ions & atoms, nv centers, quantum dots, and topological qubits.
it is the combination of qua's capabilities with an actual underlying processor, qm's pulse processor, which allows for qua's fast adoption across the emerging quantum industry.
"as the field of quantum computing continues to progress and grow more complex, teams across the industry need solutions that can scale to meet their growing needs," said itamar sivan, ceo of quantum machines.
"qua is the first- ever programming language designed with the needs of quantum research in mind and offers teams a powerful and intuitive language designed not only for their present needs but also those of the future."
"we are extremely excited to introduce qua," said yonatan cohen, co- founder and cto of quantum machines.
"with qua, teams can run even the most complex quantum- error- correction and hybrid quantum- classical algorithms, right 'out- of- the- box'.
it is a true paradigm shift that we see drastically impacting each and every one of our customers."
about quantum machines
qm's full- stack quantum orchestration platform enables an entirely new approach to controlling and operating quantum processors.
capable of running even the most complex algorithms - from near- term applications of quantum computers to challenges of quantum- error- correction - the quantum orchestration platform allows users to realize the potential of all quantum processors right out of the box via its powerful, yet intuitive, programming language qua.
media contact:
lazer cohen
scientists in australia have developed a new approach to reducing the errors that plague experimental quantum computers; a step that could remove a critical roadblock preventing them scaling up to full working machines.
by taking advantage of the infinite geometric space of a particular quantum system made up of bosons, the researchers, led by dr arne grimsmo from the university of sydney, have developed quantum error correction codes that should reduce the number of physical quantum switches, or qubits, required to scale up these machines to a useful size.
"the beauty of these codes is they are 'platform agnostic' and can be developed to work with a wide range of quantum hardware systems," dr grimsmo said.
"many different types of bosonic error correction codes have been demonstrated experimentally, such as 'cat codes' and 'binomial codes'," he said.
"what we have done in our paper is unify these and other codes into a common framework."
the research, published this week in physical review x, was jointly written with dr joshua combes from the university of queensland and dr ben baragiola from rmit university.
the collaboration is across two leading quantum research centres in australia, the arc centre of excellence for engineered quantum machines and the arc centre of excellence for quantum computation and communication technology.
robust qubits
"our hope is that the robustness offered by 'spacing things out' in an infinite hilbert space gives you a qubit that is very robust, because it can tolerate common errors like photon loss," said dr grimsmo from the university of sydney nano institute and school of physics.
scientists in universities and at tech companies across the planet are working towards building a universal, fault- tolerant quantum computer.
the great promise of these devices is that they could be used to solve problems beyond the reach of classical supercomputers in fields as varied as materials science, drug discovery and security and cryptography.
with google last year declaring it has a machine that has achieved 'quantum supremacy' -  performing an arguably useless task but beyond the scope of a classical computer -  interest in the field of quantum computing and engineering continues to rise.
but to build a quantum machine that can do anything useful will require thousands, if not millions of quantum bits operating without being overwhelmed with errors.
and qubits are, by their very nature, error prone.
the 'quantumness' that allows them to perform a completely different type of computing operation means they are highly fragile and susceptible to electromagnetic and other interference.
identifying, removing and reducing errors in quantum computation is one of the central tasks facing physicists working in this field.
fragile superpositions
quantum computers perform their tasks by encoding information utilising quantum superposition -  a fundamental facet of nature where a final outcome of a physical system is unresolved until it is measured.
until that point, the information exists in a state of multiple possible outcomes.
dr grimsmo said: "one of the most fundamental challenges for realising quantum computers is the fragile nature of quantum superpositions.
fortunately, it is possible to overcome this issue using quantum error correction."
this is done by encoding information redundantly, allowing the correction of errors as they happen during a quantum computation.
the standard approach to achieve this is to use a large number of distinguishable particles as information carriers.
common examples are arrays of electrons, trapped ions or quantum electrical circuits.
however, this creates a large network of 'physical qubits' in order to operate a single, logical qubit that does the processing work you require.
this need to create a large network of physical qubits to support the work of a single operating qubit is a non- trivial barrier towards constructing large- scale quantum machines.
indistinguishable bosons
dr grimsmo said: "in this work, we consider an alternative approach based on encoding quantum information in collections of bosons."
the most common type of boson is the photon, a packet of electromagnetic energy and massless 'light particle'.
by trapping bosons in a particular microwave or optical cavity, they become indistinguishable from one another, unlike, say, an array of trapped ions, which are identifiable by their location.
"the advantage of this approach is that large numbers of bosons can be trapped in a single quantum system such as photons trapped in a high- quality optical or microwave cavity," dr grimsmo said.
"this could drastically reduce the number of physical systems required to build a quantum computer."
the researchers hope their foundational work will help build a roadmap towards fault tolerance in quantum computing.
scientists from the fom foundation and the technical university delft, working together at the kavli institute of nanoscience, have succeeded in detecting and correcting errors during the storage of quantum states in a diamond.
this is an important step towards protecting fragile quantum information long enough to realize a functioning quantum computer.
the results were published online on february 2, 2014 in nature nanotechnology.
whereas a classical bit of information either takes on the value '0' or '1', quantum particles can be placed in superposition, meaning they can be either '0', '1' or '0' and '1' simultaneously.
these quantum bits enable powerful new ways to process information.
they are, however, also extremely vulnerable to errors, such as accidental flips from '0' to '1' or changes in the phase of a superposition.
even the tiniest of such errors continuously accumulate to inevitably erase the quantum information.
it is therefore crucial to timely detect and correct errors.
quantum error correction
during classical computations errors can be detected and corrected by making redundant copies and comparing these copies at intermediate steps.
at first glance such error correction seems impossible for quantum states.
quantum states cannot be faithfully copied and are irrevocably disturbed when measured.
how can we know if an error occurred without ever knowing what the state actually is?
the theoretical solution, found in the nineties, is based on entanglement.
this is the counterintuitive phenomenon that quantum systems can become so strongly connected that they can no longer be described separately.
by encoding the quantum state in an entangled state of multiple quantum bits it is possible to compare the states of the quantum bits to detect errors, without measuring or disturbing the encoded quantum state itself.
spins in diamond
dr. tim taminiau and his colleagues, led by fom workgroup leader prof.dr.ir.
ronald hanson, have now experimentally realized an error correction protocol at room temperature.
previous demonstrations required extremely low temperatures and high vacuum.
the team used electrons and nuclei in a diamond.
these particles carry an intrinsic property called spin, which behaves like a tiny magnet.
the two possible orientations of the spin, up or down, form the values '0' and '1' of the quantum bit.
the researchers entangled three of these quantum bits and could successfully detect and correct one type of quantum error at the time.
towards applications of quantum information
the team now plans to demonstrate that quantum error correction can be used to simultaneously protect quantum states against all types of errors for an extended period.
such long- lived quantum information can enable fundamentally secure communication and extremely fast computations.
this research was done in collaboration with researchers from the ames laboratory and iowa state university in the united states.
combining expertise in quantum technologies and cryptography, researchers have been projecting future dates that quantum computers could jeopardise the security of current cryptocurrencies, a market now worth over usd $150 billion, and assessing countermeasures to such attacks.
macquarie university physicist associate professor gavin brennen, together with australian entrepreneur dorjee sun and researchers in singapore and sydney, has announced a collaboration with blockchain company hyperchain on providing quantum security to digital currency.
the team named quantum resistant coin (qrc), includes researchers a/prof.
brennen at macquarie u., prof. miklos santha at cnrs universite paris diderot and centre for quantum technologies (cqt), a/prof.
troy lee at nanyang technological university and cqt, and senior lecturer dr. marco tomamichel at uts.
they have just released a whitepaper which finds that bitcoin and other cryptocurrencies will be vulnerable to attacks by quantum computers in as little as 10 years.
such attacks could have a disastrous effect on cryptocurrencies as thieves equipped with quantum computers could easily steal funds without detection, thus leading to a quick erosion of trust in the markets.
they also assess the risk of quantum dominated mining in so called proof of work protocols which are the basis for verifying transactions in bitcoin and many other cryptocurrencies.
leading edge blockchain company hyperchain, which provides technical services to hcash (coinmarketcap.com hshare with a market capitalisation of over usd $300 million), has now enlisted qrc as technical advisors.
they will work with hcash, hshare and hyperchain to make sure that their cryptocurrency is resistant against quantum attacks.
"i've been working on the theory of quantum computers for well over a decade and the exciting thing is that now very simple quantum machines, like the google device, are a reality," says co- author brennen, who is director of the macquarie quantum science and technology centre (qscitech) where researchers work on quantum science theory and experiment.
"understandably, there is a lot of nervousness in cryptocurrency communities about whether their digital assets will resist future attacks by very fast quantum computers.
our service is providing advice and algorithmic protocols to digital currencies and blockchains like hcash who want to certify their product will be quantum safe.
hcash has put emphasis on quantum security from the start so this collaboration will be a benefit to both teams"
brennen, comments further, "i'm excited to be part of this team joining excellent people working in pure physics and computer science at uts, cqt, and ntu with dorjee sun who is a social entrepreneur.
the open environment at macquarie was key in getting this going, in fact the whole enterprise started with a conversation with dorjee over coffee and lots of scribbled notes at the macquarie hub this winter."
more information: quantum attacks on bitcoin, and how to protect against them.
arxiv:1710.10377 [quant- ph] https://arxiv.org/abs/1710.10377v1
for the first time, researchers have demonstrated an effective method for transferring quantum information from one location to another.
this could pave the way towards a quantum data bus, an essential component for practical quantum computing.
standard computer processors come equipped with data buses capable of transferring traditional bits of information, which is necessary for tasks such as communication between the cpu and memory.
building a practical quantum computer would require a similar capability for quantum bits.
however, it is challenging to reliably transfer quantum information due to the fragile nature of quantum states.
perfect state transfer
the quantum bits were transferred with an implementation of the perfect state transfer (pst) protocol, which uses a one- dimensional lattice to connect two quantum processors.
through hamiltonian evolution over a specific time, the quantum state in the initial lattice site is transferred to the final lattice site with 100% probability.
the evolution of a perfect state transfer lattice: the input state at the first site is transferred to the output state at the final site.
(image courtesy of the researchers and nature communications.)
while most research on pst has been strictly theoretical, the recent demonstration showed the preservation of the encoded quantum states with an average fidelity of 97.1%.
an extension of standard pst, the demonstration was realized as an array of 11 evanescently- coupled waveguides using a polarization- encoded photon to store the quantum information.
bits vs. qubits
quantum computing, as opposed to traditional computing, makes use of the strange properties of quantum mechanics to greatly increase processing capabilities.
traditional bits have the binary restriction of being either a 0 or 1.
quantum bits, or qubits, are not so constrained: they can exist as a superposition of both 0 and 1.
this property can be harnessed to greatly reduce the computation time required for certain classes of problems.
"it could make the critical difference for discovering new drugs, developing a perfectly secure quantum internet and even improving facial recognition,'' said dr. alberto peruzzo, director of the quantum photonics laboratory, which helped conduct the research.
he believes his team's results are "a breakthrough that has the potential to open up quantum computing in the near future."
the research paper can be found in nature communications.
university of copenhagen researchers have developed a nanocomponent that emits light particles carrying quantum information.
less than one- tenth the width of a human hair, the miniscule component makes it possible to scale up and could ultimately reach the capabilities required for a quantum computer or quantum internet.
the research result puts denmark at the head of the pack in the quantum race.
teams around the world are working to develop quantum technologies.
the focus of researchers based at the center for hybrid quantum networks (hy- q) at the university of copenhagen's niels bohr institute is on developing quantum communication technology based on light circuits, known as nanophotonic circuits.
the ucph researchers have now achieved a major advancement.
"it is a truly major result, despite the component being so tiny," says assistant professor leonardo midolo, who has been working towards this breakthrough for the past five years.
the research team has invented a component, called a nanomechanical router, that emits quantum information carried by light particles (photons) and routes them into different directions inside a photonic chip.
photonic chips are like computer microchips -  only, they use light instead of electrons.
the component merges nano- opto- mechanics and quantum photonics -  two areas of research that, until now, have never been combined.
most spectacular of all, is the size of the component, just a tenth that of a human hair.
it is this microscopic size that makes it so promising for future applications.
"bringing the worlds of nanomechanics and quantum photonics together is a way to scale up quantum technology.
in quantum physics, it has been a challenge to scale systems.
until now, we have been able to send off individual photons.
however, to do more advanced things with quantum physics, we will need to scale systems up, which is what this invention allows for.
to build a quantum computer or quantum internet, you don't just need one photon at a time, you need lots of photons simultaneously that you can connect to each another," explains leonardo midolo.
achieving 'quantum supremacy' is realistic
to exploit quantum mechanical laws to e.g., to build a quantum computer or a quantum internet, many nanomechanical routers must be integrated in the same chip.
about 50 photons are required to have enough power for achieving what is known as "quantum supremacy."
according to midolo, the new nanomechanical router makes doing so a realistic goal:
"we have calculated that our nanomechanical router can already be scaled up to ten photons, and with further enhancements, it should be able to achieve the 50 photons needed to reach 'quantum supremacy."
the invention is also a major leap forward in controlling light in a chip.
existing technology allows for only a few routers to be integrated on a single chip due to the large device footprint.
nanomechanical routers, on the contrary, are so small that several thousand can be integrated in the same chip.
"our component is extremely efficient.
it is all about being able to emit as many photons at once, without losing any of them.
no other current technique allows for this," says leonardo midolo.
the research is carried out in the quantum photonics group at the niels bohr institute, which is a part of the newly established center for hybrid quantum networks (hy- q)
physicists at the university of sydney have discovered a method of using microwaves to probe the sounds of a quantum dot, a promising platform for building a quantum computer.
the findings have been published in nature communications today.
a quantum dot consists of a small number of electrons trapped in zero dimensions inside a solid.
the quantum mechanical properties of these electrons can be used to store and manipulate quantum data for revolutionary applications in computing, communication, sensing and bio- medical diagnostic applications.
james colless and xanthe croot, phd candidates in the university's school of physics, uncovered a way to study what happens when electrons in quantum dots interact with sound waves of the solid they are trapped in.
"the possibility of computing using quantum logic, rather than the classical logic on which today's machines are based, has changed the boundary between hard and easy problems.
previously it was thought that certain tasks - exactly modeling a complex molecule to construct new medicines or computing certain mathematical functions - were simply too hard for any computer, no matter how big," said professor david reilly, from the centre of excellence for engineered quantum systems (equs) and the university's school of physics.
"the rules of the game have now changed.
we now know that quantum mechanics allows certain interesting problems to be computed with ease, so long as you can build a machine that operates according to quantum mechanics - a daunting task!"
"our work is a further step towards understanding the issues that enable or disable quantum machines.
sound waves in solids are a key mechanism that can lead to quantum devices interacting with their environment."
these sounds waves are called phonons, and are similar to the waves one can make in a stretched slinky.
the 'slinky chain', in this case, is formed by the atoms which make up the solid.
it turns out that interactions between sound waves and electrons reveal information about the environment of the electron - akin to detecting the size and shape of a room by listening to a singer's voice in that room.
the interaction between quantum dots and the solids in which they form is a double- edged sword for the purpose of quantum computing.
on one hand, sound vibrations have been used to 'shuttle' electrons from place to place in quantum circuits - almost like a wave might pick up a surfer and take them into the beach.
"however, there are other contexts where sound interacting with electrons can cause huge problems: in particular, when you are performing a quantum algorithm and only want the electron to interact with certain parameters that the experimenter controls," said xanthe croot.
unwanted sound can significantly limit the time you have to perform the algorithm before the electron loses all the information it was storing.
understanding how the size and geometry of the quantum circuit affects these interactions is therefore extremely important.
in quantum computing, different configurations of electrons within the dot represent something similar to the 0 and 1 (or on and off) states in classical computing.
the 1 and the 0 states have different energies: if you apply microwaves with exactly this energy difference you can change the state from 0 to 1 and vice versa.
"we found that if you apply microwaves with energy slightly higher than the electron energy difference, the system creates sound of a very specific frequency.
it is almost like the electron saying, if you hit me too hard i'll scream," said ms croot.
"changing the microwave energy will change the frequency of the sound that the system creates in the solid.
the results show that some frequencies of sound interact very strongly with the system, while others less so.
there are hints in the data that the geometry of the quantum dot plays a key role in determining which frequencies will interact strongly."
decoherence can be metaphorically seen as a quantum fall from grace: when quantum bits, or qubits, are in superposition - such as a single qubit simultaneously having both 1 and 0 values - they're said to be in a state of coherence.
any coupling with the environment - whether intentional (as in an observation or measurement) or accidental - causes the superposition to collapse into a state of decoherence in which only one of all possible coherent states exists.
when two or more objects - be they subatomic particles, atoms, molecules, or even small but macroscopic diamonds - are in a state of entanglement, a change in a property of one instantaneously appears as the inverse change in the same property of the other, and does so instantaneously - i.e., no time elapses - regardless of the distance between the two entangled objects.
since entanglement is critical factor in quantum information, and decoherence can degrade or terminate entanglement (the latter referred to as entanglement sudden death), preserving coherence is vital to the development of quantum computing, quantum cryptography, quantum teleportation, quantum metrology and other quantum information applications.
recently, scientists in the department of physics at pohan ... technology (postech) have devised a way to protect entanglement by mitigating decoherence using weak measurement and quantum measurement reversal.
led by yoon- ho kim, the research team - which also included yong- su kim, jong- chan lee and osung kwon - faced a series of challenges in protecting entanglement from decoherence.
"in the past," yoon- ho kim tells physorg.com, "we've worked on experimental demonstrations of suppressing decoherence for a single- qubit state.
we therefore knew that the effect of decoherence can be suppressed by using weak measurement and reversing measurement for a single- qubit.
in the current study, our initial question was whether this general approach - that is, using weak and reversed measurements - could work for entangled states."
since their approach makes use of quantum measurement, which is often destructive and entanglement- breaking, their initial intuition was that it might not.
"in some sense," kim explains, "the main challenge was overcoming this mental barrier of intuition at first sight and getting ourselves to actually formulate the problem mathematically.
quite often in quantum physics, intuition based on daily experience is not always correct, because more often the quantum effects are counter- intuitive."
however, once their theoretical studies convinced the researchers that the approach would work for entangled states, making it possible to protect entanglement from decoherence using weak measurement and quantum measurement reversal, they focused on a clear way to demonstrate the effect experimentally.
"the challenge here," kim notes, "was to be able to vary the amount of decoherence and the strengths of weak and reversing measurements precisely so that the essence of the protocol could be clearly demonstrated.
we were able to develop a linear optical method to precisely control the amount of decoherence based on a displaced sagnac interferometer."
a sagnac interferometer uses counter- propagating light beams to measure its own angular velocity with respect to the local inertial frame.
there are other innovations possible, notes kim.
"in this work, we demonstrated a way to protect two- qubit entanglement from a particular type of decoherence.
however, the general approach to use weak measurement and reversing measurement for suppressing the effect of decoherence should be valid for other types of decoherence and for multipartite entangled states.
decoherence is often unavoidable in real life - and to date, several attempts to directly reduce decoherence have been made without much success.
i think, in the future, we might battle decoherence using the protocol described in our work, since it's a much more subtle and potentially effective way to battle decoherence.
if we find weak and reversing measurements appropriate for a given type of decoherence," kim adds, "practically all real- world quantum information implementations could benefit by utilizing the protocol."
not surprisingly, then, the next steps in the team's research involves ways of performing weak measurement and reversing measurement for other types of decoherence.
"in particular," kim tells physorg, "we're interested in a realistic quantum communication scenario - so we're hoping to find ways to perform weak measurement and reversing measurement for the types of decoherence often found in fiber and free- space quantum channels."
they're also working on apply their protocol to solid- state (or superconducting) qubits, and atomic qubits (such as trapped ions), because the particular type of decoherence considered in their study is directly responsible for loss of coherence in such systems.
"we're actively discussing such experimental possibilities," he adds, "with colleagues working on solid- state physics and atomic physics."
kim also describes how their findings may make it possible to effectively handle decoherence in quantum information by combining your scheme for protecting entanglement from decoherence with entanglement distillation, a protocol essential in long- distance quantum communications.
"let's consider a simple quantum communication scenario in which entangled photon pairs are distributed through quantum channels with decoherence," kim illustrates.
"alice prepares the entangled qubit pairs and sends one qubit to bob and the other qubit to charlie.
ideally, bob and charlie share an entangled- bit, or e- bit, which can be used for many quantum information tasks.
however, all practical quantum channels cause a certain level of decoherence to the quantum states being distributed through the channel.
if the decoherence is too big, initially entangled qubit pairs end up losing the entanglement."
as a result, bob and charlie now share two qubits with no entanglement, and which are therefore of no use in quantum information.
"however," kim explains, "using our protocol, bob and charlie can still share two qubits with some amount of entanglement even through a quantum channel with severe decoherence.
the amount of entanglement that bob and charlie share depends on the strengths of weak and reversing measurements.
bob and charlie can now repeat the process until they accumulate sufficient numbers of qubit pairs with less- than- maximum entanglement.
now, to produce a single maximally entangled qubit pairs, bob and charlie perform an entanglement distillation protocol on multiple pairs of less- than- maximum entanglement."
in other words, by using their protocol and combining it entanglement distillation, bob and charlie can share maximally entangled qubit pairs even through quantum channels with strong decoherence.
specifically addressing the study's impact on quantum information applications, kim first notes that quantum teleportation requires that two parties share maximally- entangled qubit pairs.
"in realistic situations, due to decoherence in real quantum channels, sharing maximally entangled qubit pairs over long distances would not be possible, thus preventing quantum teleportation between two parties with a very large separation.
of course, kim points out, "these two parties could use quantum memory devices to store the qubits and then move apart, but all quantum memory devices have a certain storage time, after which it is not possible to extract the identical qubit.
furthermore, a practical quantum memory device adds decoherence to the quantum state being stored.
thus, our protocol will allow long- distance quantum teleportation possible even if the quantum channel is not ideal."
secondly, a quantum computer needs to operate coherently until the results are measured and read out.
"in implementing a quantum computer," notes kim, "a qubit and/or many entangled qubits must undergo unitary transformations before decoherence affects the qubit states.
in other words, each qubit is said to have a certain characteristic decoherence time.
if an operation cannot be done within that decoherence time, it becomes meaningless as it no longer represents a unitary transformation.
by using our protocol, it should be possible to prolong the effective decoherence time of a qubit, thus making the qubit less vulnerable to external perturbation."
finally, in quantum cryptography, as in quantum teleportation, it is often essential pure state qubits must be sent from one party to another.
"if the channel between alice and bob adds decoherence," says kim, "even if alice sends a pure state, bob doesn't receive a pure state.
if this should happen, the quantum bit error rate, or qber, of the quantum cryptography system will rise - and if it rises above a certain threshold value, it becomes impossible to generate secure keys.
clearly, our protocol will have a unique position for practical quantum cryptography if a practical quantum channel is to be considered."
more generally, kim tells physorg, the team's research paves the way for dealing with decoherence in an active way by utilizing weak measurement and reversing measurement.
"the general approach of using a non- projective quantum measurement, also known as a positive operator valued measure, or povm" - a measure whose values are non- negative self- adjoint operators on a hilbert space - "has been shown to have interesting and important practical applications in quantum information.
in this work, we demonstrated that such generalized measurements can be used to actively battle decoherence - and i think it should be possible to find other practical applications of this approach," envisions kim.
"another aspect i wish to mention is something more fundamental," kim concludes.
"so far, measurement in quantum optics and quantum information has nearly always meant projective, or von neumann, measurement.
our work suggests that there could be other interesting and important applications of weak measurement and reversing measurement, not only for quantum information, but also for precision measurement, atomic optics, cavity quantum electrodynamics, mesoscopic physics, and many other areas."
more information: protecting entanglement from decoherence using weak measurement and quantum measurement reversal, nature physics 8, 117- 120 (2012), doi:10.1038/nphys2178.
researchers at the department of energy's oak ridge national laboratory have developed a quantum chemistry simulation benchmark to evaluate the performance of quantum devices and guide the development of applications for future quantum computers.
an ornl research team lead is developing a universal benchmark for the accuracy and performance of quantum computers based on quantum chemistry simulations.
the benchmark will help the community evaluate and develop new quantum processors.
(below left: schematic of one of quantum circuits used to test the rbh molecule.
top left: molecular orbitals used.
top right: actual results obtained using the bottom left circuit for rbh).
image credit: ornl
their findings were published in npj quantum information.
quantum computers use the laws of quantum mechanics and units known as qubits to greatly increase the threshold at which information can be transmitted and processed.
whereas traditional "bits" have a value of either 0 or 1, qubits are encoded with values of both 0 and 1, or any combination thereof, allowing for a vast number of possibilities for storing data.
while still in their early stages, quantum systems have the potential to be exponentially more powerful than today's leading classical computing systems and promise to revolutionize research in materials, chemistry, high- energy physics, and across the scientific spectrum.
but because these systems are in their relative infancy, understanding what applications are well suited to their unique architectures is considered an important field of research.
"we are currently running fairly simple scientific problems that represent the sort of problems we believe these systems will help us to solve in the future," said ornl's raphael pooser, principal investigator of the quantum testbed pathfinder project.
"these benchmarks give us an idea of how future quantum systems will perform when tackling similar, though exponentially more complex, simulations."
pooser and his colleagues calculated the bound state energy of alkali hydride molecules on 20- qubit ibm tokyo and 16- qubit rigetti aspen processors.
these molecules are simple and their energies well understood, allowing them to effectively test the performance of the quantum computer.
by tuning the quantum computer as a function of a few parameters, the team calculated these molecules' bound states with chemical accuracy, which was obtained using simulations on a classical computer.
of equal importance is the fact that the quantum calculations also included systematic error mitigation, illuminating the shortcomings in current quantum hardware.
systematic error occurs when the "noise" inherent in current quantum architectures affects their operation.
because quantum computers are extremely delicate (for instance, the qubits used by the ornl team are kept in a dilution refrigerator at around 20 millikelvin (or more than - 450 degrees fahrenheit), temperatures and vibrations from their surrounding environments can create instabilities that throw off their accuracy.
for instance, such noise may cause a qubit to rotate 21 degrees instead of the desired 20, greatly affecting a calculation's outcome.
"this new benchmark characterizes the 'mixed state,' or how the environment and machine interact, very well," pooser said.
"this work is a critical step toward a universal benchmark to measure the performance of quantum computers, much like the linpack metric is used to judge the fastest classical computers in the world."
while the calculations were fairly simple compared to what is possible on leading classical systems such as ornl's summit, currently ranked as the world's most powerful computer, quantum chemistry, along with nuclear physics and quantum field theory, is considered a quantum "killer app."
in other words, it is believed that as they evolve quantum computers will be able to more accurately and more efficiently perform a wide swathe of chemistry- related calculations better than any classical computer currently in operation, including summit.
"the current benchmark is a first step towards a comprehensive suite of benchmarks and metrics that govern the performance of quantum processors for different science domains," said ornl quantum chemist jacek jakowski.
"we expect it to evolve with time as the quantum computing hardware improves.
ornl's vast expertise in domain sciences, computer science and high- performance computing make it the perfect venue for the creation of this benchmark suite."
ornl has been planning for paradigm- shifting platforms such as quantum for more than a decade via dedicated research programs in quantum computing, networking, sensing and quantum materials.
these efforts aim to accelerate the understanding of how near- term quantum computing resources can help tackle today's most daunting scientific challenges and support the recently announced national quantum initiative, a federal effort to ensure american leadership in quantum sciences, particularly computing.
such leadership will require systems like summit to ensure the steady march from devices such as those used by the ornl team to larger- scale quantum systems exponentially more powerful than anything in operation today.
access to the ibm and rigetti processors was provided by the quantum computing user program at the oak ridge leadership computing facility, which provides early access to existing, commercial quantum computing systems while supporting the development of future quantum programmers through educational outreach and internship programs.
support for the research came from doe's office of science advanced scientific computing research program.
"this project helps doe better understand what will work and what won't work as they forge ahead in their mission to realize the potential of quantum computing in solving today's biggest science and national security challenges," pooser said.
next, the team plans to calculate the exponentially more complex excited states of these molecules, which will help them devise further novel error mitigation schemes and bring the possibility of practical quantum computing one step closer to reality.
ut- battelle manages ornl for doe's office of science.
the office of science is the single largest supporter of basic research in the physical sciences in the united states and is working to address some of the most pressing challenges of our time.
for more information, please visit https://science.energy.gov/.
quantum computingcyber security
yesterday, google confirmed the contents of a memo posted to (and then removed from) the nasa website in september, revealing that a team of researchers led by john martinis had achieved "quantum supremacy".
quantum supremacy is the development point at which a quantum computer is shown to be able to perform a task beyond the capabilities of even the most powerful supercomputer.
a calculation that reportedly would have taken ibm's summit - the world's most powerful commercially available computer - around 10,000 years to complete was allegedly cracked in under four minutes by google's 53- qubit sycamore machine.
for unknown reasons, the computation wasn't made with google's larger, more powerful 72- qubit bristlecone machine.
today, ibm (google's biggest rival in the quantum race) responded, casting doubt on google's achievement.
rather than 10,000 years, ibm argues that the task that google used to benchmark quantum supremacy could have been accomplished in under three days on a classical computer, "and with far greater fidelity."
ibm's point is that the original definition of quantum supremacy given in 2012 by physicist john preskill describes the point where quantum computers can do things that classical computers can't.
therefore, ibm argues, this threshold has not been met.
regardless of which company is ahead, or even if the race has been won already, widespread commercial applications for quantum computing that make ibm watson look like a ti- 82 are coming.
so, what do we even use them for?
see also:
ibm hits quantum computing 'milestone' revealing its highest quantum volume to date
just like graphics processors are good at rendering lush, particle- packed digital worlds, but wouldn't know what to what to do with a simple data analysis task, there are some things that quantum computers are going to do better than anything else, and some things they cannot.
here's gigabit's breakdown of the three applications for quantum computers that could have the biggest impact on everything from our day to day lives to the fate of hyperscale enterprises.
encryption
the biggest disruption - and why the finance industry is most nervous about a quantum dawn before it's ready for one - would most likely be in the field of encryption.
the increased power stemming from quantum computers' ability to process computations in parallel - rather than sequentially like conventional machines - could render modern standards of encryption ineffective against a quantum cyber attack (single handedly the coolest three words i've ever strung together) potentially laying bare every aspect of the financial sector, the state secrets of world governments and the personal information of billions.
as a result of a growing unease over the potential obsolescence of modern security measures, the global quantum cybersecurity market is predicted to grow from around $101mn in 2018 to more than $506mn by 2023.
weather
currently, conventional computer analysis of weather conditions can sometimes take longer than the weather itself does to change.
in the us, the head of the national oceanic and atmospheric association's chief economist, rodney f. weiher said estimated back in 2008 that nearly 30% of the country's gdp (about $6trn) was directly or indirectly affected by the weather, which can impact agriculture, logistics, retail commerce and more.
a quantum computer's ability to crunch vast amounts of data could lead to the sort of weather system modeling that could allow scientists to predict the changing weather patterns with never before seen accuracy - something that could become essential as climate change radically rewrites the predictable meteorology of our world.
chemistry
according to ibm, one of the first and most promising applications of quantum computing will be in the field of chemistry.
even for simple molecules like caffeine, the number of quantum states in the molecule can be astoundingly large - so large that all the conventional computing memory and processing power that could ever be built couldn't model it.
the ability for quantum machines to entertain the existence of both 1 and 0 simultaneously could provide the necessary power to successfully map and model increasingly simple molecules, catapulting our understanding of interactions forward dramatically, potentially opening a new era of pharmaceutical research.
"spooky action at a distance," einstein's famous, dismissive characterization of quantum entanglement, has long been established as a physical phenomenon, and researchers are keen to develop practical applications for entanglement including communication, encryption, and computing.
quantum entanglement is a phenomenon in which the production or the interactions of a number of particles cannot be described independently of each other, and must instead be described in terms of the whole system's quantum state.
two recent experiments with entanglement have been reported in the proceedings of the national academy of sciences, one proving that complex quantum states in photons can be preserved even in turbulent atmospheric conditions; the other demonstrating entanglement swapping between qubits over the 143 kilometers between the canary islands and tenerife.
in the first experiment, a group of austrian researchers sought to transmit complex, quantum- encoded information through turbulent air.
the polarization of photons is a robust technique favored for long- distance entanglement research in which two photons with mutually perpendicular polarizations exist in different locations.
the state is easily controllable and resistant to atmospheric turbulence.
but polarization is a binary state that cannot easily carry complex information.
the encoding of more complex information in an entangled system- for instance, a jpeg image of albert einstein shaking his fist at entangled photons- would require a much larger space state than the two- dimensional space state of polarization.
the researchers used photons with a twisted phase front, which carry an unbounded amount of orbital angular momentum (oam)- a much larger space state than polarization.
in principle, oam offers the possibility of practical applications like quantum encryption and the testing of fundamental properties of quantum physics.
but oam is quite sensitive to atmospheric turbulence.
however, the researchers succeeded in distributing the quantum entanglement of spatially structured photons over a free- space link across the city of vienna.
in the experiment, the sender, traditionally known in quantum entanglement experiments as alice, and the receiver, bob, were in different physical locations three kilometers apart.
alice was in a 35- meter- high radar tower with a high- fidelity, high- brightness entanglement source that produced entangled pairs.
photon a remained unchanged, while photon b's polarization state was transferred interferometrically to a generated hybrid- entangled quantum oam state.
photon a was delayed via a 30 meter fiber cable to ensure that it was not measured before photon b was transferred to bob, on the other side of the city.
after the transfer, the polarization of photon a was measured and each detection event time- stamped.
similarly, the detection of photon b was time- stamped and the two events were correlated at a sub- nanosecond regime.
the researchers verified quantum entanglement of the pairs through turbulent atmosphere.
though they were still using a two- dimensional space state, the method allows for a quantum link with up to 11 orthogonal channels of oam.
they write, "our result clearly shows that entanglement encoded in oam can be identified after long- distance transmission.
it is not fundamentally limited by atmospheric turbulence, as expected in some recent investigation, and thus could be a feasible way to distribute high- dimensional entanglement."
in the second experiment, another group that included many of the same researchers teleported an entangled state between two qubits, a process known as entanglement swapping.
this is in contrast to the teleportation of a single quantum state from one qubit to another; in essence, the researchers created an entangled state between two qubits located 143 kilometers from each other, and which had never interacted before.
this provides a solution to the no- cloning theorem, which in quantum theory states that it is impossible to create an identical copy of an unidentified quantum state.
by demonstrating the applicability of the technique outside the laboratory, the experiment opens a door to such practical applications as cloud quantum computing.
the researchers don't shy from the significance of this accomplishment, writing, "our data demonstrate successful entanglement swapping via a long- distance free- space link under the influence of highly demanding environmental conditions, in fact more challenging than expected for a satellite- to- ground link.
this proves the feasibility of a crucial element for realizing a quantum repeater in a future space- and ground- based worldwide quantum internet."
more information: paper 1: twisted photon entanglement through turbulent air across vienna.
pnas 2015 ; published ahead of print november 2, 2015, doi: 10.1073/pnas.1517574112
abstract
photons with a twisted phase front can carry a discrete, in principle, unbounded amount of orbital angular momentum (oam).
the large state space allows for complex types of entanglement, interesting both for quantum communication and for fundamental tests of quantum theory.
however, the distribution of such entangled states over large distances was thought to be infeasible due to influence of atmospheric turbulence, indicating a serious limitation on their usefulness.
here we show that it is possible to distribute quantum entanglement encoded in oam over a turbulent intracity link of 3 km.
we confirm quantum entanglement of the first two higher- order levels (with oam=+- 1 and +- 2).
they correspond to four additional quantum channels orthogonal to all that have been used in long- distance quantum experiments so far.
therefore, a promising application would be quantum communication with a large alphabet.
we also demonstrate that our link allows access to up to 11 quantum channels of oam.
the restrictive factors toward higher numbers are technical limitations that can be circumvented with readily available technologies.
paper 2: teleportation of entanglement over 143 km.
pnas 2015 ; published ahead of print november 2, 2015, doi: 10.1073/pnas.1517007112
abstract
as a direct consequence of the no- cloning theorem, the deterministic amplification as in classical communication is impossible for unknown quantum states.
this calls for more advanced techniques in a future global quantum network, e.g., for cloud quantum computing.
a unique solution is the teleportation of an entangled state, i.e., entanglement swapping, representing the central resource to relay entanglement between distant nodes.
together with entanglement purification and a quantum memory it constitutes a so- called quantum repeater.
since the aforementioned building blocks have been individually demonstrated in laboratory setups only, the applicability of the required technology in real- world scenarios remained to be proven.
here we present a free- space entanglement- swapping experiment between the canary islands of la palma and tenerife, verifying the presence of quantum entanglement between two previously independent photons separated by 143 km.
we obtained an expectation value for the entanglement- witness operator, more than 6 sds beyond the classical limit.
by consecutive generation of the two required photon pairs and space- like separation of the relevant measurement events, we also showed the feasibility of the swapping protocol in a long- distance scenario, where the independence of the nodes is highly demanded.
because our results already allow for efficient implementation of entanglement purification, we anticipate our research to lay the ground for a fully fledged quantum repeater over a realistic high- loss and even turbulent quantum channel.
quantum encryption is here but encrypting quantum computing data is a little way off.
a recent report released by inside quantum technology found that quantum key distribution (qkd), an encryption technology designed to protect critical data in the quantum computing era that was only in the experimental stage a couple of years ago, is now being supplied by major companies, including fujitsu, nokia, raytheon, and toshiba, among others.
the research firm is projecting that the qkd distribution market will grow to more than $980 million by 2024 with the telephone companies being the primary purchasers of qkd gear for their networks, which is being developed by nokia, zte, bt and ntt.
the first specialist qkd carrier, quantum xchange, is creating a qkd link between manhattan and northern new jersey.
qkd secures communications by implementing a cryptographic protocol involving components of quantum mechanics that allows two groups to produce a shared random secret key - known only to them - to encrypt and decrypt messages.
quantum mechanics is what enables a unique and compelling feature of qkd: the two authorized parties can detect whether a third party is attempting to gain knowledge of their key because the act of eavesdropping creates detectable anomalies.
qkd differs from traditional public key cryptography, which depends on the computational difficulty of certain mathematical functions.
qkd essentially uses quantum states to make sure that there's no way you can break into a qkd- protected network, said lawrence gasman, founder and president of inside quantum technology in a telephone interview with ee times.
"if you interfere with the information in any way, it ceases to exist.
it's the first uncrackable system."
the first specialist qkd carrier, quantum xchange, is creating a qkd link between manhattan and northern new jersey.
its trusted node is composed of two or more conventional qkd system endpoints.
as long as a continuous path of trusted nodes links two widely separated endpoints, those endpoints can still share quantum- derived key information over this much longer path.
(source: quantum xchange)
it's important to distinguish between the benefits of a qkd network with quantum computing, which is expected to be able to crack standard algorithms in about a decade, and encrypting the data on a quantum computer, or the data being shared between two quantum computers, said michael osborne, manager of the security research group at ibm research.
"if you have multiple quantum machines and you actually want to interact between them, what sort of a security models does one require in that scenario?"
the priority, he said, is doing the work necessary to make today's security "quantum secure," so the long- term identity, signature, and data encrypted today is safe for the lifetime that those things are required.
"there's a lot of confusion.
a lot of people see qkd as the answer to becoming quantum safe - but that only really addresses one part of a problem: securing a point- to- point connection that you have control of."
osborne added, currently, the best niche use case of qkd is by telcos that need to control that point- to- point connection to make sure there are no "man- in- the- middle" attacks.
for now, quantum encryption is being used to secure "classical" data back and forth, rather than for keeping data communications between quantum computers secure - which is some time off.
"you'd use very different techniques to actually communicate between the quantum machines.
so qkd is very much about a partial solution to a problem for protecting data as we send it around today."
as noted by the inside quantum technology report, there's lots of qkd activity in the telco space with both terrestrial and satellite networks.
by 2024, $254 million will be spent on satellite based qkd networks.
japan, italy, germany, singapore, canada, and the u.k. all have qkd deployed on satellite networks, while china is in the lead, having demonstrated space- to- ground qkd from its micias satellite and from the tiangong- 2 space lab.
other applications that may be embraced by qkd include securing national power grids and other infrastructure that is increasingly at risk due to the adoption of remote sensing and control, the report noted, but infrastructure companies tend to be very conservative with respect to technology change and are expected to embrace qkd slowly.
related articles:
1333900
1333160
1332290
1332216
1331768
gary hilson is a freelance writer and editor who has written thousands of words for print and pixel publications across north america.
his areas of interest include software, enterprise and networking technology, research and education, sustainable transportation, and community news.
his articles have been published by network computing, informationweek, computing canada, computer dealer news, toronto business times, strategy magazine, and the ottawa citizen.
this is interesting, but i had a couple of questions, not being an expert in quantum cryptography.
how is a quantum key exchange better than diffie- hellman?
also, how is the man- in- the- middle attack prevented?
it seems like you could do a quantum man- in- the- middle just like a classical one.
also, if the link is long and you need repeaters, and the repeaters have to be authenticated so that they can regenerate a key, how is this any better than using classical authentication?
it seems worse.
with a classical key exchange, you have to authenticate the other guy, but it doesn't matter how many repeaters the public key goes through on its path.
this site uses akismet to reduce spam.
learn how your comment data is processed.
quantum computers newsmaker rigetti computing has announced quantum cloud services and, along with that, a $1 million contest prize for a conclusive demonstration of quantum advantage.
takers will be those who take to the idea of exploring quantum machines' potentials through the cloud.
rigetti's platform was also noted in the context of stepping into the "cloud establishment" arena.
forbes said, "ceo chad rigetti's challenger to the cloud establishment comes with a major technological ace: what the startup says is the first- ever cloud service powered by quantum computing."
the prize, meanwhile, will go to the first person or team using the qcs to demonstrate that a quantum machine is capable of showing what was called "quantum advantage."
competition details are said to follow at the end of next month.
rigetti fleshed out his concept of quantum advantage in a blog.
"this is the inflection point where quantum computers first begin to solve practical problems faster, better, or cheaper than otherwise possible."
rigetti believes that this quantum "advantage" will be reached in new markets and domains, "changing the ways in which problems are solved across industries."
mit technology review got down to what sets quantum computing apart from traditional computing.
traditional machines use standard digital bits that can represent either 1 or 0, said - but qubits can be both at the same time.
"adding just a few extra qubits to a machine- and linking them via a phenomenon known as 'entanglement'- creates exponential leaps in computing power," said martin giles, the review's san francisco bureau chief.
giles further compared the traditional way of getting experiment results with the new approach.
"to run such experiments, researchers often program their own classical computers with hybrid quantum algorithms that then use application programming interfaces, or apis, to call on quantum machines in the cloud for specific bits of a calculation."
the results are shipped to the traditional machines.
quantum computers promise to run calculations far beyond the reach of any conventional supercomputer, but what exactly is qcs?
this is a cloud computing platform where quantum processors are integrated with a classical computing infrastructure, said the blog.
"qcs tackles the problem with a data center containing both quantum computers and classical ones in a system optimized to run entire hybrid algorithms," said giles.
rigetti computing is a five year old startup as described by forbes.
mit technology review said the company recently unveiled "the world's most powerful quantum processor, a 128- qubit model that tops the previous record holder, google's 72- qubit bristlecone chip."
"in august, we announced that we are building 128- qubit quantum computers with the low error rates needed to achieve advantage.
these systems are based on our scalable 16, 32, and 128- qubit aspen quantum processors," wrote rigetti.
what's next?
"we will be granting early access to quantum cloud services in the coming weeks," he said.
"you can sign up to reserve a qmi today at rigetti.com."
iqm finland oy (iqm), today announced an expansion through the establishment of its first subsidiary company, iqm germany, located in munich.
the company also announced the appointment of renowned quantum computing and quantum technology expert, prof. enrique solano as ceo of iqm germany.
iqm is an emerging hardware leader focused on developing quantum computers based on scalable high- speed superconducting quantum processors.
the company was last friday named as one of the ten most promising startups in finland by the finnish financial magazine talouselama.
the expansion move intends to harness local quantum expertise to create a co- design hub that tightens the interaction between quantum hardware and quantum software development.
the idea is to optimize quantum chips at the earliest development stage to meet gate design, connectivity, gate fidelity, and other technical imperatives for scalable quantum computing.
the novel co- design model lets iqm collaborate closely with its current and future software partners throughout europe to accelerate the development of useful quantum solutions for specialized applications.
prof. enrique solano will lead the munich operation as ceo of iqm germany.
the peruvian/spanish physicist is a renowned international expert in quantum computing, quantum simulation, quantum artificial intelligence, and quantum technologies.
he joins iqm from his leading positions in two quantum centers in bilbao, spain, and shanghai, china.
"munich was the obvious location for the new facility," said iqm's ceo, dr. jan goetz.
"for germany and bavaria, in particular, building a quantum computer is a strategic national priority.
that commitment has produced an ecosystem of quantum innovation, with stakeholders from industry, academia and the investor community collectively focused on advancing europe's quantum leadership."
the new facility marks iqm's first expansion outside of finland.
since its debut last july, the company has ramped up fast to build an infrastructure to produce scalable quantum devices efficiently.
at its operations lab in espoo, qubit samples are already being produced on a wafer scale every week.
the devices are benchmarked onsite for uniformity, accuracy, quality, and other imperatives for real- world applications.
the high- throughput/fast- feedback model allows iqm technologists to constantly monitor the quality of the fabrication processes, and thus ensure high- quality quantum processors.
at present, iqm's team of experts drives the business in four core areas: fabrication, scalable electronics, software, and systems integration.
the new team in munich will extend these efforts to design special- purpose processors for near- term algorithms, while also focusing on ip development.
of the company's nearly three dozen employees, 25 hold phds, almost all in physics, and specifically quantum computing.
prof. solano adds new weight to the multi- national iqm team with his record of leadership in academic and entrepreneurial ecosystems.
his previous roles include distinguished professor and director at quartist - quantum artificial intelligence for science and technology at shanghai university in china, as well as similar positions at the international center of quantum computing and quantum technologies at the university of the basque country in bilbao, spain.
he obtained his phd in physics from the universidade federal de rio de janeiro in brazil.
"the hardware- software co- design model is different and exciting and brings a design- think mindset to quantum computing," said prof. solano.
"it lets us develop and optimize next- generation chip architectures for specific applications and implement quantum computing algorithms for industries with entrenched operations in germany and throughout europe.
for example, financial models and predictions for car manufacturers, as well as models for drug design, material design, aerodynamics, nuclear reactions, biological systems, intelligent devices, use- cases for smart cities, and much more.
i'm thrilled to join the company."
prof. mikko mottonen, founder and chief scientist of iqm added: "i have long admired enrique for his ability to solve extremely difficult theoretical problems that are useful for practical application.
he's already building a team of experts at iqm to increase our understanding of quantum algorithms to develop financial models for myriad applications.
this will enable us to quickly optimize our hardware for new algorithmic inventions produced by our software partners.
he's a dream addition to the team."
for information on iqm, please visit www.meetiqm.com
mikko mottonen, chief scientist, iqm; email: mikko@meetiqm.com; +358 505 940 950 (english & finnish)
researchers working in singapore and the united states have discovered that all entangled states of two particles have a classical 'fingerprint'.
this breakthrough could help engineers guard against errors and devices that don't do what they promise in quantum computing and quantum cryptography.
goh koon tong and valerio scarani at the centre for quantum technologies at the national university of singapore, with andrea coladangelo at the california institute of technology, reported in nature communications on 26 may that a simple set of measurements can act as an identity check for any two- particle entangled state.
the presence of this fingerprint could help certify quantum computers or quantum encryption devices purchased from third parties.
an entangled quantum state is made of two or more particles held in a multitude of undecided outcomes.
such states are fuel for quantum computing and bring security to quantum communication.
the problem is, it is difficult to check that these states have the properties expected of them.
that leaves the door open for poorly- functioning devices.
"i like to see our work as bringing the power of testing quantum devices to the consumers who use them.
currently, only those who build the devices or understand the engineering aspect of them can perform the test," says goh.
quantum physicists could also use this 'self- testing' tool as a check step in lab experiments.
the work builds on results by other groups, extending findings for qubits to the more exotic qudits.
qudits are higher- dimensional quantum bits.
rather than just storing a binary bit of information - a 0 or 1 - a qudit has bigger information density, storing a 0, 1, 2, 3, 4, etc.
such states, though hard to make, are interesting because they could accelerate some computing or communication tasks.
the idea of self- testing is significant because it is generally difficult to gain a lot of information about the quantum state of a particle.
a particle's state is described by a 'wave function' that encodes the probabilities for the particle's various properties, such as polarisation or momentum.
to be sure about a quantum state, you need to know the whole wave function.
however, there is a problem here.
measuring the quantum state reveals just one value - not the full set of possibilities.
the traditional way to try to learn the full quantum state involves a technique called tomography.
this requires measuring many copies of the quantum state in different ways, counting up all the outcomes of the various measurements to give a full set of probabilities.
it also involves a laborious process of characterising the measurement devices and aligning them with the source of the quantum particles.
self- testing is more efficient, requiring fewer measurements.
it is also 'device- independent', or like blind tomography - needing no characterisation of the measurement device, as long as the device is guaranteed to detect most of the particles.
this is because the fingerprint is a pattern of results across measurements of the two particles that could only be consistently created by the weird correlations in the quantum state, not by any classical process or by chance.
seeing this pattern then means the quantum state must be present.
the famous 'chsh experiment' in quantum physics is an example of fingerprinting for a quantum state of two qubits.
to prove that fingerprint tests exist for all two- qudit states, the authors showed that these states can be considered as composed of blocks of two- level systems, akin to qubits.
even better, this mathematical equivalence points to what measurements are needed - although it's not clear yet if they are experimentally- friendly to make.
the team hope that this discovery will motivate a new wave of research to find straightforward ways to incorporate this check in experiments or devices.
so far, the signs are good.
"of all my work in the past five years, this has attracted the most attention," says scarani.
as well as hearing from colleagues interested in the result, he has been invited to give a talk on self- testing at qcrypt, an annual conference on quantum cryptography being held this year in the uk in september.
university of adelaide- led research has moved the world one step closer to reliable, high- performance quantum computing.
an international team has developed a ground- breaking single- electron "pump."
the electron pump device developed by the researchers can produce one billion electrons per second and uses quantum mechanics to control them one by one.
and it's so precise they have been able to use this device to measure the limitations of current electronics equipment.
this paves the way for future quantum information processing applications, including in defence, cybersecurity and encryption, and big data analysis.
"this research puts us one step closer to the holy grail -  reliable, high- performance quantum computing," says project leader dr giuseppe c. tettamanzi, senior research fellow, at the university of adelaide's institute for photonics and advanced sensing.
published in the journal nano letters, the researchers also report observations of electron behaviour that's never been seen before -  a key finding for those around the world working on quantum computing.
"quantum computing, or more broadly quantum information processing, will allow us to solve problems that just won't be possible under classical computing systems," says dr tettamanzi.
"it operates at a scale that's close to an atom and, at this scale, normal physics goes out the window and quantum mechanics comes into play.
"to indicate its potential computational power, conventional computing works on instructions and data written in a series of 1s and 0s -  think about it as a series of on and off switches; in quantum computing every possible value between 0 and 1 is available.
we can then increase exponentially the number of calculations that can be done simultaneously."
this university of adelaide team, in collaboration with the university of cambridge, aalto university in finland, university of new south wales, and the university of latvia, is working in an emerging field called electron quantum optics.
this involves controlled preparation, manipulation and measurement of single electrons.
although a considerable amount of work has been devoted world- wide to understand electronic quantum transport, there is much still to be understood and achieved.
"achieving full control of electrons in these nano- systems will be highly beneficial for realistic implementation of a scalable quantum computer.
we, of course, have been controlling electrons for the past 150 years, ever since electricity was discovered.
but, at this small scale, the old physics rules can be thrown out," says dr tettamanzi.
"our final goal is to provide a flow of electrons that's reliable, continuous and consistent -  and in this research, we've managed to move a big step towards realistic quantum computing.
"and, maybe equally exciting, along the way we have discovered new quantum effects never observed before, where, at specific frequencies, there is competition between different states for the capture of the same electrons.
this observation will help advances in this game- changing field."
the commercial development of quantum computers calls for breakthroughs in three important areas: quantum logic, readout of quantum information, and managing losses.
quantum information is destroyed if there are energy losses in the system.
on the other hand, quantum computers need losses to efficiently initialize quantum memory.
the new scalable fabrication process for quantum- circuit refrigerators (scar) project aims to develop the reliability and scope of the quantum- circuit refrigerators that are part of quantum processors.
the ultimate goal is to demonstrate that quantum- circuit refrigerators can be used to reliably manage losses.
scar is professor mikko mottonen's fourth project grant from the european research council (erc).
photo of a centimeter- sized silicon chip, which has two parallel superconducting resonators and quantum- circuit refrigerators connected to them.
image credit: kuan yen tan/aalto university
'we will use an electron beam writer to manufacture 10,000 quantum- circuit refrigerators, which are tunnel junctions about 100 nanometres in size.
we will systematically study which actions improve the quality of the manufacturing process and its uniformity.
the revised process may involve new cleaning methods and alternative materials for the tunnel junctions,' mikko mottonen explains.
one of the aims of the cleaning is to prevent unwanted atoms and molecules from entering the junctions and thus changing their properties over time.
'the options for the cleaning methods include the use of hydrofluoric acid, which can be used to clean the silicon surface very efficiently.
it is a rather aggressive method, however, and the junctions and masks used to make the junctions may not be able to withstand it.
aluminium has been used as a superconductor in the junctions and it does not need to be replaced.
copper has been used, for example, as a normal metal, and a gold- palladium alloy could be tried as an alternative to this.
mikko mottonen has received a eur2 million erc consolidator grant for the period 2017- 2021.
furthermore, the new scar project also supports the construction of a domestic quantum ecosystem which includes, for example, aalto university, vtt technical research centre of finland and a number of companies from the quantum technology sector.
'the quantum- circuit refrigerator was invented by my group a few years ago, and it has been further developed in other research projects.
our goal is to be able to manage losses extremely precisely and in real time,' mikko mottonen adds.
the study is being carried out by the quantum computing and devices research group, which is part of quantum technology finland (qtf), a national centre of excellence in quantum research.
the group uses the national otanano research infrastructure for its research.
featured news from related categories:
researchers at crann and the school of physics at trinity college dublin have created an innovative new device that will emit single particles of light, or photons, from quantum dots that are the key to practical quantum computers, quantum communications, and other quantum devices.
the team has made a significant improvement on previous designs in photonic systems via their device, which allows for controllable, directional emission of single photons and which produces entangled states of pairs of quantum dots.
qubits and the promise of quantum computing
the promise of quantum computers leverages the properties of quantum bits - "qubits" - to execute computations.
current computers process and store information in bits of either 0s or 1s whereas qubits can be 0 and 1 simultaneously.
that means quantum computers will have much greater computational powers over and above classical computers.
scientists are exploring different options and designs to make quantum computing a viable reality.
one proposed idea utilises photonic systems, making use of quantum properties of light at the nanoscale, as qubits.
the trinity team explores such a system in their recently published paper in the high- profile journal nano letters.
their system utilises single photons of light emitted in a controlled fashion in time and space from quantum emitters (nanoscale materials known as quantum dots).
for applications such as quantum computing, it is necessary to control emissions from these dots and to produce quantum entanglement of emission from pairs of these dots.
quantum entanglement is a fundamental property of quantum mechanics and occurs when a pair or group of particles are quantum- mechanically linked in a way such that the quantum state of each particle of the pair cannot be described independently of the state of the others.
essentially, two entangled quantum dots can emit entangled photons.
professor john donegan, crann and trinity's school of physics, said:
"the device works by placing a metal tip within a few nanometers of a surface containing the quantum dots.
the tip is excited by light and produces an electric field of such enormous intensity that it can greatly increase the number of single photons emitted by the dots.
this strong field can also couple emission from pairs of quantum dots, entangling their states in a way that is unique to quantum emitters of light."
the other significant advantage is the mechanism by which the device works over current state- of- art photonic devices for quantum computing applications.
professor ortwin hess, professor of quantum nanophotonics in trinity's school of physics and crann, added: "by scanning the metal tip over the surface containing the quantum dots, we can generate the single photon emission as required.
such a device is much simpler than current systems that attempt to fix a metal tip, or a cavity, in close proximity to a quantum dot.
we now expect that this device and its operation will have a striking effect on research in quantum emitters for quantum technologies."
the collaboration between professors hess and donegan began while professor hess was at imperial college london and will continue with his recent appointment to trinity through the sfi research professorship programme.
the team plans to fabricate devices that will demonstrate controlled single photon emission and contribute strongly to the research effort in quantum technologies in ireland.
physicists have theoretically shown that, when multiple nanoscale batteries are coupled together, they can be charged faster than if each battery was charged individually.
the improvement arises from collective quantum phenomena and is rooted in the emerging field of quantum thermodynamics- the study of how quantum effects influence the traditional laws governing energy and work.
the researchers, francesco campaioli et al., have published a paper on the fast charging of nanoscale batteries in a recent issue of physical review letters.
although a great deal of research has shown that quantum phenomena provide advantages in information processing applications, such as computing and secure communication, there have been very few demonstrations of quantum advantages in thermodynamics.
in one recent study in this area, researchers showed that quantum entanglement can allow more work to be extracted from a nanoscale energy- storage device, or "quantum battery," than would be possible without entanglement.
in the new study, the researchers build on this result to show that quantum phenomena can also enhance the charging power of quantum batteries.
they also found that the process does not necessarily require entanglement, although it does require operations that have the potential to generate entangled states.
"our work shows how entangling operations- that is, interactions between two or more bodies- are necessary to obtain a quantum advantage for the charging power of many- body batteries, whereas entanglement itself does not constitute a resource," campaioli, at monash university in australia, told phys.org.
"additionally, we show that for locally coupled batteries the quantum advantage scales with the number of interacting batteries."
the quantum advantage is not without its limits, however, and the physicists derive the upper bound on how much faster a collection of batteries can be charged with the help of quantum phenomena.
they show that for locally coupled batteries the quantum advantage grows with the number of interacting batteries.
these bounds for the quantum advantage are based on quantum speed limits, which are used, for example, to estimate the maximum speed of quantum processes, such as calculations on a quantum computer.
here, the limit is for thermodynamic processes.
overall, the results may lead to methods of improving future nanoscale energy- charging processes, as well as to a better understanding of how quantum theory and thermodynamics are related.
"our result could be used to provide optimal charging for nanodevices that rely on batteries that consist of few quantum systems, such as charge qubits, ions or atoms," campaioli said.
"our plan for future research in this field is to provide a tight upper bound to the advantage that can be obtained by means of interactions between a finite number of bodies.
furthermore, we would like to obtain an experimental realization of the above- mentioned quantum advantage."
research teams all over the world are exploring different ways to design a working computing chip that can integrate quantum interactions.
now, unsw engineers believe they have cracked the problem, reimagining the silicon microprocessors we know to create a complete design for a quantum computer chip that can be manufactured using mostly standard industry processes and components.
the new chip design, published in the journal nature communications, details a novel architecture that allows quantum calculations to be performed using existing semiconductor components, known as cmos (complementary metal- oxide- semiconductor) - the basis for all modern chips.
it was devised by andrew dzurak, director of the australian national fabrication facility at the university of new south wales (unsw), and dr menno veldhorst, lead author of the paper who was a research fellow at unsw when the conceptual work was done.
"we often think of landing on the moon as humanity's greatest technological marvel," said dzurak, who is also a program leader at australia's famed centre of excellence for quantum computation and communication technology (cqc2t).
"but creating a microprocessor chip with a billion operating devices integrated together to work like a symphony - that you can carry in your pocket!
- is an astounding technical achievement, and one that's revolutionised modern life.
"with quantum computing, we are on the verge of another technological leap that could be as deep and transformative.
but a complete engineering design to realise this on a single chip has been elusive.
i think what we have developed at unsw now makes that possible.
and most importantly, it can be made in a modern semiconductor manufacturing plant," he added.
veldhorst, now a team leader in quantum technology at qutech - a collaboration between delft university of technology and tno, the netherlands organisation for applied scientific research - said the power of the new design is that, for the first time, it charts a conceivable engineering pathway toward creating millions of quantum bits, or qubits.
"remarkable as they are, today's computer chips cannot harness the quantum effects needed to solve the really important problems that quantum computers will.
to solve problems that address major global challenges - like climate change or complex diseases like cancer - it's generally accepted we will need millions of qubits working in tandem.
to do that, we will need to pack qubits together and integrate them, like we do with modern microprocessor chips.
that's what this new design aims to achieve.
"our design incorporates conventional silicon transistor switches to 'turn on' operations between qubits in a vast two- dimensional array, using a grid- based 'word' and 'bit' select protocol similar to that used to select bits in a conventional computer memory chip," he added.
"by selecting electrodes above a qubit, we can control a qubit's spin, which stores the quantum binary code of a 0 or 1.
and by selecting electrodes between the qubits, two- qubit logic interactions, or calculations, can be performed between qubits."
a quantum computer exponentially expands the vocabulary of binary code used in modern computers by using two spooky principles of quantum physics - namely, 'entanglement' and 'superposition'.
qubits can store a 0, a 1, or an arbitrary combination of 0 and 1 at the same time.
and just as a quantum computer can store multiple values at once, so it can process them simultaneously, doing multiple operations at once.
this would allow a universal quantum computer to be millions of times faster than any conventional computer when solving a range of important problems.
but to solve complex problems, a useful universal quantum computer will need a large number of qubits, possibly millions, because all types of qubits we know are fragile, and even tiny errors can be quickly amplified into wrong answers.
"so we need to use error- correcting codes which employ multiple qubits to store a single piece of data," said dzurak.
"our chip blueprint incorporates a new type of error- correcting code designed specifically for spin qubits, and involves a sophisticated protocol of operations across the millions of qubits.
it's the first attempt to integrate into a single chip all of the conventional silicon circuitry needed to control and read the millions of qubits needed for quantum computing."
"we expect that there will still be modifications required to this design as we move towards manufacture, but all of the key components that are needed for quantum computing are here in one chip.
and that's what will be needed if we are to make quantum computers a workhorse for calculations that are well beyond today's computers," dzurak added.
"it shows how to integrate the millions of qubits needed to realise the true promise of quantum computing."
building such a universal quantum computer has been called the 'space race of the 21st century'.
for a range of calculations, they will be much faster than existing computers, and for some challenging problems they could find solutions in days, maybe even hours, when today's best supercomputers would take millions of years.
there are at least five major quantum computing approaches being explored worldwide: silicon spin qubits, ion traps, superconducting loops, diamond vacancies and topological qubits; unsw's design is based on silicon spin qubits.
the main problem with all of these approaches is that there is no clear pathway to scaling the number of quantum bits up to the millions needed without the computer becoming huge a system requiring bulky supporting equipment and costly infrastructure.
that's why unsw's new design is so exciting: relying on its silicon spin qubit approach - which already mimics much of the solid- state devices in silicon that are the heart of the us$380 billion global semiconductor industry - it shows how to dovetail spin qubit error correcting code into existing chip designs, enabling true universal quantum computation.
unlike almost every other major group elsewhere, cqc2t's quantum computing effort is obsessively focused on creating solid- state devices in silicon, from which all of the world's computer chips are made.
and they're not just creating ornate designs to show off how many qubits can be packed together, but aiming to build qubits that could one day be easily fabricated - and scaled up.
"it's kind of swept under the carpet a bit, but for large- scale quantum computing, we are going to need millions of qubits," said dzurak.
"here, we show a way that spin qubits can be scaled up massively.
and that's the key."
the design is a leap forward in silicon spin qubits; it was only two years ago, in a paper in nature, that dzurak and veldhorst showed, for the first time, how quantum logic calculations could be done in a real silicon device, with the creation of a two- qubit logic gate - the central building block of a quantum computer.
"those were the first baby steps, the first demonstrations of how to turn this radical quantum computing concept into a practical device using components that underpin all modern computing," said mark hoffman, unsw's dean of engineering.
"our team now has a blueprint for scaling that up dramatically.
"we've been testing elements of this design in the lab, with very positive results.
we just need to keep building on that - which is still a hell of a challenge, but the groundwork is there, and it's very encouraging.
it will still take great engineering to bring quantum computing to commercial reality, but clearly the work we see from this extraordinary team at cqc2t puts australia in the driver's seat," he added.
other cqc2t researchers involved in the design published in the nature communications paper were henry yang and gertjan eenink, the latter of whom has since joined veldhorst at qutech.
the unsw team has struck a a$83 million deal between unsw, telstra, commonwealth bank and the australian and new south wales governments to develop, by 2022, a 10- qubit prototype silicon quantum integrated circuit - the first step in building the world's first quantum computer in silicon.
in august, the partners launched silicon quantum computing pty ltd, australia's first quantum computing company, to advance the development and commercialisation of the team's unique technologies.
the nsw government pledged a$8.7 million, unsw a$25 million, the commonwealth bank a$14 million, telstra a$10 million and the australian government a$25 million.
researchers from griffith university and the university of queensland have overcome one of the key challenges to quantum computing by simplifying a complex quantum logic operation.
they demonstrated this by experimentally realising a challenging circuit- the quantum fredkin gate- for the first time.
"the allure of quantum computers is the unparalleled processing power that they provide compared to current technology," said dr raj patel from griffith's centre for quantum dynamics.
"much like our everyday computer, the brains of a quantum computer consist of chains of logic gates, although quantum logic gates harness quantum phenomena."
the main stumbling block to actually creating a quantum computer has been in minimising the number of resources needed to efficiently implement processing circuits.
"similar to building a huge wall out lots of small bricks, large quantum circuits require very many logic gates to function.
however, if larger bricks are used the same wall could be built with far fewer bricks," said dr patel.
"we demonstrate in our experiment how one can build larger quantum circuits in a more direct way without using small logic gates."
at present, even small and medium scale quantum computer circuits cannot be produced because of the requirement to integrate so many of these gates into the circuits.
one example is the fredkin (controlled- swap) gate.
this is a gate where two qubits are swapped depending on the value of the third.
usually the fredkin gate requires implementing a circuit of five logic operations.
the research team used the quantum entanglement of photons- particles of light- to implement the controlled- swap operation directly.
"there are quantum computing algorithms, such as shor's algorithm for finding prime numbers, that require the controlled- swap operation.
the quantum fredkin gate can also be used to perform a direct comparison of two sets of qubits (quantum bits) to determine whether they are the same or not.
this is not only useful in computing but is an essential feature of some secure quantum communication protocols where the goal is to verify that two strings, or digital signatures, are the same," said professor tim ralph from the university of queensland.
professor geoff pryde, from griffith's centre for quantum dynamics, is the project's chief investigator.
"what is exciting about our scheme is that it is not limited to just controlling whether qubits are swapped, but can be applied to a variety of different operations opening up ways to control larger circuits efficiently," said professor pryde.
"this could unleash applications that have so far been out of reach."
the research has been published as 'a quantum fredkin gate' in science advances.
if quantum computing is the future, then info- tech staffers will need to learn about new kinds of components.
most pictures of quantum computers show what's become known as a steampunk chandelier, with several tiers of golden vertical rods, metal cables running everywhere, but nothing at all resembling a computer.
what is all that stuff, and how does it compare to what's in a traditional server?
see: it leader's guide to the future of quantum computing (tech pro research)
the actual processor, which researchers call the payload, is in a microchip- like package about 1 inch by three- quarters of an inch.
that's at the bottom of the assembly in a part called the cryostat- its name was selected because it needs to be cold and stable.
as for the shiny rube goldberg contraption above it: "all of that is in support of the chip itself," explained ibm's bob sutor, the vice president in charge of quantum computing strategy.
most of the rods are gold- plated brass that moves cooling fluids, because gold reflects away heat in the form of infrared radiation, he explained.
and most of the cables contain microwave signals, which send low- energy pulses that command or read data from each qubit, depending on the shape of the wave.
finally, the whole assembly is nearly vacuum sealed.
see: quantum computing: an insider's guide (techrepublic download)
sutor emphasized the importance of keeping the payload cold.
balmy room temperature of 72 degree fahrenheit is 295.372 degrees kelvin.
outer space is about 2.7 degrees kelvin, and yet quantum chips need to be kept at just .01 degrees above absolute zero.
(there goes the notion of warming up your data center.)
the programming is performed on a normal computer, which has an interface to those microwave signal generators and readers.
coding there is not unlike assembly language, where you're directly interacting with a processor and playing traffic cop for which information goes where.
but forget the notion of quantum memory or storage.
"today there is no notion of on- the- side quantum storage.
when will we have quantum ram?
this is going to be related to fault tolerance, qubits that will hold their value long enough," which is probably a decade from now, sutor said.
so these systems are strictly volatile for now.
qubits are hard enough for today's scientists to keep stable when the computers are turned on, let alone when they're off.
right now, ibm's biggest system has 50 qubits.
in the future, "let's say you get to 300.
the number of pieces of data a quantum computer can access is more than the number of atoms in the observable universe," sutor said.
big blue highlighted its latest design at the consumer electronics show this month, which seems like an odd choice considering there is nothing consumer- oriented about quantum computing.
they put the computer inside a dark chassis, just like an ibm mainframe, with an 81- square- feet footprint.
the official announcement cites it as the "world's first integrated quantum computing system for commercial use," but that implies much that doesn't exist- "for commercial use" is the company's goal, but barely at all what is possible today.
see: ces 2019: did ibm just reveal first commercial quantum computer?
(zdnet)
currently, you can write simple programs and interact with a system that lives in an ibm laboratory.
the company is secretive about how many such computers it's built: there are a few in one lab, a few in another, and so on.
it's likely the total number is less than a dozen or two.
each one is a custom improvement over the previous one, and ibm fabricates each chip itself.
"we are not at the point of having an assembly line," sutor said.
at this rate, is our first sentence true- is quantum computing the future?
"these are not backroom science experiments," sutor asserted, with a caveat: "the world is a hybrid.
it will continue to be a hybrid for decades if not centuries."
computer security is among the first real applications.
qubits are better at making randomness than any conventional computer on earth, and random numbers are essential for encryption.
in theory, by the time a hacker determined the key to your lock, the lock would have changed.
"unhackable" is a dangerous term, but quantum random number generators could be much closer than anything current, and these are already real products you can buy.
also see
quantum computing has the potential to revolutionize technology, medicine, and science by providing faster and more efficient processors, sensors, and communication devices.
but transferring information and correcting errors within a quantum system remains a challenge to making effective quantum computers.
in a paper in the journal nature, researchers from purdue university and the university of rochester, including john nichol, an assistant professor of physics, and rochester phd students yadav p. kandel and haifeng qiao, demonstrate their method of relaying information by transferring the state of electrons.
the research brings scientists one step closer to creating fully functional quantum computers and is the latest example of rochester's initiative to better understand quantum behavior and develop novel quantum systems.
the university recently received a $4 million grant from the department of energy to explore quantum materials.
quantum computers
a quantum computer operates on the principles of quantum mechanics, a unique set of rules that govern at the extremely small scale of atoms and subatomic particles.
when dealing with particles at these scales, many of the rules that govern classical physics no longer apply and quantum effects emerge; a quantum computer is able to perform complex calculations, factor extremely large numbers, and simulate the behaviors of atoms and particles at levels that classical computers cannot.
quantum computers have the potential to provide more insight into principles of physics and chemistry by simulating the behavior of matter at unusual conditions at the molecular level.
these simulations could be useful in developing new energy sources and studying the conditions of planets and galaxies or comparing compounds that could lead to new drug therapies.
"you and i are quantum systems.
the particles in our body obey quantum physics.
but, if you try to compute what happens with all of the atoms in our body, you cannot do it on a regular computer," nichol says.
"a quantum computer could easily do this."
quantum computers could also open doors for faster database searches and cryptography.
"it turns out that almost all of modern cryptography is based on the extreme difficulty for regular computers to factor large numbers," nichol says.
"quantum computers can easily factor large numbers and break encryption schemes, so you can imagine why lots of governments are interested in this."
bits vs. qubits
a regular computer consists of billions of transistors, called bits.
quantum computers, on the other hand, are based on quantum bits, also known as qubits, which can be made from a single electron.
unlike ordinary transistors, which can be either "0" or "1," qubits can be both "0" and "1" at the same time.
the ability for individual qubits to occupy these "superposition states," where they are simultaneously in multiple states, underlies the great potential of quantum computers.
just like ordinary computers, however, quantum computers need a way to transfer information between qubits, and this presents a major experimental challenge.
"a quantum computer needs to have many qubits, and they're really difficult to make and operate," nichol says.
"the state- of- the art right now is doing something with only a few qubits, so we're still a long ways away from realizing the full potential of quantum computers."
all computers, including both regular and quantum computers and devices like smart phones, also have to perform error correction.
a regular computer contains copies of bits so if one of the bits goes bad, "the rest are just going to take a majority vote" and fix the error.
however, quantum bits cannot be copied, nichol says, "so you have to be very clever about how you correct for errors.
what we're doing here is one step in that direction."
manipulating electrons
quantum error correction requires that individual qubits interact with many other qubits.
this can be difficult because an individual electron is like a bar magnet with a north pole and a south pole that can point either up or down.
the direction of the pole -  whether the north pole is pointing up or down, for instance -  is known as the electron's magnetic moment or quantum state.
if certain kinds of particles have the same magnetic moment, they cannot be in the same place at the same time.
that is, two electrons in the same quantum state cannot sit on top of each other.
"this is one of the main reasons something like a penny, which is made out of metal, doesn't collapse on itself," nichol says.
"the electrons are pushing themselves apart because they cannot be in the same place at the same time."
if two electrons are in opposite states, they can sit on top of each other.
a surprising consequence of this is that if the electrons are close enough, their states will swap back and forth in time.
"if you have one electron that's up and another electron that's down and you push them together for just the right amount of time, they will swap," nichol says.
"they did not switch places, but their states switched."
to force this phenomenon, nichol and his colleagues cooled down a semiconductor chip to extremely low temperatures.
using quantum dots -  nanoscale semiconductors -  they trapped four electrons in a row, then moved the electrons so they came in contact and their states switched.
"there's an easy way to switch the state between two neighboring electrons, but doing it over long distances -  in our case, it's four electrons -  requires a lot of control and technical skill," nichol says.
"our research shows this is now a viable approach to send information over long distances."
a first step
transmitting the state of an electron back and forth across an array of qubits, without moving the position of electrons, provides a striking example of the possibilities allowed by quantum physics for information science.
"this experiment demonstrates that information in quantum states can be transferred without actually transferring the individual electron spins down the chain," says michael manfra, a professor of physics and astronomy at purdue university.
"it is an important step for showing how information can be transmitted quantum- mechanically -  in manners quite different than our classical intuition would lead us to believe."
nichol likens this to the steps that led from the first computing devices to today's computers.
that said, will we all someday have quantum computers to replace our desktop computers?
"if you had asked that question of ibm in the 1960s, they probably would've said no, there's no way that's going to happen," nichol says.
"that's my reaction now.
but, who knows?"
the development of technologies that can process information based on the laws of quantum physics is predicted to have profound impacts on modern society.
for example, quantum computers may hold the key to solving problems that are too complex for today's most powerful supercomputers, and a quantum internet could ultimately protect the information of the world from malicious attacks.
a) chip schematic.
black lines represent single mode waveguides for the single photons, red and blue pulses represent the photon energy in each path.
yellow bars represent external phase control.
b) teleportation setup.
c) entanglement swapping setup.
d) 4- photon ghz state preparation.
image credit: university of bristol
however, these technologies all rely on 'quantum information', which is typically encoded in single quantum particles that are extremely difficult to control and measure.
scientists from the university of bristol, in collaboration with the technical university of denmark (dtu), have successfully developed chip- scale devices that are able to harness the applications of quantum physics by generating and manipulating single particles of light within programmable nano- scale circuits.
these chips are able to encode quantum information in light generated inside the circuits and can process the 'quantum information' with high efficiency and extremely low noise.
this demonstration could enable a significant boost in the ability to produce more complex quantum circuits that are required in quantum computing and communications.
their work, published in the journal nature physics, hosts a range of quantum demonstrations.
in one of the breakthrough experiments, researchers at the university of bristol's quantum engineering technology labs (qet labs) demonstrate the quantum teleportation of information between two programmable chip for the first time, which they remark is a cornerstone of quantum communications and quantum computing.
quantum teleportation offers quantum state transfer of a quantum particle from one place to another by utilising entanglement.
teleportation is not only useful for quantum communication but is a fundamental building- block of optical quantum computing.
establishing an entangled communication link between two chips in the lab however has proven to be highly challenging.
bristol co- author dan llewellyn said: "we were able to demonstrate a high- quality entanglement link across two chips in the lab, where photons on either chip share a single quantum state.
"each chip was then fully programmed to perform a range of demonstrations which utilise the entanglement.
"the flagship demonstration was a two- chip teleportation experiment, whereby the individual quantum state of a particle is transmitted across the two chips after a quantum measurement is performed.
this measurement utilises the strange behaviour of quantum physics, which simultaneously collapses the entanglement link and transfers the particle state to another particle already on the receiver chip."
another co- author, dr. imad faruque, also from bristol, added: "based on our previous result of on- chip high quality single- photon sources, we have built an even more complex circuit containing four sources.
"all of these sources are tested and found to be nearly identical emitting nearly identical photons, which is an essential criterion for the set of experiments we had performed, such as entanglement swapping."
the results showed an extremely high- fidelity quantum teleportation of 91 percent.
in addition, the researchers were able to demonstrate some other important functions of their designs, such as entanglement swapping (required for quantum repeaters and quantum networks) and four- photon ghz states (required in quantum computing and the quantum internet).
according to co- author dr. yunhong ding, from dtu, low loss, high stability, and excellent controllability are extremely important for integrated quantum photonics.
he said: "this experiment was made possible because of the state of the art low- loss silicon photonics technology based on high- quality fabrication at the dtu."
lead author, dr. jianwei wang, now at peking university, said: "in the future, a single si- chip integration of quantum photonic devices and classical electronic controls will open the door for fully chip- based cmos- compatible quantum communication and information processing networks."
featured news from related categories:
researchers at the institute of quantum optics and quantum information, the university of vienna, and the universitat autonoma de barcelona have achieved a new milestone in quantum physics: they were able to entangle three particles of light in a high- dimensional quantum property related to the 'twist' of their wavefront structure.
the results from their experiment appear in the journal nature photonics.
entanglement is a counterintuitive property of quantum physics that has long puzzled scientists and philosophers alike.
entangled quanta of light seem to exert an influence on each other, irrespective of how much distance is between them.
consider for example a metaphorical quantum ice dancer, who has the uncanny ability to pirouette both clockwise and counter- clockwise simultaneously.
a pair of entangled ice- dancers whirling away from each other would then have perfectly correlated directions of rotation: if the first dancer twirls clockwise then so does her partner, even if skating in ice rinks on two different continents.
"the entangled photons in our experiment can be illustrated by not two, but three such ice dancers, dancing a perfectly synchronized quantum mechanical ballet," explains mehul malik, the first author of the paper.
"their dance is also a bit more complex, with two of the dancers performing yet another correlated movement in addition to pirouetting.
this type of asymmetric quantum entanglement has been predicted before on paper, but we are the first to actually create it in the lab."
from fundamentals to applications: layered quantum cryptography
the scientists created their three- photon entangled state by using yet another quantum mechanical trick: they combined two pairs of high- dimensionally entangled photons in such a manner that it became impossible to ascertain where a particular photon came from.
besides serving as a test bed for studying many fundamental concepts in quantum mechanics, multi- photon entangled states such as these have applications ranging from quantum computing to quantum encryption.
along these lines, the authors of this study have developed a new type of quantum cryptographic protocol using their state that allows different layers of information to be shared asymmetrically among multiple parties with unconditional security.
"the experiment opens the door for a future quantum internet with more than two partners and it allows them to communicate more than one bit per photon," says anton zeilinger.
many technical challenges remain before such a quantum communication protocol becomes a practical reality.
however, given the rapid progress in quantum technologies today, it is only a matter of time before this type of entanglement finds a place in the quantum networks of the future.
demonstrating that limits were made to be broken, physicists have overcome what was previously considered to be a natural and universal limit on the efficiency of a quantum cryptography task called blind quantum computing.
the new method offers significant efficiency improvements and, in some cases, requires exponentially fewer communication resources to implement than previous methods did.
the physicists, carlos a. perez- delgado and joseph f. fitzsimons at the singapore university of technology and design, have published a paper on the improved blind quantum computing method in a recent issue of physical review letters.
fitzsimons is also with the centre for quantum technologies at the national university of singapore.
as its name suggests, in blind quantum computing, a computer performs a task blindly- the input, computation, and output remain unknown to the computer.
the scientists explain that this capability "allows a user to delegate a computation to an untrusted server while keeping the computation hidden."
as the technology develops, it is expected to provide greater security than classical protocols for a variety of applications.
like all computing tasks, blind quantum computing requires a minimum number of qubits, gates, and other communication resources in order to perform a computation.
recent research has suggested that there is a natural lower limit on these communication requirements, which is based on the so- called "no- programming theorem."
because this lower bound suggests that blind quantum computing protocols will always require a certain minimum amount of resources, it effectively limits the efficiency with which these protocols can be carried out.
teleportation for error correction
in the new paper, the physicists have shown that this limit holds only in certain scenarios, and it can be overcome by using a technique called "iterated gate teleportation."
the technique is based on standard gate teleportation, in which quantum states are rapidly transmitted from one gate to another by taking advantage of quantum entanglement between the gates.
in the iterated version, additional gate teleportation steps are repeatedly carried out to correct errors based on the results of the preceding teleportation steps.
"the really important part of gate teleportation is that it provides a way to perform the desired computation on one half of an entangled state before the desired input is even known, resulting in a special resource state," fitzsimons told phys.org.
"once you have the input, you can perform a special set of measurements between the input and the resource state.
for one possible outcome of the measurements, the effect is to perform the encoded computation on the chosen input.
however, it is impossible to control which measurement outcome you get, and any other outcome results in some unwanted error which needs to be corrected.
our iterated teleportation protocol is simply using teleportations to correct errors introduced by previous teleportation steps in such a way that the errors diminish each round and eventually disappear."
the physicists showed that just a few of these additional teleportation steps can correct errors that would otherwise need to be corrected using many more resources.
in this way, the technique exponentially reduces the amount of communication requirements, even below the minimum required by the no- programming theorem limit.
"to me, at least, this is quite a surprising result, as it had been previously proved that for deterministic computation, programs encoded using quantum states are no shorter than those encoded using classical bits," fitzsimons said.
"it is natural to think that, in order to delegate a computation, it is necessary to describe the program to be implemented, and hence the required communication would scale with the size of the program.
"it turns out, however, that this is not the case.
our protocol gets around this limit by encoding a large number of gates into each quantum state, and then adaptively correcting for any errors introduced by teleportation.
if you think about this in terms of a program, the program itself would have to contain the quantum states to correct for every possible measurement result, resulting in exponential overhead.
this explains why our result is not in tension with the no- programming theorem: our protocol essentially reads only a small portion of the full program, though which portion that is cannot be predetermined."
the future of blind quantum computing
the physicists expect that the iterated gate teleportation approach can be applied to computing tasks other than blind quantum computing, offering potential efficiency improvements in these areas, as well.
"it is also possible to extend blind quantum computation protocols to include tests which ensure that the desired computation has been performed correctly," fitzsimons said.
"this isn't something that is very practical at the moment, since in order for it to be really useful, we first need to have relatively large quantum computers.
to date, blind and verifiable quantum computation has only been experimentally demonstrated in four- qubit systems.
however, as the technology progresses we expect that the importance of securely running programs on remote quantum servers will become increasingly important, just as cloud computing has emerged in the classical world.
the advantage of blind quantum computing and verification protocols is that they offer a type of security which simply is not possible using purely classical protocols."
in the future, the researchers plan to further develop blind quantum computing protocols for new cryptographic applications.
"the term quantum cryptography has become synonymous with quantum key distribution," fitzsimons said.
"although my group has fairly diverse research interests, a unifying theme is that we are interested in finding new quantum protocols for cryptographic tasks beyond key distribution.
delegated computation is proving to be a great source of new cryptographic problems, and so a lot of our efforts are focused there.
perhaps the most important question for us at the moment is whether there exist blind quantum computing protocols which do not require any quantum communication or entanglement between parties.
i am very grateful to the national research foundation which generously supports our research in this area."
finding a way to build a quantum computer that works more efficiently than a classical computer has been the holy grail of quantum information processing for more than a decade.
"there is quite a strong competition at the moment to realize these protocols," mark tame tells physorg.com.
the latest experiment performed as a collaboration by a queen's university theoretical group and an experimental group in vienna has "allowed us to pick up the pace" of quantum computing.
the joint project's experiment is reported in physical review letters in an article titled, "experimental realization of deutsch's algorithm in a one- way quantum computer."
"this is the first implementation of deutsch's algorithm for cluster states in quantum computing," tame explains.
tame along with members of the queen's group in belfast, including mauro paternostro and myungshik kim joined a group from the university of vienna, including robert prevedel, pascal bohi, and anton zeilinger (who is also associated with the institute for quantum optics and quantum information at the austrian academy of sciences) to perform this experiment.
"when performing a quantum algorithm," says tame, "the standard approach is based on logical gates that are applied in a network similar to classical computing."
tame points out that this method of quantum computing is not practical or efficient.
"our quantum computer model uses cluster states, which are highly entangled multi- partite quantum states."
the irish and austrian group's quantum computer makes use of four entangled photons in a cluster state.
tame explains how it works:
"our setup is completely based on light, where quantum information is encoded on each photon.
the information is in the polarization of each photon, horizontal or vertical, and superpositions in between.
an ultra- violet laser pumps a crystal and produces an entangled pair of photons in one direction.
the laser beam then hits a mirror and bounces back to form another pair of entangled photons on its second passage through the crystal.
these four photons are then made to interact at beamsplitters to form the entangled cluster state resource on which we perform the quantum computation."
next, tame says, come the calculations.
"we perform deutsch's algorithm as a sequence of the measurements.
when you measure in a specific basis, you can manipulate the quantum information in the photons using their shared entanglement."
he continues with an illustration related to classical computing: "you can think of the cluster state as the 'hardware', and the measurements as the 'software'."
now that the groups in belfast and vienna have proved that deutsch's algorithm works for a cluster- based quantum computer, the next step is to apply it to larger systems.
"right now it's really just a proof of principle," explains tame.
"we've shown it can be done, but we need to build larger cluster states and perform more useful computations."
tame admits that this next step is where it gets trickier.
"quantum systems like this can be influenced by small fluctuations in the environment.
it can be difficult to get accurate computations using larger resources."
he says that noise resistant protocols need to be developed in order to maintain the coherence of the quantum information.
"there's not a lot of noise in the lab during the implementation of experiments on small numbers of qubits.
but as we increase this number there are physical and technological concerns that need to be solved.
this is a key issue."
and does tame have any idea how to solve some of these issues?
"we have some schemes at the moment.
it's a work in progress."
he pauses.
"but for now it's exciting to have this proof that quantum computing can be efficiently performed with deutsch's algorithm."
quantum physics is both mysterious and difficult to grasp.
barry sanders, director of the university of calgary's institute for quantum information science, is hoping to change that.
sanders, who is also the icore chair of quantum information science, has produced a four- minute animated movie with a team of animators and scientists.
the film is intended for funding agencies, the public, and interdisciplinary teams building quantum computers, so they can see how a quantum computer would work and its underlying science.
for the first time, a detailed description on the making of sanders' animation- solid state quantum computer in silicon- was published this month in the new journal of physics.
this issue is devoted to the leading uses of visualization in astrophysics, biophysics, geophysics, medical physics and quantum physics and sanders is one the guest editors for this issue.
"the goal of our animated movie about the quantum computer is to convey to a non- expert audience the nature of quantum computation: its power, how it would work, what it would look like," says sanders, who also has an article published in the december issue of physics world on the making of his four- minute animation.
"the animation incorporates state- of- the- art techniques to show the science and the technology in the most accurate and exciting way possible while being true to the underlying principles of quantum computing," says sanders.
the animated movie was completed last year but the clips have not been publicly distributed before now.
quantum computers harness the power of atoms and molecules and have the potential to calculate significantly faster than any existing computer could.
some hard computational problems that can't be solved ever by foreseeable computers become easily solved on quantum computers, which could make today's secure communication obsolete.
basic quantum computers that can perform certain calculations exist; but a practical quantum computer is still years away.
"there is a history of simple visualization over the last century to convey quantum concepts," says sanders.
he notes that erwin schrodinger introduced his eponymous cat, which is left in a tragic state of being in a superposition of life and death, an illustration of the strangeness of quantum theory.
and the uncertainty principle associated with werner heisenberg and his fictional gamma ray microscope, has found its way into common english parlance.
"the imagery of the early days of quantum mechanics played a crucial role in understanding and accepting quantum theory.
our work takes this imagery a quantum leap forward by using the state- of- the- art animation techniques to explain clearly and quickly the nature of quantum computing which is, by its very nature, counterintuitive."
a team from the department of energy's oak ridge national laboratory has conducted a series of experiments to gain a better understanding of quantum mechanics and pursue advances in quantum networking and quantum computing, which could lead to practical applications in cybersecurity and other areas.
ornl quantum researchers joseph lukens, pavel lougovski, brian williams, and nicholas peters -  along with collaborators from purdue university and the technological university of pereira in colombia -  summarized results from several of their recent academic papers in a special issue of the optical society's optics & photonics news, which showcased some of the most significant results from optics- related research in 2019.
their entry was one of 30 selected for publication from a pool of 91.
conventional computer "bits" have a value of either 0 or 1, but quantum bits, called "qubits," can exist in a superposition of quantum states labeled 0 and 1.
this ability makes quantum systems promising for transmitting, processing, storing, and encrypting vast amounts of information at unprecedented speeds.
to study photons -  single particles of light that can act as qubits -  the researchers employed light sources called quantum optical frequency combs that contain many precisely defined wavelengths.
because they travel at the speed of light and do not interact with their environment, photons are a natural platform for carrying quantum information over long distances.
interactions between photons are notoriously difficult to induce and control, but these capabilities are necessary for effective quantum computers and quantum gates, which are quantum circuits that operate on qubits.
nonexistent or unpredictable photonic interactions make two- photon quantum gates much more difficult to develop than standard one- photon gates, but the researchers reached several major milestones in recent studies that addressed these challenges.
for example, they made adjustments to existing telecommunications equipment used in optics research to optimize them for quantum photonics.
their results revealed new ways to use these resources for both traditional and quantum communication.
"using this equipment to manipulate quantum states is the technological underpinning of all these experiments, but we did not expect to be able to move in the other direction and improve classical communication by working on quantum communication," lukens said.
"these interesting and unanticipated findings have appeared as we delve deeper into this research area."
one such tool, a frequency beam splitter, divides a single beam of light into two frequencies, or colors, of light.
"imagine you have a beam of light going down an optical fiber that has a particular frequency, say, red," lukens said.
"then, after going through the frequency beam splitter, the photon will leave as two frequencies, so it will be both red and blue."
the members of this team were the first researchers to successfully design a quantum frequency beam splitter with standard lightwave communications technology.
this device takes in red and blue photons simultaneously, then produces energy in either the red or the blue frequency.
by using this method to deliberately change the frequencies of photons, the team tricked the stubborn particles into beneficial interactions based on quantum interference, the phenomenon of photons interfering with their own trajectories.
"it turned out that off- the- shelf devices can deliver impressive control at the single- photon level, which people didn't know was possible," lougovski said.
additionally, the researchers completed the first demonstration of a frequency tritter, which splits a beam of light into three different frequencies instead of two.
their results indicated that multiple quantum information processing operations can run at the same time without introducing errors or damaging the data.
another key accomplishment was the team's design and demonstration of a coincidence- basis controlled- not gate, which enables one photon to control a frequency shift in another photon.
this device completed a universal quantum gate set, meaning any quantum algorithm can be expressed as a sequence within those gates.
"quantum computing applications require much more impressive control levels than any sort of classical computing," lougovski said.
the team also encoded quantum information in multiple independent values known as degrees of freedom within a single photon, which allowed them to observe quantum entanglement- like effects without needing two separate particles.
entanglement usually involves two linked particles in which changes made to the state of one particle also apply to the other.
finally, the researchers have completed quantum simulations of real- world physics problems.
in collaboration with scientists at the air force research laboratory, they are now developing tiny, specialized silicon chips similar to those common in microelectronics in pursuit of even better photonic performance.
"in theory, we can get all these operations onto a single photonic chip, and we see a lot of potential for doing similar quantum experiments on this new platform," lukens said.
"that's the next step to really move this technology forward."
future quantum computers will allow scientists to simulate incredibly complex scientific problems that would be impossible to study on current systems, even supercomputers.
in the meantime, the team's findings could help researchers embed photonic systems into current high- performance computing resources.
"we have a very diverse and talented team," lougovski said.
"the most important thing is we're getting results."
this research was funded by ornl's laboratory directed research and development program.
researchers at purdue university and tohoku university have built quasi- quantum "probabilistic computer" using a modified type of magnetoresistive random- access memory (mram) to approximate the behavior of a qubit, the building block of a quantum computer.
in classical computing, a bit can either hold a value of 0 or 1, while a qubit can hold values of 0 and 1 simultaneously.
the purdue/tohoku probabilistic computer uses a p- bit, which "rapidly fluctuate" between 0 or 1.
in a whitepaper published in nature on wednesday detailing their proof- of- concept, researchers were able to factor 945 and 35,161 into primes using an 8 p- bit machine.
quantum computers that can factor long integers in trivial amounts of time could be used to break widely- used rsa encryption schemes, prompting research into post- quantum cryptography.
while quantum computers have been capable of this scale of integer factorization for years, the mram- derived p- bit design can be operated at room temperature, while quantum computers rely on aggressive helium cooling to operate.
different types of qubits- superconducting, topological, trapped ion, etc.- are being actively evaluated by researchers globally, though the relative novelty of these designs, combined with their inherent physical properties, presents difficulties in scaling up for many- qubit machines, an encumbrance not applicable to p- bits.
combined with the modest cooling requirements, the prospect of scalability for p- bit systems in the near- term appears quite possible- the p- bit design was demonstrated to provide ten times higher energy efficiency, with a 300x area advantage compared to conventional computers.
innothe capabilities of p- bits are not identical to true qubits, however, leading purdue's supriyo datta to call p- bits "a poor man's qubit."
according to the whitepaper, "for a subclass of quantum systems, quantum annealing can be approximated with replicated p- bit networks."
d- wave is the largest commercial purveyor of quantum annealer systems, which are oriented toward solving quadratic unconstrained binary optimization (qubo) problems.
current commercial use of quantum annealers includes path optimization and training for machine learning, as these tasks can easily be expressed as qubo problems.
for more on quantum computing, check out "aliro aims to make quantum computers usable by traditional programmers" and "first 53- qubit ibm q system to roll out at ny quantum computation center" on techrepublic.
also see
programming for quantum computers is similar in one respect to programming in the late 1980s and early 1990s.
an ecosystem of architecture options- including 68k, powerpc, ia32, arm, alpha, mips, superh, sparc, and likely others- all had different strengths and weaknesses.
some applications ran quite well on specific platforms, while the process of porting was typically laborious and not quite as performant as the original.
aliro technologies, a software company spinning out of harvard's quantum computing lab, is attempting to bridge that gap for hybrid classical- quantum programs by providing hardware- agnostic solutions for programmers and researchers.
this essentially adds an abstraction layer, preventing the need to learn the intricacies of vendor- specific implementations, as aliro's platform uses validation tools to determine the best available quantum system for the task.
noisy intermediate- scale quantum (nisq) systems, the currently available type of quantum computers, hold a great deal of potential but additional work is needed to increase the number of qubits in nisq systems, thereby extending the capabilities of quantum computers.
the design of qubits, and how they are connected, is similar in many ways to the architectural diversity in previous decades.
"i think there'll be a lot of competition in the next 10 years or longer with superconducting [qubits] that's popular now, but trapped ion is coming along, there are cold atom plays, there's atomic.
i saw your work about topological qubits," jim ricotta, ceo of aliro, told techrepublic.
"there's going to be all these different hardware technologies... and what aliro wants to do is build a cross platform stack, that can make it easy to get your algorithm running on quantum, at least probably in the quantum- classical accelerator mode, that's where things are starting.
and let us figure out what the hardware is, and which qubits to use, and how to allocate them."
presently, aliro is working with ibm q and rigetti quantum computers, as well as with designs from two other firms that are not yet commercially available.
these heterogeneous designs are benchmarked on aliro's platform, with an eye toward interactivity and workload management.
for quantum- classical acceleration, "developers have a choice of how much of their workload they want to shift to the quantum computer, versus keep it in the classical realm," ricotta said, noting the balance of how much time developers want to commit to using quantum systems.
"currently, circuits are a very common way of expressing quantum programs.
i don't think that'll be the case all the time, but right now circuits are the way, and there are different ways to map circuits onto machines, and onto qubits.
our software lets you try some variations, and seek optimal solutions to those kinds of problems.
you do the benchmarking to verify whether [your program is] optimal or not," ricotta continued.
quantifying value of a quantum computer
stages of quantum computing are generally divided into quantum supremacy- the threshold at which quantum computers are theorized to be capable of solving problems, which traditional computers would not (practically) be able to solve- is likely decades away.
while quantum volume, a metric that "enables the comparison of hardware with widely different performance characteristics and quantifies the complexity of algorithms that can be run," according to ibm, has gained acceptance from nist and analyst firm gartner as a useful metric.
aliro proposes the idea of "quantum value," as the point at which organizations using high performance computing today can achieve results from using quantum computers to accelerate their workload.
"we're dealing with enterprises that want to get business value from these machines.... we think there's going to be a lot of different metrics.
what metrics enterprises are interested in helps them decide whether to invest or not," ricotta said.
how aliro's stack compares to full virtualization or software containers
given the relatively meager capabilities of nisq systems, the prospect of adding an abstraction layer on top of an already low- performing system will likely make some recoil at the prospect of performance regression through the use of middleware.
on a scale of one to 10, in which 10 is full hardware virtualization, five is software containers like docker, and one is tool- assisted porting, ricotta puts aliero's platform between three and four.
"we're not ready for many levels of abstraction above the quantum hardware, but we're ready for a little bit.
when you get down to the equivalent of the machine language, these things are very, very different, and it's not just what kind of qubits they are.
it's noise characteristics, it's connectivity," ricotta said.
"riggeti and ibm q machines both use superconducting josephson junctions around the same number- approximately, the same order of magnitude of qubits- but they are connected in different ways.
therefore, you may take a program, and you might allocate logical physical qubits in a different way for a rigetti versus an ibm.
[aliro's platform] can make your program work better and give you easier access to this to this powerful new tool."
aliro is funded in part by samsung next's q fund.
samsung's interest in quantum computing is decidedly long- term.
"on the demand side, there is a need for increasing parallelism on massive problems, like machine learning, or material discovery, or drug discovery.
the search space is too massive for any classical [computer].
on the supply side, we have our own things that we do, on the [semiconductor fabrication plant] side of things," ajay singh, senior director of samsung next, told techrepublic.
"in the last 15 years, the cost of a new fab is increased 11 times, to now roughly $10 billion.
that tells you the performance per dollar on the supply side- if you were to start crunching on more chips, it is actually becoming less and less efficient for you, from an economic standpoint."
for more on quantum computing, check out "ibm releases quantum computing textbook and video tutorials" and "how helium shortages will impact quantum computer research" on techrepublic.
also see
cloudquantum computing
ibm has announced it has built and tested its two most powerful quantum processors yet, representing a major step towards its mission to commercialise the technology.
translating the theory of quantum computing into practical, commercially applicable reality has been a goal of researchers for decades, and ibm's unveiling of new 16 and 17- qubit processors marks a key milestone on that journey.
its smaller processor will be made available to researchers and programmers at no cost via the ibm cloud platform.
the larger processor will be a prototype commercial device, forming the core of the company's first imb q early- access commercial systems.
the company hopes to achieve 50- qubit processing over the next few years, and in so doing open up vast computational power to science and business.
quantum computers exploit the peculiar properties of quantum mechanics to hugely increase the amount of information a machine can handle.
while conventional computers store information as bits - 1s and 0s - quantum computers use quantum bits, or qubits, which store information as 1s, 0s, or both at the same time.
this, along with the other quantum phenomena of entanglement and tunnelling, enables quantum computers to manipulate colossal combinations of states at once.
potential applications include untangling the unimaginable complexity of molecular and chemical interactions - a feat impossible even with all the world's current computing power combined - in order to discover new materials and medicines.
it could also be turned towards business, supercharging ai capabilities or solving complex optimisation problems in logistics and supply chains, financial modelling or risk analysis.
"the significant engineering improvements announced today will allow ibm to scale future processors to include 50 or more qubits, and demonstrate computational capabilities beyond today's classical computing systems," said arvind krishna, senior vice president and director of ibm research and hybrid cloud.
"these powerful upgrades to our quantum systems, delivered via the ibm cloud, allow us to imagine new applications and new frontiers for discovery that are virtually unattainable using classical computers alone."
video: ibm's jerry chow explains the potential of quantum computing
scientists at the university of sydney have demonstrated the ability to "see" the future of quantum systems, and used that knowledge to preempt their demise, in a major achievement that could help bring the strange and powerful world of quantum technology closer to reality.
the applications of quantum- enabled technologies are compelling and already demonstrating significant impacts - especially in the realm of sensing and metrology.
and the potential to build exceptionally powerful quantum computers using quantum bits, or qubits, is driving investment from the world's largest companies.
however a significant obstacle to building reliable quantum technologies has been the randomisation of quantum systems by their environments, or decoherence, which effectively destroys the useful quantum character.
the physicists have taken a technical quantum leap in addressing this, using techniques from big data to predict how quantum systems will change and then preventing the system's breakdown from occurring.
the research is published today in nature communications.
"much the way the individual components in mobile phones will eventually fail, so too do quantum systems," writes the paper's senior author, professor michael j. biercuk of the university's school of physics and chief investigator at the australian research council's centre for engineered quantum systems.
"but in quantum technology the lifetime is generally measured in fractions of a second, rather than years."
biercuk says his group had demonstrated it was possible to suppress decoherence in a preventive manner.
the key was to develop a technique to predict how the system would disintegrate.
"humans routinely employ predictive techniques in our daily experience; for instance, when we play tennis we predict where the ball will end up based on observations of the airborne ball," he says.
"this works because the rules that govern how the ball will move, like gravity, are regular and known.
but what if the rules changed randomly while the ball was on its way to you?
in that case it's next to impossible to predict the future behavior of that ball.
"and yet this situation is exactly what we had to deal with because the disintegration of quantum systems is random.
moreover, in the quantum realm observation erases quantumness, so our team needed to be able to guess how and when the system would randomly break.
we effectively needed to swing at the randomly moving tennis ball while blindfolded."
the team turned to machine learning for help in keeping their quantum systems - qubits realised in trapped atoms - from breaking.
what might look like random behavior actually contained enough information for a computer program to guess how the system would change in the future.
it could then predict the future without direct observation, which would otherwise erase the system's useful characteristics.
the predictions were remarkably accurate, allowing the team to use their guesses preemptively to compensate for the anticipated changes.
doing this in real time allowed the team to prevent the disintegration of the quantum character, extending the useful lifetime of the qubits.
"we know that building real quantum technologies will require major advances in our ability to control and stabilise qubits - to make them useful in applications," biercuk says.
the techniques apply to any qubit, built in any technology, including the special superconducting circuits being used by major corporations.
"we're excited to be developing new capabilities that turn quantum systems from novelties into useful technologies.
the quantum future is looking better all the time," says biercuk.
ibm has announced its plans to begin offering the world's first commercial universal quantum- computing service- called ibm q, the system will be made available to those who wish to use it for a fee sometime later this year.
the new system will build on ibm's quantum experience, a software development platform for programmers and developers interested in designing and building actual quantum- based applications.
quantum computers are new and different, and still very much under development.
they run using quantum bits rather than the digital 0s and 1s we all know so well, and are therefore expected one day to be capable of solving problems or running applications that today are impossible.
imagine, the team at ibm suggests, a computer that could faithfully model nature in its nearly infinite variety of interactions and relationships.
as part of the roll out of ibm q, ibm will also release a new api for quantum experience and an upgraded simulator.
later this year, the company will also release a full sdk.
the company expects it to help programmers around the world learn to code in a whole new way that will result in unprecedented software applications able to take advantage of the unique abilities of a universal quantum computer.
even as ibm begins selling quantum computing services, the company will continue to develop the quantum computing hardware at the core of its system.
they have set a goal of building a 50- qubit system over the next few years.
however, they have not revealed the quantum volume customers can expect when the system goes online- current quantum experience users have access to 5 qubit computing.
the company has embraced the approach of incorporating qubits with superconducting circuits by keeping them at temperatures just above absolute zero.
in its announcement, ibm suggests it believes customers (or academics) will want to use quantum experience to build their own applications using quantum programming, which will then execute via the cloud on ibm q.
they note that quantum experience was launched just less than a year ago, but already over 40,000 users have built and run 275,000 test applications and 15 research papers have resulted thus far.
australian engineers have created a new quantum bit which remains in a stable superposition for 10 times longer than previously achieved, dramatically expanding the time during which calculations could be performed in a future silicon quantum computer.
the new quantum bit, made up of the spin of a single atom in silicon and merged with an electromagnetic field - known as 'dressed qubit' - retains quantum information for much longer that an 'undressed' atom, opening up new avenues to build and operate the superpowerful quantum computers of the future.
the result by a team at australia's university of new south wales (unsw), appears today in the online version of the international journal, nature nanotechnology.
"we have created a new quantum bit where the spin of a single electron is merged together with a strong electromagnetic field," said arne laucht, a research fellow at the school of electrical engineering & telecommunications at unsw, and lead author of the paper.
"this quantum bit is more versatile and more long- lived than the electron alone, and will allow us to build more reliable quantum computers."
building a quantum computer has been called the 'space race of the 21st century' - a difficult and ambitious challenge with the potential to deliver revolutionary tools for tackling otherwise impossible calculations, such as the design of complex drugs and advanced materials, or the rapid search of massive, unsorted databases.
its speed and power lie in the fact that quantum systems can host multiple 'superpositions' of different initial states, which in a computer are treated as inputs which, in turn, all get processed at the same time.
"the greatest hurdle in using quantum objects for computing is to preserve their delicate superpositions long enough to allow us to perform useful calculations," said andrea morello, leader of the research team and a program manager in the centre for quantum computation & communication technology (cqc2t) at unsw.
"our decade- long research program had already established the most long- lived quantum bit in the solid state, by encoding quantum information in the spin of a single phosphorus atom inside a silicon chip, placed in a static magnetic field," he said.
what laucht and colleagues did was push this further: "we have now implemented a new way to encode the information: we have subjected the atom to a very strong, continuously oscillating electromagnetic field at microwave frequencies, and thus we have 'redefined' the quantum bit as the orientation of the spin with respect to the microwave field."
the results are striking: since the electromagnetic field steadily oscillates at a very high frequency, any noise or disturbance at a different frequency results in a zero net effect.
the researchers achieved an improvement by a factor of 10 in the time span during which a quantum superposition can be preserved.
specifically, they measured a dephasing time of t2*=2.4 milliseconds - a result that is 10- fold better than the standard qubit, allowing many more operations to be performed within the time span during which the delicate quantum information is safely preserved.
"this new 'dressed qubit' can be controlled in a variety of ways that would be impractical with an 'undressed qubit',", added morello.
"for example, it can be controlled by simply modulating the frequency of the microwave field, just like in an fm radio.
the 'undressed qubit' instead requires turning the amplitude of the control fields on and off, like an am radio.
"in some sense, this is why the dressed qubit is more immune to noise: the quantum information is controlled by the frequency, which is rock- solid, whereas the amplitude can be more easily affected by external noise".
since the device is built upon standard silicon technology, this result paves the way to the construction of powerful and reliable quantum processors based upon the same fabrication process already used for today's computers.
the unsw team leads the world in developing quantum computing in silicon, and morello's team is part of the consortium of unsw researchers who have struck a a$70 million deal between unsw, the researchers, business and the australian government to develop a prototype silicon quantum integrated circuit - the first step in building the world's first quantum computer in silicon.
a functional quantum computer would allow massive increases in speed and efficiency for certain computing tasks - even when compared with today's fastest silicon- based 'classical' computers.
in a number of key areas - such as searching large databases, solving complicated sets of equations, and modelling atomic systems such as biological molecules and drugs - they would far surpass today's computers.they would also be enormously useful in the finance and healthcare industries, and for government, security and defence organisations.
quantum computers could identify and develop new medicines by greatly accelerating the computer- aided design of pharmaceutical compounds (and minimising lengthy trial and error testing), and develop new, lighter and stronger materials spanning consumer electronics to aircraft.
they would also make possible new types of computational applications and solutions that are beyond our ability to foresee.
quantum physics is moving out of the laboratory and into everyday life.
despite headline results about quantum computers solving problems impossible for classical computers, technical challenges are standing in the way of getting quantum physics into the real world.
new research published in nature communications from teams at aalto university and lund university could provide an important tool in this quest.
one of the open questions in quantum research is how heat and thermodynamics coexist with quantum physics.
this research field of quantum thermodynamics is one of the areas professor jukka pekola, the leader of the qtf centre of excellence of the academy of finland, has worked on in his career.
"this field has been dominated by theory, and only now are important experiments are starting to emerge," says professor pekola.
his research group has set about creating quantum thermodynamic nanodevices that can solve open questions experimentally.
quantum states, like those governing the qubits that power quantum computers, interact with their surrounding world, and these interactions are what quantum thermodynamics deals with.
measuring these systems requires the detection of energy changes so exceptionally small they are hard to pick out from background fluctuations, like attempting to determine if a candle in a room has been blown out using only a thermometer.
another problem is that quantum states can change when measured, simply because they have been measured.
this is analogous to causing a cup of water to boil by putting a thermometer in it.
the team had to make a thermometer that was able to measure very small changes without interfering with any of the quantum states they plan to measure.
doctoral student bayan karimi works in qtf and marie curie training network questech.
her device is a calorimeter, which measures the heat in a system.
it uses a strip of copper about 1000 times thinner than a human hair.
"our detector absorbs radiation from the quantum states.
it is expected to determine how much energy they have, and how they interact with their surroundings.
there is a theoretical limit to how accurate a calorimeter can be, and our device is now reaching that limit," says karimi.
scientists at the rdecom research laboratory, the army's corporate research laboratory (arl) have found a novel way to safeguard quantum information during transmission, opening the door for more secure and reliable communication for warfighters on the battlefield.
recent advancements of cutting- edge technologies in lasers and nanophysics, quantum optics and photonics have given researchers the necessary tools to control and manipulate miniature quantum systems, such as individual atoms or photons- the smallest particles of light.
these developments have given rise to a new area of science- quantum information science, or qis, that studies information encoded in quantum systems and encompasses quantum computing, quantum communication and quantum sensing among other subfields.
quantum information science is believed to have the potential to shape the way information is processed in the future.
the army's corporate research laboratory invests in qis research to guarantee continuous technological superiority in this rapidly developing field, which in turn will bring about multiple new technologies in computation, encryption, secure communication and precise measurements.
however, to utilize quantum information, scientists need to figure out robust ways to process and transmit it- a task being tackled by drs.
daniel jones, brian kirby, and michael brodsky from the laboratory's computational and information sciences directorate.
"in our classical world, information is often corrupted during manipulation and transmission- everyone is familiar with noisy cell phone connections in poor reception areas," brodsky said.
"thus, communication engineers have been working on a variety of techniques to filter out the noise."
in classical communications, the filtering is rather straightforward as it is done locally, that is in the very place the information is received, such as directly in your phone or internet router.
in the quantum world, things become much more intricate.
the lab's research team has been looking into ways of filtering noise from little bits of quantum information- quantum bits or qubits sent across fiber- optic telecom links.
they discovered that the filtering does not necessarily need to be done by the receiving party.
"the nature of the quantum states in which the information is encoded is such that the filtering could be more easily done at a different location in the network," kirby said.
that's right, to fix a qubit sent over a certain route, one could actually apply a filter to other qubits that traverse a different route.
over the last year, the researchers have been looking into the problem of transmission of entangled pairs of photons over optical fibers.
"we started with developing an understanding of how physical properties of real telecom fibers, such as inherent residual birefringence and polarization dependent loss, or pdl, affect the quality of quantum communications," jones said.
"we exploited a novel mathematical approach, which has led to the development of a simple and elegant geometrical model of the pdl effects on polarization entanglement," kirby added.
the developed model predicts both the quality of transmitted quantum states as well as the rate at which quantum information could be transmitted.
furthermore, the lab's team invented a new technique that helps reduce the deleterious effects of the noise.
the developed models were experimentally validated using the recently built quantum networking testbed at the lab, which simulates the practical telecom fiber infrastructure.
"we believe that this research has a potential to revolutionize cybersecurity and to enable secure secret sharing and authentication for the warfighter of the future," brodsky said.
"in addition, it will have an impact on developing better sensors for position navigation and timing as well as quantum computers that might result in the synthesis of novel special materials with on demand properties."
according to the researchers, in order to make quantum technology a reality, a large- scale field- deployed testbed must be built, thus guiding the development of both quantum hardware and software.
a journal paper documenting the research titled "tuning quantum channels to maximize polarization entanglement for telecom photon pairs" is featured in the prestigious nature partner journal quantum information.
more information: daniel e. jones et al.
tuning quantum channels to maximize polarization entanglement for telecom photon pairs, npj quantum information (2018).
doi: 10.1038/s41534- 018- 0107- x
at the quantum europe conference, imec announced that it is ramping- up its r&d activities focused on quantum computing.
imec will implement qubits and supporting nanoelectronic functionality for quantum computing, leveraging its advanced silicon (si) platform that was established within the framework of its industrial affiliation program with additional support from the eu through e.g.
ecsel projects senate and take- 5.
widely seen as a possible solution to complex computing problems which are intractable on classical computers, quantum computing uses quantum physics to create and manipulate quantum states within electronic devices (qubits) to enhance the performance over that of existing, 'classical' approaches.
of the many device proposals for qubit implementation, the ones compatible with existing si technology will provide the most viable solution for interfacing with the outside world.
the goal of imec's initiative is to establish a bridge between the most advanced transistor technology and emerging quantum technology options.
imec will support the transition of new quantum technologies, from the physics lab to technology feed into the supply chain.
imec's platform will be open for universities, smes and industrial partners of imec's quantum technologies programs.
a ucf physicist has discovered a new material that has the potential to become a building block in the new era of quantum materials, those that are composed of microscopically condensed matter and expected to change our development of technology.
researchers are entering the quantum age, and instead of using silicon to advance technology they are finding new quantum materials, conductors that have the ability to use and store energy at the subatomic level.
assistant professor madhab neupane has spent his career learning about the quantum realm and looking for these new materials, which are expected to become the foundation of the technology to develop quantum computers and long- lasting memory devices.
these new devices will increase computing power for big data and greatly reduce the amount of energy required to power electronics.
big companies recognize the potential and they are investing in research.
microsoft has invested in its station q, a lab dedicated solely to studying the field of topological quantum computing.
google has teamed up with nasa on a quantum ai lab that studies how quantum computing and artificial intelligence can mesh.
once the quantum phenomena are well understood and can be engineered, the new technologies are expected to change the world, much like electronics did at the end of the 20th century.
neupane's discovery, published today in nature communications is a big step in making that reality happen.
"our discovery takes us one step closer to the application of quantum materials and helps us gain a deeper understanding of the interactions between various quantum phases," neupane said.
the material neupane and his team discovered, hf2te2p- chemically composed of hafnium, tellurium and phosphorus- is the first material that has multiple quantum properties, meaning there is more than one electron pattern that develops within the electronic structure, giving it a range of quantum properties.
neupane's research group is using its specialized equipment for advanced- spectroscopic characterization of quantum materials to develop their work further.
"with the discovery of such an incredible material, we are at the brink of having a deeper understanding of the interplay of topological phases and developing the foundation for a new model from which all technology will be based off, essentially the silicon of a new era," neupane said.
researchers at the national institute of standards and technology's quantum information program have taken another step toward their goal of building a working quantum computer by demonstrating a simplified ion trap.
for the first time, a planar chip that could be scaled up to accommodate more ions has been built using microlithography techniques.
previous quantum- computing components built with one- of- a- kind laboratory setups might now have an integrated system for coordinated quantum logic operations resembling the integrated circuits of electronics.
the new ion trap approach uses a strategy similar to previous traps built at nist: levitate ultracold ions above metal electrodes and manipulate them with laser beams.
the latest advance is able to do that in a purely planar geometry, which simplifies the previous, multilevel- electrode architecture.
the chip can accommodate up to 12 ions simultaneously.
if this proves to be the "vlsi" technology for quantum logic, nist researchers may be close to their goal of building a 10- qubit quantum cpu.
quantum computers would differ radically from electronic ones; they would be able to perform vastly more operations on a given set of hardware components.
but so far, getting any component to work has been the big challenge.
the key to quantum information processes is some physical means of representing bits by quantum states.
next, there must be some mechanism to allow the quantum states to interact to create basic logic operations.
and during those operations, the state of quantum coherence must be maintained.
any interaction with the surrounding environment will destroy coherence, and indeed, simply reading the contents of a register will be enough to bring quantum operations to a halt.
in the nist scheme, ions cooled to temperatures close to absolute zero are used to represent quantum bits, known as qubits.
thus, an ion trap with eight ions would represent a quantum byte of information.
the advantage of the quantum representation of information comes from the fact that these ions are in a superposition of two states, which represent both logical 0 and 1.
thus, any quantum operation on the eight ions represents simultaneous operation on all combinations of eight conventional bits, for a total of 256 operations.
scaling up the size of the registers will therefore produce an exponential increase in processing capability.
because of this exponential scaling of performance, even very small quantum computers, using tens of qubits, could surpass current supercomputer technology.
from one to a dozen magnesium atoms can be loaded into the national institute of standards and technology's single- level ion trap.
there are other potential advantages.
the quantum states of the ions can be "entangled" so that a change to one ion's state is instantly transferred to an entangled ion, producing a uniquely efficient interconnect.
nist researchers have also demonstrated teleportation at the quantum level, in which the state of an ion, rather than the ion itself, is transported to another ion.
in addition, optical communications technology has been applied to transporting quantum information via single photons.
that technology has already reached the commercial stage, in the form of quantum encryption systems that securely distribute keys.
the overall vision is to create registers, quantum logic gates and interconnect that can all function within a state of undisturbed quantum coherence.
quantum processors would then be linked by quantum information networks using teleportation or single- photon optical systems.
most of the components for this comprehensive quantum information system have been at least demonstrated in the lab.
a scalable technology for implementing it all is the next requirement.
nist has other programs that may contribute to the effort, such as a josephson junction quantum- bit initiative or the single- photon technology effort.
researchers at delft university of technology have created a quantum circuit to listen to the weakest radio signal allowed by quantum mechanics.
this new quantum circuit opens the door to possible future applications in areas such as radio astronomy and medicine (mri).
it also enables experiments to shed light on the interplay between quantum mechanics and gravity.
the results have been published in science.
the usual solution to a weak radio signal is to find a bigger signal, for instance, by picking a different radio station or by moving to the other side of the room.
however,m what if we could just listen more carefully?
weak radio signals are not just a challenge for people trying to find their favourite radio station, but also for magnetic resonance imaging (mri) scanners at hospitals, as well as for the telescopes scientists use to peer into space.
in a quantum leap in radio frequency detection, researchers in the group of prof. gary steele in delft demonstrated the detection of photons or quanta of energy, the weakest signals allowed by the theory of quantum mechanics.
quantum chunks
one of the strange predictions of quantum mechanics is that energy comes in tiny little chunks called quanta.
what does this mean?
"say i am pushing a kid on a swing," says lead researcher mario gely.
"in the classical theory of physics, if i want the kid to go a little bit faster i can give them a small push, giving them more speed and more energy.
quantum mechanics says something different: i can only increase the kid's energy one 'quantum step' at a time.
pushing by half of that amount is not possible."
for a kid on a swing, these quantum steps are so tiny that they are too small to notice.
until recently, the same was true for radio waves.
however, the research team in delft developed a circuit that can actually detect these chunks of energy in radio frequency signals, opening up the potential for sensing radio waves at the quantum level.
from quantum radio to quantum gravity?
beyond applications in quantum sensing, the group in delft is interested in taking quantum mechanics to the next level: mass.
while the theory of quantum electromagnetism was developed nearly 100 years ago, physicists are still puzzled today on how to fit gravity into quantum mechanics.
"using our quantum radio, we want to try to listen to and control the quantum vibrations of heavy objects, and explore experimentally what happens when you mix quantum mechanics and gravity," gely said.
"such experiments are hard, but if successful we would be able to test if we can make a quantum superposition of space- time itself, a new concept that would test our understanding of both quantum mechanics and general relativity."
"think what we can do if we teach a quantum computer to do statistical mechanics," posed michael mcguigan, a computational scientist with the computational science initiative at the u.s. department of energy's brookhaven national laboratory.
at the time, mcguigan was reflecting on ludwig boltzmann and how the renowned physicist had to vigorously defend his theories of statistical mechanics.
boltzmann, who proffered his ideas about how atomic properties determine physical properties of matter in the late 19th century, had one extraordinarily huge hurdle: atoms were not even proven to exist at the time.
fatigue and discouragement stemming from his peers not accepting his views on atoms and physics forever haunted boltzmann.
today, boltzmann's factor, which calculates the probability that a system of particles can be found in a specific energy state relative to zero energy, is widely used in physics.
for example, boltzmann's factor is used to perform calculations on the world's largest supercomputers to study the behavior of atoms, molecules, and the quark "soup" discovered using facilities such as the relativistic heavy ion collider located at brookhaven lab and the large hadron collider at cern.
while it took a sea change to show boltzmann was right, computer scientists now are at the precipice of a new computing wave, making the leap from supercomputers and bytes to quantum systems and quantum bits (or "qubits").
these quantum computers have the potential to unlock some of the most mysterious concepts in physics.
and, oddly, these so- called mysteries may seem a bit familiar to many.
time and temperature brought to you by...
although most people are well acquainted with the notions of time and temperature and check on them several times a day, it turns out these basic concepts remain enigmatic in physics.
boltzmann's factor helps model temperature effects that can be used to predict and control atomic behavior and physical properties, and they work great on classical computers.
however, on a quantum computer, the quantum logic gates used in the computation (akin to logic gates found in digital circuits) are represented by complex numbers, as opposed to boltzmann's factor, which by definition, is real.
this issue offered mcguigan and his student/coauthor raffaele miceli an interesting problem to tackle using a quantum computing testbed provided by way of brookhaven lab's access agreement to ibm's universal quantum computing systems, through the ibm q hub at oak ridge national laboratory.
the collaboration allows brookhaven (among others in network) access to ibm's commercial quantum systems, including 20- and 53- qubit systems for experiments.
"on a quantum computer, there is another way to simulate finite temperature called thermo field dynamics, which is able to compute quantities that are both time- and temperature- dependent," mcguigan explained.
"in this formalism, you construct a double of the system, called the thermo double, then proceed with the calculation on a quantum computer as the computation can be represented in terms of quantum logic gates with complex numbers.
"in the end, you can sum the double states and generate an effective boltzmann's factor for calculations at finite temperature," he continued.
"there also are certain advantages of the formalism.
for example, you can study the effects of finite temperature and how the system evolves in real time as time and temperature are separated using this quantum algorithm.
one disadvantage is that it requires twice as many qubits as a zero temperature calculation to handle the double states."
miceli and mcguigan demonstrated how to implement the quantum algorithm for thermo field dynamics for finite temperature on a simple system involving a few particles and found perfect agreement with the classical computation.
their work used resources from both classical and quantum computing.
according to mcguigan, they used qiskit open- source quantum computing software that allowed them to create their algorithm in the cloud.
qiskit then transpiled that code to pulses that communicate with a quantum computer in real time (in this case, an ibm q device).
optimizers that run classical algorithms further enable the back and forth between the traditional and quantum systems.
"our experiment shows quantum systems have an advantage of representing real- time calculations exactly rather than rotating from imaginary time to real time to find a result," mcguigan explained.
"it offers a truer picture of how a system evolves.
we can map the problem to a quantum simulation that lets it evolve."
into the cosmos
quantum cosmology is another area where mcguigan anticipates that new quantum computing options will have profound impact.
despite the multitude of advances in understanding the universe made possible by modern supercomputers, some physical systems remain beyond their reach.
the mathematical complexity, which usually includes accounting for full quantum gravity theory, is simply too great to obtain exact solutions.
however, a true quantum computer, complete with the ability to exploit entanglement and superposition, would expand the options for new, more precise algorithms.
"quantum systems can realize path integrals in real time, giving us access to large- scale simulations of the universe," mcguigan said.
"you can visualize the calculated wavefunction of the universe as it evolves forward without first formulating a full theory of quantum gravity."
again, using the qiskit package and access to ibm q hardware, mcguigan and his collaborator charles kocher, a student at brown university, employed a mix of classical computational methods and vqe to run varied experiments, including one that examined systems with gravity coupled to a boson field called an inflaton, a hypothetical particle that plays an important role in modern cosmology.
their work showed the hybrid vqe yielded wavefunctions consistent with the wheeler- dewitt equation, which mathematically combines quantum mechanics with albert einstein's theory of relativity.
inspiration on an expanding scale
while early quantum experiments are leading to different perspectives of the basics behind physics, quantum computing is expected to contribute major advances toward solving longstanding problems impacting doe's missions.
among them, it can be a tool for unveiling new materials, solving energy challenges, or adding to fundamental understandings (like time and temperature) in high energy physics and cosmology.
in turn, these changes could cascade into more readily recognizable areas.
for example, drug developers need more realized quantum mechanics to understand the structure of molecules.
quantum computers can enable discoveries by affording simulations of the full quantum mechanics that would provide a truly practical point of view.
"there seems to always be interest in the basics behind physics," mcguigan said.
"it has been of interest to the public for millennia.
right now, the combination of theoretical expertise and actual technology is converging with quantum computing.
yet, it still is a very human endeavor."
for now, using near- term quantum computers to solve small thermo field problems or to take a new look at an old universe is inspiring researchers to scale up their algorithms as they do bigger things in science.
"we get emboldened to do different things.
we all do," mcguigan said.
"other groups around the world, such as the perimeter institute in canada and universiteit van amsterdam in the netherlands, are already extending the thermo field double quantum algorithm to even bigger systems.
with the emergence of large near- term quantum computers of 50- 100 qubits, the goal is to run finite temperature simulations on realistic systems involving many particles.
it is exciting to have an actual quantum computer to test these ideas and problems that we once had no solutions for.
quantum mechanics with no tradeoffs- that is what science is all about."
more information: raffaele miceli et al.
thermo field dynamics on a quantum computer, 2019 new york scientific data summit (nysds) (2019).
doi: 10.1109/nysds.2019.8909787
charles d. kocher et al.
simulating 0+1 dimensional quantum gravity on quantum computers: mini- superspace quantum cosmology and the world line approach in quantum field theory, 2018 new york scientific data summit (nysds) (2018).
doi: 10.1109/nysds.2018.8538963
machine learning techniques have so far proved to be very promising for the analysis of data in several fields, with many potential applications.
however, researchers have found that applying these methods to quantum physics problems is far more challenging due to the exponential complexity of many- body systems.
quantum many- body systems are essentially microscopic structures made up of several interacting particles.
while quantum physics studies have focused on the collective behavior of these systems, using machine learning in these investigations has proven to be very difficult.
with this in mind, a team of researchers at harvard university recently developed a quantum circuit- based algorithm inspired by convolutional neural networks (cnns), a popular machine learning technique that has achieved remarkable results in a variety of fields.
in their paper, published in nature physics, the researchers outlined this new architecture and evaluated its accuracy in recognizing quantum states associated with a 1- d, symmetry- protected topological phase.
"our work is largely motivated by recent experimental progress to build quantum computers and the development of artificial intelligence based on neural network methods," soonwon choi, one of the researchers who carried out the study, told phys.org.
"in some sense, the idea to combine machine learning techniques and quantum computers/simulators is very natural: in both fields, we are trying to extract meaningful information from a large amount of complex data."
as a theoretical physicist investigating quantum many- body systems, choi had often wondered whether there might be a more efficient way of analyzing the large amount of complex data obtained using quantum simulators.
artificial neural networks soon caught his attention, as they led to noteworthy results in several other tasks.
transforming traditional machine learning approaches so that they could be effectively applied in quantum physics, however, appeared to be challenging.
the main reason for this is that existing quantum simulators are quite small, thus they are unable to support a large- scale cnns and other machine learning techniques that are being used in conventional computers.
"we had to make sure that all important features of conventional machine learning techniques are kept while our new algorithm is as compact as possible," choi explained.
"one of the objectives of the present work was to generalize a specific, well- known machine learning architecture called convolutional neural network (cnn) for a compact quantum circuit, and demonstrate its capabilities with simplistic but meaningful examples."
in their study, choi and his colleagues assumed that cnns owe their great success to two important features.
firstly, the fact that they are made out of smaller local units (i.e., multiple layers of quasi- local quantum gates).
secondly, their ability to process input data in a hierarchical fashion.
the researchers found a connection between these two characteristics and two renowned physics concepts known as locality and renormalization.
"locality is natural in physics because we believe that the law of nature is fundamentally local," choi said.
"renormalization, on the other hand, is a very interesting concept.
in physics, certain universal features of a quantum many- body system, such as the phase (e.g., liquid, gas, solid, etc.)
of materials do not depend on (or are not sensitive to) microscopically detailed information of the system, but rather governed by only a few important hidden parameters.
renormalization is a theory technique to identify those important parameters starting from microscopic description of a quantum system."
the researchers observed that renormalization processes share some similarities with pattern recognition applications, particularly those in which machine learning is used to identify objects in pictures.
for instance, when a cnn trained for pattern recognition tasks analyzes pictures of animals, it focuses on a universal feature (i.e., trying to identify what animal is portrayed in the image), regardless of whether individual animals of the same type (e.g., cats) look slightly different.
this process is somewhat similar to renormalization techniques in theoretical physics, which can also help to distill universal information.
in their study, choi and his colleagues tried to develop an architecture with the same key qualities as cnns, but that would also be applicable to quantum physics problems.
"the resultant quantum circuit involves only log(n) number of parameters to be optimized for n- qubit input data, which is double exponential improvement compared to a naive approach, in which exp(n) number of parameters are optimized," choi explained.
"when the number of parameters to optimize becomes this small, one may worry that our circuit is not capable of complex information processing tasks.
however, we have demonstrated that despite its small size, our quantum cnn is still capable of recognizing different quantum phases and designing quantum error correction schemes."
the researchers evaluated the technique they developed, called quantum convolutional neural network (qcnn), on a quantum physics- specific problem that involved recognizing quantum states associated with a 1- d symmetry protected topological phase.
remarkably, their technique was able to recognize these quantum states, outperforming existing approaches.
as it is fairly compact, the qcnn could also potentially be implemented in small quantum computers.
"in my view, the most meaningful finding in our work is the connection between well- known physics concepts, renormalization (or more precisely, multiscale entanglement renormalization ansatz), and a successful information processing technique in artificial intelligence, cnn," choi said.
"similar connections have been already suggested several years back, but here we have successfully substantiated the connection by explicitly demonstrating it with a clean example."
choi and his colleagues are among the first to successfully create a cnn- inspired architecture that incorporates quantum physics.
the examples outlined in their paper are also simple enough to be experimentally applied to existing and forthcoming quantum devices.
their results suggest that renormalization could be a promising quantum information processing technique and they thus intend to explore this idea further.
"we have demonstrated that our method allows to design quantum error correction schemes tailored for a given experimental system," choi said.
"it would be very exciting to see its action in exiting quantum computing platforms and improve their performance."
in their future work, choi and his colleagues will first try to use their findings to develop new quantum computers.
in addition, they would like to carry out further research investigating the relationship between cnns or other neural network based methods and renormalization techniques.
"while we have demonstrated a nice example for one- dimensional quantum systems, more in- depth study of the connection in its full generality is still missing," choi added.
"in particular, studying the connection in two- dimensional quantum systems would be an exciting future direction."
researchers have passed an important milestone in building high- precision quantum gates with record- breaking precision of 99.9%
a quantum logic gate developed at oxford university can create quantum- entangled particles with 99.9% precision.
researchers at the university of oxford have passed an important milestone in building high- precision quantum gates with a record- breaking precision of 99.9%.
considering the fact that, theoretically, we need high- fidelity quantum logic gates with over 99% precision to achieve fault- tolerant quantum computations, this new technology can significantly speed up developing practical quantum computers.
quantum computers vs. classic computers
quantum computers are not going to be a replacement for classic computers like the ones you have in your home.
however, they obviously hold the potential to marginalize even today's most powerful computers in solving particular problems.
in other words, quantum computers are not generally faster than the classic ones; they are only faster for the type of problems which can utilize the endless possibilities offered by the quantum mechanics.
quantum computing uses quantum bits or qubits to represent data.
while in classical computing each bit is either one or zero, qubits behave quantumly according to the laws of quantum physics.
due to the superposition phenomenon of quantum mechanics, a qubit can be one, zero, or a superposition of these two states.
superposition means that a quantum system can be in multiple states at the same time.
extending the superposition phenomenon for a general quantum system, we find that an n- qubit system contains as much information as a 2n- bit classical computing system.
we should note that although qubits can assume a superposition of a large number of states, as soon as we measure a qubit, it falls into one of its base states and all the other information carried by the qubit before measuring is lost.
with many quantum superpositions available, a quantum computer provides computational parallelism in solving complex problems which require us to deal with very large amounts of data.
according to andrea morello, professor of electrical engineering and telecommunications at the university of new south wales, we do not expect a quantum computer to necessarily expedite a single information processing operation.
instead, a quantum computer can exponentially reduce the number of operations required to perform a particular algorithm.
as chris ballance, the lead author of the research, explains a quantum computer is fundamentally different from our everyday computers and it is not merely a different technology.
quantum entanglement
the new gate is based on putting two atoms in a state of quantum entanglement.
entanglement, which is fundamental to quantum computing, is an extremely strong connection between two quantum particles in a way that the state of particles cannot be described independently of one another.
according to a co- author of the research, professor david lucas of oxford university's department of physics and balliol college, two entangled particles share a joint quantum state and when we measure a property of one of the particles, we obtain information about the other particle too.
the example of identical twins is a tangible analogy to discuss two entangled particles.
a common tale is that identical twins can feel each other's pain or emotions even from a long distance away.
this is representative of what happens for two entangled particles.
in a highly entangled quantum system, a large amount of information is conveyed by the relationship between the entangled particles and you will have to study all of the particles at once to analyze the system.
quantum logic gates
a quantum logic gate receives two independent atoms and puts them in a state of quantum entanglement.
the gate precision tells us how well the gate does this.
the new gate which has the precision of 99.9% which means that it can successfully generate the entangled particles 999 times out of 1000 and only one of all these experiments lead to qubits which are not correctly entangled.
quantum computation faces many challenges and the technology is still in its infancy.
one of these challenges is to build the required quantum logic gates, then we need to develop the techniques which can utilize multiple gates to process data.
as professor lucas explains, a precision of 99.9% is theoretically enough to build a quantum computer.
however, in practice, it would be very difficult and expensive.
he hopes that a future precision of 99.99% will enhance the prospect of a practical quantum computer.
the new method is presented in a paper entitled 'high- fidelity quantum logic gates using trapped- ion hyperfine qubits' in the journal physical review letters.
los alamos national laboratory scientists have developed a new quantum computing algorithm that offers a clearer understanding of the quantum- to- classical transition, which could help model systems on the cusp of quantum and classical worlds, such as biological proteins, and also resolve questions about how quantum mechanics applies to large- scale objects.
"the quantum- to- classical transition occurs when you add more and more particles to a quantum system," said patrick coles of the physics of condensed matter and complex systems group at los alamos national laboratory, "such that the weird quantum effects go away and the system starts to behave more classically.
for these systems, it's essentially impossible to use a classical computer to study the quantum- to- classical transition.
we could study this with our algorithm and a quantum computer consisting of several hundred qubits, which we anticipate will be available in the next few years based on the current progress in the field."
answering questions about the quantum- to- classical transition is notoriously difficult.
for systems of more than a few atoms, the problem rapidly becomes intractable.
the number of equations grows exponentially with each added atom.
proteins, for example, consist of long strings of molecules that may become important biological components or sources of disease, depending on how they fold up.
although proteins can be comparatively large molecules, they are small enough that the quantum- to- classical transition, and algorithms that can handle it, become important when trying to understand and predict how proteins fold.
in order to study aspects of the quantum- to- classical transition on a quantum computer, researchers first need a means to characterize how close a quantum system is to behaving classically.
quantum objects have characteristics of both particles and waves.
in some cases, they interact like tiny billiard balls, in others they interfere with each other in much the same way that waves on the ocean combine to make larger waves or cancel each other out.
the wave- like interference is a quantum effect.
fortunately, a quantum system can be described using intuitive classical probabilities rather than the more challenging methods of quantum mechanics, when there is no interference.
the lanl group's algorithm determines how close a quantum system is to behaving classically.
the result is a tool they can use to search for classicality in quantum systems and understand how quantum systems, in the end, seem classical to us in our everyday life.
this work was initially funded by asc bml funds, and subsequently by the department of energy's high energy physics quantised program and in part by the los alamos national laboratory directed research and development program.
for full funding documentation, see the paper's acknowledgements section.
note that no government funds from distinct sources were used to fund the same research.
primary funding and research from the doe hep quantised program was directed by principal investigator -  andrew sornborger, information sciences, ccs- 3, lanl
a team of researchers made up of representatives from google, lawrence berkeley national labs, tufts university, uc santa barbara, university college london and harvard university reports that they have successfully created a scalable quantum simulation of a molecule for the first time ever.
in a paper uploaded to the open access journal physical review x, the team describes the variational quantum eigensolver (vqe) approach they used to create and solve one of the first real- world quantum computer applications.
as research continues with the development of a true quantum computer, some in the field have turned their attention to selecting certain types of problems that such computers could solve, as opposed to what are now being called classical computers.
one such problem is solving the molecular electronic structure problem, which as google quantum software engineer ryan babbush notes in a blog post involves searching for the lowest electron energy configuration of a given molecule.
what this means in practice is using a machine to compute the energies of molecules- doing so for some, such as methane, is relatively easy and can be done very quickly on a classical computer, but others, such as propane, can take days.
this makes it an ideal test case for a quantum computer.
to calculate molecular energies on a quantum computer, the researchers used the vqe approach because it translates well as a quantum equivalent of a neural network, i.e., quantum bits could be used to represent molecular wave functions.
once they had built and programmed the system, they tested it by computing the energy of a hydrogen molecule.
their results very closely matched prior results found using classical computers.
the researchers are aiming to create a quantum computer that is capable not only of computing single molecule energies, but entire chemical systems.
as one example, they would like to be able to simulate what happens as bacteria do their work in producing fertilizer- a process they note that currently consumes approximately 2 percent of global energy produced.
that would mean developing a universal quantum computer, which, not coincidently, is a goal google has set for itself.
more information: p. j. j. o'malley et al.
scalable quantum simulation of molecular energies, physical review x (2016).
doi: 10.1103/physrevx.6.031007
abstract
we report the first electronic structure calculation performed on a quantum computer without exponentially costly precompilation.
we use a programmable array of superconducting qubits to compute the energy surface of molecular hydrogen using two distinct quantum algorithms.
first, we experimentally execute the unitary coupled cluster method using the variational quantum eigensolver.
our efficient implementation predicts the correct dissociation energy to within chemical accuracy of the numerically exact result.
second, we experimentally demonstrate the canonical quantum algorithm for chemistry, which consists of trotterization and quantum phase estimation.
we compare the experimental performance of these approaches to show clear evidence that the variational quantum eigensolver is robust to certain errors.
this error tolerance inspires hope that variational quantum simulations of classically intractable molecules may be viable in the near future.
at room temperature, carbon- 13 nuclei in diamond create stable, controllable quantum register
surmounting several distinct hurdles to quantum computing, physicists at harvard university have found that individual carbon- 13 atoms in a diamond lattice can be manipulated with extraordinary precision to create stable quantum mechanical memory and a small quantum processor, also known as a quantum register, operating at room temperature.
the finding brings the futuristic technology of quantum information systems into the realm of solid- state materials under ordinary conditions.
the results, described this week in science, could revolutionize scientists' approach to quantum computing, which is built on the profound eccentricity of quantum mechanics and could someday far outperform conventional supercomputers in solving certain problems.
"these experiments lay the groundwork for development of a new approach to quantum information systems," says mikhail d. lukin, professor of physics in harvard's faculty of arts and sciences.
earlier advances in quantum computing have occurred inside high vacuums cooled to fractions of a degree above absolute zero.
individual quantum bits, or qubits -  the building blocks of a quantum computer, encoding information much as a conventional computer bit stores information as zeroes and ones -  are extremely fragile.
usually they decay very rapidly, losing quantum information within a tiny fraction of a second unless the qubit is suspended in high vacuum under these specialized, extreme conditions.
this short "coherence time" has been a major impediment to advances in quantum computing.
quantum mechanics dictates that coherence is destroyed -  and quantum information lost -  through contact with virtually anything, which is why previous attempts at quantum computing have occurred under such extreme circumstances.
this need for absolute isolation has vexed scientists for more than a decade, not only because it is difficult to achieve experimentally -  not to mention in a practical computer -  but because it has complicated the ability to manipulate a quantum computer's input or read its output.
the new advance makes use of spinning properties of atomic nuclei, fundamental building blocks of matter with sub- nanometer dimensions, to encode quantum bits.
acting as tiny magnets, such nuclear spins are well known for their exceptional stability.
but in practice the very weak interactions of nuclear spins with their surroundings -  the very reason for their near- perfect isolation -  means that it is essentially impossible to address and manipulate individual nuclei, and harder still to control interactions between them.
for instance, many billions of nuclei are required in conventional mri machines, which work by detecting signals from spinning nuclei.
"the problem is, what makes single nuclear spin so stable -  its weak interaction with its surroundings -  also prevents us from directly manipulating it," lukin says.
"how do you control something that can't interact with anything""
you do it gingerly and indirectly, the harvard physicists report in science.
they found that nuclear spins associated with single atoms of carbon- 13 -  which make up some 1.1 percent of natural diamond -  can be manipulated via a nearby single electron whose own spin can be controlled with optical and microwave radiation.
the excitation of an electron by focusing laser light on a nitrogen vacancy center, a stable defect in a diamond lattice where nitrogen replaces an atom of carbon and develops an electronic spin in its ground state, causes the single electron's spin to act as a very sensitive magnetic probe with extraordinary spatial resolution.
using the nitrogen center as an intermediary, a single carbon- 13 atom's nuclear spin is cooled to near absolute zero, creating in the process a single, isolated quantum bit with a coherence time that approaches seconds.
the controlled interaction between the electron and nuclear spins allows the latter to be used as very robust quantum memory.
the harvard physicists also observed and manipulated coupling between individual nuclear spins, thus demonstrating a way to increase the number of qubits working in the quantum register.
because the electron spin and nuclear spin are controlled independently, the experiments lay the groundwork for development of larger, scalable systems in which such quantum registers are connected via optical photons.
"beyond specific applications in quantum information science," the authors write, "our measurements show that the electron spin can be used as a sensitive local magnetic probe that allows for a remarkable degree of control over individual nuclear spins."
yale university scientists have found a way to observe quantum information while preserving its integrity, an achievement that offers researchers greater control in the volatile realm of quantum mechanics and greatly improves the prospects of quantum computing.
quantum computers would be exponentially faster than the most powerful computers of today.
"our experiment is a dress rehearsal for a type of process essential for quantum computing," said michel devoret, the frederick william beinecke professor of applied physics & physics at yale and principal investigator of research published jan. 11 in the journal science.
"what this experiment really allows is an active understanding of quantum mechanics.
it's one thing to stare at a theoretical formula and it's another thing to be able to control a real quantum object."
in quantum systems, microscopic units called qubits represent information.
qubits can assume either of two states- "0" or "1"- or both simultaneously.
correctly recognizing, interpreting, and tracking their state is necessary for quantum computing.
however, the act of monitoring them usually damages their information content.
the yale physicists successfully devised a new, non- destructive measurement system for observing, tracking and documenting all changes in a qubit's state, thus preserving the qubit's informational value.
in principle, the scientists said, this should allow them to monitor the qubit's state in order to correct for random errors.
"as long as you know what error process has occurred, you can correct," devoret said.
"and then everything's fine.
you can basically undo the errors."
"that's the key," said michael hatridge, a postdoctoral associate in physics at yale and lead author of the science paper, "the ability to talk to the qubit and hear what it's telling you."
he continued: "a major problem with quantum computing is the finite lifetime of information stored in the qubits, which steadily decays and which must be corrected.
we now know that it is possible to do this correction by feedback involving a continuous measurement.
our work advances the prospects of large- scale quantum computers by opening the door to continuous measurement- based quantum feedback."
the yale physicists successfully measured one qubit.
the challenge ahead is to measure and control many at once, and the team is developing ultra- fast digital electronics for this purpose.
"we are on the threshold between the ability to measure and control one or two qubits, and many," hatridge said.
researchers at the department of energy's oak ridge national laboratory have developed a quantum chemistry simulation benchmark to evaluate the performance of quantum devices and guide the development of applications for future quantum computers.
their findings were published in npj quantum information.
quantum computers use the laws of quantum mechanics and units known as qubits to greatly increase the threshold at which information can be transmitted and processed.
whereas traditional "bits" have a value of either 0 or 1, qubits are encoded with values of both 0 and 1, or any combination thereof, allowing for a vast number of possibilities for storing data.
while still in their early stages, quantum systems have the potential to be exponentially more powerful than today's leading classical computing systems and promise to revolutionize research in materials, chemistry, high- energy physics, and across the scientific spectrum.
but because these systems are in their relative infancy, understanding what applications are well suited to their unique architectures is considered an important field of research.
"we are currently running fairly simple scientific problems that represent the sort of problems we believe these systems will help us to solve in the future," said ornl's raphael pooser, principal investigator of the quantum testbed pathfinder project.
"these benchmarks give us an idea of how future quantum systems will perform when tackling similar, though exponentially more complex, simulations."
pooser and his colleagues calculated the bound state energy of alkali hydride molecules on 20- qubit ibm tokyo and 16- qubit rigetti aspen processors.
these molecules are simple and their energies well understood, allowing them to effectively test the performance of the quantum computer.
by tuning the quantum computer as a function of a few parameters, the team calculated these molecules' bound states with chemical accuracy, which was obtained using simulations on a classical computer.
of equal importance is the fact that the quantum calculations also included systematic error mitigation, illuminating the shortcomings in current quantum hardware.
systematic error occurs when the "noise" inherent in current quantum architectures affects their operation.
because quantum computers are extremely delicate (for instance, the qubits used by the ornl team are kept in a dilution refrigerator at around 20 millikelvin (or more than - 450 degrees fahrenheit), temperatures and vibrations from their surrounding environments can create instabilities that throw off their accuracy.
for instance, such noise may cause a qubit to rotate 21 degrees instead of the desired 20, greatly affecting a calculation's outcome.
"this new benchmark characterizes the 'mixed state,' or how the environment and machine interact, very well," pooser said.
"this work is a critical step toward a universal benchmark to measure the performance of quantum computers, much like the linpack metric is used to judge the fastest classical computers in the world."
while the calculations were fairly simple compared to what is possible on leading classical systems such as ornl's summit, currently ranked as the world's most powerful computer, quantum chemistry, along with nuclear physics and quantum field theory, is considered a quantum "killer app."
in other words, it is believed that as they evolve quantum computers will be able to more accurately and more efficiently perform a wide swathe of chemistry- related calculations better than any classical computer currently in operation, including summit.
"the current benchmark is a first step towards a comprehensive suite of benchmarks and metrics that govern the performance of quantum processors for different science domains," said ornl quantum chemist jacek jakowski.
"we expect it to evolve with time as the quantum computing hardware improves.
ornl's vast expertise in domain sciences, computer science and high- performance computing make it the perfect venue for the creation of this benchmark suite."
ornl has been planning for paradigm- shifting platforms such as quantum for more than a decade via dedicated research programs in quantum computing, networking, sensing and quantum materials.
these efforts aim to accelerate the understanding of how near- term quantum computing resources can help tackle today's most daunting scientific challenges and support the recently announced national quantum initiative, a federal effort to ensure american leadership in quantum sciences, particularly computing.
such leadership will require systems like summit to ensure the steady march from devices such as those used by the ornl team to larger- scale quantum systems exponentially more powerful than anything in operation today.
access to the ibm and rigetti processors was provided by the quantum computing user program at the oak ridge leadership computing facility, which provides early access to existing, commercial quantum computing systems while supporting the development of future quantum programmers through educational outreach and internship programs.
support for the research came from doe's office of science advanced scientific computing research program.
"this project helps doe better understand what will work and what won't work as they forge ahead in their mission to realize the potential of quantum computing in solving today's biggest science and national security challenges," pooser said.
next, the team plans to calculate the exponentially more complex excited states of these molecules, which will help them devise further novel error mitigation schemes and bring the possibility of practical quantum computing one step closer to reality.
in the digital age, each one of us carries out activities daily that would be impossible without cryptographic techniques.
the security of our information, however, risks being thrown into crisis by the advent of future quantum computers, equipped with vast computing resources, potentially able to overcome current cryptographic techniques.
a new generation of devices under development by companies such as microsoft, google, and ibm will multiply the computing capabilities of computers and will probably make obsolete the encryption systems currently in use, based on the transmission of radio waves.
quantum cryptography is a method of transmitting secret information that offers the guarantee of maximum security.
unlike conventional cryptography based on calculation hypotheses, quantum cryptography has a significant advantage: its security is based on the laws of physics proving to be unconditionally safe with quantum cryptographic techniques.
quantum mechanics aims to describe the heart of matter, where natural phenomena occur on a subatomic scale.
current systems of quantum cryptography rely on encoding a computer bit in a property of a single photon, which is the fundamental constituent of light and electromagnetic radiation.
the collaboration agreement between imec and the national university of singapore (nus) aims to jointly develop scalable, robust, and efficient quantum technologies for the distribution of secure keys for the internet of the future.
in the coming years, large- scale quantum computers will make most of the current cryptography techniques insecure.
to avoid this, two major global directions have been pursued: a post- quantum cryptography approach and another hardware- based approach called quantum cryptography.
post- quantum cryptography is essentially about updating existing algorithms and cryptographic standards.
it still maintains a security profile that is still based on unproven hypotheses.
it consists of the definition and the study of cryptographic systems capable of guaranteeing high levels of security even against attackers equipped with quantum computers.
the first challenge in this area consists of identifying mathematical problems that are difficult to solve for an attacker who is not significantly affected by the existence of quantum computers.
quantum cryptography, on the other hand, offers a much stronger security guarantee.
with this approach, two essential constitutive elements are quantum key distribution (qkd) and quantum random number generation (qrng).
now, however, the methods and processes that enable these quantum technologies are limiting and expensive.
as a result, these bottlenecks have made quantum cryptography unattractive for widespread diffusion.
imec and nus aim to solve some of these bottlenecks (figure 1).
"our approach consists of developing and integrating all qkd key components in a single silicon- photonics- based chip, which ensures a cost- effective solution," said joris van campenhout, r&d program director at imec.
dr. charles lim, assistant professor at nus, said, "the development of chip- based prototypes will allow us to turn today's qkd technologies into an efficient communication networking solution."
the quantum distribution of keys makes it possible to transmit a secret key from one user to another, reaching the condition of perfect secrecy from a mathematical point of view and therefore making any interception attempts useless.
furthermore, the quantum characteristics of the physical phenomena used make it intrinsically inevitable to detect the presence of any passive attackers.
quantum cryptography is an alternative to the use of public key protocols, such as rsa, to generate and exchange secret keys.
the objective of qkd is to guarantee the secrecy of a distributed key.
in turn, legitimate subjects can use this key for encryption.
the confidentiality of the transmission data is guaranteed by a systemic chain through the quantum- distributed key and the encryption algorithm.
if one of these two "parameters" fails, the entire chain is compromised.
quantum cryptography is thought to be secure for various reasons.
first, the quantum no- cloning theorem states that an unknown quantum state cannot be cloned.
moreover, in a quantum system, which can be in one of the two states, any attempt to measure it will disturb the system itself.
if a quantum message is intercepted by malicious users, it will become useless to the recipient.
the measurements of a quantum property are irreversible, which means that the quantum message cannot return to its original state.
fundamentally, the no- cloning theorem protects the uncertainty principle of quantum mechanics, effectively representing an essential ingredient in quantum cryptography, prohibiting interceptors from creating copies of a transmitted quantum cryptographic key.
the futuristic vision of a quantum communication with the qkd implementation could be the ideal choice, also given the advent of quantum computers and therefore of optoelectronics/photonics (figure 2).
cryptography is increasingly considered a protection tool for cybersecurity, becoming essential for banks and financial institutions that, being responsible for managing a large amount of sensitive data, are confirmed as one of the main objectives of cyberattacks.
ibm corp.'s almaden research center here today announced it has performed the world's most advanced quantum- computer calculation to date.
scientists at the san jose- based r&d center claimed they have developed "a billion- billion custom- designed molecules in a test tube," enabling a seven- qubit quantum computer that solved a simple math problem in a data- security cryptographic application.
a quantum computer gets its power by taking advantage of certain quantum properties of atoms or nuclei that allow them to worktogether as quantum bits, or "qubits," which serve simultaneously as the computer's processor and memory.
"this result reinforces the growing realization that quantum computers may someday be able to solve problems that are so complex that even the most powerful supercomputers working for millions of years can't calculate the answers," said nabil amer, manager and strategist of ibm research's physics of information group.
in the quantum- computer demonstration, a team of ibm scientists and stanford university graduate students also reported the first demonstration of "shor's algorithm."
this is a method developed in 1994 by at&t scientist peter shor for using the futuristic quantum computer to find a number's factors.
today, factoring a large number is so difficult for conventional computers.
while the potential for quantum computing is huge, commercial systems are still many years away.
the first quantum computing applications would likely to be co- processors for specific functions, such as solving difficult mathematical problems, modeling quantum systems and performing unstructured searches.
the microprocessor inside a computer is a single multipurpose chip that has revolutionised people's life, allowing them to use one machine to surf the web, check emails and keep track of finances.
now, researchers from the university of bristol in the uk and nippon telegraph and telephone (ntt) in japan, have pulled off the same feat for light in the quantum world by developing an optical chip that can process photons in an infinite number of ways.
it's a major step forward in creating a quantum computer to solve problems such as designing new drugs, superfast database searches, and performing otherwise intractable mathematics that aren't possible for super computers.
the fully reprogrammable chip brings together a multitude of existing quantum experiments and can realise a plethora of future protocols that have not even been conceived yet, marking a new era of research for quantum scientists and engineers at the cutting edge of quantum technologies.
the work is published in the journal science on 14 august.
since before newton held a prism to a ray of sunlight and saw a spectrum of colour, scientists have understood nature through the behaviour of light.
in the modern age of research, scientists are striving to understand nature at the quantum level and to engineer and control quantum states of light and matter.
a major barrier in testing new theories for quantum science and quantum computing is the time and resources needed to build new experiments, which are typically extremely demanding due to the notoriously fragile nature of quantum systems.
this result shows a step change for experiments with photons, and what the future looks like for quantum technologies.
dr anthony laing, who led the project, said: "a whole field of research has essentially been put onto a single optical chip that is easily controlled.
the implications of the work go beyond the huge resource savings.
now anybody can run their own experiments with photons, much like they operate any other piece of software on a computer.
they no longer need to convince a physicist to devote many months of their life to painstakingly build and conduct a new experiment."
the team demonstrated the chip's unique capabilities by re- programming it to rapidly perform a number of different experiments, each of which would previously have taken many months to build.
bristol phd student jacques carolan, one of the researchers, added: "once we wrote the code for each circuit, it took seconds to re- programme the chip, and milliseconds for the chip to switch to the new experiment.
we carried out a year's worth of experiments in a matter of hours.
what we're really excited about is using these chips to discover new science that we haven't even thought of yet."
the device was made possible because the world's leading quantum photonics group teamed up with nippon telegraph and telephone (ntt), the world's leading telecommunications company.
professor jeremy o'brien, director of the centre for quantum photonics at bristol university, explained: "over the last decade, we have established an ecosystem for photonic quantum technologies, allowing the best minds in quantum information science to hook up with established research and engineering expertise in the telecommunications industry.
it's a model that we need to encourage if we are to realise our vision for a quantum computer."
the university of bristol's pioneering 'quantum in the cloud' is the first and only service to make a quantum processor publicly accessible and plans to add more chips like this one to the service so others can discover the quantum world for themselves.
research teams all over the world are exploring different ways to design a working computing chip that can integrate quantum interactions.
now, unsw engineers believe they have cracked the problem, reimagining the silicon microprocessors we know to create a complete design for a quantum computer chip that can be manufactured using mostly standard industry processes and components.
the new chip design, published in the journal nature communications, details a novel architecture that allows quantum calculations to be performed using existing semiconductor components, known as cmos (complementary metal- oxide- semiconductor) - the basis for all modern chips.
it was devised by andrew dzurak, director of the australian national fabrication facility at the university of new south wales (unsw), and dr menno veldhorst, lead author of the paper who was a research fellow at unsw when the conceptual work was done.
"we often think of landing on the moon as humanity's greatest technological marvel," said dzurak, who is also a program leader at australia's famed centre of excellence for quantum computation and communication technology (cqc2t).
"but creating a microprocessor chip with a billion operating devices integrated together to work like a symphony - that you can carry in your pocket!
- is an astounding technical achievement, and one that's revolutionised modern life.
"with quantum computing, we are on the verge of another technological leap that could be as deep and transformative.
but a complete engineering design to realise this on a single chip has been elusive.
i think what we have developed at unsw now makes that possible.
and most importantly, it can be made in a modern semiconductor manufacturing plant," he added.
veldhorst, now a team leader in quantum technology at qutech - a collaboration between delft university of technology and tno, the netherlands organisation for applied scientific research - said the power of the new design is that, for the first time, it charts a conceivable engineering pathway toward creating millions of quantum bits, or qubits.
"remarkable as they are, today's computer chips cannot harness the quantum effects needed to solve the really important problems that quantum computers will.
to solve problems that address major global challenges - like climate change or complex diseases like cancer - it's generally accepted we will need millions of qubits working in tandem.
to do that, we will need to pack qubits together and integrate them, like we do with modern microprocessor chips.
that's what this new design aims to achieve.
"our design incorporates conventional silicon transistor switches to 'turn on' operations between qubits in a vast two- dimensional array, using a grid- based 'word' and 'bit' select protocol similar to that used to select bits in a conventional computer memory chip," he added.
"by selecting electrodes above a qubit, we can control a qubit's spin, which stores the quantum binary code of a 0 or 1.
and by selecting electrodes between the qubits, two- qubit logic interactions, or calculations, can be performed between qubits."
a quantum computer exponentially expands the vocabulary of binary code used in modern computers by using two spooky principles of quantum physics - namely, 'entanglement' and 'superposition'.
qubits can store a 0, a 1, or an arbitrary combination of 0 and 1 at the same time.
and just as a quantum computer can store multiple values at once, so it can process them simultaneously, doing multiple operations at once.
this would allow a universal quantum computer to be millions of times faster than any conventional computer when solving a range of important problems.
but to solve complex problems, a useful universal quantum computer will need a large number of qubits, possibly millions, because all types of qubits we know are fragile, and even tiny errors can be quickly amplified into wrong answers.
"so we need to use error- correcting codes which employ multiple qubits to store a single piece of data," said dzurak.
"our chip blueprint incorporates a new type of error- correcting code designed specifically for spin qubits, and involves a sophisticated protocol of operations across the millions of qubits."
"it's the first attempt to integrate into a single chip all of the conventional silicon circuitry needed to control and read the millions of qubits needed for quantum computing."
"we expect that there will still be modifications required to this design as we move towards manufacture, but all of the key components that are needed for quantum computing are here in one chip".
"and that's what will be needed if we are to make quantum computers a workhorse for calculations that are well beyond today's computers," dzurak added.
"it shows how to integrate the millions of qubits needed to realise the true promise of quantum computing."
building such a universal quantum computer has been called the 'space race of the 21st century'.
for a range of calculations, they will be much faster than existing computers, and for some challenging problems they could find solutions in days, maybe even hours, when today's best supercomputers would take millions of years.
there are at least five major quantum computing approaches being explored worldwide: silicon spin qubits, ion traps, superconducting loops, diamond vacancies and topological qubits; unsw's design is based on silicon spin qubits.
the main problem with all of these approaches is that there is no clear pathway to scaling the number of quantum bits up to the millions needed without the computer becoming huge a system requiring bulky supporting equipment and costly infrastructure.
that's why unsw's new design is so exciting: relying on its silicon spin qubit approach - which already mimics much of the solid- state devices in silicon that are the heart of the us$380 billion global semiconductor industry - it shows how to dovetail spin qubit error correcting code into existing chip designs, enabling true universal quantum computation.
unlike almost every other major group elsewhere, cqc2t's quantum computing effort is obsessively focused on creating solid- state devices in silicon, from which all of the world's computer chips are made.
and they're not just creating ornate designs to show off how many qubits can be packed together, but aiming to build qubits that could one day be easily fabricated - and scaled up.
"it's kind of swept under the carpet a bit, but for large- scale quantum computing, we are going to need millions of qubits," said dzurak.
"here, we show a way that spin qubits can be scaled up massively.
and that's the key."
the design is a leap forward in silicon spin qubits; it was only two years ago, in a paper in nature, that dzurak and veldhorst showed, for the first time, how quantum logic calculations could be done in a real silicon device, with the creation of a two- qubit logic gate - the central building block of a quantum computer.
physicists have implemented the first experimental demonstration of everlasting quantum coherence- the phenomenon that occurs when a quantum system exists in a superposition of two or more states at once.
typically, quantum coherence lasts for only a fraction of a second before decoherence destroys the effect due to interactions between the quantum system and its surrounding environment.
the collaboration of physicists, led by gerardo adesso at the university of nottingham and with members from the uk, brazil, italy, and germany, have published a paper on the demonstration of the extreme resilience of quantum coherence in a recent issue of physical review letters.
"quantum properties can be exploited for disruptive technologies but are typically very fragile," adesso told phys.org.
"here we report an experiment which shows for the first time that quantum coherence in a large ensemble of nuclear spins can be naturally preserved ('frozen') under exposure to strong dephasing noise at room temperature, without external control, and for timescales as long as a second and beyond."
quantum coherence is an inherently quantum property that arises due to the wave- like nature of matter.
most importantly for potential applications, quantum coherence allows a quantum system to occupy a superposition of states.
this trait leads to quantum parallelism, which is the key ingredient that allows some quantum devices to outperform classical ones in a wide range of applications.
for instance, many research groups are currently working on harnessing quantum coherence to develop quantum algorithms, quantum cryptography, quantum metrology, and other quantum technologies.
however, a major obstacle to developing these technologies is to overcome the fragile, fleeting nature of quantum coherence.
while researchers have developed methods to slow down or correct the effects of decoherence, these methods are generally very resource- demanding.
the method presented in the new study does not attempt to slow down or correct decoherence, but instead it reveals a natural mechanism under which resilience to decoherence spontaneously emerges.
the results show that, under certain conditions, quantum coherence remains completely unaffected by common mechanisms of decoherence that typically destroy coherence.
the new mechanism was predicted to exist in a study published last year by some of the same authors.
in the new study, the researchers have experimentally observed this effect for the first time.
the scientists demonstrated the mechanism in composite systems whose subsystems are all affected by decoherence, yet the overall composite system maintains its quantum coherence for as long as desired.
"the trick lies in the fact that local decoherence acts in a preferred direction, which is perpendicular to the one in which coherence is measured," adesso explained.
"consequently, the resulting quantum states are overall degraded by such noise, but their observed coherence remains unaffected during the dynamics if the initial conditions are suitably chosen."
the researchers implemented the method using set- ups that involve room- temperature liquid- state nuclear magnetic resonance (nmr) quantum simulators, and demonstrated the effect in two- and four- qubit ensembles.
"we used two different nmr set- ups," said first author isabela silva, at the university of sao paulo and the university of nottingham.
"the first, owned by ivan oliveira's group in brazil, consisted of a simple chloroform sample labeled with carbon- 13 to encode the two- qubit system in the hydrogen and carbon nuclei.
the four- qubit system was instead a heteronuclear sample specially developed in steffen glaser's group in germany.
to manipulate this four- channel heteronuclear system independently, a prototype nmr probe head was also developed.
both systems are affected by natural and independent dephasing channels.
therefore, once initial quantum states satisfying special constraints are prepared, the quantum coherence freezing can be automatically observed, with no need for external control."
the researchers predict that the surprising effect can occur in larger systems composed of any even number of qubits.
odd- numbered qubit systems do not exhibit the resilience because the specific initial conditions supporting the phenomenon cannot be met due to the different geometry of quantum states in such instances.
the researchers also showed that the mechanism appears to be universal, since it does not depend on the specific measure used to quantify the amount of coherence.
the researchers expect that this trait will make the mechanism especially useful for future applications.
"the universality paves the way toward designing a novel generation of quantum- enhanced devices able to harness coherence for unscathed performance in realistic and adverse conditions," adesso said.
besides technological innovations, the results may also shed light on the quantum coherence that occurs naturally in biological systems, such as the light- harvesting systems in plants.
previous research has shown that some biological systems can maintain quantum coherence for very long times in certain noisy environments.
"the new study raises the possibility that these systems may have evolved an ability to harness natural mechanisms for coherence protection, similar to the one reported here," said coauthor rosario lo franco at the university of palermo in italy.
when a quantum computer is finally able to calculate operations faster than the most powerful supercomputers currently in existence, it will achieve what is known as "quantum supremacy".
google quantum ai lab revealed a new gate- based superconducting quantum computing chip called bristlecone last week with a square array of 72 qubits (a portmanteau for quantum bits).
they are going for quantum supremacy, but they may be a few qubits short.
classical bits versus qubits
qubits are analogous to classical binary bits but are primarily differentiated by their ability to be in a superposition of both 0 and 1 states at the same time, whereas classical bits are either in a 0 state or a 1 state.
for classical bits, the 0 and 1 represent a mutable dichotomy (on and off, yes or no, true or false), however a processed bit is known for its dichotomy of two different dc voltage levels (x volts for the "0" state, y volts for the "1" state).
these switch through what's known as "the forbidden zone", which represents the rate of transference between the two dc voltage levels that represent the two bits in either the "0" state or the "1" state.
though the qubit has a similar dichotomy of states to a classical bit, meaning it can be 0 or 1, the superposition of a qubit is represented by two basis vectors, |0) and |1), which represent [0 1] and [1 0], or "ket 0" and "ket 1".
these basis vectors, or base states, can then be combined into pairs and multiple qubits.a
a key difference between classical bits and qubits is that qubits can hold two bits of information using superdense coding, while a classical bit has a limit of one bit.
using superdense coding, two bits or information are encoding in a state of a single qubit under the assumption that each are sharing an entangled state.
with this assumption, the limit of classical bits that can be encoded in a qubit in two, thus the term "superdense".
this doubling of efficiency per qubit compared to classical bits is why adding qubits to a quantum computer increases its computing power exponentially versus adding classical bits to a classical computer, which does not.
google's bristlecone and the reality of quantum supremacy
the first quantum computer had 2 qubits in 1998, and the previous record for highest qubit count in a quantum computer was 50 qubits by ibm in 2017.
google's john martinis, who is leading their quantum computing efforts believes that achieving quantum supremacy is reachable milestone in the next few months or by the year's end.
well, don't get your hopes up, because there definitely won't be a commercial version available to try out at home anytime soon.
what happens when quantum supremacy is achieved?
first, creating qubits are insanely complicated because their circuits must be made of superconductive material, which needs an environment of ultra- low (in bristlecone's case less than - 450 degrees fahrenheit) temperatures to keep electrical resistance to an absolute minimum.
it's not just temperature that needs to be controlled, it's any level of vibration.
the tiniest vibration can cause qubits to lose their quantum state.
when this happens, the rate of erroneous calculations begins to slink upward.
another factor that affects the ability of a quantum computer to produce errors is the number of qubits.
the more qubits there are, the more error- prone the computer becomes.
more qubits mean more complexity and more power dedicated towards creating the perfect conditions to stabilize the fragile quantum state.
the computational stability is perpetually caught in this extremely fragile house of cards, though enough strides have been made in a few supporting areas including supercooling technology to keep the dream of quantum supremacy within site.
also, the speed of classical supercomputers isn't standing still, and the two most powerful among them sit inside china's borders, so it might be impossible to test against them.
ibm in 2017 believed that a 49- qubit computer would do the trick, but simulations performed on a classical computer indicated that a 100- qubit computer would need to be achieved before quantum supremacy could be achieved, and any useful quantum applications could be developed.
ibm q is an example of an initiative to build commercially available universal quantum computing systems based on a quantum processor prototype.
google, whose majority income is based on advertising revenue, via adwords, is pushing research in a few different bleeding edge technologies like ai, might beat ibm to achieve "quantum supremacy" over china's classical supercomputers with bristlecone.
how to benchmark a quantum computer
how do you create a problem that can't be solved on classical computers to test a quantum computer?
the task should be impossible to perform on classical computers, but martinis and researchers at google are going as far as they can, attempting to solve an algorithm that ranks extremely close to the limits of known classical supercomputers.
then, if they add just one more qubit, they may just pass the fastest supercomputers.
even if google achieves quantum supremacy, what then?
there are virtually zero quantum computing applications that have any usefulness, so besides discovery, on a practical level, what can quantum computers do?
they are difficult to manage, costly to maintain, incredibly fragile and are ridiculous even as hypothetical replacements for less cumbersome classical computers.
classical computers will also continue to improve, and over- hyped technologies can easily fall into yet another layer of technological obscurity through too much media exposure.
most of the legwork that needs to occur to keep a possible 100- qubit quantum stable requires the most focus now, before any quantum applications that are beyond the capabilities of classical supercomputers can be made viable.
potential quantum applications
what are the problems that scale better for quantum computers to solve versus classical computers?
one category might turn out to be machine learning.
algorithms are being developed by companies like d- wave systems to leverage quantum- computing capabilities to perform machine learning tasks better than classical computers, which are already doing extremely sophisticated things like beating the best human go players.
quantum computing could help with optimization problems, which generally revolve around trying to find the best answer out of a complicated and large set of alternatives.
designing molecules might be an example of a near- future quantum application.
nasa is investigating the usefulness of quantum computing to solve problems like locating exoplanets and tackling enormous logistical issues.
bottom line
the problems of superconductivity and tiny vibrations make quantum computing in its current state an unbelievably fragile enterprise, suited only for extremely wealthy companies like google and prestigious legacy companies and organizations that pioneered computing applications, like ibm and nasa.
it's hard to get a grasp even from experts, who disagree on the viability of quantum computing at almost every level.
but google's martinis is determined to break through to achieve quantum supremacy.
after that, he wants to lead the way with proving quantum computing's usefulness by creating some killer quantum apps.
few people doubt the "quantumness" of entanglement.
quantifying the quantum correlation of entanglement is something that is relatively regular right now.
however, things change a bit when it comes to quantum correlations other than entanglement.
however, there is a growing interest in the use of non- entanglement quantum correlations in a number of possible future applications.
"a few years ago, scientists proposed quantum discord as a quantum correlation measure that goes beyond the entanglement paradigm," roberto serra tells physorg.com.
"quantum discord may be present, even in separate, non- entangled states.
however, some doubt was being shed on the quantum qualities of non- entangled states because of the difficulty in quantifying the correlations."
in order to remedy the difficulty in "seeing" the correlations in a laboratory setting, serra, a scientist at the federal university of abc in sao paulo, brazil, worked with a group to create a technique that makes it possible to recognize nonclassical correlations in quantum discord.
serra worked with a team from different brazilian institutions of higher learning, including the brazilian agricultural research corporation's and the brazilian center for physics research both in rio de janeiro, and the physics institute of sao carlos, in sao paulo state.
the results of the work can be seen in physical review letters: experimentally witnessing the quantumness of correlations."
"nuclear magnetic resonance systems at room temperature were used to test principles of quantum computation with a good level of success," serra explains.
"the quantum nature of these demonstrations was questioned because there is no entanglement in such a system.
in our experiment we reveled directly the quantum nature of this system at room temperature.
we used a sample of chloroform molecules, since it's the simplest two- qubit system.
we folded a qubit in the carbon nucleus and another one in the hydrogen nucleus."
next the brazilian scientists were able to manipulate the system.
even though they used hot quantum bits, the system actually works as a quantum mechanical one.
"we displaced the system from the thermal equilibrium by a very tiny deviation, and the phase coherence present there could encode quantum correlations as the measured by the quantum discord," serra says.
"our methods can be applied to another system, such as an optical system.
this can enable us to say if a given system is purely classical in nature, or if it has truly quantum correlations," he continues.
serra thinks that using this test, which is relatively simple to perform in a laboratory setting, could help lay to rest the debate over whether or not these other types of correlations are truly quantum.
"we test the quantumness of discord at room temperature, and this very robust quantumness can be used to get an advantage in quantum protocols," serra insists.
he believes that this method can already be used for metrology.
"we are involved now in a test of principles in quantum metrology using this type of system, and exploiting this very tiny nonclassical correlation.
we are testing those right now, to see about advantages over classical protocols, and we hope to have new results in the next few months."
"we hope to develop future applications, and advance our comprehension about the rule played by this kind of quantumness in tasks as, for example, quantum communications," serra continues.
"we are building collaborations between theoretical and experimental researchers, and we hope that we can do more to show the usefulness of other quantum correlations beyond entanglement."
more information: r. auccaise, j. maziero, l.c.
celeri, d.o.
soares- pinto, e.r.
deazevedo, t.j. bonagamba, r.s.
sarthour, i.s.
oliviera, and r.m.
serra, "experimentally witnessing the quantumness of correlations," physical review letters (2011).
available online: http://link.aps.org/doi/10.110 ... srevlett.107.070501/
a new kind of quantum computer is being proposed by scientists from the tu wien (vienna) and japan (national institute of informatics and ntt basic research labs).
the quantum computer is the holy grail of quantum technology.
its computing power would eclipse even the fastest classical computers we have today.
a team of researchers from tu wien (vienna) the national institute for informatics (tokyo) and ntt basic research labs in japan has now proposed a new architecture for quantum computing, based on microscopic defects in diamond.
a reliable quantum computer capable of solving complex problems would have to consist of billions of quantum systems, and such a device is still out of reach.
but the researchers are convinced that the basic elements of their newly proposed architecture are better suited to be miniaturized, mass- produced and integrated on a chip than previously suggested quantum computing concepts.
experiments towards the new quantum computing architecture are already being undertaken at tu wien.
fragile quantum superpositions
for decades, scientists have been trying to use quantum systems for logical calculations.
"in a classical computer, one bit can only store a number: zero or one.
quantum physics, however, allows superpositions of states.
a quantum bit can be in the state zero and the state one at the same time - and this opens up unbelievable possibilities for computing", says jorg schmiedmayer (tu wien).
such superposition states can be implemented in different kinds of quantum systems, such as ions, captured in electromagnetic traps, or in superconducting quantum bits.
the architecture which has now been published in the journal "physical review x" is different: nitrogen atoms which can occupy two different spin states are injected into a small diamond.
every nitrogen defect is trapped in an optical resonator made of two mirrors.
via glass fibres, photons are coupled to the quantum system consisting of the resonator, the diamond and the nitrogen atom.
this way, it is possible to read and manipulate the state of the quantum system without destroying the quantum properties of the spins in the diamond.
realistic quantum computers need error correction
each system - made up of mirrors, diamond and a nitrogen defect - can store one quantum bit of information: zero, one, or an arbitrary superposition of both.
but usually such a quantum bit is very unstable.
error correction procedures are needed to build a quantum computer that works reliably.
"if error correction is used, a quantum bit cannot be stored in one single quantum particle any more.
instead, a complex architecture of interconnected quantum systems is required", says michael trupke (tu wien).
the researchers calculated how the resonators, diamonds and nitrogen atoms can be assembled to create an error resistant two dimensional quantum system, a so- called "topologically protected quantum computer".
according to the calculations, about 4.5 billion such quantum systems would be sufficient to implement the algorithm "shor- 2048", which is able to calculate prime factors of a 2048- bit- number.
this huge number of quantum elements is required in any quantum computer architecture, no matter whether ion traps, superconducting quantum bits or nitrogen spins in diamonds are used.
"our approach has the big advantage that we know how to make the elements smaller.
this architecture has great potential for miniaturization and mass production", says michael trupke.
"whole industries are working with diamonds, materials science is progressing rapidly.
there are still many obstacles to overcome, but connecting nitrogen spins in solid materials opens up a path that could finally lead to a functioning quantum computer."
only the beginning - just like the transistor
trupke compares the current state of quantum computing with the early days of electronic computing: "when the first transistors were built, nobody could imagine placing them on a small chip by the billions.
today, we carry around such chips in our pockets.
these nitrogen spins in diamond could develop just like transistors did in classical computer science."
at tu wien, researchers have begun to create a small- scale realisation of this new architecture.
"we have the great advantage of being able to collaborate with a number of internationally renowned research teams in materials research and quantum technology right here at tu wien", says jorg schmiedmayer.
friedrich aumayr works on methods to inject the nitrogen atoms into the diamonds, peter mohn obtains numerical data in large- scale computer simulations.
the microcavity arrays are the result of an ongoing collaboration with ulrich schmid at the centre for micro- and nanostructures (zmns) within tu wien.
diamond chips are routinely analysed in the university's own x- ray center.
there may still be a long way to go before algorithms like shor- 2048 run on a quantum computer.
but scientists believe that it should become possible to entangle quantum building blocks, creating larger cluster cells, within the next few years.
"once this happens, the scale- up will be fast", says kae nemoto of the national institute of informatics.
"in the end," schmiedmayer says, "it all depends on whether we manage to enter an era of mass production and miniaturization in quantum technology.
i do not see any physical laws that should keep us from doing that."
imagine a place where anything possible always happens, like a tv screen that displays all the channels at once.
if that seems beyond imagination, you are not alone.
the world of quantum physics is so weird that even the scientists who study it say it challenges everyday concepts of common sense.
the field has grown from a realization that at the smallest scale - the realm where atoms and molecules roam - the classical equations that isaac newton used to describe the physical world no longer apply.
in this realm, matter behaves differently, and many realities can co- exist.
particles like electrons, for instance, occupy several locations at the same time, behaving more like fuzzy waves than solid pebbles.
fortunately, such weirdness mostly confines itself to the inner life of atoms.
but a new quantum world is coming, where scientists hope to preserve the quirky diversity of the subatomic realm.
this would allow them to devise superfast computers, design new drugs and guarantee security for sending secret messages.
harnessing the power of the quantum realm requires coordinated planning from experts in fields ranging from physics and chemistry to electrical engineering.
and that puts usc college's daniel lidar in a perfect position to help prepare for the quantum future.
a physicist with joint appointments in the departments of chemistry and electrical engineering, lidar is a leader in current efforts to transform quantum physics from theoretical curiosity to cutting- edge information technology.
as the son of two scientists (a biochemist and pharmacologist), lidar was constantly exposed to scientific thinking while growing up in israel and holland.
he earned his ph.d. in physics from hebrew university in jerusalem in 1997, and soon thereafter began exploring the emerging field of quantum information theory.
after a postdoctoral position at berkeley and several years on the faculty at the university of toronto, he migrated to usc last fall.
he was drawn by southern california's growing status as the world's leading region for the new quantum research enterprise.
"this is a real hub," he said, noting that usc, caltech and uc santa barbara all boast strong programs.
"southern california is probably the world capital of activity in my field."
in the mid- 1990s, bell labs mathematician peter shor initiated the quantum information revolution by proving that a computer using quantum programming could crack the toughest of today's secret codes, used for governmental, military and financial communication.
about the same time, other research showed that only another quantum system could provide absolute protection against any illicit eavesdropping.
work by lidar and his collaborators has focused on how to protect the delicate process of quantum computing from attack - by nature itself or malicious hackers.
so far, quantum computations have been performed only in rudimentary laboratory experiments.
if feasible on a larger scale, quantum computers could solve some difficult problems at a fraction of the speed of today's fastest supercomputers.
the trick relies on those multiple quantum realities.
like the tv screen showing every channel at once, a quantum computer could process all the numbers in its memory simultaneously, rather than one computation at a time.
it's a bit like finding which of a thousand keys opens a lock; instead of trying one at a time, you could just spin one key in the lock until it opened.
certain problems that would tax a supercomputer for a trillion years could yield to a quantum computer in minutes.
but such speed is available only as long as the multiple quantum calculations can be protected from outside interference.
and the same process nature uses to make rocks and people solid, instead of fuzzy like electrons, conspires to keep that time very, very short.
that process, known as quantum decoherence, is usually an immediate and inevitable result of interaction with the environment - collisions with atoms or mere particles of light can cause a frail ensemble of multiple quantum realities to crash.
lidar and colleagues have shown, though, that some quantum computing set- ups are at least partially immune to the ravages of decoherence.
by designing an apparatus with "decoherence free subspaces," quantum information can be preserved in the face of environmental insults.
the solution is to make sure that external effects exert a symmetric effect on the quantum storage sites.
(if one bit of information is altered, so is its partner, so the two together retain a record of the stored information.)
a more difficult challenge may arise on a future "quantum internet" where quantum computers share data.
nobody had considered the potential for quantum viruses afflicting such a network until last year, when lidar and post- doc lian- ao wu proposed a scheme for fighting such "quantum malware" in a paper to be published in the journal quantum information processing.
"essentially the proposal is to do the analog of backup," said lidar, an associate professor hired as part of the college's senior faculty initiative.
only legitimate users of a system would be told when "real" data is being transmitted.
during the remaining down time, the quantum data could be stored on a secure device, off the network, while bogus transmissions serve as a decoy for intruders.
a hacker would never know when the system was vulnerable, and constant intrusion attempts would be easy to detect.
"it's the first look at this problem," lidar said, and much further work will be needed to devise foolproof protection and a quantum virus cleanser if infection is successful.
for now, of course, quantum viruses are of no serious concern, as there is no quantum network to attack.
but lidar foresees a growing likelihood that quantum technology will soon play a significant role in sending secure messages and eventually in computing.
"it's a field that is likely to have a widespread impact in the context of secure information transmission," he said.
"it is the most secure method of information transmission that we know of."
as for quantum computing, its advantages are limited to certain types of problems; quantum computers are likely never to be good for word processing.
but they could prove valuable in economically important realms such as designing drugs from scratch, by computing the quantum rules governing how biological molecules interact.
any such uses depend, of course, on effective hardware for building quantum computing devices, which might require advances in nanotechnology approaches for fabricating the necessary materials.
thus while lidar focuses on theory, he emphasizes the need to develop the experimental side of the field as well.
"my dream for usc would be to develop not only as a leading theoretical place, which i believe it is .
.
.
but also to strongly develop the experimental capabilities here," he said.
"that would really put us on the map."
one of the widely accepted properties of quantum entanglement is secrecy.
since scientists and researchers began working with quantum key distribution, entanglement has been considered an essential part of keeping communications private.
what if entanglement didn't always mean secrecy, though?
new work is shedding light on the nature of entanglement and quantum key distribution - and possibly proving that a high degree of entanglement does not necessarily lead to complete secrecy.
"entanglement, or quantum correlation, is responsible for enabling quantum key distribution, a method to distribute a perfectly secret key that can be used to encrypt messages," matthias christandl tells physorg.com via email.
"we have now discovered that not every type of entanglement is useful for quantum key distribution... [w]e showed that sometimes particles can be very strongly entangled, but at the same time be nearly useless for cryptography!"
christandl is a scientist who recently moved from the university of munich, germany, to eth zurich, switzerland.
christandl worked with norbert schuch at the max planck institute for quantum physics in garching, germany, and with andreas winter at the university of bristol in the u.k. and the national university of singapore to illustrate an example of a quantum state that is entangled, but that does not offer the secrecy needed for quantum cryptography.
their work appears in physical review letters: "highly entangled states with almost no secrecy."
so far, the findings are only a theoretical observation.
"we studied the possibilities that the world of single atoms and weak bursts of light offers when security is concerned," explains christandl.
the two particles that are used in this theoretical exploration are in what christandl calls an "antisymmetric state."
observing this entangled state in the laboratory may not be something that occurs anytime really soon, though.
"this state may be difficult to create in the laboratory," says christandl, "as one needs a high number of maximally entangled particles in order to create the state.
...however, the insights gained in this work might offer a route to new and better protocols for quantum cryptography."
christandl and his colleagues are looking at ways to better understand how secrecy works in quantum physics.
the idea that, theoretically, even high degrees of quantum entanglement may not lead to complete secrecy, could change the way that scientists view quantum cryptography.
it could also encourage the study of different quantum states to determine which are better for quantum key distribution, and which should be avoided for such purposes.
while there are no plans to set up an experiment to test this idea, christandl thinks that the theoretical findings can help advance the study of quantum physics.
"a better understanding might lead to novel quantum protocols and quantum technologies in the future."
the next step for christandl and his peers is to look for entanglement that offers no secrecy.
"this would push our findings to the extreme," he says, "offering an analog of the famous 'bound entanglement.'
such a finding would conclusively revise the view that entanglement and secrecy go hand in hand."
more information: matthias christandl, norbert schuch, and andreas winter, "highly entangled states with almost no secrecy," physical review letters (2010).
available online: http://link.aps.org/doi/10.110 ... ysrevlett.104.240405
scientists at the department of energy's oak ridge national laboratory are conducting fundamental physics research that will lead to more control over mercurial quantum systems and materials.
their studies will enable advancements in quantum computing, sensing, simulation, and materials development.
the researchers' experimental results were recently published in physical review b rapid communication and optics letters.
quantum information is considered fragile because it can be lost when the system in which it is encoded interacts with its environment, a process called dissipation.
scientists with ornl's computing and computational sciences and physical sciences directorates and vanderbilt university have collaborated to develop methods that will help them control- or drive- the "leaky," dissipative behavior inherent in quantum systems.
"our goal is to develop experimental platforms that allow us to probe and control quantum coherent dynamics in materials," said benjamin lawrie, a research scientist in the quantum sensing team in ornl's quantum information science group.
"to do that, you often have to be able to understand what's going on at the nanoscale."
bringing perspectives from quantum information science, nanoscience and electron microscopy, the scientists exploit existing knowledge of matter and the physics of light and sound to examine the quantum nature of nanostructures- structures that measure about one- billionth of a meter.
one project focused on driving nitrogen vacancy center defects in nanodiamonds with plasmons.
the naturally occurring defects are created when a nitrogen atom forms in place of the typical carbon atom, adjacent to an atomless vacancy.
the defects are being investigated for use in tests of entanglement, a state that will allow substantially more information to be encoded in a quantum system than can be accomplished with classical computing.
electrons generate an electric field.
when an electron beam is applied to a material, the material's electrons are spurred to motion- a state called excitation- creating a magnetic field that can then be detected as light.
working with plasmons, electron excitations that couple easily with light, allows scientists to examine electromagnetic fields at the nanoscale.
matthew feldman, a vanderbilt university graduate student conducting doctoral research at ornl through the national defense science and engineering graduate fellowship program and a member of the quantum sensing team, used a high- energy electron beam to excite nitrogen vacancy centers in diamond nanoparticles, causing them to emit light.
he then used a cathodoluminescence microscope owned by ornl's materials science and technology division, which measures the visible- spectrum luminescence in irradiated materials, to collect the emitted photons and characterize high- speed interactions among nitrogen vacancy centers, plasmons and vibrations within the nanodiamond.
in other research, jordan hachtel, a postdoctoral fellow with ornl's center for nanophase materials sciences, used the cathodoluminescence microscope to excite plasmons in gold nanospirals.
he explored how the geometry of the spirals could be harnessed to focus energy in nanoscale systems.
andy lupini served the project as a microscopy consultant, providing expertise regarding equipment optimization and troubleshooting.
precise control over nanoscale energy transfer is required to enable long- lived entanglement in a model explored by eugene dumitrescu, a research scientist in ornl's quantum information science group.
dumitrescu's research, published in physical review a in late 2017, showed that the photon statistics feldman collected could be used in calculations to show entanglement.
"this work advances our knowledge of how to control light- matter interactions, providing experimental proof of a phenomenon that had previously been described by simulations," lawrie said.
closed systems, in which quantum information can be kept away from its surroundings, theoretically can prevent dissipation, but real- world quantum systems are open to numerous influences that result in information leakage.
"the elephant in the room in discussions of quantum systems is decoherence," feldman said.
"if we can model an environment to influence how a quantum system works, we can enable entanglement."
dumitrescu agreed.
"we know quantum systems will be leaky.
one remedy is to drive them," he said.
"the driving mechanisms we're exploring cancel out the effects of dissipation."
dumitrescu used the analogy of a musical instrument to explain the researchers' attempts to control quantum systems.
"if you pluck a violin string, you get the sound, but it begins to dissipate through the environment, the air," he said.
"but if you slowly draw the bow across the string, you get a more stable, longer- lasting sound.
you've brought control to the system."
feldman thinks these are fascinating times for quantum physicists because the field of quantum computing is at the same phase classical computing was in the mid- 20th century.
"what excites me most is how current research could change our understanding of quantum systems and materials," he said.
more information: matthew a. feldman et al, colossal photon bunching in quasiparticle- mediated nanodiamond cathodoluminescence, physical review b (2018).
doi: 10.1103/physrevb.97.081404
jordan a. hachtel et al.
polarization- and wavelength- resolved near- field imaging of complex plasmonic modes in archimedean nanospirals, optics letters (2018).
doi: 10.1364/ol.43.000927
building a slide deck, pitch, or presentation?
here are the big takeaways:
google has announced the release of the 72- qubit square array bristlecone quantum processor, which the company believes is adequate to demonstrate quantum supremacy.
bristlecone is the evolution of google's prior 9- qubit linear quantum processor, which had error rates of 1% for readout, 0.1% for single- qubit gates, and 0.6% for two- qubit gates.
google unveiled its bristlecone quantum processor monday at the annual american physical society meeting in los angeles.
the traditional gate- based processor is intended as a test platform for researching error rates and scalability of google's approach to implementing qubits in quantum systems.
one of the primary encumbrances to building a quantum computer is the comparatively high error rate that quantum processors are prone to.
these systems require extremely precise environmental controls, as fluctuations in the operating environment can cause errors in a given computation to occur.
presently, reliably repeating operations in computers with a relatively small number of qubits is the first challenge to overcome before quantum computers can be scaled beyond tens of qubits.
google's previous design was a linear nine- qubit system, which the company was able to refine to relatively low error rates.
according to google, it achieved best case error rates of 1% for readout, 0.1% for single- qubit gates, and 0.6% for two- qubit gates.
the bristlecone processor scales the same design for coupling, control, and readout in a square array of 72 qubits.
see: it leader's guide to the future of quantum computing (tech pro research)
on bristlecone, google intends to facilitate the development of quantum algorithms on real hardware rather than simulated hardware, as well as investigate first and second level error correction schemes, and attempt to demonstrate quantum supremacy in the future.
quantum supremacy is the threshold at which a quantum computer performs in a way that traditional computers are practically incapable of doing (or, otherwise, the use of a traditional computer for a given task would be infeasible.)
as a basic type of benchmark, quantum supremacy can be established by executing shor's algorithm to factor integers with a superpolynomial speedup compared to the fastest algorithm executable on a traditional computer.
google plans to use an apparently original test to establish quantum supremacy.
according the announcement:
our theory team has developed a benchmarking tool for [quantum supremacy].
we can assign a single system error by applying random quantum circuits to the device and checking the sampled output distribution against a classical simulation.
if a quantum processor can be operated with low enough error, it would be able to outperform a classical supercomputer on a well- defined computer science problem.
google believes that quantum supremacy is possible at 49 qubits, with a circuit depth higher than 40, and a two- qubit error rate less than 0.5%.
that said, bristlecone is a transitory, proof- of- concept processor, which, according to the researchers, "requires harmony between a full stack of technology ranging from software and control electronics to the processor itself.
getting this right requires careful systems engineering over several iterations."
of note, the achievement of quantum supremacy would be one step closer to breaking public- key cryptography, as schemes such as rsa are built around the notion that factoring large numbers is practically too difficult.
that said, this type of factorization requires a quantum computer with thousands of qubits, which is beyond what is presently available.
other organizations are also researching quantum computing techniques.
ibm has a 50- qubit machine in development, as well as a cloud service allowing researchers to test quantum algorithms.
d- wave advertises their line of quantum computers as having thousands of qubits, though these systems are designed specifically for quadratic unconstrained binary optimization.
d- wave's definition and measurement of qubit does not strictly conform to the standard used by academics and other companies, making the comparison inexact.
also see
"a quantum memory is a crucial component of future quantum information processing technologies.
among these technologies, a quantum communications system based on light will enable vastly improved performance over conventional systems, and allow quantum computers to be connected," ian walmsley tells physorg.com.
walmsley is a scientist at the university of oxford in the united kingdom.
"building such as system will require a means to effect the temporary storage of single light quanta - photons."
one of the obstacles involved in building a quantum memory is that it is difficult to get a true quantum effect due to the decoherence associated with noise, when working at room temperature.
"with previous demonstrations in this quantum regime, ultracold atomic gases or cryogenic solid state materials have been used as a storage medium," walmsley explains.
walmsley is part of a group working to create room temperature solutions for quantum computing.
the group, working out of clarendon laboratory at oxford, includes klaus reim, patrick michelberger, ka chung lee, joshua nunn and nathan langford as well as walmsley.
the results of their efforts can be seen in physical review letters: "single- photon- level quantum memory at room temperature."
"our breakthrough is two- fold," walmsley explains.
"using a so- called raman interaction allowed a dramatic increase in potential bandwidth.
the second advantage is that you can use these warm vapors, allowing quantum operation at room temperature."
in order to make their quantum memory work, walmsley and his colleagues store information in the collective state of atoms in a warm vapor.
"this concept has been around for years, but we are looking at how to make it work practically at room temperature."
the team at oxford uses a strong off- resonant control pulse to store a weak quantum light pulse.
"it's two step," he says.
"we put in the quantum light with a control pulse.
because their frequencies are tuned out of resonance with the atoms, neither is absorbed without the other.
this allows you larger bandwidth."
"additionally, because neither is absorbed without the other, you don't have extraneous atoms that have absorbed energy from the control pulse alone, and which can then give away this energy in the form of noise photons," walmsley continues.
"it's these extra photons that have added to the noise in previous attempts, and been a deal- breaker for room- temperature quantum memories."
the use of warm atomic cesium vapors show that quantum operation can be achieved in ambient conditions.
walmsley says that, already, their technique offers applications.
"even though the memory isn't perfect yet, there are some things we can do, like entanglement distillation."
he explains that he thinks that this technique could improve the efficiency of quantum repeaters.
"the idea of a quantum repeater has been around for about 12 years now, but without a memory you get a degree of degraded quality so that signals are lost.
theoretically, our system could make quantum repeaters a reality."
room temperature quantum memory would be a great step forward for quantum communications and quantum information processing.
quantum repeaters might need to be placed in remote areas, or in areas that are warm.
reliable quantum memory will be needed in the coming years as secure quantum communications are in greater demand.
walmsley hopes that his group can be at the forefront of turning the possibilities into realities.
"while there are things we can do now, there is still a great deal of room for improvement," he says.
"we want to improve efficiency, and the stability of the memory."
another important point will be to shrink the technology.
"we want to miniaturize it so that it is small enough to integrate into fiber optic networks," walmsley continues.
"this is a definite breakthrough, but we still have some way to go."
more information: k.f.
reim, p. michelberger, k.c.
lee, j. nunn, n.k.
langford, and i.a.
walmsley, "single- photon- level quantum memory at room temperature," physical review letters (2011).
available online: http://link.aps.org/doi/10.110 ... ysrevlett.107.053603
the world's first entanglement of quantum bits (qubits) in a solid- state superconducting josephson junction circuit has been reported by university of maryland researchers.
though far from a quantum logic circuit and even farther from a quantum computer, the demonstration holds hope that engineering improvements could someday produce such a computer.
"our findings indicate that you could use josephson junctions to build a quantum computer," said professor fred wellstood, leader of the project and director of the university's center for superconductivity research.
others contributing to the project include andrew berkley, mark gubrud, joseph foley, matt kenyon, jan olaf gaudestad, huizhong xu and roberto ramos.
unlike the usual physical states such as voltage or current used to encode information in electronic circuits, quantum states have only a probable existence until they are measured.
that counters intuition, but it also offers a high degree of parallelism since all possible states of an elementary particle have a probable existence simultaneously.
there are many different quantum states- from electron spin direction to photon polarization angles- but all share the common ability to exist in a nebulous state that is not resolved until an observation of them is made.
by encoding "1s" and "0s" in these unresolved nebulous states, quantum computers promise to perform parallel operations on both values in a single step.
quantum entanglement is important because it enables the final result to be read out without disturbing the sequence of parallel operations.
wellstood said entanglement blurs the distinction between individual particles so that it is impossible to describe the particles separately no matter how far away they are physically separated.
"we view entanglement as essential to quantum computing because it packs more information into quantum bits than is possible with classical computing bits.
six quantum bits, for instance, can represent 64 pieces of information," said berkley.
wellstood claims his approach "scales up" more easily than competing methods because it is based on solid- state electronic devices, not free- floating subatomic particles.
by demonstrating entanglement between two josephson junctions, wellstood said he has provided important evidence that quantum computers are possible.
the josephson junction device used by the team is composed of two superconductors separated by an insulating layer so thin that electrons can tunnel through it.
otherwise he uses the same techniques used to make conventional ics, concluding that his approach is well suited for scaling up.
"we can go to the thousands of devices you need to build a real working quantum computer," said wellstood.
wellstood's work builds on that of at&t's albert chan, whose nearest- neighbor qubits were shown to be capable of interbit coupling that potentially performs calculations without resolving the qubits nebulous quantum state.
qunnect, inc., a deep tech company committed to globalizing quantum communications, received a $1.5m phase- ii small business innovation research award from the us department of energy.
qunnect is developing a device suite to enable ultra- secure, long- distance quantum communications.
all products are engineered to operate at room temperature, a critical design requirement for real- world deployment of quantum technologies.
the company's first product, a quantum memory device, can store, coherently manipulate, temporally synchronize, and retrieve quantum- states on- demand.
the phase- ii award supports the development of features to integrate and use these devices on standard telecom fiber bed infrastructure.
through collaborations with brookhaven national laboratory and esnet, qunnect's products will be field- tested on fiber networks connecting brookhaven national lab to new york city.
as a stand- alone product, the quantum memories will function as a quantum buffer, similar to a data buffer in traditional telecom, regulating data flow at different nodes in the network.
the quantum buffers will also be an integral component in the company's plan to build a quantum repeater to enable quantum entanglement- based communications, shattering the distance- limitations currently experienced by first- generation quantum communications technologies.
"we champion the department of energy's vision to build the quantum internet, and we are very appreciative to have their support," said qunnect ceo mehdi namazi.
"qunnect is committed to engineering field- stable devices that enable long- distance, quantum- secure communication on the existing telecom infrastructure.
we believe this hybrid approach will accelerate the adoption of quantum technologies by early users, who will then develop the next generation of quantum communication protocols."
about qunnect inc.
qunnect was founded in 2017 by eden figueroa, mehdi namazi, mael flament and robert brill to commercialize the technologies of the quantum information sciences & technology laboratory at stony brook university.
noel goddard leads operations.
the company closed a $800k financing round in 2019 which included quantonation and accelerate ny seed fund.
qunnect is a client of the clean energy business incubator program (cebip) at stony brook university, which is funded by the ny state energy research and development agency (nyserda).
their r&d facility is located in the center for excellence in wireless information technology, in stony brook, ny.
the company is also a member of new lab, the deep tech innovation incubator located in the brooklyn navy yard in new york city.
with quantum computing on the rise, we take a look at the technology and at industry leader, honeywell, to see what's happening in this revolutionary space.
recently, at their 2019 ignite conference, microsoft announced a new service known as "azure quantum," a partnership between microsoft and honeywell that provides users with cloud access to the quantum computers of honeywell, ionq, and qci.
alongside google, ibm, and a handful of others, microsoft is considered one of the leaders in the field of quantum computing, and this new service includes access to their open- source quantum development kit (utilizing q#, microsoft's quantum computer programming language), marking the service as a potentially attractive choice for potential developers.
what is quantum computing?
at the very core of quantum computing lies the qubit.
where traditional computing methods store and quantify basic information using bits in binary values of 0 or 1, quantum computing utilizes qubits.
qubits, like binary bits, still possess the two possible outcomes of "0" or "1".
by the principle of quantum superposition, however, quantum computing states that these two quantum states of "0" or "1" can be superposed on one- another and result in another valid quantum state.
additionally, the reverse is also true, meaning that any quantum state can also be represented as the sum of any two (or more) quantum states.
as a result, qubits can exist in this superposition, and send two bits worth of information as a single unit of quantum information, or one qubit.
because of this, the parallel nature of quantum computers allows them to work on potentially millions of computations at once instead of operating on the binary level of our modern computers.
this has caused quantum computing to become perhaps the most promising emergent computational model for large- scale information processing.
quantum computing in industry
quantum computers are far more suited to industrial and commercial applications over small- scale use for, say, personal devices.
the promise of quantum computing in industry is an unparalleled ability to run calculations millions of times faster than standard computers, providing industry solutions in cybersecurity, database analytics, trade optimization, artificial intelligence, and even code- breaking.
as mentioned earlier in the article, there is a select group of leaders in the quantum computing space.
giants such as google, microsoft, ibm, hewlett- packard, and intel are racing to be the first to break the barrier and achieve "quantum supremacy."
additionally, ibm has already begun selling the use of their quantum supercomputer to commercial and research clients, showing us that commercial quantum computing is becoming more of a reality by the day.
honeywell and quantum computing
according to their quantum solutions page, honeywelll has been active in the quantum computing space for more than a decade.
as tony uttley, president of honeywell quantum solutions, explained last year, the company began investigating the technology because they already had many of the proficiencies needed: "state- of- the- art hardware and software control systems, advanced electronics, optics and photonics..." honeywell researchers proposed that combining some of honeywell's existing technologies would allow them to build a quantum computer of their own.
uttley also explained his view on a more sweeping reason it makes sense for honeywell to develop quantum computers: "...at its heart, quantum computing is a controls problem."
"...at its heart, quantum computing is a controls problem."
honeywell's trapped ion technology
the company refers to themselves as "future shapers" in this space, which can be seen in their trapped- ion quantum technology.
essentially, honeywell's quantum computers use trapped ions as qubits to complete computations and transfer information.
their industry- leading solution has led to record- setting fidelity, and their newly- developed parallel quantum computing has further increased the speed of qubit operations.
they've accomplished this feat by incorporating multiple parallel, independent operating zones for the trapped ions via multiple optical beams.
the future of quantum computing
the recent partnership with microsoft is a step for honeywell towards the commercialization of this technology.
as far back as 2016, ibm has allowed engineers access to their quantum computer via the cloud.
honeywell will do the same in early 2020, utilizing microsoft's cloud technology to give access to their own computers to allow public use for program development.
uttley said last year that among the first industries that could benefit from this technology is pharmaceuticals, where quantum computing could allow for much faster r&d.
it could also, however, affect many other industries, including the optimization of manufacturing and oil and gas.
honeywell
while quantum computing is still in its infancy, it is becoming more of a reality with each passing day.
it may only be a matter of time before automation functions via quantum algorithms in a facility near you.
how familiar are you with quantum computing?
will you be checking out honeywell's program to try your hand at developing quantum algorithms?
share your thoughts in the comments below.
scientists at the university of sydney have for the first time demonstrated improvement in quantum computers by using codes designed to detect and discard errors in the logic gates of such machines.
"this is really the first time that the promised benefit for quantum logic gates from theory has been realised in an actual quantum machine," said dr robin harper, lead author of a new paper published this week in the journal, physical review letters.
quantum logic gates are formed by entangled networks of a small number of quantum bits, or qubits.
they are the switches that allow quantum computers to run algorithms, or recipes, to process information and perform calculations.
dr harper and his colleague professor steven flammia, from the school of physics and university of sydney nano institute, used ibm's quantum computer to test error detection codes.
they demonstrated an order of magnitude improvement in reducing infidelity, or error rates, in quantum logic gates, the switches that will form the basis of fully functioning quantum computers.
dr jay gambetta, ibm fellow and principal theoretical scientist with ibm q, said: "this paper is a great example of how scientists can use our publicly available cloud systems to probe fundamental problems.
here harper and flammia show that ideas of fault tolerance can be explored on real devices we are building and already deploying, today."
quantum technologies are still in their infancy but promise to revolutionise computing in the 21st century by performing calculations thought to be beyond the ability of the largest and fastest supercomputers.
they will do this using the unusual properties of matter at the quantum scale that allow them to process information using qubits.
these are computing elements that utilise the fact that quantum objects can exist in an indeterminate state, known as superposition, and can become 'entangled', a phenomenon describing behaviour not seen in conventional computers.
however, electronic 'noise' easily disrupts these states, quickly producing errors in quantum computations, which makes development of useful machines very difficult.
"current devices tend to be too small, with limited interconnectivity between qubits and are too 'noisy' to allow meaningful computations," dr harper said.
"however, they are sufficient to act as test beds for proof of principle concepts, such as detecting and potentially correcting errors using quantum codes."
whereas the classical switches in your laptop or mobile phone can run for many years without error, at this stage quantum switches begin to fail after just fractions of a second.
"one way to look at this is through the concept of entropy," said professor flammia.
"all systems tend to disorder.
in conventional computers, systems are refreshed easily and reset using dram and other methods, effectively dumping the entropy out of the system, allowing ordered computation," he said.
"in quantum systems, effective reset methods to combat entropy are much harder to engineer.
the codes we use are one way to dump this entropy from the system," said professor flammia, who today was awarded the pawsey medal by the australian academy of science.
using codes to detect and discard errors on ibm's quantum device, dr harper and professor flammia showed error rates dropping from 5.8 percent to 0.60 percent.
so rather than one in 20 quantum gates failing, just one in 200 would fail, an order of magnitude improvement.
"this is an important step forward to develop fault tolerance in quantum systems to allow them to scale up to meaningful devices," dr harper said.
the physicists, who are both researchers with the arc centre of excellence for engineered quantum systems, emphasised that this was a demonstration of fault tolerant gates on pairs of qubits.
"there is still a long way to go before the quantum community can demonstrate fault tolerant computing," dr harper said.
he said that other groups have shown improvements in other facets of quantum devices using codes.
the next step is to synthesise and test these approaches on larger- scale devices of a few dozen qubits that enable the reuse and reinitialisation of qubits.
companies such as ibm, google, rigetti and ionq have started or are about to start allowing quantum researchers to test their theoretical approaches on these small, noisy machines.
"these experiments are the first confirmation that the theoretical ability to detect errors in the operation of logical gates using quantum codes is advantageous in present- day devices, a significant step towards the goal of building large- scale quantum computers," dr harper said.
quantum corp. (nasdaq: qmco) today announced achievement award winners for its top global channel partners in media and entertainment.
the awards recognize sales achievements for quantum's worldwide reseller partners in fy2020.
"our partners are on the front line for helping our customers realize the value quantum solutions bring to solve their most pressing needs, and this was another strong year of momentum and growth for quantum's channel worldwide," said james mundle, channel chief, quantum.
"at virtualq | nab partners 2020, we reaffirm our commitment to deliver compelling and innovative solutions that address the challenges of explosive video and unstructured data growth.
the channel has been a critical part of quantum's success.
i'm excited about the solutions and recent acquisitions that we're highlighting at this year's event, and i am already looking forward to next year when we hopefully can return to celebrating our successes in person."
quantum's top global partners in media and entertainment for 2020:
global partner of the year: integrated media technologies inc. (imt).
imt brings expertise in media asset management and workflow consulting, pre- sales, sales, and quantum certified professional services and managed services.
imt is a trusted extension of quantum's sales force, and achieved the highest revenue in annual sales among all of quantum channel partners in fy2020.
north american partner of the year: fcpworks.
fcpworks is an integrator and marketing consultancy focused on apple and final cut pro x based professional video solutions.
fcpworks' understanding and experience with cutting- edge 4k and beyond workflows with final cut is unique in the post- production world.
fcpworks not only deployed one of the first f- series systems last year, but also sold into multiple large deployments.
north american growth partner of the year: diversified.
diversified is a leading systems and media technology integration company.
from initial design consultation to deployment to managed services, diversified is one of quantum's most trusted partners, achieving more than 216% yoy growth in fy2020.
apj media and entertainment partner of the year: media village tech.
media village tech is a leading systems and media technology integration company located in korea.
they are using quantum's f- series to accelerate development of virtual reality (vr) content to be delivered over 5g cellular networks.
emea media and entertainment partner of the year: jigsaw24.
jigsaw24's quantum business has grown significantly year- over- year.
the experience and technical knowledge they have demonstrated around complex media workflows means they're able to tackle challenges at all stages of production and post.
jigsaw24 has a longstanding partnership with quantum and continues to build upon this strong foundation.
latin america media and entertainment partner of the year: seal telecom.
seal telecom is a brazilian multinational which integrates the most advanced technologies and end- to- end solutions through 11 affiliates serving brazil, argentina, mexico, chile, colombia, andean region and usa.
in fy2020 seal telecom grew their business with quantum by 256%.
join quantum at virtualq | nab partners 2020
due to the covid- 19 pandemic, in place of quantum's traditional pre- nab partner event held in las vegas, the company will host a video event for partners on april 23 at 9:00 am pdt / 12:00 pm edt.
during this session quantum executives and key team members will share information on the upcoming virtualq | nab end user event, including tips on how to schedule appointments with customers; learning sessions; quantum's vision for "changed workflows forever," and important updates to quantum's product roadmap and marketing initiatives.
about quantum
quantum technology and services help customers capture, create and share digital content - and preserve and protect it for decades.
with solutions built for every stage of the data lifecycle, quantum's platforms provide the fastest performance for high- resolution video, images, and industrial iot.
that's why the world's leading entertainment companies, sports franchises, researchers, government agencies, enterprises, and cloud providers are making the world happier, safer, and smarter on quantum.
see how at www.quantum.com.
quantum, the quantum logo are registered trademarks of quantum corporation and its affiliates in the united states and/or other countries.
all other trademarks are the property of their respective owners.
"safe harbor" statement: this press release contains "forward- looking" statements.
all statements other than statements of historical fact are statements that could be deemed forward- looking statements.
specifically, but without limitation, statements regarding our partner's continued growth with quantum, and how that growth may effect quantum's financials, and our partner's success in accelerating the development of new type of content, such as virtual reality content are forward- looking statements within the meaning of the safe harbor.
all forward- looking statements in this press release are based on information available to quantum on the date hereof.
these statements involve known and unknown risks and uncertainties, including the ongoing impact of the covid- 19 pandemic on the economy, which is outside of quantum's control, the company's ability to grow revenue and extend its global reach through its partners and continue the company's transformation, the company's ability to develop and deliver the solutions needed to meet the increasing demand for managing video and unstructured data, and other factors that may cause quantum's actual results to differ materially from those implied by the forward- looking statements.
more detailed information about these risk factors are set forth in quantum's periodic filings with the securities and exchange commission, including, but not limited to, those risks and uncertainties listed in the section entitled "risk factors," in quantum's annual report on form 10- k filed with the securities and exchange commission on august 6, 2019.
quantum expressly disclaims any obligation to update or alter its forward- looking statements, whether as a result of new information, future events or otherwise, except as required by applicable law.
researchers at crann and the school of physics at trinity college dublin have created an innovative new device that will emit single particles of light, or photons, from quantum dots that are the key to practical quantum computers, quantum communications, and other quantum devices.
the team has made a significant improvement on previous designs in photonic systems via their device, which allows for controllable, directional emission of single photons and which produces entangled states of pairs of quantum dots.
qubits and the promise of quantum computing
the promise of quantum computers leverages the properties of quantum bits - "qubits" - to execute computations.
current computers process and store information in bits of either 0s or 1s whereas qubits can be 0 and 1 simultaneously.
that means quantum computers will have much greater computational powers over and above classical computers.
scientists are exploring different options and designs to make quantum computing a viable reality.
one proposed idea utilises photonic systems, making use of quantum properties of light at the nanoscale, as qubits.
the trinity team explores such a system in their recently published paper in the high- profile journal nano letters.
their system utilises single photons of light emitted in a controlled fashion in time and space from quantum emitters (nanoscale materials known as quantum dots).
for applications such as quantum computing, it is necessary to control emissions from these dots and to produce quantum entanglement of emission from pairs of these dots.
quantum entanglement is a fundamental property of quantum mechanics and occurs when a pair or group of particles are quantum- mechanically linked in a way such that the quantum state of each particle of the pair cannot be described independently of the state of the others.
essentially, two entangled quantum dots can emit entangled photons.
professor john donegan, crann and trinity's school of physics, said:
"the device works by placing a metal tip within a few nanometers of a surface containing the quantum dots.
the tip is excited by light and produces an electric field of such enormous intensity that it can greatly increase the number of single photons emitted by the dots.
this strong field can also couple emission from pairs of quantum dots, entangling their states in a way that is unique to quantum emitters of light."
the other significant advantage is the mechanism by which the device works over current state- of- art photonic devices for quantum computing applications.
professor ortwin hess, professor of quantum nanophotonics in trinity's school of physics and crann, added: "by scanning the metal tip over the surface containing the quantum dots, we can generate the single photon emission as required.
such a device is much simpler than current systems that attempt to fix a metal tip, or a cavity, in close proximity to a quantum dot.
we now expect that this device and its operation will have a striking effect on research in quantum emitters for quantum technologies."
story source:
materials provided by trinity college dublin.
note: content may be edited for style and length.
honeywell (nyse: hon) today announced it has achieved a breakthrough in quantum computing that accelerates the capability of quantum computers and will enable the company to release the world's most powerful quantum computer within the next three months.
the company also announced it has made strategic investments in two leading quantum computing software providers and will work together to develop quantum computing algorithms with jpmorgan chase.
together, these announcements demonstrate significant technological and commercial progress for quantum computing and change the dynamics in the quantum computing industry.
within the next three months, honeywell will bring to market the world's most powerful quantum computer in terms of quantum volume, a measure of quantum capability that goes beyond the number of qubits.
quantum volume measures computational ability, indicating the relative complexity of a problem that can be solved by a quantum computer.
when released, honeywell's quantum computer will have a quantum volume of at least 64, twice that of the next alternative in the industry.
in a scientific paper that will be posted to the online repository arxiv later today and is available now on honeywell's website, honeywell has demonstrated its quantum charge coupled device (qccd) architecture, a major technical breakthrough in accelerating quantum capability.
the company also announced it is on a trajectory to increase its computer's quantum volume by an order of magnitude each year for the next five years.
this breakthrough in quantum volume results from honeywell's solution having the highest- quality, fully- connected qubits with the lowest error rates.
"building quantum computers capable of solving deeper, more complex problems is not just a simple matter of increasing the number of qubits," said paul smith- goodson, analyst- in- residence for quantum computing, moor insights & strategy.
"quantum volume is a powerful tool that should be adopted as an interim benchmarking tool by other gate- based quantum computer companies."
honeywell chairman and chief executive officer darius adamczyk said companies should start now to determine their strategy to leverage or mitigate the many business changes that are likely to result from new quantum computing technology.
"quantum computing will enable us to tackle complex scientific and business challenges, driving step- change improvements in computational power, operating costs and speed," adamczyk said.
"materials companies will explore new molecular structures.
transportation companies will optimize logistics.
financial institutions will need faster and more precise software applications.
pharmaceutical companies will accelerate the discovery of new drugs.
honeywell is striving to influence how quantum computing evolves and to create opportunities for our customers to benefit from this powerful new technology."
to accelerate the development of quantum computing and explore practical applications for its customers, honeywell ventures, the strategic venture capital arm of honeywell, has made investments in two leading quantum software and algorithm providers cambridge quantum computing (cqc) and zapata computing.
both zapata and cqc complement honeywell's own quantum computing capabilities by bringing a wealth of cross- vertical market algorithm and software expertise.
cqc has strong expertise in quantum software, specifically a quantum development platform and enterprise applications in the areas of chemistry, machine learning and augmented cybersecurity.
zapata creates enterprise- grade, quantum- enabled software for a variety of industries and use cases, allowing users to build quantum workflows and execute them freely across a range of quantum and classical devices.
honeywell also announced that it will collaborate with jpmorgan chase, a global financial services firm, to develop quantum algorithms using honeywell's computer.
"honeywell's unique quantum computer, along with the ecosystem honeywell has developed around it, will enable us to get closer to tackling major and growing business challenges in the financial services industry," said dr. marco pistoia, managing director and research lead for future lab for applied research & engineering (flare), jpmorgan chase.
honeywell first announced its quantum computing capabilities in late 2018, although the company had been working on the technical foundations for its quantum computer for a decade prior to that.
in late 2019, honeywell announced a partnership with microsoft to provide cloud access to honeywell's quantum computer through microsoft azure quantum services.
honeywell's quantum computer uses trapped- ion technology, which leverages numerous, individual, charged atoms (ions) to hold quantum information.
honeywell's system applies electromagnetic fields to hold (trap) each ion so it can be manipulated and encoded using laser pulses.
honeywell's trapped- ion qubits can be uniformly generated with errors more well understood compared with alternative qubit technologies that do not directly use atoms.
these high- performance operations require deep experience across multiple disciplines, including atomic physics, optics, cryogenics, lasers, magnetics, ultra- high vacuum, and precision control systems.
honeywell has a decades- long legacy of expertise in these technologies.
today, honeywell has a cross- disciplinary team of more than 100 scientists, engineers, and software developers dedicated to advancing quantum volume and addressing real enterprise problems across industries.
honeywell (www.honeywell.com) is a fortune 100 technology company that delivers industry- specific solutions that include aerospace products and services; control technologies for buildings and industry; and performance materials globally.
our technologies help aircraft, buildings, manufacturing plants, supply chains, and workers become more connected to make our world smarter, safer, and more sustainable.
for more news and information on honeywell, please visit www.honeywell.com/newsroom.
researchers from the tyndall national institute in cork have created micro- structures shaped like small pyramids that can create entangled photons.
does this mean that quantum computers are closer than we realize?
eve of quantum computing
quantum computers have been the stuff of science fiction for the past few decades.
in recent times, quantum computers have slowly become more of a reality with some machines successfully solving real world problems such as games and path finding algorithms.
but why are quantum computers so desired by tech firms and why is there so much research into the field?
silicon has been incredibly loyal to the tech world for the past 50 years, giving us the point contact transistor in 1947.
now, silicon is at the center of technology with computers, tablets, smartphones, the iot, and even everyday items.
in fact, you cannot walk down a city street without being in range of some wi- fi network or influence from a small silicon device.
however, silicon, for all its uses, has started to reach its maximum potential with moore's law starting to fracture.
it has been estimated that within the next 10- 15 years, transistor count will no longer increase at the expected rate and will eventually level off, attaining some maximum value.
when this happens, devices will not become more powerful thanks to transistor increase and this will directly impact how technology advances.
while there are some methods that can be employed to continue the trend in increasing computer power (such as the use of hardware solutions to increase software speed and more efficiently written code), they can only go so far.
other more exotic methods include the use of computers which are designed in a similar fashion to brains that can adapt to specific problems.
one computer design, however, is particularly sought after as it could increase computational power exponentially.
this computational power comes in the form of quantum computing.
many are mislead into believing that quantum computers will replace classical computing- based devices (i.e., just about every computer ever made)- this is a myth.
quantum computers, if introduced to the public, would work alongside classical devices as they are very good at solving specific problems.
for example, a quantum computer could be used to solve encryption algorithms much faster than classical computers but running a program like word or chrome would be better done on an intel i7 instead of a d- wave quantum computer.
the challenges of quantum computing
given all of the interest and research into quantum computers, why are they not currently available to the public or even to big businesses and tech companies?
the issue comes down to how quantum computers work and their requirements.
firstly, quantum devices typically require conditions that simply do not exist outside laboratories.
for example, such devices rely on superconductors which need to be kept extremely cold (as low as 0.02k).
secondly, materials that express quantum properties such as entanglement and superposition are needed.
attaining such materials has been achieved using many techniques including trapping silicon atoms in diamond to produce quantum emitters.
however, these techniques do not currently provide mass production capabilities, unlike silicon chips which can be made in the billions with relative ease.
one research team from the tyndall national institute have devised a method for creating structures that can produce entangled photons.
nanostructure pyramid photon emitters
using common semiconductor fabrication techniques and easily obtained materials, the team have created small pyramid structures called "dots" that can emit entangled photons in a specific direction.
what makes this research critical to quantum computers is the ability to direct the entangled photos while being able to control the emission via an external electrical source.
if classical computers are to interact with quantum computers, the two need to be able to communicate which is why it is imperative that quantum information can be generated using classical methods and then encoded back into electrical current.
quantum dot led cross- section.
image courtesy of roisin kelly, tyndall national institute, university college cork
the tiny pyramids are made using epitaxial growth (growth of a crystal structure on top of a pre- existing crystal layer), inside an inverted pyramid structure that is patterned using standard lithography on a 111 (crystal orientation) gaas substrate.
this production technique allows the pyramids to be manufactured by most (if not all) semiconductor manufacturers which is crucial for mass production.
it also allows for the devices to be designed on a piece of silicon that could also hold transistors and other components which brings quantum computers a step closer to being a reality.
the quantum leds.
image courtesy of tung- hsun chung, tyndall national institute, university college cork
read more
basic analysis of a quantum processor
record- breaking quantum logic gate
summary
the big step that these quantum dots represent is that they allow the control and production of particles that can be used for quantum computing and can easily be manufactured with current technology.
while the devices on their own cannot process quantum information, they could play a big part in future quantum devices.
whatever happens, it can be said that quantum power will be needed soon if silicon stagnates in the near future.
at ces 2019, the tuesday keynote by ibm ceo ginni rometty, walked attendees through the ibm lens of core areas of computing: and one of the four was quantum computing- which comes as no surprise for those who have been following ibm's aggressive role in taking it out of the labs and into the hands of those wishing to explore it more.
reporting for venturebeat, kyle wiggers said "since 2016, as part of its ibm q initiative, it's offered access to a quantum computer located in yorktown heights, new york, that's executed more than seven million quantum programs, the results of which have been cited in more than 120 published academic papers.
and that's just the start, rometty said."
ibm revealed its intentions for a data center mainly consisting of quantum computers.
the quantum computational data center in poughkeepsie, new york, will be a world's first.
the center will be for commercial clients- and hold that thought.
arvind krishna, senior vice president of hybrid cloud and director of ibm research, has spoken of their work to develop "practical quantum applications for business and science."
this certainly resonate with the noise ibm has made in the past, that we need not resign ourselves to quantum computing as an out of reach concept primarily seen in labs.
last year, ibm already was spreading the word that it saw quantum computing going mainstream within five years.
ibm has had some quantum devices and simulators available for use through the cloud.
meanwhile, ibm also unveiled q system one.
information age reporter andrew ross said it was "the world's first integrated universal approximate quantum computing system designed for scientific and commercial use.
"what a time to be alive!
is quantum computing no longer just a concept or theory?"
he asked.
that question can be assumed to be rhetorical.
for ibm, it is no longer just a concept or theory.
ibm is sharing details of what went into the design of q system one for commercial use.
the case is 9 feet tall and 9 feet wide.
a half- inch thick borosilicate glass forms a sealed, airtight enclosure.
the glass door opens effortlessly.
the team assembled the system for mechanical testing at goppion's headquarters in milan over the course of two weeks in the summer.
goppion is a milan- based manufacturer of high- end museum display cases.
frederic lardinois in techcrunch: "the 20- qubit system combines into a single package the quantum and classical computing parts it takes to use a machine like this for research and business applications."
quantum computers are a different kind of computer based on the laws of quantum mechanics.
that said, what would ibm have in mind that would make a difference for business and science in a newer commercialization phase?
an ibm video suggests the commercialization move could enable breakthroughs in materials and drug discovery, financial services and artificial intelligence.
many would find it difficult to think about quantum computing on their computers when goals are confined to print out forms or check the weather or write a report.
"but try asking one to simulate a single molecule like caffeine to understand how it impacts out brains?"
asks a presenter in an ibm research video.
encoding information into quantum states, quantum computers make calculations that we only dream of today.
("right now, we're not quite there yet," said lardinois, "but the company also notes that these systems are upgradable.)"
such as?
simply consider there are problems "too complex and exponential in nature for classical systems to handle," in the words of an ibm news release.
"future applications of quantum computing may include finding new ways to model financial data and isolating key global risk factors to make better investments, or finding the optimal path across global systems for ultra- efficient logistics and optimizing fleet operations for deliveries."
what's next in ibm's plan?
in the second half of this year, expect to hear about the opening of the ibm q quantum computational center in poughkeepsie.
it will house advanced cloud- based quantum computing systems.
these will be accessible to members of the ibm q network.
the latter is described as a worldwide community of fortune 500 companies, startups, academic institutions, and national research labs, exploring practical applications for business and science.
ibm says it's ready to go commercial with its ibm q quantum computer, offered to date for free on a cloud- enabled quantum computing platform.
at the heart of the emerging commercial offering, ibm is featuring a prototype 50 quantum bit (qubit) processor along with its 20- qubit processor already offered online.
although a true "universal" quantum computer is years away, the new offering provides a platform for research and development of new computing approaches based on quantum computing.
quantum computing is considered to pose both promise and threat in offering a solution to advanced problem sets beyond the capability of existing computers - even supercomputers.
for example, elliptic curve cryptography (ecc) has gained popularity because it offers a combination of relatively low overhead and high security.
ultimately, the security of ecc and other crypto algorithms relies on the inability of current supercomputers to unravel encrypted messages to find the encryption key (the elliptic curve discrete logarithm problem).
quantum computers can execute the kind of compute- intensive algorithms required to break ecc and other security algorithms.
although universal quantum computers might not appear for another decade, ibm is also providing a software development kit designed to allow development of quantum computing programs without the need for a quantum computer.
available as an open- source repository on github, the ibm quantum information software kit (qiskit) is a python- based library that lets developers create quantum computing programs.
developers can run their programs on simulators or on actual hardware using the ibm q platform.
read more about ibm q on embedded's sister site, ee times.
building a slide deck, pitch, or presentation?
here are the big takeaways:
researchers at microsoft are competing with alphabet, ibm, and others to create less error- prone quantum computers.
microsoft is taking a different approach than other companies, believing their method will allow them to make quantum computers that are commercially viable.
microsoft is slowly making headway in the race toward commercially- viable quantum computing, tapping into the unique properties of a certain particle to address issues engineers at many tech companies have been struggling with for decades.
alphabet, ibm, and a number of smaller companies are all competing for "quantum supremacy," a disputed term referring to the point at which quantum computers will be able to handle calculations beyond the capacity of the world's best supercomputers.
"[quantum supremacy] is very catchy, but it's a bit confusing and oversells what quantum computers will be able to do," simon benjamin, a quantum expert at oxford university, told mit's technology review.
he added that even as the abilities of quantum computers improve, classic computers will still be faster and cheaper.
"using a quantum computer would be like chartering a jumbo jet to cross the road," benjamin said.
see: it leader's guide to the future of quantum computing (tech pro research)
quantum computing relies on quantum bits or qubits, which store information.
classic computers store information as either 1s or 0s, but qubits are special because they "can exist in multiple states of 1 and 0 at the same time- a phenomenon known as superposition," according to mit's technology review.
"what this boils down to is that even though a few extra bits make only a modest difference to a classical computer's power, adding extra qubits to a quantum machine can increase its computational power exponentially," they wrote.
"that's why, in principle, it doesn't take all that many qubits to outgun even the most powerful of today's supercomputers."
the problems come in creating and maintaining qubits, which require conditions akin to outer space and can lose their fragile quantum state, making them more likely to be significantly error- prone, according to mit while scientists and engineers are working to deal with this through software and additional qubits, the exponential errors that come with higher numbers of qubits have kept commercially- viable quantum computing a distant dream.
"if you had 50 or 100 qubits and they really worked well enough, and were fully error- corrected- you could do unfathomable calculations that can't be replicated on any classical machine, now or ever," robert schoelkopf, a yale professor and founder of a company called quantum circuits, told the mit technology review.
"the flip side to quantum computing is that there are exponential ways for it to go wrong."
alphabet announced earlier this month, at the annual american physical society meeting in los angeles, that their quantum computing chip "bristlecone" had 72 qubits, surpassing ibm's 50- qubit processor released last year.
but the more qubits you add, the more potential errors that come along with it, leading some researchers to ask how the computers can be trusted with calculations that cannot be checked by any other machine.
alphabet has tried to address this issue by performing calculations right at the edge of what most supercomputers are capable of, believing that if they can show they can get that far, the addition of even one more qubit would prove the quantum computer could vastly outperform a classic computer.
microsoft is trying something different than the other companies, focusing their attention on majorana fermions, particles that can be their own antiparticle, according to bloomberg.
when these particles are used in quantum computers, the error rates are significantly lower than those of the machines heralded by alphabet and ibm.
the goal, according to microsoft, is to eventually rent access to quantum computers for a variety of purposes, including artificial intelligence (a), drug design through chemistry, and financial industry models.
while alphabet and ibm are racing to get to quantum supremacy first, microsoft is focusing on how to make quantum computers viable machines that can be used in any number of fields and produce accurate information, the bloomberg report noted.
microsoft has not even been able to create a qubit yet, but they believe that once they can, their quantum computer will be 10,000 times more accurate than anything currently being produced by alphabet, ibm, and d- wave systems inc., which was the first company to sell limited- application quantum computers.
todd holmdahl, a microsoft executive, told bloomberg that they are at most five years away from producing a quantum computer that could be sold on the market.
also see
research in battery chemistry is getting a boost from quantum computing that in turn is furthering research to improve quantum computing.
scientists from ibm and mitsubishi chemical have simulated the initial steps of the reaction mechanism between lithium and oxygen in lithium- air (li- air) batteries.
it's the first research of its kind to have been simulated on a quantum computer as outlined in a paper recently published on arxiv, computational investigations of the lithium superoxide dimer rearrangement on noisy quantum devices, and it lays the groundwork for modeling the entire lithium- oxygen reaction on a quantum computer.
understanding the underlying mechanism of the li- air battery chemistry could lead to significantly more efficient batteries for everything from mobile devices to cars.
the capabilities of quantum computing were being tested as well.
specifically, the variational quantum eigensolver algorithm was used to perform computational investigations on the rearrangement of the lithium superoxide dimer with both quantum simulators and quantum devices.
while the research demonstrated that quantum simulators could obtain energy values similar to the exact ones, the calculations on quantum hardware underestimated energies - even after the application of readout error mitigation.
mitsubishi chemical was already tackling the problem with classical computing before approaching ibm, said dr. gavin jones, ibm q ambassador and a research staff member in the quantum applications in chemistry and science group at ibm research - almaden.
the problem is that such a large chemical system requires more qubits (quantum bits) than there is room for in any of ibm's quantum computers.
"we needed to figure out a way to actually reduce the number of qubits down to some level where we could actually use a quantum computer or quantum simulators to study the problem," jones said.
the research paper outlines the qubit reductions and the use of quantum simulators as well as quantum hardware to study the problem, and then checks the accuracy of the simulators and the calculations being done on the quantum hardware, said jones.
"we showed that with simulators we could do this really, really accurately."
however, once the research team moved on to the quantum hardware, which is noisy and prone to decoherence errors, the reactant energy was underestimated.
"we want to be able to use much better methods to study this process, so we can get better answers."
better hardware goes together with that, he said, and in turn drives the need for better software.
molecular structures of focus in computational investigations of the lithium superoxide dimer rearrangement on noisy quantum devices, as part of the process of lithium peroxide formation (source: ibm research)
a functional battery relies on lithium dioxide, but lithium peroxide can form instead.
the goal of studying the battery from the electrochemical perspective is to reduce the formation of undesirable lithium peroxide.
it may take years to be able to study the entire problem with quantum computing, said jones, but ultimately it will help researchers understand how to prevent lithium peroxide formation.
"maybe we could add something in, some sort of additive that will prevent that from happening.
maybe there's something else that we can do - from an electrochemical perspective - that could improve that," jones said.
li- air battery research collaboration between industry and academia goes back to the mid- 1990s, said dr. naoki yamamoto, associate professor and the chair of the keio university quantum computing center, which is part of the ibm q hub.
"because both the charge and discharge process in the lithium- air battery are very complicated and sensitive to the surrounding environment, it is still hard to elucidate the reaction mechanism at the atomic level, which [has] limited the progress of the technology."
prior to turning to quantum computing, he said, reactants in the initial steps of the charge and discharge process were modeled using small lithium- oxygen molecular clusters, and the reaction mechanism was studied by using a classical computer.
"these new study results accurately predict the reaction surface, but the density functional theory - which is a widely- used approach - is not good enough."
this means a much higher level of quantum mechanical theory is needed in order to accurately predict the properties of the li- air battery using a computational approach, said yamamoto, but such theory adds significantly to the computational expense of calculations on classical computing.
this year, researchers have been able to study the initial step of the charge process in the li- air battery on a real quantum computing device.
the results to date - combined with the anticipated improvement of quantum volume in the next few years - could make it possible to examine specific properties of the li- air battery.
so far, he said, the researchers have been able to more accurately measure reactant energy in a li- air battery by studying it on a real quantum computing device.
reactant energy had previously been underestimated by about 20 kcal/mol in real device, and the researchers were able to reduce that value to about 6 kcal/mol.
the next step is to further reduce the value of underestimation calculated with the real device to less than 1 kcal/mol.
in japan, said yamamoto, there are some large industry- academic projects in progress for bringing the technology of the li- air battery to the market.
however, there remains significant work ahead in materials science to make an efficient battery.
"i hope quantum computing can help us effectively explore such new materials in the near future."
related articles
gary hilson is a freelance writer and editor who has written thousands of words for print and pixel publications across north america.
his areas of interest include software, enterprise and networking technology, research and education, sustainable transportation, and community news.
his articles have been published by network computing, informationweek, computing canada, computer dealer news, toronto business times, strategy magazine, and the ottawa citizen.
a localization phenomenon boosts the accuracy of solving quantum many- body problems with quantum computers.
these problems are otherwise challenging for conventional computers.
this brings such digital quantum simulation within reach using quantum devices available today.
quantum computers promise to solve certain computational problems exponentially faster than any classical machine.
"a particularly promising application is the solution of quantum many- body problems utilizing the concept of digital quantum simulation," says markus heyl from max planck institute for the physics of complex in dresden, germany.
"such simulations could have a major impact on quantum chemistry, materials science and fundamental physics."
within digital quantum simulation, the time evolution of the targeted quantum many- body system is realized by a sequence of elementary quantum gates by discretizing time evolution, a process called trotterization.
"a fundamental challenge, however, is the control of an intrinsic error source, which appears due to this discretization," says markus heyl.
together with international colleagues, they showed in a recent science advances article that quantum localization by constraining the time evolution through quantum interference strongly bounds these errors for local observables.
more robust than expected
"digital quantum simulation is thus intrinsically much more robust than what one might expect from known error bounds on the global many- body wave function," heyl says.
this robustness is characterized by a sharp threshold as a function of the utilized time granularity measured by the so- called trotter step size.
the threshold separates a regular region with controllable trotter errors, where the system exhibits localization in the space of eigenstates of the time- evolution operator, from a quantum chaotic regime where errors accumulate quickly rendering the outcome of the quantum simulation unusable.
"our findings show that digital quantum simulation with comparatively large trotter steps can retain controlled trotter errors for local observables," says markus heyl.
"it is thus possible to reduce the number of quantum gate operations required to represent the desired time evolution faithfully, thereby mitigating the effects of imperfect individual gate operations."
this brings digital quantum simulation for classically challenging quantum many- body problems within reach for current day quantum devices.
ibm is expanding its quantum computing community with the addition of several startups from around the world to its ibm q network.
the new members include two companies from canada, quantum benchmark and 1qbit, as well as zapata computing, strangeworks, qc ware,q- ctrl, qxbranch, and cambridge quantum computing.
ibm research made the announcement this week at its ibm q summit silicon valley event in palo alto, calif.
the network was launched late last year and builds on the interest ibm has received for its ibm q experience website, in place since may 2016, which has garnered a great deal of interest, said bob sutor, vice president for ibm q strategy and ecosystem at ibm research.
"people can go and really use the quantum computer," he said.
"that's very significant because quantum computing has been theory for a long time."
in a telephone interview with ee times, sutor said when the site debuted nearly two years it was hard to predict who would be interested in using a quantum computer, but it now has 80,000 people doing a range of things.
initially, the program was geared toward "premier users" such as jp morgan chase and daimler, who really wanted very early access to the top of the line machines that could be produced by ibm research, as well as universities who might operate as hubs.
"the hubs that help us extend the reach of the q network and individual partners who would want to partner with ibm," sutor said.
now ibm q is looking to open up to the startup community, said jeff welser, a director, ibm research - almaden.
"there are a large number of startups that are interested in thinking about the software and applications that can be opened up with the capability of quantum computing," welser said.
this includes everything from building the toolkit to the middleware that helps make these things programmable and useful, he said, and expanding the network is next natural step in in growing the eco system around quantum computing.
"it's all about scaling, all about getting as many of the smart people in the world, and all the different types of organizations thinking about this.
the eco system has to be much bigger, from algorithms to applications, in many different fields," he said.
ibm q's dilution refrigerator for cooling, a key challenge in quantum computing, as photographed by graham carlow 2018 asce.
welser said ibm is being "excruciatingly honest" about the status of quantum computing today.
"this is going to play out over the next few years, in fact decades," he said.
"we have to really work with all these partners figure out what are the applications and when will quantum computers be powerful enough to really show a discernible difference from what you can do today using classical computers."
welser said ibm is also being open about is the importance of not just building and chasing the number of cubits produced - those cubits must improve the error rates, otherwise volume doesn't mean a better system.
from a hardware perspective, quantum computing can use a lot of ecosystems already in place, said welser, but the members of the ibm q network will be taking on new challenges.
"you have to have extremely efficient cooling systems and you have to be able to route signals in and out of a very cold environment, without introducing a new noise into what's happening there," he said.
"we are seeing more people thinking about the various components that do that."
what's different about quantum computing is that it's not digital technology, it's analog.
but quantum computing will be simplified in that the hardware will be part of a cloud- based system, said welser.
"we want something we can be building in a data center than can actually run for multiple users at once on the cloud," he added.
"that will be the model that will certainly dominate in quantum, given the infrastructure necessary."
on the software side, it also a completely different technology, said sutor.
there are a range gates that are based on specific quantum mechanical behavior.
tied in with these gates, and depending on how many cubits you have, there is a programming language that sits on top of them.
"you are literally describing a set of gates that operate on multiple cubits and from that as with traditional you build up higher and higher constructs," he said.
"right now, we are at the lower levels - the algorithmic levels, and the beginning of the library levels."
ibm scientists work in the ibm q computation center at the thomas j watson research center in yorktown heights, new york.
the new center houses ibm's most advanced quantum computers, which are accessible via the ibm cloud to organizations participating in the ibm q network.
(photo by connie zhou)
sutor said the startups joining the q network are a mixed bag making all parts of the ecosystem, with some focused on tools and libraries, and other focused on the middle of quantum computing, to enable all the software stacks, to be on top of it.
quantum benchmark is a venture- backed software company led by a team of the top research scientists and engineers in quantum computing, with headquarters in kitchener- waterloo, canada.
it's focused on solutions to mitigate the inevitable errors that arise during a quantum computation, which will lead to incorrect output.
the company's true- q technology helps users determine the quantum advantage that is achievable with any given quantum computing hardware for any application of interest.
quantum benchmark founder and ceo joseph emerson also thinks it's important to manage expectations around quantum computing because although it provides new capabilities not offered by classic computing, it's error prone in new ways.
"any quantum computing of the future will require very sophisticated error correction strategies, software and algorithms to kind of suppress these uniquely quantum errors that arise and kind of spoil the magic of the quantum computation," emerson said.
his research specialty for almost two decades now has been finding ways to drive and identify and suppress those errors, and it could be another decade before quantum computing can completely compensate for these errors.
emerson said it's possible that in five years quantum computing will have some tangible real- world impacts, and it will start by solving niche problems.
"we're at the tip of the ice berg in terms of understanding the capabilities of quantum computing," emerson said.
- gary hilson is a general contributing editor with a focus on memory and flash technologies for ee times.
related articles:
gary hilson is a freelance writer and editor who has written thousands of words for print and pixel publications across north america.
his areas of interest include software, enterprise and networking technology, research and education, sustainable transportation, and community news.
his articles have been published by network computing, informationweek, computing canada, computer dealer news, toronto business times, strategy magazine, and the ottawa citizen.
yale university researchers have demonstrated how to build a quantum computer operating on quantum bits, or qubits, which hold a superposition of quantum states.
the computer uses a superconducting "cooper box" to store oscillating microwave photons which can be read and written without disturbing their quantum states.
qubits based on the superposition of quantum states can be used to make integrated circuits.
"heisenberg's uncertainty principle says you can't measure the velocity and position of a particle, and likewise in qed [quantum electrodynamic] circuits you can't measure the voltage and the current at the same time," explained yale university professor steven girvin.
quantum computers derive their power from enabling a superposition of quantum states to simultaneously perform many parallel operations.
those operations allow quantum computers to perform tasks like breaking encryption codes that are impossible for digital computers.
many quantum- state mechanisms have been demonstrated in physics labs, some of which could serve as building blocks for future quantum computers.
likewise, yale's "qutons," or "qubit on a photon," invention may enable quantum computers to be placed on chips even sooner.
the advantages of yale's method include the relatively small size of its qubit repositories about a square micron and the ability to read a qubit's state without disturbing it the bane of quantum computers to date.
the yale approach stored qubits in a cooper- box with over 1 billion superconducting aluminum atoms acting together, thereby providing a kind of quantum momentum that allowed a "probing" photon to read out a qubit's state from the cooper- box without changing its state.
the researchers predict that their cookbook for quantum computing circuitry using quantum electrodynamics could spawn a range of lab experiments.
soon, the researchers predicted, experiments will test quantum circuitry for a new breed of quantum ics using optics and photons that interface on- chip with traditional electronic circuits.
quantum computingcloud
at its annual re: invent conference yesterday, amazon web services (aws) unveiled three new services aimed at advancing the state of quantum computing.
"nearly a decade ago, i wrote about the quantum compute cloud on april fool's day.
the future has arrived and you now have the opportunity to write quantum algorithms and to run them on actual quantum computers," wrote jeff barr in an aws announcement.
quantum computing has made some significant advances this year, meaning that amazon is throwing its hat into an already quite crowded ring.
both ibm and google made massive investments in quantum computing this year, both bringing the technology into the public discussion in october as google claimed to have achieved quantum supremacy, only to have ibm roundly refute it.
it's a time when the race to dominate the quantum computing field is hotly contested.
there's just one problem: aws doesn't actually have a quantum computer.
google has the sycamore and the bristlecone, ibm has its q system one, and aws has a plan.
as an industry leader of cloud solutions, aws is happy to sell quantum as a service, acting as a middleman between researchers and companies who want to "tinker around with qubits" and three companies - d- wave, ionq and rigetti - who all have their own supercomputers.
see also:
as a result, aws is moving forward with three new services based around its quantum offering.
amazon braket - a fully managed service that allows scientists, researchers, and developers to begin experimenting with computers from multiple quantum hardware providers in a single place.
bra- ket notation is commonly used to denote quantum mechanical states, and inspired the name of the service.
aws center for quantum computing - a research center adjacent to the california institute of technology (caltech) that will bring together the world's leading quantum computing researchers and engineers in order to accelerate development of quantum computing hardware and software.
amazon quantum solutions lab - a new program to connect aws customers with quantum computing experts from amazon and a very select set of consulting partners.
"we believe that quantum computing will be a cloud- first technology and that the cloud will be the main way customers access the hardware," said charlie bell, senior vice president, utility computing services, aws, said in a press release.
"with our amazon braket service and amazon quantum solutions lab, we're making it easier for customers to gain experience using quantum computers and to work with experts from aws and our partners to figure out how they can benefit from the technology.
and with our aws center for quantum computing and academic partnerships, we join the effort across the scientific and industrial communities to help accelerate the promise of quantum computing."
the development of technologies which can process information based on the laws of quantum physics are predicted to have profound impacts on modern society.
for example, quantum computers may hold the key to solving problems that are too complex for today's most powerful supercomputers, and a quantum internet could ultimately protect the worlds information from malicious attacks.
however, these technologies all rely on "quantum information," which is typically encoded in single quantum particles that are extremely difficult to control and measure.
scientists from the university of bristol, in collaboration with the technical university of denmark (dtu), have successfully developed chip- scale devices that are able to harness the applications of quantum physics by generating and manipulating single particles of light within programmable nanoscale circuits.
these chips are able to encode quantum information in light generated inside the circuits and can process the "quantum information" with high efficiency and extremely low noise.
this demonstration could enable a significant boost in the ability to produce more complex quantum circuits that are required in quantum computing and communications.
their work, published in the journal nature physics and available for free in preprint form on the arxiv preprint server, hosts a range of quantum demonstrations.
in one of the breakthrough experiments, researchers at the university of bristol's quantum engineering technology labs (qet labs) demonstrate the quantum teleportation of information between two programmable chip for the first time, which they remark is a cornerstone of quantum communications and quantum computing.
quantum teleportation offers quantum state transfer of a quantum particle from one place to another by utilizing entanglement.
teleportation is not only useful for quantum communication but is a fundamental building- block of optical quantum computing.
establishing an entangled communication link between two chips in the lab however has proven to be highly challenging.
bristol co- author dan llewellyn said: "we were able to demonstrate a high- quality entanglement link across two chips in the lab, where photons on either chip share a single quantum state.
"each chip was then fully programmed to perform a range of demonstrations which utilize the entanglement.
"the flagship demonstration was a two- chip teleportation experiment, whereby the individual quantum state of a particle is transmitted across the two chips after a quantum measurement is performed.
this measurement utilizes the strange behavior of quantum physics, which simultaneously collapses the entanglement link and transfers the particle state to another particle already on the receiver chip."
another co- author, dr. imad faruque, also from bristol, added: "based on our previous result of on- chip high quality single- photon sources, we have built an even more complex circuit containing four sources.
"all of these sources are tested and found to be nearly identical emitting nearly identical photons, which is an essential criterion for the set of experiments we had performed, such as entanglement swapping."
the results showed extremely high- fidelity quantum teleportation of 91 percent.
in addition, the researchers were able to demonstrate some other important functionality of their designs, such as entanglement swapping (required for quantum repeaters and quantum networks) and four- photon ghz states (required in quantum computing and the quantum internet).
according to co- author dr. yunhong ding, from dtu, low loss, high stability, and excellent controllability are extremely important for integrated quantum photonics.
he said: "this experiment was made possible because of the state of the art low- loss silicon photonics technology based on high- quality fabrication at the dtu."
lead author, dr. jianwei wang, now at peking university, said: "in the future, a single si- chip integration of quantum photonic devices and classical electronic controls will open the door for fully chip- based cmos- compatible quantum communication and information processing networks."
more information: daniel llewellyn et al.
chip- to- chip quantum teleportation and multi- photon entanglement in silicon, nature physics (2019).
doi: 10.1038/s41567- 019- 0727- x
chip- to- chip quantum teleportation and multi- photon entanglement in silicon.
https://arxiv.org/abs/1911.07839
a new computer program that spots when information in a quantum computer is escaping to unwanted states will give users of this promising technology the ability to check its reliability without any technical knowledge for the first time.
researchers from the university of warwick's department of physics have developed a quantum computer program to detect the presence of 'leakage', where information being processed by a quantum computer escapes from the states of 0 and 1.
their method is presented in a paper published today (19 march) in the journal physical review a, and includes experimental data from its application on a publicly accessible machine, that shows that undesirable states are affecting certain computations.
quantum computing harnesses the unusual properties of quantum physics to process information in a wholly different way to conventional computers.
taking advantage of the behaviour of quantum systems, such as existing in multiple different states at the same time, this radical form of computing is designed to process data in all of those states simultaneously, lending it a huge advantage over conventional computing.
in conventional computing, quantum computers use combinations of 0s and 1s to encode information, but quantum computers can exploit quantum states that are both 0 and 1 at the same time.
however, the hardware that encodes that information may sometimes encode it incorrectly in another state, a problem known as 'leakage'.
even a miniscule leakage accumulating over many millions of hardware components can cause miscalculations and potentially serious errors, nullifying any quantum advantage over conventional computers.
as a part of a much wider set of errors, leakage is playing its part in preventing quantum computers from being scaled up towards commercial and industrial application.
armed with the knowledge of how much quantum leakage is occurring, computer engineers will be better able to build systems that mitigate against it and programmers can develop new error- correction techniques to take account of it.
dr animesh datta, associate professor of physics, said: "commercial interest in quantum computing is growing so we wanted to ask how we can say for certain that these machines are doing what they are supposed to do.
"quantum computers are ideally made of qubits, but as it turns out in real devices some of the time they are not qubits at all -  but in fact are qutrits (three state) or ququarts (four state systems).
such a problem can corrupt every subsequent step of your computing operation.
"most quantum computing hardware platforms suffer from this issue -  even conventional computer drives experience magnetic leakage, for example.
we need quantum computer engineers to reduce leakage as much as possible through design, but we also need to allow quantum computer users to perform simple diagnostic tests for it.
"if quantum computers are to enter common usage, it's important that a user with no idea of how a quantum computer works can check that it is functioning correctly without requiring technical knowledge, or if they are accessing that computer remotely."
the researchers applied their method using the ibm q experience quantum devices, through ibm's publicly accessible cloud service.
they used a technique called dimension witnessing: by repeatedly applying the same operation on the ibm q platform, they obtained a dataset of results that could not be explained by a single quantum bit, and only by a more complicated, higher dimensional quantum system.
they have calculated that the probability of this conclusion arising from mere chance is less than 0.05%.
while conventional computers use binary digits, or 0s and 1s, to encode information in transistors, quantum computers use subatomic particles or superconducting circuits known as transmons to encode that information as a qubit.
this means that it is in a superposition of both 0 and 1 at the same time, allowing users to compute on different sequences of the same qubits simultaneously.
as the number of qubits increases, the number of processes also increases exponentially.
certain kinds of problems, like those found in codebreaking (which relies on factoring large integers) and in chemistry (such as simulating complicated molecules), are particularly suited to exploiting this property.
transmons (and other quantum computer hardware) can exist in a huge number of states: 0, 1, 2, 3, 4 and so on.
an ideal quantum computer only uses states 0 and 1, as well as superpositions of these, otherwise errors will emerge in the quantum computation.
dr george knee, whose work was funded by a research fellowship from the royal commission for the exhibition of 1851, said: "it is quite something to be able to make this conclusion at a distance of several thousand miles, with very limited access to the ibm chip itself.
although our program only made use of the permitted 'single qubit' instructions, the dimension witnessing approach was able to show that unwanted states were being accessed in the transmon circuit components.
i see this as a win for any user who wants to investigate the advertised properties of a quantum machine without the need to refer to hardware- specific details."
in order to determine how fast quantum technologies can ultimately operate, physicists have established the concept of "quantum speed limits."
quantum speed limits impose limitations on how fast a quantum system can transition from one state to another, so that such a transition requires a minimum amount of time (typically on the order of nanoseconds).
this means, for example, that a future quantum computer will not be able to perform computations faster than a certain time determined by these limits.
although physicists have been investigating different quantum speed limits for different types of quantum systems, it has not been clear what the best way to do this is, or how many different quantum speed limits there are.
now in a new paper published in physical review x, diego paiva pires et al., from the uk and brazil, have used techniques from information geometry to show that there are an infinite number of quantum speed limits.
they also develop a way to determine which of these speed limits are the strictest, or in other words, which speed limits offer the tightest lower bounds.
as the researchers explain, the search for the ultimate quantum speed limits is closely related to the very nature of time itself.
"in recent years, there has been an intense theoretical and experimental research activity to understand, on one hand, a fundamental concept in quantum mechanics such as time, and to devise, on the other hand, efficient schemes for the implementation of quantum technologies," coauthor gerardo adesso, at the university of nottingham, told phys.org.
"a basic question that combines and underpins both areas of research is: 'how fast can a quantum system evolve in time?'
establishing general and tight quantum speed limits is crucial to assess how fast quantum technologies can ultimately be, and can accordingly guide in the design of more efficient protocols operating at or close to the ultimate bounds."
in order to determine how fast a quantum system can evolve from one state to another, it's necessary to be able to distinguish between the two states, and there are multiple ways to do this.
in the new study, the physicists used a general method based on information geometry.
from a geometric perspective, two distinguishable states can be represented by two points on the surface of some shape, such as a sphere or other manifold.
previous research has shown that there are an infinite number of corresponding metrics that can be used to measure the distinguishability of two quantum states.
in the new study, the physicists have shown that each of these metrics corresponds to a different quantum speed limit.
the "strictest" quantum speed limit is determined by the metric that gives the shortest distance (also known as a 'geodesic') between the two points, or states, as measured along the manifold's curved surface.
"a different quantum speed limit arises from each of these metrics in such a way that the tightest bound for a given dynamics is specified by the metric whose geodesic is best tailored to the given dynamical path," explained coauthor marco cianciaruso, also at nottingham.
overall, the new approach unifies most of the previous results by interpreting them under a single, new framework.
on one hand, the researchers could derive the tightest bounds to date on quantum speed limits for some relevant instances, such as quantum bits undergoing dissipative and decohering evolutions.
on the other hand, they could also show that bounds that have been previously proposed for other instances are truly the optimal bounds- no tighter bounds will ever be found.
in the future, the researchers plan to experimentally investigate the quantum speed limits derived here using nuclear magnetic resonance (nmr) techniques.
"our findings are expected to have an impact on the fields of quantum information, computation, simulation, and metrology," said diogo soares- pinto at the sao carlos institute of physics, who supervised the project.
a recent report claimed google has built a quantum computer that was able to perform a calculation in three minutes that would have taken about 10,000 years for today's fastest computer.
google hasn't commented.
according to financial times, a paper by google researchers was posted recently on nasa's website before being taken down.
it claimed to perform the calculation giving its quantum computer the designation of "quantum supremacy."
industry survey
covid- 19 impact on sensors & electronics industry
as your resource for sensors and electronics news, fierceelectronics is committed to our community and want to bring you the latest information on how covid- 19 is impacting our industry and shine a light on the technologies being used to fight the pandemic.
we'd appreciate you taking this brief, 5- minute survey on how covid- 19 is impacting you and the information that you find most helpful now.
the survey findings, which we will share with the community, will help to shape our coverage moving forward.
ft quoted the paper as saying, "this dramatic speed- up relative to all known classical algorithms provides an experimental realization of quantum supremacy on a computational task and heralds the advent of a much- anticipated computing paradigm.
to our knowledge this marks the first computation that can only be performed on a quantum processor."
they added that using quantum computing to solve practical problems is still years off, they added.
without a doubt quantum computing is real.
whether the report and google's achievement is real is another matter.
in quantum computing, quantum bits (known as qubits) can represent both zeros and ones at the same time.
when qubits are put in sequence they can represent different states, making it possible to complete millions of computations quickly.
google once said it could reach quantum supremacy in 2017.
its first system with 72 qubits was difficult to control so the researchers there changed the system to a 53- qubit architecture that was codenamed sycamore.
quantum computing is considered a game- changer for cryptography, ai and machine learning.
ibm, microsoft, intel and other companies are pursuing quantum computing research.
two contrasting strategies have come close in the race to develop large- scale quantum computers.
one strategy is based on trapping ions, while the other is based on a traditional technology.
both can now develop a basic device, which would be able to run a range of quantum software.
previous attempts were centered on running a quantum algorithm, like shor's algorithm for factoring numbers.
a quantum device that is large enough to operate these algorithms should be able to exceed traditional pcs.
however, this strategy was shown to have limited potential, because if standard computers are developed based on this strategy, then a different laptop would need to be used to run each app.
this is the reason why focus has shifted towards producing programmable quantum computers.
in may 2016 ibm set to develop a similar device, which could be used by anyone over the internet.
as only five quantum bits or qubits were included in this computer, it cannot handle major issues, however it can be programmed like a standard computer.
a similar device was also developed by google researchers, but it remains inaccessible to the general public.
both of these pcs employ superconducting qubits, which were developed using methods from the computer chip sector.
now, researchers at the university of maryland have developed the first fully programmable and reprogrammable five- qubit computer using a unique method.
each qubit is an electrically charged particle or ion trapped in a magnetic field.
the qubits were built from ytterbium ions which were held in place by lasers and magnetic fields, a technology that emerged from atomic clocks.
the five ytterbium atoms (ions) can be exploited by lasers, infusing them with exact amounts of energy and affecting their interactions with one another.
thus allows the quantum pc to be programmed and reprogrammed with different types of algorithms.
ions are nature's quantum units.
if you have a bunch of them in a processor, all of them are identical, and that is a significant advantage.
shantanu debnath, quantum physicist, university of maryland
this quantum technology could herald an era of quantum computing, which could aid researchers to run difficult simulations and create instant answers to complex calculations.
earlier studies indicated that quantum pcs can rapidly perform calculations and also show that these abilities would enable the pcs to resolve issues relatively faster than standard pcs, for example, solving encryption data would take much longer in traditional pcs.
earlier, several research teams developed tiny yet functional quantum pcs, but these were designed to run step- by- step set of operations or a single algorithm.
the unusual nature of quantum physics controls the functioning of quantum pcs.
the field proposed that basic building blocks of the universe, including atoms, exist in flux states called "superpositions", meaning that atoms can simultaneously rotate in two opposite directions.
such a superposition makes quantum computing essentially different from conventional pcs.
in traditional pcs, data is represented as 1's and 0's, which are binary digits called "bits" and indicated by turning switch- like transistors on or off.
however, qubits are used by quantum pcs that are in superpositions, meaning they are on and off at the same time.
this allows a qubit to simultaneously carry out two calculations.
subsequently, the new device was tested on three algorithms, which quantum pcs can perform rapidly.
the deutsch- jozsa algorithm is only used to test quantum- computing capabilities; the bernstein- vazirani algorithm can be used to probe errors in quantum computing, and the last is the quantum fourier transform algorithm, an element used in quantum- computing encryption- breaking applications.
until now, there hasn't been any quantum- computing platform that had the capability to program new algorithms into their system.
they're usually each tailored to attack a particular algorithm.
shantanu debnath, quantum physicist, university of maryland
entangled talk
trapped- ion qubits are better than the superconducting qubits, as they can remotely communicate with one another due to the unusual property of quantum entanglement.
this enables the pc to process the information much more easily.
"any ion can interact within any other," says debnath.
"quantum entanglement is at the heart of parallel processing and speed- up."
superconducting qubits, in contrast, can only exchange information with their adjacent neighbor.
this means in order to communicate, two distant qubits would need to work through all the qubits present in between.
"that is something they are going to pay for in the long term," says debnath.
the advantage is that present chip fabrication technology should make it relatively easier to build a large number of superconducting qubits.
according to simon devitt of the riken center for emergent matter science in saitama, japan, superconductors and ion traps are the two most sophisticated quantum hardware methods available, but only time will tell which method will forge ahead.
quantum information technology is certainly going through a second renaissance, and technological advances and investment are increasing at a faster and faster pace.
results like this are great to see and i hope they keep coming.
simon devitt, riken center for emergent matter science
the combined algorithms, deutsch- jozsa and bernstein- vazirani, ran 95 and 90% of the time, respectively.
the quantum fourier transform algorithm, which is considered to be the most difficult quantum calculations, had a success rate of 70%.
the team intends to test more algorithms on the device in the coming days.
debnath said.
"we'd like this system to serve as a test bed for examining the challenges of multiqubit operations, and find ways to make them better," debnath told live science.
the study findings have been published in the journal, nature.
a team of researchers from shanghai jiao tong university and the university of science and technology of china has developed a chip that allows for two- dimensional quantum walks of single photons on a physical device.
in their paper published on the open access site, science advances the group describes the chip and why they believe developing it was important.
quantum walks are the quantum version of classical random walks, which are a mathematical means for describing a natural random walk, e.g., simply wandering around randomly.
to describe such walks, mathematicians and computer scientists use probability distribution grids that show a current position and possible next steps.
quantum walks are used to build models that depict randomly grown, sophisticated and complex networks such as the human neural network.
they can also be used to create networks for actual use in applications, and might one day be used in quantum- based robots.
as the researchers note, a quantum computer should provide exponential advantages over classical systems due to their nature.
to that end, scientists have been working to implement quantum walks in a physical machine as part of developing a truly useful quantum computer.
in this new effort, the researchers report that they have developed a chip that carries out quantum walks on a two- dimensional 49x49 grid- the largest created so far by any team.
the three- dimensional chip, the team reports, was created using a technique called femtosecond writing.
it uses the external geometry of photonic waveguide arrays as a means for carrying out the quantum walks using a single photon.
they note also that they tested the chip by observing patterns and variance profiles and comparing them to simulation studies.
they suggest further that in addition to making progress toward a truly useful quantum computer, the chip could also be used to boost the performance of analog quantum computing or quantum simulators.
if researchers can create quantum computers with very large, or even unlimited size grids, it might be possible to create and use networks as complex as the human nervous system.
more information: hao tang et al.
experimental two- dimensional quantum walk on a photonic chip, science advances (2018).
doi: 10.1126/sciadv.aat3174
abstract
quantum walks, in virtue of the coherent superposition and quantum interference, have exponential superiority over their classical counterpart in applications of quantum searching and quantum simulation.
the quantum- enhanced power is highly related to the state space of quantum walks, which can be expanded by enlarging the photon number and/or the dimensions of the evolution network, but the former is considerably challenging due to probabilistic generation of single photons and multiplicative loss.
we demonstrate a two- dimensional continuous- time quantum walk by using the external geometry of photonic waveguide arrays, rather than the inner degree of freedoms of photons.
using femtosecond laser direct writing, we construct a large- scale three- dimensional structure that forms a two- dimensional lattice with up to 49 x 49 nodes on a photonic chip.
we demonstrate spatial two- dimensional quantum walks using heralded single photons and single photon- level imaging.
we analyze the quantum transport properties via observing the ballistic evolution pattern and the variance profile, which agree well with simulation results.
we further reveal the transient nature that is the unique feature for quantum walks of beyond one dimension.
an architecture that allows a quantum walk to freely evolve in all directions and at a large scale, combining with defect and disorder control, may bring up powerful and versatile quantum walk machines for classically intractable problems.
rmit university researchers have trialled a quantum processor capable of routing quantum information from different locations in a critical breakthrough for quantum computing.
the work opens a pathway towards the "quantum data bus", a vital component of future quantum technologies.
the research team from the quantum photonics laboratory at rmit in melbourne, australia, the institute for photonics and nanotechnologies of the cnr in italy and the south university of science and technology of china, have demonstrated for the first time the perfect state transfer of an entangled quantum bit (qubit) on an integrated photonic device.
quantum photonics laboratory director dr alberto peruzzo said after more than a decade of global research in the specialised area, the rmit results were highly anticipated.
"the perfect state transfer has emerged as a promising technique for data routing in large- scale quantum computers," peruzzo said.
"the last 10 years has seen a wealth of theoretical proposals but until now it has never been experimentally realised.
"our device uses highly optimised quantum tunnelling to relocate qubits between distant sites.
"it's a breakthrough that has the potential to open up quantum computing in the near future."
the difference between standard computing and quantum computing is comparable to solving problems over an eternity compared to a short time.
"quantum computers promise to solve vital tasks that are currently unmanageable on today's standard computers and the need to delve deeper in this area has motivated a worldwide scientific and engineering effort to develop quantum technologies," peruzzo said.
"it could make the critical difference for discovering new drugs, developing a perfectly secure quantum internet and even improving facial recognition.''
peruzzo said a key requirement for any information technology, along with processors and memories, is the ability to relocate data between locations.
full scale quantum computers will contain millions, if not billions, of quantum bits (qubits) all interconnected, to achieve computational power undreamed of today.
while today's microprocessors use data buses that route single bits of information, transferring quantum information is a far greater challenge due to the intrinsic fragility of quantum states.
"great progress has been made in the past decade, increasing the power and complexity of quantum processors," peruzzo said.
robert chapman, an rmit phd student working on the experiment, said the protocol they developed could be implemented in large scale quantum computing architectures, where interconnection between qubits will be essential.
"we experimentally relocate qubits, encoded in single particles of light, between distant locations," chapman said.
"during the protocol, the fragile quantum state is maintained and, critically, entanglement is preserved, which is key for quantum computing."
the research, experimental perfect state transfer of an entangled photonic qubit, will be published in nature communications on april 18.
scientists have developed a topological photonic chip to process quantum information, promising a more robust option for scalable quantum computers.
the research team, led by rmit university's dr alberto peruzzo, has for the first time demonstrated that quantum information can be encoded, processed and transferred at a distance with topological circuits on the chip.
the research is published in science advances.
the breakthrough could lead to the development of new materials, new generation computers and deeper understandings of fundamental science.
in collaboration with scientists from the politecnico di milano and eth zurich, the researchers used topological photonics -  a rapidly growing field that aims to study the physics of topological phases of matter in a novel optical context -  to fabricate a chip with a 'beamsplitter' creating a high precision photonic quantum gate.
"we anticipate that the new chip design will open the way to studying quantum effects in topological materials and to a new area of topologically robust quantum processing in integrated photonics technology," says peruzzo, chief investigator at the arc centre of excellence for quantum computation and communication technology (cqc2t) and director, quantum photonics laboratory, rmit.
"topological photonics have the advantage of not requiring strong magnetic fields, and feature intrinsically high- coherence, room- temperature operation and easy manipulation" says peruzzo.
"these are essential requirements for the scaling- up of quantum computers."
replicating the well known hong- ou- mandel (hom) experiment -  which takes two photons, the ultimate constituents of light, and interfere them according to the laws of quantum mechanics -  the team was able to use the photonic chip to demonstrate, for the first time, that topological states can undergo high- fidelity quantum interference.
hom interference lies at the heart of optical quantum computation which is very sensitive to errors.
topologically protected states could add robustness to quantum communication, decreasing noise and defects prevalent in quantum technology.
this is particularly attractive for optical quantum information processing.
"previous research had focussed on topological photonics using 'classical' - laser- light, which behaves as a classical wave.
here we use single photons, which behave according to quantum mechanics" says lead- author jean- luc tambasco, phd student at rmit.
demonstrating high- fidelity quantum interference is a precursor to transmitting accurate data using single photons for quantum communications -  a vital component of a global quantum network.
"this work intersects the two thriving fields of quantum technology and topological insulators and can lead to the development of new materials, new generation computers and fundamental science" says peruzzo.
the research is part of the photonic quantum processor program at cqc2t.
the centre of excellence is developing parallel approaches using optical and silicon processors in the race to develop the first quantum computation system.
cqc2t's australian researchers have established global leadership in quantum information.
having developed unique technologies for manipulating matter and light at the level of individual atoms and photons, the team have demonstrated the highest fidelity, longest coherence time qubits in the solid state; the longest- lived quantum memory in the solid state; and the ability to run small- scale algorithms on photonic qubits.
so far it exists mainly in theory, but if invented, the large- scale quantum computer would change computing forever.
rather than the classical data- encoding method using binary digits, a quantum computer would process information millions of times faster through the use of quantum states of matter.
but there's a reason quantum computers aren't in every home yet.
scientists are still working on how to make a computer do calculations reliably on the quantum scale.
cornell physicists have come up with a key piece of the theoretical puzzle, bringing science one step closer to revealing the first quantum computer.
abolhassan vaezi, a bethe postdoctoral fellow working with eun- ah kim, associate professor of physics, authored a paper published earlier this month in physical review x that answers a long- standing problem in quantum computing.
he made a "fractional topological superconductor," an exotic state of matter in which emergent quasi- particles perform quantum computations without error.
to work on quantum computing, physicists explore electronic interactions in two dimensions, where never- before- seen states of matter can exist.
the states are exotic because particles can carry what's called fractional charges - that is, rather than being limited to a charge of 1 (1 electron), within two- dimensional planes, charges can have a value of, for example, 1/3.
in this theoretical realm, quantum computation could become reality.
scientists had already theorized that for a quantum computing system to be robust and able to store data effectively, it would have to employ such fractionalized excitations of particles, some of which are called non- abelian anyons.
the simplest example of a non- abelian anyon is called a majorana fermion, which has been conjectured to exist in many physical systems and to form the basis for fault tolerance, meaning resistant to error.
in his paper, vaezi demonstrates more efficient non- abelian quasi- particles, fibonacci anyons, through the use of a superconducting vortex, at the interface of a 2/3 fractional quantum hall- superconductor structure.
he demonstrates how this system undergoes a phase transition to the fibonacci state that is the most coveted platform for building fault- tolerant quantum computers.
this discovery provides a novel mechanism for how to make universal topological quantum computers.
intel's horse ridge chip is expected to speed up the development of commercially viable quantum computers and burn a path to scaling larger systems.
intel labs has unveiled a new cryogenic control chip designed to speed up the development of full- stack, commercially- viable quantum computers capable of processing multiple qubits.
the company claims that this development will blaze a path toward scaling larger systems.
controlling multiple qubits simultaneously
rather than use the traditional binary (ones and zeros) system to process information (like today's cpus utilize), quantum platforms take advantage of qubits (quantum bits), which use the spin of an electron or particle in superposition to crunch numbers.
this distinction is explored in more detail in our article on the fundamentals of quantum computing.
intel
intel states that its horse ridge cryogenic control chip is capable of controlling multiple qubits at the same time, significantly speeding up the data- crunching power necessary in building large- scale quantum computers- a major milestone in quantum practicality.
"while there has been a lot of emphasis on the qubits themselves, the ability to control many qubits at the same time had been a challenge for the industry.
intel recognized that quantum controls were an essential piece of the puzzle we needed to solve in order to develop a large- scale commercial quantum system," stated intel's director of quantum hardware jim clarke.
qubits can be in multiple states at the same time, rather than just two (one or zero).
intel claims that this opens up innumerable new possibilities for quantum computers.
image from intel
"that's why we are investing in quantum error correction and controls.
with horse ridge, intel has developed a scalable control system that will allow us to significantly speed up testing and realize the potential of quantum computing."
finfet technology process
intel collaborated with engineers from quantum computer research conglomerate qutech, who helped fabricate the horse ridge chip using intel's 22nm finfet technology process, which produces 3d tri- gate transistors down to nanometer scale.
the 22nm finfet process will allow intel to fast- track intel's ability to r&d a commercially viable quantum computer.
eliminating errors from a freezer
intel's advancement in developing quantum computers is a challenging endeavor, considering qubits are notoriously unstable; the superposition state of the electron tends to collapse while processing data at a rapid rate, which leads to errors.
the company feels that if they can apply their wide- spread transistor technology to qubits, they will have an advantage in progressing quantum computing.
image from intel
to overcome the problem, engineers created redundancies, using a number of qubits and housing them in cryogenically- controlled chambers calibrated at near absolute zero.
microwave pulses were then used to control the qubits within that chamber, which provided a level of stability to process information with little to no errors.
the horse ridge chip
intel capitalized on this information and designed the horse ridge chip to reside in the freezer, thus gaining the ability to garner data without any loss or corruption.
this is said to also remove the interconnects (in the hundreds) required for the microwave pulse devices.
the chip is purportedly smaller than those currently in use, which also allows them to better control more than one qubit.
intel
the horse ridge chip can also be used as a benchmark platform other engineers can use to determine if their system is viable; to this end, intel provides the software stack needed for development.
under the hood, the horse ridge chip is a mixed- signal soc (system on chip) that acts as an rf processor and controls the qubit states within a 4- kelvin freezer.
this is achieved using an instruction program that translates into electromagnetic microwave pulses.
future research in temperature for quantum computers
most quantum computers operate in the millikelvin range or just a fraction of a degree higher than absolute zero (- 270degc).
however, silicon qubits would allow them to function at higher temperatures (1- kelvin or more), meaning the required refrigeration process could be simplified for those quantum systems.
intel feels this simplification is possible in future platforms.
intel feels confident that this breakthrough takes us one step closer to quantum computing, which could, for example, significantly advance medical practices.
image from intel
the company explains, "as research progresses, intel aims to have cryogenic controls and silicon spin qubits operate at the same temperature level.
this will enable the company to leverage its expertise in advanced packaging and interconnect technologies to create a solution with the qubits and controls in one streamlined package."
read up on other quantum computing developments
explaining the most recent record for quantum computing: a 51- qubit quantum computer array
the quantum race: roundup on quantum cryptosecurity, programming languages, and development
most quantum physics research to date has used particles such as atoms and electrons to observe quantum mechanical behaviour.
professor mika sillanpaa of aalto university is now working in the relatively new field of using supercool temperatures to observe quantum features in larger objects
when considering tiny constituents of matter, such as single atoms or molecules, the laws of physics seem to contradict common sense.
atoms or small elementary particles can properly be understood only by quantum physics, which tells that matter and energy consist of small packets, quanta.
on the other hand, according to quantum physics, they both can also behave as waves.
without such detailed knowledge of the fundamental laws of nature, modern electronics, for example, could not have been constructed.
professor mika sillanpaa of the department of applied physics and o.v.
lounasmaa laboratory at aalto university is carrying out basic research on micromechanical resonators measured at ultralow temperatures.
since everything is built with atoms, macroscopic sized objects should, in principle, follow the counterintuitive quantum laws.
quanta are never directly observed, because the quantum waves in sizable objects usually immediately cancel each other out, leaving behind the everyday world.
however, if well protected from noise of the surroundings, tangible objects can retain some quantum features.
"we use quite sophisticated cryogenic equipment to cool our samples close to - 273degc, known as absolute zero," sillanpaa explains.
"at this temperature, the energies of single vibrational quanta are not excessively disturbed by random motion of atoms due to temperature.
this allows us to observe quantum- mechanical behaviour in relatively macroscopic objects such as the micromechanical oscillators that we work with."
in sillanpaa's work, the micromechanical resonators are housed inside a superconducting cavity resonator.
when the two quantum resonators are put together, they begin to exchange quanta, and their resonant motion thus becomes amplified.
this is very similar to what happens in a guitar, where the string and the guitars' echo chamber resonate at the same frequency, but instead occurring in the realms of quantum physics.
instead of the musician playing with the guitar string, the energy source is provided by a microwave laser.
quantum computing
recently, sillanpaa's group successfully connected a superconducting quantum bit, or qubit, with a micrometre- sized drumhead and transferred information from the qubit to the resonator and back again.
"this work represents the first step towards creating exotic mechanical quantum states," he states.
"for example, the transfer makes it possible to create a state in which the resonator simultaneously vibrates and doesn't vibrate."
a qubit is the quantum- mechanical equivalent of the bits used in computers.
a traditional bit can be in a state of 0 or 1, whereas a qubit can be in both states at the same time.
in theory, this situation allows for a quantum calculation in which the operations are performed simultaneously for many possible computational pathways.
in the case of a single qubit, this means zero and one, but as the number of qubits increases, the amount of possible numbers and simultaneous calculations grows exponentially.
the quantum state of a qubit is very fragile and easily disturbed between and during the operations.
the key to successful quantum calculation is being able to protect the qubit state from disturbances in the environment.
although sillanpaa's erc project is basic research aimed at understanding the laws of nature, there is a technological motivation in the distance: future quantum information processing.
micromechanical resonators can serve as an intermediator of quantum information from the quantum bits via optical fibers even to the other side of the earth, which could form the basis of a quantum internet.
quantum computing and quantum information processing technology have attracted attention in recently emerging fields.
among many important and fundamental issues in science, solving the schroedinger equation (se) of atoms and molecules is one of the ultimate goals in chemistry, physics and their related fields.
se is the first principle of non- relativistic quantum mechanics, whose solutions, termed wave functions, can afford any information of electrons within atoms and molecules, predicting their physicochemical properties and chemical reactions.
dr. k. sugisaki, profs.
k. sato and t. takui and coworkers, all researchers from osaka city university (ocu) in japan, have found a novel quantum algorithm enabling us to perform full configuration interaction (full- ci) calculations suitable for "chemical reactions" without exponential/combinatorial explosion.
full- ci gives the exact numerical solutions of se, which are intractable problems even for supercomputers.
such a quantum algorithm contributes to the acceleration of implementing practical quantum computers.
since 1929, chemistry and physics have sought to predict complex chemical reactions by invoking full- ci approaches, but they have never been successful until now.
full- ci calculations are potentially capable of predicting chemical reactions.
the researchers of the current study report a new full- ci approach implemented on quantum computers for the first time.
the paper is published in acs central science.
they write, "as dirac claimed in 1929 when quantum mechanics was established, the exact application of mathematical theories to solve se leads to equations too complicated to be solvable.
in fact, the number of variables to be determined in the full- ci method grows exponentially against the system size, and it easily runs into astronomical figures such as exponential explosion.
for example, the dimension of the full- ci calculation for benzene molecule c6h6, in which only 42 electrons are involved, amounts to 1044, which is impossible to be dealt with by any supercomputer.
worse, molecular systems during the dissociation process are characterized by extremely complex electronic structures (multiconfigurational nature), and relevant numerical calculations are impossible on any supercomputer."
according to the ocu research group, quantum computers date back to feynman's suggestion in 1982 that quantum mechanics can be simulated by a computer itself built of quantum mechanical elements that obey quantum mechanical laws.
more than 20 years later, prof. aspuru- guzik, harvard univ.
(toronto univ.
since 2018) and coworkers have proposed a quantum algorithm capable of calculating the energies of atoms and molecules not exponentially but polynomially against the number of the variables of the systems, making a breakthrough in the field of quantum chemistry on quantum computers.
when aspuru's quantum algorithm is applied to the full- ci calculations on quantum computers, good approximate wave functions close to the exact wave functions of se under study are required.
otherwise, bad wave functions need an extreme number of steps of repeated calculations to reach the exact ones, hampering the advantages of quantum computing.
this problem becomes extremely serious for the analyses of chemical reactions, which have a multiconfigurational nature due to electrons not participating in chemical bonding during the bond dissociation.
the ocu researchers have tackled this problem, one of the most intractable issues in quantum science and chemistry, and made a breakthrough in implementing a new quantum algorithm generating particular wave functions called configuration state functions (csfs) in polynomial computing time.
the previously proposed algorithms for quantum computing, however, inevitably involve the dissociation and formation of many chemical bonds, and as a result, generate many electrons not participating in chemical bonds, making the quantum algorithms difficult to apply.
this is termed the "quantum dilemma."
the ocu researchers have introduced a diradical character, yi(0 ~ 1), to measure and characterize the nature of open shell electronic structures, and have exploited the diradical characters to construct multiconfigurational wave functions required for chemical reactions, executing the full- ci calculations along the whole reaction pathway on quantum computers.
this new procedure requires no time- consuming post- hartree- fock calculations, avoiding the exponential explosion of the calculation, solving the "quantum dilemma" for the first time.
the ocu group writes, "this is the first example of a practical quantum algorithm that makes quantum chemical calculations for predicting chemical reaction pathways realizable on quantum computers equipped with a sizable number of qubits.
the implementation empowers practical applications of quantum chemical calculations on quantum computers in many important fields of chemistry and materials science."
physicists sergei filippov (mipt and russian quantum center at skolkovo) and mario ziman (masaryk university in brno, czech republic, and the institute of physics in bratislava, slovakia) have found a way to preserve quantum entanglement of particles passing through an amplifier and, conversely, when transmitting a signal over long distances.
details are provided in an article published in the journal physical review a.
quantum entangled particles are considered to be the basis of several promising technologies, including quantum computers and communication channels secured against tapping.
quantum entangled particles are quantum objects that can be described in terms of a common quantum state.
two quantum entangled particles can be in different places, at any distance from each other, but they still are to be considered as a whole.
this effect has no analogues in classical physics, and it has been actively studied for the past few decades.
physicists have learned to entangle photons and have found application for them, including opticalfiber communication channels which are impossible to tap.
when trying to intercept the transmission of data over such a channel, quantum entanglement of photons is inevitably destroyed and the legitimate recipient of the message immediately detects interference.
in addition to this, quantum entanglement allows for carrying out quantum teleportation, wherein a quantum object, for example, an atom, in a certain state in one laboratory transmits its quantum state to another object in another laboratory.
it is quantum entangled particles that play the key role in this process, and it is not necessarily about the quantum entanglement of the atoms between which the transmission of the state takes place.
the latter atom becomes absolutely identical to the former one, which in its turn transfers into a different state during the teleportation.
if all atoms of an object were transferred like this, the second laboratory would have its exact copy.
the laws of quantum mechanics do not allow for the teleportation of objects and people, but it is already possible to quantum teleport single photons and atoms, which opens up exciting opportunities for the creation of new computing devices and communication lines.
due to specific quantum effects, a quantum computer will be able to efficiently solve certain problems, for example, hacking codes used in banking, but for now it is still just a theoretical possibility.
in practice, quantum computing and teleportation are obstructed by a process called decoherence.
decoherence is the destruction of the quantum state due to the interaction of a quantum system with the outside world.
for experiments in quantum computing, scientists use single atoms caught in magnetic traps and cooled to temperatures close to absolute zero.
after going through kilometers of fiber, photons cease to be quantum entangled in most cases and become ordinary, unrelated light quanta.
to create an effective quantum computing system, scientists have to solve a number of problems, including preserving quantum entanglement when the signal abates and when it passes through an amplifier.
fiber- optic cables on the ocean bed contain a great deal of special amplifiers composed of optical glass and rare earth elements.
it is these amplifiers that make it possible to watch high- resolution videos stored on a server in california from the mipt campus or a university in beijing.
in their article, filippov and ziman say that a certain class of signals can be transmitted so that the risk ofruining quantum entanglement becomes much lower.
in this case, neither the attenuation nor the amplification of a signal ruins the entanglement.
to achieve this effect, it is necessary to have the particles in a special, non- gaussian state, or, as physicists put it, "the wave function of the particles in the coordinate representation should not be in the form of a gaussian wave packet."
a wave function is a basic concept of quantum mechanics, and gaussian distribution is a major mathematical function used not only by physicists but also by statisticians, sociologists and economists.
quantum mechanics differs from classical mechanics in that there are neither material points, nor clearly specified boundaries for bodies in it.
each object can be described by a wave function - each point in space corresponds to a complex number at each moment.
if this number is squared* one may find an object at a given point.
to get information on the momentum, energy, or other physical characteristic, the same wave function has to be exposed to a so- called operator.
* in fact, since the amplitude is expressed as a complex number, it is necessary to multiply the numberby a complex conjugate.
this detail was omitted due to reader unfamiliarity with complex numbers.
gaussian distribution is a function that in its simplest form (without additional coefficients) looks like e- x2.
in diagrams, it appears as a bell curve.
many processes in nature are described via this function when the results of observations are processed using mathematical methods.
ordinary photons, which are used in most quantum entanglement experiments, are also described by a gaussian function.
the probability of finding a photon at a given point (a translation of the expression "in the coordinate representation") first increases and then decreases according to the rule of the gaussian bell curve.
in this case "it would be impossible to send the entanglement far, even if the signal is very strong," sergei filippov told mipt's press service.
using photons whose wave function has a different shape should increase the number of entangled photon pairs reaching the destination.
however, this does not mean that a signal could be transmitted through a very opaque environment and at very long distances.
if the signal/noise ratio falls below a certain critical threshold, quantum entanglement vanishes in any case.
here's the scenario: you have sensitive data and a problem that only a quantum computer can solve.
you have no quantum devices yourself.
you could buy time on a quantum computer, but you don't want to give away your secrets.
what can you do?
writing in physical review x on 11 july, researchers in singapore and australia propose a way you could use a quantum computer securely, even over the internet.
the technique could hide both your data and program from the computer itself.
their work counters earlier hints that such a feat is impossible.
the scenario is not far- fetched.
quantum computers promise new routes to solving problems in cryptography, modelling and machine learning, exciting government and industry.
such problems may involve confidential data or be commercially sensitive.
technology giants are already investing in building such computers -  and making them available to users.
for example, ibm announced on 17 may this year that it is making a quantum computer with 16 quantum bits accessible to the public for free on the cloud, as well as a 17- qubit prototype commercial processor.
seventeen qubits are not enough to outperform the world's current supercomputers, but as quantum computers gain qubits, they are expected to exceed the capabilities of any machine we have today.
that should drive demand for access.
"we're looking at what's possible if you're someone just interacting with a quantum computer across the internet from your laptop.
we find that it's possible to hide some interesting computations," says joseph fitzsimons, a principal investigator at the centre for quantum technologies (cqt) at the national university of singapore and associate professor at singapore university of technology and design (sutd), who led the work.
quantum computers work by processing bits of information stored in quantum states.
unlike the binary bits found in our regular (i.e., classical) computers, each a 0 or 1, qubits can be in superpositions of 0 and 1.
the qubits can also be entangled, which is believed to be crucial to a quantum computer's power.
the scheme designed by fitzsimons and his colleagues brings secrecy to a form of quantum computing driven by measurements.
in this scheme, the quantum computer is prepared by putting all its qubits into a special type of entangled state.
then the computation is carried out by measuring the qubits one by one.
the user provides step- wise instructions for each measurement: the steps encode both the input data and the program.
researchers have shown previously that users who can make or measure qubits to convey instructions to the quantum computer could disguise their computation.
the new paper extends that power to users who can only send classical bits -  i.e.
most of us, for now.
this is surprising because some computer science theorems imply that encrypted quantum computation is impossible when only classical communication is available.
the hope for security comes from the quantum computer not knowing which steps of the measurement sequence do what.
the quantum computer can't tell which qubits were used for inputs, which for operations and which for outputs.
"it's extremely exciting.
you can use this unique feature of the measurement- based model of quantum computing -  the way information flows through the state -  as a crypto tool to hide information from the server," says team member tommaso demarie of cqt and sutd.
although the owner of the quantum computer could try to reverse engineer the sequence of measurements performed, ambiguity about the role of each step leads to many possible interpretations of what calculation was done.
the true calculation is hidden among the many, like a needle in a haystack.
the set of interpretations grows rapidly with the number of qubits.
"the set of all possible computations is exponentially large -  that's one of the things we prove in the paper -  and therefore the chance of guessing the real computation is exponentially small," says fitzsimons.
one question remains: could meaningful computations be so rare among all the possible ones that the guessing gets easier?
that's what the researchers need to check next.
nicolas menicucci at the centre for quantum computation and communication technology at rmit university in melbourne, australia, and atul mantri at sutd, are coauthors on the work.
"quantum computers became famous in the '90s with the discovery that they could break some classical cryptography schemes -  but maybe quantum computing will instead be known for making the future of cloud computing secure," says mantri.
cite this page:
mla
apa
chicago
centre for quantum technologies at the national university of singapore.
"why you might trust a quantum computer with secrets, even over the internet: researchers suggest you could operate a quantum computer in the cloud without revealing your data or the program you're running."
sciencedaily.
sciencedaily, 12 july 2017.
<www.sciencedaily.com/releases/2017/07/170712110601.htm>.
universities space research association (usra) today announced that darpa has awarded the organization and its partners rigetti computing and the nasa quantum artificial intelligence laboratory (quail) to work as a team to advance the state of art in quantum optimization.
usra, as the prime contractor of the award, will manage the collaboration.
the collaboration will focus on developing a superconducting quantum processor, hardware - aware software and custom algorithms that take direct advantage of the hardware advances to solve scheduling and asset allocation problems.
in addition, the team will design methods for benchmarking the hardware against classical computers to determine quantum advantage.
usra senior vice president bernie seery noted, "this is a very exciting public- private partnership for the development of forefront quantum computing technology and the algorithms that will be used to address pressing, strategically significant challenges.
we are delighted to receive this award and look forward to working with our partner institutions to deliver value to darpa."
in particular, the work will target scheduling problems whose complexity goes beyond what has been done so far with the quantum approximate optimization algorithm (qaoa).
usra's research institute for advanced computer science (riacs) has been working on quantum algorithms for planning and scheduling for nasa quail since 2012.
"the innovations on quantum gates performed by rigetti coupled perfectly with the recent research ideas at quail, enabling an unprecedented hardware- theory co- design opportunity," explains dr. venturelli, usra associate director for quantum computing and project pi for usra.
understanding how to use quantum computers for scheduling applications could have important implications for national security such as real time strategic asset deployment, as well as commercial applications including global supply chain management, network optimizations or vehicle routing.
the grant is a part of the darpa optimization with noisy intermediate- scale quantum program (onisq).
the goal of this program is to establish that quantum information processing using nisq devices has a quantitative advantage for solving real- world- combinatorial optimization problems using the qaoa method.
about usra
founded in 1969, under the auspices of the national academy of sciences at the request of the u.s. government, the universities space research association (usra) is a nonprofit corporation chartered to advance space- related science, technology and engineering.
usra operates scientific institutes and facilities, and conducts other major research and educational programs, under federal funding.
usra engages the university community and employs in- house scientific leadership, innovative research and development, and project management expertise.
more information about usra is available at www.usra.edu.
about the research institute for advanced computer science
usra manages and operates the research institute for advanced computer science which focuses on interdisciplinary research and challenging applications associated with nasa's mission to develop innovative information systems and other technologies.
to implement this approach research staff undertake collaborative projects with research groups at nasa and elsewhere, integrating computer science with other disciplines to support nasa's mission.
more information about the quantum computing group is available at https://riacs.usra.edu/quantum/.
about rigetti computing
rigetti computing is an integrated quantum systems company.
through our quantum cloud services platform, rigetti machines can be integrated into any public, private or hybrid cloud.
rigetti serves customers in finance, insurance, government, defense, and energy with custom software and full- stack quantum computing solutions focused on simulation, optimization, and machine learning applications.
the company is based in berkeley, ca with offices in fremont, ca, washington, d.c., london, and australia.
pr contact:
suraiya farukhi, ph.d.
san francisco work at ibm corp. on the theory and practice of quantum computing suggests that the industry may be closer to practical cpus that could process information in the form of quantum bits, or "qubits," rather than conventional binary bits.
the new thinking was discussed today (dec. 11) in a plenary lecture at the ieee international electron devices meeting here.
david divincenzo of ibm's t. j. watson research center (yorktown heights, n.y.) surveyed the prospects for quantum computing, concluding that practical, solid- state devices may soon emerge to support the theoretical projections of vast computing power arising from this technology.
"the principles and promise of quantum computers," divincenzo said, lie in the "requirements for the physical implementation of quantum computers in atomic physics, quantum optics, nuclear and electron magnetic- resonance spectroscopes, superconducting electronics and quantum- dot physics."
proving a point
divincenzo already has a track record in quantum theory.
for instance, in 1995 he mathematically proved that two- qubit operations were sufficient to execute any quantum algorithm.
thus, engineers need not design more than two- qubit physical devices to reap all the parallel- processing benefits of any future quantum algorithm.
in divincenzo's view, all quantum devices will require a new formal basis to express the kind of algorithmic parallelisms that could be realized with quantum computers.
unlike conventional encodings of information, quantum devices allow the superposition of multiple discrete states simultaneously on the same qubit.
thus, multiplying two qubits together is equivalent to simultaneously multiplying every possible string of values that a conventional computer register could hold.
that kind of operation demands a new mathematical formalism in order to craft effective quantum algorithms, said divincenzo.
"making bits that obey the quantum- mechanical principles [and] efficient algorithms for some otherwise intractable problems, like prime factoring, becomes possible," he said.
divincenzo listed a sevenfold set of requirements for physical implementation of quantum computing.
they are scalable well- defined qubits, resettable states, long superposition times, a universal set of quantum gates, easy qubit measurements, easy qubit- to- digital conversions and easy qubit telecommunications.
today, laboratory implementations of quantum devices concentrate on perhaps one or two of the seven requirements, he said, but to create commercial devices all seven must be met.
divincenzo summarized current attempts at building a quantum "transistor," most of which only have a superficial resemblance to today's silicon transistors.
using a voltage- controlled gate to switch the qubit may be the only recognizable commonfeature.
the most striking new feature, according to divincenzo, is the fact that a single atomic element will embody the memory element in a microscopic domain, probably in the spin direction (either up or down) of a single atom or electron.
the quantum gate will most likely be switched with a voltage- controlled pulse at the gate, which superimposes a new state into the currently executing quantum algorithm.
aside from these generalizations, there is little similarity among the various current proposals for implementing quantum computers.
methodologies, so far, are based on basic physics rather than chip technologies, divicenzo said, but advances in quantum dots embrace standard solid- state physics for integrated circuits.
in particular, divincenzo described an ion- trap computer that holds qubits in pairs of energy levels of ions held in a linear electromagnetic trap.
other atomic- physics- based proposals use the position of atoms in a trap or lattice or the vibrational quanta of trapped electrons, ions or atoms as their qubits.
the presence or absence of a photon in an optical cavity has also been proposed as the basic qubit mechanism, and superconducting devices are being proposed that store qubits as charge or "flux."
on the solid- state chip side, impurities can introduce well- characterized discrete energy- level spectra directly onto silicon chips.
separately, researchers are at work on various quantum- dot approaches, storing the qubit in the spin state, the orbital state or the charge state of quantum dots, according to divincenzo.
"every implementation detail of a qubit from its initialization to its interaction with neighboring bits, the errors that it might be subjected to and its readout have to be thought out and investigated from scratch," he said.
divincenzo has narrowed the overall development effort, however, by applying his seven criteria to the known theories today.
the result is to single out, by elimination, a model for the solid- state implementations of the future.
the right spin
in divincenzo's model, qubits are represented by the spins of individual electrons trapped in an array of quantum dots.
many proposals for building quantum- dot arrays are on record, but divincenzo's model will reset the system by cooling the device to a predetermined temperature.
basic two- bit gates would be built by changing the height of an electrostatic barrier between quantum dots.
according to divincenzo, the superposition time of such arrays has already been determined in preliminary laboratory experiments measuring spins in semiconductors, to be long enough to allow quantum algorithms to execute.
and other laboratory measurements, he said, also suggest that spin can be conveniently converted into directly measurable electron position.
a team at the university of sydney and microsoft, in collaboration with stanford university in the us, has miniaturised a component that is essential for the scale- up of quantum computing.
the work constitutes the first practical application of a new phase of matter, first discovered in 2006, the so- called topological insulators.
beyond the familiar phases of matter - solid, liquid, or gas - topological insulators are materials that operate as insulators in the bulk of their structures but have surfaces that act as conductors.
manipulation of these materials provide a pathway to construct the circuitry needed for the interaction between quantum and classical systems, vital for building a practical quantum computer.
theoretical work underpinning the discovery of this new phase of matter was awarded the 2016 nobel prize in physics.
the sydney team's component, coined a microwave circulator, acts like a traffic roundabout, ensuring that electrical signals only propagate in one direction, clockwise or anti- clockwise, as required.
similar devices are found in mobile phone base- stations and radar systems, and will be required in large quantities in the construction of quantum computers.
a major limitation, until now, is that typical circulators are bulky objects the size of your hand.
this invention, reported by the sydney team today in the journal nature communications, represents the miniaturisation of the common circulator device by a factor of 1000.
this has been done by exploiting the properties of topological insulators to slow the speed of light in the material.
this minaturisation paves the way for many circulators to be integrated on a chip and manufactured in the large quantities that will be needed to build quantum computers.
the leader of the sydney team, professor david reilly, explained that the work to scale- up quantum computing is driving breakthroughs in related areas of electronics and nanoscience.
"it is not just about qubits, the fundamental building blocks for quantum machines.
building a large- scale quantum computer will also need a revolution in classical computing and device engineering," professor reilly said.
"even if we had millions of qubits today, it is not clear that we have the classical technology to control them.
realising a scaled- up quantum computer will require the invention of new devices and techniques at the quantum- classical interface."
lead author of the paper and phd candidate alice mahoney said: "such compact circulators could be implemented in a variety of quantum hardware platforms, irrespective of the particular quantum system used."
a practical quantum computer is still some years away.
scientists expect to be able to carry out currently unsolveable computations with quantum computers that will have applications in fields such as chemistry and drug design, climate and economic modelling, and cryptography.
today, ibm (nyse: ibm) and the university of tokyo unveiled a landmark collaboration with the launch of the quantum innovation initiative consortium (qiic).
expanding from the december 2019 japan- ibm quantum partnership initiative, qiic, aims to accelerate the collaboration between industry, academia, and government to advance japan's leadership in quantum science, business, and education.
qiic's main goal is to strategically accelerate quantum computing r&d activities in japan by bringing together academic talent from across the country's universities and prominent research associations and large- scale industry.
the consortium plans to further develop technology for quantum computing in japan and build an ecosystem to improve student skills and expertise, opening doors to future scientific discoveries and practical quantum applications.
headquartered at the university of tokyo, member organizations of qiic will collaborate to engage students, faculty, and industry researchers with seminars, workshops, and events to foster new quantum business opportunities in japan.
organizations in agreement to join the consortium include keio university, toshiba, hitachi, mizuho, mufg, jsr, dic, toyota, mitsubishi chemicals and ibm japan.
these organizations in consortium will also be part of the ibm q network - the world's first community of fortune 500 companies, startups, academic institutions and research labs - to advance quantum computing and the development of practical applications for it.
as part of the network, they will have access to ibm's expertise and resources, and cloud development environment, as well as cloud- based access to the ibm quantum computation center, which includes ibm's most- advanced quantum computers.
in addition to cloud- based access to the ibm's fleet of quantum systems, the qiic will also have access to an ibm q system one, a dedicated system planned for installation in japan in 2021.
the first of its kind in the region, and only the second such installation outside of the us, this system - along with a separate testbed system to be part of a system technology development lab - will support the consortium's goals of next- generation quantum hardware research and development, including cryogenic components, room temperature electronics, and micro- signal generators.
according to professor makoto gonokami, president of the university of tokyo:
"society 5.0 is the concept of a better future with inclusive, sustainable and a knowledge- intensive society where information and services create value underpinned by digital innovation.
the key to realizing this society is to utilize real data in real- time.
in order to achieve this, it is necessary to protect and nurture the global environment, an entity of physical space and cyberspace as one, by taking it as a global commons (a concept that encompasses global resources and the ecosystems) which is sustainable and reliable, while the fusion of physical space and cyberspace progresses.
"quantum technology and quantum computers are indispensable technologies to make that happen.
i believe that japan will play an important role in implementing quantum computing technology to society ahead of rest of the world, and that industry- academia- government collaboration is necessary for this.
the qiic will accelerate quantum technology research and its implementation to the society 5.0 while firmly sharing each other's wisdom and promoting the close sharing of information."
"today, i am extremely excited and proud to launch this new consortium that will help foster economic growth and quantum technology leadership in japan.
the qiic will greatly advance japan's entire quantum computing ecosystem, bringing experts from industry, government and academia together to collaborate on research and development," said dario gil, director of ibm research.
"quantum computing has the potential to tackle some of the world's greatest challenges in the future.
we expect that it will help us accelerate scientific discovery so that we can develop vaccines more quickly and accurately, create new materials to address climate change or design better energy storage technologies.
the potential is massive, and we will only reach this future if we work together - uniting the best minds from the public and private sectors.
universities, businesses and governments have to collaborate so that we can unleash the full potential of quantum computing."
qiic's members are forging a path for japan's discovery of practical quantum applications for the benefit of society.
the cooperation between industry, academia, and government aims to create a new community for quantum computation research and use cases.
about ibm quantum
ibm quantum is an industry- first initiative to build quantum systems for business and science applications.
for more information about ibm's quantum computing efforts, please visit www.ibm.com/ibmq.
for more information about the ibm q network, as well as a full list of all partners, members, and hubs, visit https://www.research.ibm.com/ibm- q/network/
about the university of tokyo
the university of tokyo was established in 1877 as the first national university in japan.
as a leading research university, the university of tokyo is conducting academic research in almost all fields at both undergraduate and graduate schools.
the university aims to provide its students with a rich and varied academic environment that ensures opportunities for acquiring both academic and professional knowledge and skills.
media contacts
chris nay
a team from the department of energy's oak ridge national laboratory has conducted a series of experiments to gain a better understanding of quantum mechanics and pursue advances in quantum networking and quantum computing, which could lead to practical applications in cybersecurity and other areas.
researchers in ornl's quantum information science group summarized their significant contributions to quantum networking and quantum computing in a special issue of optics & photonics news.
credit: christopher tison and michael fanto/air force research laboratory.
ornl quantum researchers joseph lukens, pavel lougovski, brian williams, and nicholas peters- along with collaborators from purdue university and the technological university of pereira in colombia- summarized results from several of their recent academic papers in a special issue of the optical society's optics & photonics news, which showcased some of the most significant results from optics- related research in 2019.
their entry was one of 30 selected for publication from a pool of 91.
conventional computer "bits" have a value of either 0 or 1, but quantum bits, called "qubits," can exist in a superposition of quantum states labeled 0 and 1.
this ability makes quantum systems promising for transmitting, processing, storing, and encrypting vast amounts of information at unprecedented speeds.
to study photons- single particles of light that can act as qubits- the researchers employed light sources called quantum optical frequency combs that contain many precisely defined wavelengths.
because they travel at the speed of light and do not interact with their environment, photons are a natural platform for carrying quantum information over long distances.
interactions between photons are notoriously difficult to induce and control, but these capabilities are necessary for effective quantum computers and quantum gates, which are quantum circuits that operate on qubits.
nonexistent or unpredictable photonic interactions make two- photon quantum gates much more difficult to develop than standard one- photon gates, but the researchers reached several major milestones in recent studies that addressed these challenges.
for example, they made adjustments to existing telecommunications equipment used in optics research to optimize them for quantum photonics.
their results revealed new ways to use these resources for both traditional and quantum communication.
"using this equipment to manipulate quantum states is the technological underpinning of all these experiments, but we did not expect to be able to move in the other direction and improve classical communication by working on quantum communication," lukens said.
"these interesting and unanticipated findings have appeared as we delve deeper into this research area."
one such tool, a frequency beam splitter, divides a single beam of light into two frequencies, or colors, of light.
"imagine you have a beam of light going down an optical fiber that has a particular frequency, say, red," lukens said.
"then, after going through the frequency beam splitter, the photon will leave as two frequencies, so it will be both red and blue."
the members of this team were the first researchers to successfully design a quantum frequency beam splitter with standard lightwave communications technology.
this device takes in red and blue photons simultaneously, then produces energy in either the red or the blue frequency.
by using this method to deliberately change the frequencies of photons, the team tricked the stubborn particles into beneficial interactions based on quantum interference, the phenomenon of photons interfering with their own trajectories.
"it turned out that off- the- shelf devices can deliver impressive control at the single- photon level, which people didn't know was possible," lougovski said.
additionally, the researchers completed the first demonstration of a frequency tritter, which splits a beam of light into three different frequencies instead of two.
their results indicated that multiple quantum information processing operations can run at the same time without introducing errors or damaging the data.
another key accomplishment was the team's design and demonstration of a coincidence- basis controlled- not gate, which enables one photon to control a frequency shift in another photon.
this device completed a universal quantum gate set, meaning any quantum algorithm can be expressed as a sequence within those gates.
"quantum computing applications require much more impressive control levels than any sort of classical computing," lougovski said.
the team also encoded quantum information in multiple independent values known as degrees of freedom within a single photon, which allowed them to observe quantum entanglement- like effects without needing two separate particles.
entanglement usually involves two linked particles in which changes made to the state of one particle also apply to the other.
finally, the researchers have completed quantum simulations of real- world physics problems.
in collaboration with scientists at the air force research laboratory, they are now developing tiny, specialized silicon chips similar to those common in microelectronics in pursuit of even better photonic performance.
"in theory, we can get all these operations onto a single photonic chip, and we see a lot of potential for doing similar quantum experiments on this new platform," lukens said.
"that's the next step to really move this technology forward."
future quantum computers will allow scientists to simulate incredibly complex scientific problems that would be impossible to study on current systems, even supercomputers.
in the meantime, the team's findings could help researchers embed photonic systems into current high- performance computing resources.
"we have a very diverse and talented team," lougovski said.
"the most important thing is we're getting results."
this research was funded by ornl's laboratory directed research and development program.
ibm has an operational 50- qubit quantum computer.
intel has shipped a 49- qubit quantum processor to its research partners for testing.
rigetti has an operational 19- qubit quantum computer.
d- wave has an operational 2048- qubit annealing quantum computer, and fujitsu has an operational 1024- qubit annealing quantum computer.
the last two are not so- called general- purpose systems, but they are still relevant to the industry racing to quantum supremacy.
quantum supremacy is the crossover point when quantum computers can solve or massively accelerate relevant problems that classical computers cannot solve today.
proof of quantum supremacy also requires that the result of the quantum program can be validated.
google joined this race when it announced this week its internal delivery of bristlecone for testing with its research partners.
google did not say it has run programs on the new quantum processor.
however it did say the following:
"the purpose of this gate- based superconducting system is to provide a testbed for research into system error rates and scalability of our qubit technology, as well as applications in quantum simulation, optimization, and machine learning...we are cautiously optimistic that quantum supremacy can be achieved with bristlecone, and feel that learning to build and operate devices at this level of performance is an exciting challenge!"
it is not clear that anyone outside of google and its research partners will ever be granted access to a bristlecone- based quantum computer.
it is nevertheless an impressive feat.
in the spirit of moore's law, tirias research extrapolated possible linear and exponential trajectories for the number of qubits the industry is likely to attain in a general- purpose quantum computer by the end of 2020.
if growth is linear, the industry might end up with between 100 and 200 qubits per quantum processor.
given that google has yet to put bristlecone through its paces that is not a bad bet.
if growth is exponential, the industry might end up with on the order of 1,000 qubits per quantum processor in only two years.
best- case predictions for quantum computers still fall short of what some say are the ultimate goals.
click to enlarge.
source: tirias research
i predict that before a general- purpose quantum computer can demonstrate quantum supremacy, the industry must first figure out how to build a reliable logical qubit and then figure out how to connect at least a few thousand of them into a usable computer.
the challenge is that no one has yet demonstrated a logical qubit, error corrected or otherwise fault- tolerant.
quantum computer designers may need tens of physical qubits (microsoft's goal) to thousands or tens of thousands of qubits (pretty much everyone else's goal).
neither of our simple extrapolations comes anywhere close to enabling mainstream quantum computer designers to build a useful quantum computer with thousands of logical qubits before the mid- 2020s.
to do that, several scientific and engineering breakthroughs must be achieved- events which are notoriously difficult to schedule.
it will be a long road to building a fault- tolerant quantum computer.
therefore, for very specific classes of problems, it is possible that an annealing quantum computer might demonstrate quantum supremacy first.
the good news is that by the end of this year we should know much more about quantum computing's likely trajectory.
i'll be at the international conference on quantum communication, measurement and computing (qcmc) next week asking a lot of questions.
- paul teich is a principal analyst at tirias research
in the early 2000s, high- performance computing experts repurposed gpus - common video game console components used to speed up image rendering and other time- consuming tasks - as co- processors that help cpus in supercomputers accelerate system operations.
two decades later, quantum processing units, or qpus, promise to enhance existing cpu- gpu computer architectures.
future cpu- gpu- qpu supercomputers could tackle complex workloads that would be unmanageable with current systems.
xacc enables the programming of quantum code alongside standard classical code and integrates quantum computers from a number of vendors.
this animation illustrates how qpus complete calculations and return results to the host cpu, a process that could drastically accelerate future scientific simulations.
credit: michelle lehman/oak ridge national laboratory, u.s. dept.
of energy
to help researchers harness the potential power of qpus, a team from the department of energy's oak ridge national laboratory developed an advanced software framework called xacc.
xacc offloads portions of quantum- classical computing workloads from the host cpu to an attached quantum accelerator, which calculates results and sends them back to the original system.
depending on the complexity of a given problem, this process might occur several times throughout a simulation.
"we built upon the accelerated node model of computing and adapted it to optimize quantum- classical interactions," said alex mccaskey, a computer scientist at ornl who has been developing and refining the framework since 2016.
classical computers use "bits" valued at 0 or 1, whereas quantum computers use quantum bits, or "qubits," that can be encoded with 0, 1 or any combination of those values simultaneously.
this ability shows immense promise for better data storage and analysis, indicating that quantum processors could eventually overtake classical processors in terms of power, speed and other key metrics.
because quantum techniques could accelerate scientific computing, researchers are increasingly conducting research on novel quantum hardware platforms.
to support that research, scientists require secure, system- level and user- friendly quantum- classical software frameworks.
the team designed xacc to fill this gap and published its features and applications in a quantum science and technology special issue focused on quantum software.
"at its core, xacc is a way for users to program quantum- classical systems at a level familiar to those in the hpc community," mccaskey said.
"as hardware continues to improve, we are envisioning new ways to reduce system noise, speed up simulations and integrate new quantum software with existing classical tools and techniques."
xacc's unique "plug and play" capability makes the ornl- developed resource compatible with any available quantum computer.
currently, xacc works with quantum computing platforms developed by ibm, rigetti, d- wave and ionq, and the framework will support additional systems that come online in the near future.
the ornl researchers were the first to build and demonstrate this type of hardware- agnostic software framework for today's quantum computers.
the framework provides users with additional flexibility by supporting c++ and python, and the team plans to extend this list to include julia and other popular computer programming languages.
these features allow xacc to integrate cpu- qpu processes into small- scale computing applications and large- scale hpc workflows.
many scientific problems scale exponentially, which means adding a single particle to an existing simulation would double the amount of space required to calculate accurate results.
classical computers can only simulate systems of a certain size before straining memory limits, but future quantum systems might not have the same limitation and could thus enable new discoveries in fields such as quantum chemistry, nuclear physics, high energy physics and machine learning.
"encoding scientific problems onto quantum computers would allow us to take advantage of that exponential scaling space to hopefully solve larger problems in a way that is faster and more energy efficient than with purely classical methods," mccaskey said.
in previous tests, the team proved that xacc can benchmark quantum chemistry applications by evaluating various molecules.
and in 2018, ornl scientists used xacc to complete the first successful simulation of an atomic nucleus using a quantum computer.
recently, the team completed a series of additional xacc demonstrations using resources provided by ornl's compute and data environment for science, which facilitates research across the lab.
anyone can access xacc through the eclipse foundation, a major supplier of open- source software, and the framework marks the foundation's first quantum computing project.
the researchers are currently preparing to run large- scale quantum program simulations with xacc on ornl's summit, the fastest supercomputer in the world, which has a cpu- gpu hybrid architecture.
going forward, the xacc team will focus on new programming mechanisms that allow users to control the state and movement of qubits by manipulating ultrashort quantum pulses.
obtaining direct pulse- level control could improve efficiency and optimize quantum- accelerated applications.
"the end goal is for xacc to serve as a foundational framework from which we can build a comprehensive software infrastructure for scientific quantum- classical computing," mccaskey said.
along with mccaskey, the xacc team includes dmitry lyakh, euguene dumitrescu, sarah powers, travis humble, thien nguyen, tyler kharazi, zach parks, daniel claudino, anthony santana, jay jay billings, greg watson, robert smith, vicente leyton ortega, cameron reid, prasanna date, pavel lougovski and raphael pooser.
an improved method for measuring quantum properties offers new insight into the unique characteristics of quantum systems.
the properties of quantum physical systems are fundamentally different to those of classical systems in a way that makes them attractive for applications such as computing and communications.
however, it is often difficult to determine whether a system is in a quantum or classical physical state.
franco nori and colleagues from the riken center for emergent matter science, together with collaborators in taiwan, have now developed a mechanism that permits the reliable detection of quantum properties- even in complex systems1.
the unique behavior of quantum states arises from the superposition of different states- a property known as quantum coherence.
the physicist erwin schrodinger famously compared the concept of quantum coherence to a theoretical experiment in which a cat is sealed in a box with a vial of poison to be released by a random quantum mechanism.
without looking inside the box, it cannot be known whether the cat is dead or alive; the cat is therefore in a quantum coherent state.
while some quantum states are used for computing, they also occur in nature- in certain biomolecules, for example.
measuring the properties of quantum systems is important to further their technological utility.
unfortunately, existing measurement methods are impractical due to their complexity and the constraints they place on the quantum states that can be detected.
"our main goal was to devise an unambiguous test that is easy and practical to implement, and which relies on as little 'foreknowledge' of the system as possible, to determine its quantum properties," explains neill lambert, a member of the research team.
the detection scheme developed by nori, lambert and colleagues involves the introduction of two 'quantum witnesses' that allow the comparison of two runs of an experiment: one in which the state of a system is observed twice, and one where it is only observed once.
this procedure effectively sums the results of multiple random experiments to test whether there is any deviation from the expected classical values, which would provide evidence for a quantum state (fig.
1).
for schrodinger's cat, such a deviation would suggest that the cat is neither dead nor alive but is instead in a quantum combination of both states.
among the many possible quantum systems to which this method could be applied, experiments involving biological molecules are particularly interesting, says nori.
"the question of whether quantum coherence exists in biological organisms, for example in a photosynthetic complex, has triggered a surge of interest into the relationship between quantum coherence and biological function."
quantum computing is too 'way out there' at the moment for intel to seriously fund research in the area, according to patrick gelsinger, chief technology officer of intel.
speaking at the intel developer forum in munich, gelsinger said: "we are funding research into bio- computing, molecular computing and nanosystems, the only one we are not funding is quantum computing.
"quantum computing is way out there.
so far only 3 bit quantum computing has been possible, what is that compared to the procesing power we have today?"
he maintains that silicon is the way forward for the forseeable future.
he said: "i don't think anything will replace silicon, it is a manufacturing thing, how would you build quantum computers.
they are so far away from today's manufacturing technologies, it is way out there 20- 30 years away."
however intel does intend to enhance its silicon technologies, its approach will involve integrating specific areas of new technologies with silicon.
"i am interested in taking some parts of the new technologies and integrating them with silicon.
complementing silicon with bio, molocular and nano characteristics is an area where we expect to see breakthroughs," gelsinger said.
researchers at the department of energy's oak ridge national laboratory have developed a quantum chemistry simulation benchmark to evaluate the performance of quantum devices and guide the development of applications for future quantum computers.
their findings were published in npj quantum information.
quantum computers use the laws of quantum mechanics and units known as qubits to greatly increase the threshold at which information can be transmitted and processed.
whereas traditional "bits" have a value of either 0 or 1, qubits are encoded with values of both 0 and 1, or any combination thereof, allowing for a vast number of possibilities for storing data.
while still in their early stages, quantum systems have the potential to be exponentially more powerful than today's leading classical computing systems and promise to revolutionize research in materials, chemistry, high- energy physics, and across the scientific spectrum.
but because these systems are in their relative infancy, understanding what applications are well suited to their unique architectures is considered an important field of research.
"we are currently running fairly simple scientific problems that represent the sort of problems we believe these systems will help us to solve in the future," said ornl's raphael pooser, principal investigator of the quantum testbed pathfinder project.
"these benchmarks give us an idea of how future quantum systems will perform when tackling similar, though exponentially more complex, simulations."
pooser and his colleagues calculated the bound state energy of alkali hydride molecules on 20- qubit ibm tokyo and 16- qubit rigetti aspen processors.
these molecules are simple and their energies well understood, allowing them to effectively test the performance of the quantum computer.
by tuning the quantum computer as a function of a few parameters, the team calculated these molecules' bound states with chemical accuracy, which was obtained using simulations on a classical computer.
of equal importance is the fact that the quantum calculations also included systematic error mitigation, illuminating the shortcomings in current quantum hardware.
systematic error occurs when the "noise" inherent in current quantum architectures affects their operation.
because quantum computers are extremely delicate (for instance, the qubits used by the ornl team are kept in a dilution refrigerator at around 20 millikelvin (or more than - 450 degrees fahrenheit), temperatures and vibrations from their surrounding environments can create instabilities that throw off their accuracy.
for instance, such noise may cause a qubit to rotate 21 degrees instead of the desired 20, greatly affecting a calculation's outcome.
"this new benchmark characterizes the 'mixed state,' or how the environment and machine interact, very well," pooser said.
"this work is a critical step toward a universal benchmark to measure the performance of quantum computers, much like the linpack metric is used to judge the fastest classical computers in the world."
while the calculations were fairly simple compared to what is possible on leading classical systems such as ornl's summit, currently ranked as the world's most powerful computer, quantum chemistry, along with nuclear physics and quantum field theory, is considered a quantum "killer app."
in other words, it is believed that as they evolve quantum computers will be able to more accurately and more efficiently perform a wide swathe of chemistry- related calculations better than any classical computer currently in operation, including summit.
"the current benchmark is a first step towards a comprehensive suite of benchmarks and metrics that govern the performance of quantum processors for different science domains," said ornl quantum chemist jacek jakowski.
"we expect it to evolve with time as the quantum computing hardware improves.
ornl's vast expertise in domain sciences, computer science and high- performance computing make it the perfect venue for the creation of this benchmark suite."
ornl has been planning for paradigm- shifting platforms such as quantum for more than a decade via dedicated research programs in quantum computing, networking, sensing and quantum materials.
these efforts aim to accelerate the understanding of how near- term quantum computing resources can help tackle today's most daunting scientific challenges and support the recently announced national quantum initiative, a federal effort to ensure american leadership in quantum sciences, particularly computing.
such leadership will require systems like summit to ensure the steady march from devices such as those used by the ornl team to larger- scale quantum systems exponentially more powerful than anything in operation today.
access to the ibm and rigetti processors was provided by the quantum computing user program at the oak ridge leadership computing facility, which provides early access to existing, commercial quantum computing systems while supporting the development of future quantum programmers through educational outreach and internship programs.
support for the research came from doe's office of science advanced scientific computing research program.
"this project helps doe better understand what will work and what won't work as they forge ahead in their mission to realize the potential of quantum computing in solving today's biggest science and national security challenges," pooser said.
next, the team plans to calculate the exponentially more complex excited states of these molecules, which will help them devise further novel error mitigation schemes and bring the possibility of practical quantum computing one step closer to reality.
more information: alexander j. mccaskey et al, quantum chemistry as a benchmark for near- term quantum computers, npj quantum information (2019).
doi: 10.1038/s41534- 019- 0209- 0
intensive research is being carried out on quantum simulators: they promise to precisely calculate the properties of complex quantum systems, when conventional and even supercomputers fail.
in a cooperative project, theorists from the the max planck institute of quantum optics in garching anf the consejo superior de investigaciones cientificas (csic) have now developed a new toolbox for quantum simulators and published it in science advances.
it uses the nobel prize- winning principle of topology to allow quantum bits, for example individual atoms, to communicate with each other via "topological radio channels."
the "radio channels" are provided by a light field that travels in waveguide in a robust manner with the help of topology.
the concept offers room for completely new ideas, ranging from basic research to quantum information.
"how can we make two distant quantum bits 'talk' to each other?"
asks alejandro gonzalez- tudela.
"this is an essential challenge in the field of quantum information and simulation!"
until recently, the theoretical physicist was a postdoctoral fellow in the department of ignacio cirac, director at the max planck institute of quantum optics in garching, and today he is a permanent researcher at the instituto de fisica fundamental iff- csic in madrid.
together with cirac and two spanish colleagues from the instituto de ciencias de materiales de madrid, he has now published a scientific paper that introduces a completely new toolbox to photonics.
photonics is a branch of physics that deals with the interaction between light and matter and its technical application.
one possible application is the so- called quantum simulation, which goes back to an idea of the famous us nobel prize winner richard feynman.
if one wants to calculate the behavior of a quantum system as accurately as possible on a conventional computer, the necessary computing power doubles with each new quantum particle in the system.
because of this mathematical avalanche, even relatively small quantum systems consisting of just a few dozen particles overrun the performance of even conventional supercomputers.
for this reason, feynman had the idea decades ago to simulate the behavior of a quantum system with the help of another quantum system.
in principle, such a quantum simulator is a specialized quantum computer whose individual quantum bits can be easily controlled from the outside- in contrast to the rather inaccessible quantum system whose behavior it is supposed to simulate.
such quantum simulators have been the subject of intensive research for many years.
for example, they promise to provide a better understanding of material properties such as superconductivity or complex magnetism.
they also play an important role at the institute in garching.
for example, a simulator can consist of a cloud of ultracold atoms trapped in a spatial lattice of laser light.
if these quantum bits- or qubits for short- are to interact with each other, they do so by exchanging light quanta, photons.
however, an atom normally emits such a photon in some random direction.
it would be much more efficient for quantum simulations if the qubit could target its photon directly to its next or next but one neighbor.
robust photon radio
gonzalez- tudela and his team have now developed a theoretical principle that enables such a targeted "photon radio" between atoms.
"we have to pack the qubits and photons into a waveguide," explains the theorist.
however, how do you "wire" an ensemble of atoms floating in a light grid in space with such waveguides and make them talk in a robust way?
the answer of the four theorists is: with extremely tricky light.
the trick is essentially to transfer the mathematical concept of topology from solid state physics to photonics.
in solid state physics, it has triggered a real hype in recent years because it can produce completely new, previously unknown material properties.
in 2016, the three british physicists david thouless, duncan haldane and michael kosterlitz were awarded the nobel prize in physics for successfully introducing topological concepts to solid state physics.
in principle, the question is how many holes a geometric body has.
a coffee cup, for example, has a hole in its handle just like a donut ring in its center, and thus both have the topological number one.
the consequence: from a purely geometric point of view, the cup and donut can easily be transformed into each other.
on the other hand, violent topological resistance is encountered when a one- hole donut is to be transformed into a three- hole pretzel.
in physics, this hole number rule has the consequence that the topology can enormously stabilize certain physical properties against disturbances.
and this leads to the second major challenge in quantum information and thus quantum simulation: ubiquitous disturbances cause the highly sensitive quantum information to decay rapidly.
"this so- called decoherence is the biggest problem of quantum information," says gonzalez- tudela.
the captivating properties of topology soon led clever minds to the conclusion that the sensitive quantum bits could be packaged in physical systems with such topological properties.
this is being researched in solid state physics, for example, and large companies such as microsoft are also investing heavily in this research.
topological toolbox
gonzalez- tudela and his three co- authors have now devised a toolbox with which such topological concepts can be transferred into photonics.
some systems, such as ultracold atoms in light grids, are already very advanced in their controllability.
they therefore offer many possibilities for quantum simulation.
the toolbox of the four theorists opens a new space for many creative ideas.
simply put, it consists of a set of quantum bits, for example single atoms arranged in a line.
they can interact with a cleverly constructed, linear "light bath" that behaves like the waveguide the theoretical physicists were looking for.
if one now manipulates the various adjusting screws of the system, the quantum bits can exchange photons as desired via this waveguide.
but not only that: for example, a qubit can send its information in one direction, but remain completely dark in the opposite direction.
such interactions are extremely difficult to be produced in the micro world of atoms.
thus the toolbox of the four theorists offers many new possibilities to let quantum bits communicate with each other.
this is exactly what future quantum simulators need.
the concept is also universal: it can also be realized in some quantum systems composed by many qubits that are currently being researched.
the new work of the four theorists could become the nucleus for completely new ideas, ranging from pure basic research to quantum information.
scientists at the university of sydney have for the first time demonstrated improvement in quantum computers by using codes designed to detect and discard errors in the logic gates of such machines.
"this is really the first time that the promised benefit for quantum logic gates from theory has been realised in an actual quantum machine," said dr. robin harper, lead author of a new paper published this week in the prestigious journal, physical review letters.
quantum logic gates are formed by entangled networks of a small number of quantum bits, or qubits.
they are the switches that allow quantum computers to run algorithms, or recipes, to process information and perform calculations.
dr. harper and his colleague professor steven flammia, from the school of physics and university of sydney nano institute, used ibm's quantum computer to test error detection codes.
they demonstrated an order of magnitude improvement in reducing infidelity, or error rates, in quantum logic gates, the switches that will form the basis of fully functioning quantum computers.
dr. jay gambetta, ibm fellow and principal theoretical scientist with ibm q, said: "this paper is a great example of how scientists can use our publicly available cloud systems to probe fundamental problems.
here harper and flammia show that ideas of fault tolerance can be explored on real devices we are building and already deploying, today."
quantum technologies are still in their infancy but promise to revolutionise computing in the 21st century by performing calculations thought to be beyond the ability of the largest and fastest supercomputers.
they will do this using the unusual properties of matter at the quantum scale that allow them to process information using qubits.
these are computing elements that utilise the fact that quantum objects can exist in an indeterminate state, known as superposition, and can become 'entangled', a phenomenon describing behaviour not seen in conventional computers.
however, electronic 'noise' easily disrupts these states, quickly producing errors in quantum computations, which makes development of useful machines very difficult.
"current devices tend to be too small, with limited interconnectivity between qubits and are too 'noisy' to allow meaningful computations," dr. harper said.
"however, they are sufficient to act as test beds for proof of principle concepts, such as detecting and potentially correcting errors using quantum codes."
whereas the classical switches in your laptop or mobile phone can run for many years without error, at this stage quantum switches begin to fail after just fractions of a second.
"one way to look at this is through the concept of entropy," said professor flammia.
"all systems tend to disorder.
in conventional computers, systems are refreshed easily and reset using dram and other methods, effectively dumping the entropy out of the system, allowing ordered computation," he said.
"in quantum systems, effective reset methods to combat entropy are much harder to engineer.
the codes we use are one way to dump this entropy from the system," said professor flammia, who today was awarded the prestigious pawsey medal by the australian academy of science.
using codes to detect and discard errors on ibm's quantum device, dr. harper and professor flammia showed error rates dropping from 5.8 percent to 0.60 percent.
so rather than one in 20 quantum gates failing, just one in 200 would fail, an order of magnitude improvement.
"this is an important step forward to develop fault tolerance in quantum systems to allow them to scale up to meaningful devices," dr. harper said.
the physicists, who are both researchers with the arc centre of excellence for engineered quantum systems, emphasised that this was a demonstration of fault tolerant gates on pairs of qubits.
"there is still a long way to go before the quantum community can demonstrate fault tolerant computing," dr. harper said.
he said that other groups have shown improvements in other facets of quantum devices using codes.
the next step is to synthesise and test these approaches on larger- scale devices of a few dozen qubits that enable the reuse and reinitialisation of qubits.
companies such as ibm, google, rigetti and ionq have started or are about to start allowing quantum researchers to test their theoretical approaches on these small, noisy machines.
"these experiments are the first confirmation that the theoretical ability to detect errors in the operation of logical gates using quantum codes is advantageous in present- day devices, a significant step towards the goal of building large- scale quantum computers," dr. harper said.
bipartisan, bicameral support.
when was the last time you heard that phrase?
well, debate around the national quantum initiative act (nqia) at the end of 2018 demonstrated just that- bipartisan, bicameral u.s. congressional support.
the consensus is because the digitization of nearly everything is creating an explosion of both structured and unstructured data and fueling the desire to collect, analyze and act on it.
this is driving exponential demand for compute performance and spurring research into new, specialized computing architectures- among these, quantum computing.
quantum computing holds the promise of solving complex problems that are insurmountable today, even with our most powerful supercomputers.
that's because quantum computers use qubits that can exist in multiple states simultaneously, offering the potential to compute a large number of calculations in parallel, speeding time to resolution.
development of commercially viable quantum computers is still in its infancy, but with the passage of the nqia on december 21, 2018, the u.s. took a major step to promote its research, development and- ultimately- its commercialization.
under the nqia, the u.s. authorized $1.275 billion to promote research and development initiatives against quantum information sciences (qis) across a number of federal agencies.
through the passage of the nqia, the national science foundation and the department of energy will receive funding and directives to promote "human resources development in all aspects of quantum information science and engineering" and "research experiences and training" respectively.
in short- the bill promotes the development of a quantum computing workforce.
but this is no simple feat, and it will require a lot more conversation around what it means to qualify for a job in quantum computing.
we need to answer one key question: do you need to have focused your academic studies on quantum physics to work in quantum computing?
i would argue, no.
i think there is a common misconception in the technology sector that suggests you need to be an expert in quantum before you enter the field.
but, at intel, that is not the case.
we have qualified experts in chemistry and materials science driving the fabrication, testing and refinement of our quantum computing technologies and we value their individualized expertise and diversity of thinking for its ability to push our theories further.
that said, i frequently chat with truly talented engineers working in quantum- adjacent fields who do not initially see how their expertise and vision can help further the r&d and growth of quantum computing.
and that is a major opportunity for progress.
to address this issue, we can turn our attentions to the scientific curriculum in the country.
we can begin by integrating the basics of quantum mechanics into educational programming, beginning in high school.
we can provide exposure to the basics- superposition and entanglement- that will help students connect quantum applications to the fields they are studying and understand quantum phenomena in the world around them.
then, we can further that understanding throughout the collegiate science curriculum.
we should continually integrate applications from chemistry to physics to computer science and engineering to illustrate how quantum adjacent disciplines are necessary for the advancement of the field.
quantum computing represents a step- change from the technologies that have driven progress over the past 50 years.
if we can assume that quantum computing is poised to become the major computational system driving progress for the next century, we must ensure we have the talent to develop and progress this new paradigm.
last year's passage of the nqia is a major step forward to ensuring the united states continues to lead in quantum information science.
but the next critical step, is for congress to appropriate the funds called for in the bill, as fast as possible.
as we enter the congressional appropriations season, we hope lawmakers will consider how crucial it is that they put money behind the key initiative they passed last year.
- jim clarke is the director of quantum hardware at intel labs.
a huge consortium of european researchers is solving some of the fundamental obstacles blocking real quantum computing applications in the short term.
at the same time, it is helping to pave the way to a quantum computer.
it is not easy being quantum.
the rules are different, often they do not seem to make sense, and as soon as you look at one thing, everything else changes.
quantum science is difficult and challenging, but that is the main reason it is so darned interesting.
quantum mechanics led to the systematic exploitation of materials at a subatomic scale, leading to the laser, transistor and all solid- state physics such as semiconductors and microprocessors.
it illuminated biology and chemistry, because it showed that mystifying, almost incomprehensible subatomic principles governed the nature of matter.
up to now, science has exploited quantum phenomena on a macro scale - how it impacts electrons in a conducting material, for example - and to explain why materials behave in seemingly strange ways under specific conditions.
huge consortium
but now a huge consortium of 35 european scientific and industrial actors is working together to study how to directly exploit quantum phenomena like uncertainty, entanglement and others in real- world applications.
the qubit applications integrated project, or qap for short, is the start of the road to quantum computing.
"we are not looking to create a quantum computer directly," explains professor ian walmsley, co- coordinator of the qap project.
"other people are working on that, and it will take a long time to solve that problem."
"we are, however, looking at some of the problems facing real- world quantum applications that we could deploy now.
these are problems that must be solved anyway, if a quantum computer is to become possible.
problems like the storage of information encoded on a photon.
"but by focusing on these problems, we can perhaps create important new products that could be developed in the short and medium term, and we could solve some of the fundamental problems affecting the advent of quantum computing."
tied up over entanglement
it is a very effective approach and, luckily, the consortium has a wide choice of topics to consider.
the work is divided into five sections, looking at issues such as the storage of quantum information and transmission of certain quantum states, like entanglement, over long distances using repeaters.
unsurprisingly, the consortium will study networks, too, and will be looking at quantum applications for simulation of extremely complex problems.
"finally, all this will need a focused dose of theory that helps frame the right questions and to understand the experimental results," notes walmsley.
it is an ambitious programme but qap has the resources to make it happen.
apart from the 35 scientific and industrial partners, most of them leading authorities in their field, qap enjoys a four- year research period and a budget of almost eur13m.
an even greater resource, however, is the multidisciplinary nature of the consortium, from computer scientists and applied mathematicians to experimental physicists, as well as some very impressive industrial scientists and engineers.
using exotic components such as color codes, new phases of quantum matter, and extra dimensions, a team of physicists has shown that it's theoretically possible to construct a quantum computer that has the ability to correct itself whenever an error occurs.
"the greatest significance of our work is showing that self- correcting quantum computing at a finite temperature is not impossible as a matter of principle," physicist hector bombin told phys.org.
bombin was at mit in cambridge, massachusetts, while performing the study and is currently at the perimeter institute in waterloo, ontario.
"in fact, the original motivation came from claims to the contrary by other authors.
we provide explicit constructions that can be checked directly, without numerical simulations."
bombin and his coauthors have published a paper on their proposed self- correcting quantum computer in a recent issue of the new journal of physics.
error correction in quantum computers cannot be performed the same way as in classical computers, where information is stored multiple times for redundancy.
since copying quantum information is impossible due to the no- cloning theorem, physicists must find other ways to protect quantum information against errors.
as bombin and his coauthors explain in their paper, quantum computers can be classified into three categories based on their protection against errors.
the first type is bare quantum computers, which do not have any type of error correction.
these quantum computers have already been realized with ion traps and optical lattices.
the second type is externally protected quantum computers, which can be acted upon externally in order to repair errors.
although this type has not been successfully implemented yet, theoretical studies indicate that there are no fundamental obstacles to reach them when quantum technologies are fully developed.
the third type is internally protected (or self- correcting) quantum computers, which is the most demanding type because they can correct themselves whenever an error occurs.
the standard classical computers that we use today are self- correcting, which is one of the properties that makes them so successful.
but developing a self- correcting quantum computer is much more difficult.
illustrating just how difficult it is, the physicists say that the task will amount to finding a new quantum state of matter.
in their paper, the physicists theoretically showed how to construct a self- correcting quantum computer using a candidate for a new quantum state of matter called color codes.
color codes are a class of topological codes, which themselves have gained attention as a new phase of quantum matter due to their topologically ordered states.
the physicists explain that color codes have very special transversality properties and a mathematical structure that is "colored."
in their model, the physicists constructed a color code in six and seven dimensions and explained how the code can be used to implement universal quantum computation that provides self- correction.
to demonstrate the self- protection capabilities of the quantum computer, the physicists showed that it has a stable memory which, if exposed to a local noise for a short time, can preserve its quantum information.
if a self- correcting quantum computer could be built, it would have advantages over externally protected quantum computers, bombin explained.
"external correction requires complex architectures involving enormous numbers of physical qubits to operate effectively on just a few logical qubits," he said.
"if we had at hand suitable quantum phases of matter to use as quantum registers, architectures would dramatically simplify.
in fact, the usual problem with conventional experimental approaches to quantum computing is scalability.
in the case of self- correcting quantum computers, the problem is to find a suitable phase, but scalability should be much more straightforward."
very recently, several other papers have been published that also address the possibility of self- correcting quantum computers.
while some of these proposals are similar to the one here, the physicists note that these proposals do not work at a fixed temperature, while the one presented here does.
although each proposal has its own advantages, operating at a fixed temperature makes their model the most demanding and realistic scenario, although much more work is needed to build such a computer.
among the challenges that the researchers face is lowering the dimensionality of their model.
"a major goal is to explore theoretically quantum phases of matter in two and three spatial dimensions with the goal of finding candidates for self- correcting quantum memories," bombin said.
"the self- correcting property is related to the confinement of excitations, and this may serve as a guide for research."
the one thing everyone knows about quantum mechanics is its legendary weirdness, in which the basic tenets of the world it describes seem alien to the world we live in.
superposition, where things can be in two states simultaneously, a switch both on and off, a cat both dead and alive.
or entanglement, what einstein called "spooky action- at- distance" in which objects are invisibly linked, even when separated by huge distances.
but weird or not, quantum theory is approaching a century old and has found many applications in daily life.
as john von neumann once said: "you don't understand quantum mechanics, you just get used to it."
much of electronics is based on quantum physics, and the application of quantum theory to computing could open up huge possibilities for the complex calculations and data processing we see today.
imagine a computer processor able to harness super- position, to calculate the result of an arbitrarily large number of permutations of a complex problem simultaneously.
imagine how entanglement could be used to allow systems on different sides of the world to be linked and their efforts combined, despite their physical separation.
quantum computing has immense potential, making light work of some of the most difficult tasks, such as simulating the body's response to drugs, predicting weather patterns, or analysing big datasets.
such processing possibilities are needed.
the first transistors could only just be held in the hand, while today they measure just 14 nm - 500 times smaller than a red blood cell.
this relentless shrinking, predicted by intel founder gordon moore as moore's law, has held true for 50 years, but cannot hold indefinitely.
silicon can only be shrunk so far, and if we are to continue benefiting from the performance gains we have become used to, we need a different approach.
quantum fabrication
advances in semiconductor fabrication have made it possible to mass- produce quantum- scale semiconductors - electronic circuits that exhibit quantum effects such as super- position and entanglement.
the image, captured at the atomic scale, shows a cross- section through one potential candidate for the building blocks of a quantum computer, a semiconductor nano- ring.
electrons trapped in these rings exhibit the strange properties of quantum mechanics, and semiconductor fabrication processes are poised to integrate these elements required to build a quantum computer.
while we may be able to construct a quantum computer using structures like these, there are still major challenges involved.
in a classical computer processor a huge number of transistors interact conditionally and predictably with one another.
but quantum behaviour is highly fragile; for example, under quantum physics even measuring the state of the system such as checking whether the switch is on or off, actually changes what is being observed.
conducting an orchestra of quantum systems to produce useful output that couldn't easily by handled by a classical computer is extremely difficult.
but there have been huge investments: the uk government announced ps270m ... quantum technologies in 2014 for example, and the likes of google, nasa and lockheed martin are also working in the field.
it's difficult to predict the pace of progress, but a useful quantum computer could be ten years away.
the basic element of quantum computing is known as a qubit, the quantum equivalent to the bits used in traditional computers.
to date, scientists have harnessed quantum systems to represent qubits in many different ways, ranging from defects in diamonds, to semiconductor nano- structures or tiny superconducting circuits.
each of these has is own advantages and disadvantages, but none yet has met all the requirements for a quantum computer, known as the divincenzo criteria.
the most impressive progress has come from d- wave systems, a firm that has managed to pack hundreds of qubits on to a small chip similar in appearance to a traditional processor.
quantum secrets
the benefits of harnessing quantum technologies aren't limited to computing, however.
whether or not quantum computing will extend or augment digital computing, the same quantum effects can be harnessed for other means.
the most mature example is quantum communications.
quantum physics has been proposed as a means to prevent forgery of valuable objects, such as a banknote or diamond, as illustrated in the image below.
here, the unusual negative rules embedded within quantum physics prove useful; perfect copies of unknown states cannot be made and measurements change the systems they are measuring.
these two limitations are combined in this quantum anti- counterfeiting scheme, making it impossible to copy the identity of the object they are stored in.
the concept of quantum money is, unfortunately, highly impractical, but the same idea has been successfully extended to communications.
the idea is straightforward: the act of measuring quantum super- position states alters what you try to measure, so it's possible to detect the presence of an eavesdropper making such measurements.
with the correct protocol, such as bb84, it is possible to communicate privately, with that privacy guaranteed by fundamental laws of physics.
quantum communication systems are commercially available today from firms such as toshiba and id quantique.
while the implementation is clunky and expensive now it will become more streamlined and miniaturised, just as transistors have miniaturised over the last 60 years.
improvements to nanoscale fabrication techniques will greatly accelerate the development of quantum- based technologies.
and while useful quantum computing still appears to be some way off, it's future is very exciting indeed.
the journal science has echoed a novel experiment in the field of quantum physics in which several members of the quantum information theory and quantum metrology research group of the department of theoretical physics and history of science at the upv/ehu's faculty of science and technology participated, led by geza toth, ikerbasque research professor, and carried out at the university of hannover.
in the experiment, they achieved quantum entanglement between two ultra- cold atomic clouds, known as bose- einstein condensates, in which the two ensembles of atoms were spatially separated from each other.
quantum entanglement was discovered by schrodinger and later studied by einstein and other scientists in the last century.
it is a quantum phenomenon that has no counterparts in classical physics.
the groups of entangled particles lose their individuality and behave as a single entity.
any change in one of the particles leads to an immediate response in the other, even if they are spatially separated.
"quantum entanglement is essential in applications such as quantum computing, since it enables certain tasks to be performed much faster than in classical computing," explained the leader of the quantum information theory and quantum metrology group geza toth.
unlike the way in which quantum entanglement between clouds of particles has been created up to now, and which involves using incoherent and thermal clouds of particles, in this experiment they used a cloud of atoms in the bose- einstein condensate state.
as toth explained, "bose- einstein condensates are achieved by cooling down the atoms to very low temperatures, close to absolute zero.
at that temperature, all the atoms are in a highly coherent quantum state; in a sense, they all occupy the same position in space.
in that state quantum entanglement exists between the atoms of the ensemble."
subsequently, the ensemble was split into two atomic clouds.
"we separated the two clouds from each other by a distance, and we were able to demonstrate that the two parts remained entangled with each other," he continued.
the demonstration that entanglement can be created between two ensembles in the bose- einstein condensate state could lead to an improvement in many fields in which quantum technology is used, such as quantum computing, quantum simulation and quantum metrology, since these require the creation and control of large ensembles of entangled particles.
"the advantage of cold atoms is that it is possible to create highly entangled states containing quantities of particles outnumbering any other physical systems by several orders of magnitude, which could provide a basis for large scale quantum computing," said the researcher.
quantum materials- a family that includes superfluids, superconductors, exotic magnets, ultracold atoms and recently- discovered 'topological insulators'- display on a large scale some of the remarkable quantum effects usually associated with microscopic and subatomic particles.
but, while quantum mechanics explains the behaviour of microscopic particles, applying quantum theory to larger systems is far more challenging.
"while the potential of quantum materials, such as superconductors, is undeniable, we need to fully grasp the underlying quantum physics at play in these systems to establish their true capabilities," says chris vale, an associate professor at the centre for quantum and optical science, who led the research.
"that's a big part of the motivation for what we do."
associate professor vale and his colleagues, including sascha hoinka and paul dyke, also at swinburne, developed a new way to explore the behaviour of this family of materials.
they detected when a 'fermi gas' of lithium atoms, a simple quantum material, entered a quantum 'superfluid' state.
new system checks theories against experiment
their system allows theories of superconductivity and related quantum effects to be precisely checked against experiment, to see whether the theories are accurate and how they could be refined.
the researchers' advance was based on the fact that quantum materials' special properties emerge when their constituent particles enter a synchronised state.
the zero- resistance flow of electrons through superconductors, for example, emerges when electrons can team up to form 'cooper pairs'.
the team's sophisticated experimental set- up allowed this co- ordinated quantum behaviour to be detected.
by fine- tuning the interaction of their lasers with the fermi gas, associate professor vale and his colleagues were for the first time able to detect the elusive, low energy goldstone mode, an excitation that only appears in systems that have entered a synchronised quantum state.
"because our experiment provides a well- controlled environment and the appearance of the goldstone mode is very clear, our measurements provide a benchmark that quantum theories can be tested against before they're applied to more complex systems like superconductors," associate professor vale says.
"by developing methods to understand large systems that behave quantum mechanically, we're building the knowledge base that will underpin future quantum- enabled technologies."
the team's research has been published in the online journal nature physics.
on september 20, the financial times reported that a google research paper temporarily posted online said google's quantum computer has reached "quantum supremacy," a threshold at which quantum computers are theorized to be capable of solving problems, which traditional computers would not (practically) be able to solve.
if true, this represents a major milestone in the advancement of quantum computing.
according to the financial times, the report said that google's quantum computer performed a computation in just over 3 minutes that would take 10,000 years to perform on ibm summit, the world's top- ranking supercomputer.
the paper indicated, "to our knowledge, this experiment marks the first computation that can only be performed on a quantum processor."
the specific computation in question is at present unclear, though if google has truly reached quantum supremacy, this would necessarily be a milestone worthy of celebration.
instead, google quietly removed the research paper and declined to comment on the report to techrepublic sister site cnet.
industry reaction to the news has largely been sceptical, as expectations that google's advancement is either overblown, or not applicable to practical business use of quantum computers.
"the experiment and the 'supremacy' term will be misunderstood by nearly all," said dario gil, director of ibm research, in a written statement.
"quantum computers are not 'supreme' against classical computers because of a laboratory experiment designed to essentially (and almost certainly exclusively) implement one very specific quantum sampling procedure with no practical applications.
in fact, quantum computers will never reign 'supreme' over classical computers, but will rather work in concert with them, since each have their unique strengths."
"for quantum to positively impact society, the task ahead is to continue to build and make widely accessible truly programmable quantum computing systems that can implement, reproducibly and reliably, a broad array of quantum algorithms and programs," gil said.
"this is the only path forward for practical solutions to be realized in quantum computers.
only then will we get to the era of quantum advantage, when we will get to use "quantum + classical" systems in concert to accelerate discovery in science and to create commercial value in business.
that goal, no matter current claims, has not been realized."
gil said that over- hyping "quantum supremacy" could lead to a quantum winter, similar to the "ai winter" of reduced funding and interest in artificial intelligence research, a concern also raised by gartner research vice president matthew brisse.
"if [the research paper] is validated, and we can actually show quantum supremacy, that's a huge leap in the quantum computing industry," brisse said.
"i've taken over 300 calls this to date, nobody is categorically super concerned about 'how does it work?
', it is all about 'how do i turn this into competitive advantage?'"
brisse said.
intel's reaction to the news was moderately warmer.
"google's recent update on the achievement of quantum supremacy is a notable mile marker as we continue to advance the potential of quantum computing.
achieving a commercially viable quantum computer will require advancements across a number of pillars of the technology stack," said jim clarke, director of quantum hardware at intel labs.
"we along with the industry are working to quickly advance all of those areas to realize the true potential of quantum computing.
and while development is still at mile one of the marathon, we strongly believe in the potential of this technology."
for more on quantum computing, check out "aliro aims to make quantum computers usable by traditional programmers" and "why post- quantum encryption will be critical to protect current classical computers" at techrepublic.
also see
scientists from the russian quantum center (moscow), lead by prof. alexander lvovsky, have developed a method to restore quantum entanglement and have tested this method in an experiment.
this research significantly broadens the possibilities of quantum communication and quantum cryptography.
the results are published in nature photonics.
the phenomenon of quantum entanglement (or quantum correlation), in which the states of two or more objects such as atoms, photons or ions are linked, is the basis of modern quantum technology.
this phenomenon plays a particularly important role in secure quantum communication systems, which completely eliminate the possibility of undetected "wiretapping."
however, entangled states are very fragile, breaking easily during transmission due to noise or optical losses.
in addition, the very creation of entanglement is a difficult process requiring complex experimental setups.
therefore, the possibility of transmission of quantum information is very limited.
existing equipment is only capable of transferring data at a distance of about 100 kilometers.
many research groups are now looking for ways to increase this limited distance.
otherwise, practical applications of quantum technologies are extremely limited.
lvovsky's group in the quantum optics laboratory at the russian quantum center conducted a series of experiments in which they managed to restore the level of quantum correlation between pulses of light in two optical channels, which was almost completely destroyed after a 20x optical loss.
this corresponds to the level of loss in 65 kilometers of ordinary fiber optic cable.
a nonlinear crystal of periodically poled potassium titanyl phosphate was used as the source of entangled photons in the experiment.
picosecond pulses of light generated by a titanium- sapphire laser were "fired" at the crystal.
as a result, entangled photon pairs were produced in the crystal and directed into two different optical channels.
in one of them, the light was subjected to 20- fold attenuation using darkened glass, causing the level of entanglement to fall almost to zero.
then a special amplification procedure was applied, restoring the quantum properties of light in the channel to levels close to the levels measured before the loss.
this procedure involves mixing the light pulse in the channel with a single "auxiliary" photon in a beam splitter (a partially transparent mirror).
a single photon detector is placed at one of the outputs of the beam splitter.
if the detector "clicks", this means that the photon has entered into the beam splitter and left.
it would seem that the state of the second pulse to enter the beam splitter (a part of the entangled pair) should not change.
but, because of paradoxical properties of quantum interference, the state changes toward the "strengthening" of its quantum properties.
this phenomenon, discovered by lvovsky and colleagues in 2002, has been named quantum catalysis, because the "auxiliary" photon, like a chemical catalyst, is not itself involved in the reaction, but changes the state of light in the other channel.
"at that time, it seemed no more than a curious phenomenon, of which there are plenty in quantum physics.
now it turns out to be of great practical use- it allows one to restore entanglement of quantum light," says lvovsky.
according to him, this work is a step toward building a quantum repeater- a device capable of reducing the loss of quantum information over fiber optic connections.
in the future, this could enable a global system of transmission of quantum information, and overcome constraints on quantum cryptography.
"of course there is a price for the restoration of entanglement- out of one million weakly entangled pairs of photons, there is one highly entangled one.
but in this regard, the level of correlation is restored to the original, and while the data transmission rate is somewhat reduced, we can get a stable connection at a much greater distance," said study co- author alexander ulanov.
at the technology conference "web summit 2017" (lisbon), the volkswagen group and google announced comprehensive research cooperation in the field of quantum computing.
the two companies will explore the utilization of quantum computers together, with aims to build up specialist knowledge and to carry out practically oriented research.
as part of this collaboration, a team of specialists from volkswagen and google will work together using a google quantum computer.
quantum computers can solve certain highly complex tasks considerably faster than conventional supercomputers.
in some cases, a solution will only be possible with quantum computers.
volkswagen group it wants to make progress in three development areas on the google quantum computer.
the specialists intend to continue the development of traffic optimization, to explore structures for new materials, especially high- performance batteries for electric vehicles, and to work on artificial intelligence with new machine learning processes.
google's quantum computer.
copyright: google, eric lukero.
martin hofmann, chief information officer of the volkswagen group, says: "quantum computing technology opens up new dimensions and represents the fast- track for future- oriented topics.
we at volkswagen want to be among the first to use quantum computing for corporate processes as soon as this technology is commercially available.
thanks to our cooperation with google, we have taken a major step towards this goal."
hartmut neven, director of the google quantum artificial intelligence laboratory, says: "volkswagen has enormous expertise in solving important, real- world engineering problems, and it is an honor for us to collaborate on how quantum computing may be able to make a difference in the automotive industry."
this collaboration will focus on research for practically applications.
specialists from the volkswagen information technology centers (it labs) in san francisco and munich will develop algorithms, simulations and optimizations together with the google experts.
they will carry out this work using google universal quantum computers.
this architecture is suitable for many experimental computing operations.
"volkswagen's collaboration with google marks the beginning of quantum computing in the automotive industry, and is a paramount step to addressing modern mobility challenges unlikely to be solved with binary digital electronic computers," said abdallah shanti, executive vice president and group chief information & digital officer for region americas, volkswagen of america, inc. "through this partnership, volkswagen intends to unlock the potential of this technology, and share our learnings to motivate the development of quantum computers and algorithms."
volkswagen group it development areas
volkswagen group it intends to explore the potential of this quantum computer in several different areas.
in the first project, the volkswagen specialists are working on the further development of traffic optimization.
they are building on the research project which they have already successfully completed and now want to consider additional variables in addition to reducing travelling times.
these include urban traffic guidance systems, available electric charging stations or vacant parking spaces.
in a second project, the volkswagen specialists aim to simulate and optimize the structure of high- performance batteries for electric vehicles and other materials.
volkswagen group research and development experts expect this approach to provide new information for vehicle construction and battery research.
a third project concerns the development of new machine learning processes.
machine learning is a key technology for the development of advanced ai systems, which are a prerequisite for autonomous driving.
the volkswagen group is the first automotive company in the world to work intensively on quantum computing technology.
in march 2017, volkswagen announced its first successful research project completed on a quantum computer: a traffic flow optimization for 10.000 taxis in the chinese capital beijing.
the most basic type of quantum information processing is quantum entanglement.
in a new study published in epj b, zhaonan zhang from shaanxi normal university, xi'an, china, and colleagues have provided a much finer characterisation of the distributions of entanglement in multi- qubit systems than previously available.
these findings can be used in quantum cryptography to estimate the quantity of information an eavesdropper can capture regarding the secret encryption key.
encrypted communication is achieved by sending quantum information in basic units called quantum bits, or qubits.
the most basic type of quantum information processing is quantum entanglement.
however, this process remains poorly understood.
better controlling quantum entanglement could help to improve quantum teleportation, the development of quantum computers, and quantum cryptography.
now, a team of chinese physicists have focused on finding ways to enhance the reliability of quantum secret sharing.
in a new study published in epj b, zhaonan zhang from shaanxi normal university, xi'an, china, and colleagues provide a much finer characterisation of the distributions of entanglement in multi- qubit systems than previously available.
in the context of quantum cryptography, these findings can be used to estimate the quantity of information an eavesdropper can capture regarding the secret encryption key.
physicists working on new ways of securing quantum encrypted messages are exploiting the fact that, at the quantum scale, a given qubit can only be entangled with one other qubit; this unique trait is referred to as monogamy of entanglement.
in practical terms, the quantum rules for entanglement are explained by considering three qubits, called a, b and c, belonging to alice, bob and charlie, respectively.
if alice and bob share quantum information via a two- qubit system, called ab, they cannot share any entangled states with charlie's qubit c.
however, there is also another kind of entanglement, called polygamy, in which qubits display partial entanglement with several qubits at the same time.
in this study, the authors develop a series of equations explaining the conditions for monogamy and polygamy, which are much better characterised than previous work.
specifically, they first investigate three- qubit systems under certain restrictions and then derive a general result for multi- qubit systems.
a computer science professor at amherst college who recently devised and conducted experiments to test the speed of a quantum computing system against conventional computing methods will soon be presenting a paper with her verdict: quantum computing is, "in some cases, really, really fast."
"ours is the first paper to my knowledge that compares the quantum approach to conventional methods using the same set of problems," says catherine mcgeoch, the beitzel professor in technology and society (computer science) at amherst.
"i'm not claiming that this is the last word, but it's a first word, a start in trying to sort out what it can do and can't do."
the quantum computer system she was testing, produced by d- wave just outside vancouver, bc, has a thumbnail- sized chip that is stored in a dilution refrigerator within a shielded cabinet at near absolute zero, or .02 degrees kelvin in order to perform its calculations.
whereas conventional computing is binary, 1s and 0s get mashed up in quantum computing, and within that super- cooled (and non- observable) state of flux, a lightning- quick logic takes place, capable of solving problems thousands of times faster than conventional computing methods can, according to her findings.
"you think you're in dr. seuss land," mcgeoch says.
"it's such a whole different approach to computation that you have to wrap your head around this new way of doing things in order to decide how to evaluate it.
it's like comparing apples and oranges, or apples and fish, and the difficulty was coming up with experiments and analyses that allowed you to say you'd compared things properly.
it definitely was the oddest set of problems i've ever coped with."
mcgeoch, author of a guide to experimental algorithmics (cambridge university press, 2012), has 25 years of experience setting up experiments to test various facets of computing speed, and is one of the founders of "experimental algorithmics," which she jokingly calls an "oddball niche" of computer science.
her specialty is, however, proving increasingly helpful in trying to evaluate different types of computing performance.
that's why she spent a month last fall at d- wave, which has produced what it claims is the world's first commercially available quantum computing system.
geordie rose, d- wave's founder and chief technical officer, retained mcgeoch as an outside consultant to help devise experiments that would test its machines against conventional computers and algorithms.
mcgeoch will present her analysis at the peer- reviewed 2013 association for computing machinery (acm) international conference on computing frontiers in ischia, italy, on may 15.
her 10- page- paper, titled "experimental evaluation of an adiabiatic quantum system for combinatorial optimization," was co- authored with cong wang, a graduate student at simon fraser university.
mcgeoch says the calculations the d- wave excels at involve a specific combinatorial optimization problem, comparable in difficulty to the more famous "travelling salesperson" problem that's been a foundation of theoretical computing for decades.
briefly stated, the travelling salesperson problem asks this question: given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the original city?
questions like this apply to challenges such as shipping logistics, flight scheduling, search optimization, dna analysis and encryption, and are extremely difficult to answer quickly.
the d- wave computer has the greatest potential in this area, mcgeoch says.
"this type of computer is not intended for surfing the internet, but it does solve this narrow but important type of problem really, really fast," mcgeoch says.
"there are degrees of what it can do.
if you want it to solve the exact problem it's built to solve, at the problem sizes i tested, it's thousands of times faster than anything i'm aware of.
if you want it to solve more general problems of that size, i would say it competes - it does as well as some of the best things i've looked at.
at this point it's merely above average but shows apromising scaling trajectory."
mcgeoch, who has spent her academic career in computer science, doesn't take a stance on whether the d- wave is a true quantum computer or not, a notionsome physicists take issue with.
"whether or not it's a quantum computer, it's an interesting approach to solving these problems that is worth studying," she says.
whether the d- wave computer will ever have mass market appeal is also difficult for mcgeoch to assess.
while the 439- qubit model she tested does have incredible computing power, there is that near- zero kelvin chip operating temperature requirement that would make home or office use a chilly proposition.
at present, she thinks the power of the d- wave approach is too narrowly focused to be of much use to the average personal computer user.
"the founder of ibm famously predicted that only about five of his company's first computers would be sold because he just didn't see the need for that much computing power," mcgeoch says.
"who needs to solve those big problems now?
i'd say it's probably going to be big companies like google and government agencies."
and, while conventional approaches to solving these problems will likely continue to improve incrementally, this fast quantum approach has the potential to expand to larger variety of problems than it does now, mcgeoch says.
"within a year or two i think these quantum computing methods will solve more and bigger problems significantly faster than the best conventional computing options out there," she says.
at the same time, she cautions that her first set of experiments represents a snapshot moment of the state of quantum computing versus conventional computing.
"this by no means settles the question of how fast the quantum computer is," she says.
"that's going to take a lot more testing and a variety of experiments.
it may not be a question that ever gets answered because there's always going to be progress in both quantum and conventional computing."
a team of researchers at oak ridge national laboratory has demonstrated that it is possible to use cloud- based quantum computers to conduct quantum simulations and calculations.
the team has written a paper describing their efforts and results and uploaded it to the arxiv preprint server.
as work progresses toward the development of quantum computers able to tackle some of the most difficult problems in computer science, attention has shifted to the means by which such machines would be used.
for example, if researchers build a big, expensive quantum computer able to model how atoms and particles behave under unusual conditions, how would research physicists access and use it?
that has led to the idea of cloud quantum computing so that anyone could access and use it from practically anywhere.
that idea has been put into practice by two companies investing seriously in a quantum computer- based future.
ibm has developed what it calls q experience, and rigetti has developed 19q.
the former has a quantum processor with 16 qubits while the later has 19.
in addition to building their computers, both companies have also developed software that makes the systems available on the internet.
to test the possibilities of such a platform, the team at oak ridge set themselves the task of using a quantum computer to calculate the nuclear binding energy of the deuterium nucleus (how much energy it would take to separate the neutron and proton).
the team used both cloud quantum computing systems, which required tweaking software to deal with the differing number of qubits the machines were able to use.
the team reports that the cloud responded with a binding energy that was within 2 percent of the actual measure.
the researchers report that their efforts prove that cloud- based quantum computing works, and that it will be ready for prime- time when truly powerful machines are developed capable of such tasks as simulating quantum physical systems or revealing reaction mechanisms in complex chemical systems.
more information: cloud quantum computing of an atomic nucleus, arxiv:1801.03897 [quant- ph] https://arxiv.org/abs/1801.03897
abstract
we report a quantum simulation of the deuteron binding energy on quantum processors accessed via cloud servers.
we use a hamiltonian from pionless effective field theory at leading order.
we design a low- depth version of the unitary coupled- cluster ansatz, use the variational quantum eigensolver algorithm, and compute the binding energy to within a few percent.
our work is the first step towards scalable nuclear structure computations on a quantum processor via the cloud, and it sheds light on how to map scientific computing applications onto nascent quantum devices.
one of the key concepts in quantum physics is entanglement, in which two or more quantum systems become so inextricably linked that their collective state can't be determined by observing each element individually.
now yale researchers have developed a "universal entangler" that can link a variety of encoded particles on demand.
the discovery represents a powerful new mechanism with potential uses in quantum computing, cryptography, and quantum communications.
the research is led by the yale laboratory of robert schoelkopf and appears in the journal nature.
quantum calculations are accomplished with delicate bits of data called qubits, which are prone to errors.
to implement faithful quantum computation, scientists say, they need "logical" qubits whose errors can be detected and rectified using quantum error correction codes.
"we've shown a new way of creating gates between logically- encoded qubits that can eventually be error- corrected," said schoelkopf, the sterling professor of applied physics and physics at yale and director of the yale quantum institute.
"it's a much more sophisticated operation than what has been performed previously."
the entangling mechanism is called an exponential- swap gate.
in the study, researchers demonstrated the new technology by deterministically entangling encoded states in any chosen configurations or codes, each housed in two otherwise isolated, 3- d superconducting microwave cavities.
"this universal entangler is critical for robust quantum computation," said yvonne gao, co- first author of the study.
"scientists have invented a wealth of hardware- efficient, quantum error correction codes- each one cleverly designed with unique characteristics that can be exploited for different applications.
however, each of them requires wiring up a new set of tailored operations, introducing a significant hardware overhead and reduced versatility."
the universal entangler mitigates this limitation by providing a gate between any desired input states.
"we can now choose any desired codes or even change them on the fly without having to re- wire the operation," said co- first author brian lester.
the discovery is just the latest step in yale's quantum research work.
yale scientists are at the forefront of efforts to develop the first fully useful quantum computers and have done pioneering work in quantum computing with superconducting circuits.
in a new study, physicists have teleported photonic qubits made of pairs of entangled photons that are generated by an led containing an embedded quantum dot.
the novel set- up has advantages compared to the conventional method of generating entangled photons using a laser, and could lead to a simplified technique for implementing quantum teleportation in quantum information applications.
the researchers, j. nilsson, et al., at toshiba research europe limited and the university of cambridge, both in cambridge, uk, have published their paper on demonstrating quantum teleportation using an led in a recent issue of nature photonics.
as the scientists explain, quantum teleportation- a process in which quantum information is destroyed so that it may be transferred simultaneously to another location- has been proposed as a way to create quantum communication networks and quantum computing protocols despite the no- cloning theorem.
according to the no- cloning theorem, quantum information cannot be copied.
although no- cloning enables quantum cryptography to have a high degree of security, it also limits the options to create quantum communication networks and increases the losses in quantum computing due to imperfect measurements.
teleporting the information may offer a solution for these two areas.
in quantum communication networks, teleportation can establish a quantum channel between two nodes.
in quantum computing, teleportation can transfer qubits from successful logic operations, while the other qubits can be thrown out.
although quantum teleportation can be implemented with different systems, the researchers here argue that photonic qubits are best suited for the largest number of applications.
one of the most important parts of the photonic teleportation process is having a light source that produces single pairs of entangled photons.
although lasers can be used to generate the photons, they involve practical complexities and sometimes generate multiple photon pairs, and these problems have inhibited their use in quantum information technologies.
"we can also produce entangled photon pairs by pumping a non- linear crystal with a laser," coauthor andrew shields at toshiba research europe limited told phys.org.
"although this has facilitated many experiments in quantum optics in the past, it has the disadvantage that the process sometimes produces two (or more) pairs.
these multiple pairs cause errors in quantum information processing schemes that become increasingly problematic as we scale to larger numbers of photons.
thus developing true quantum light sources (i.e., one that produces just one entangled pair at a time) is seen as essential to achieving useful applications in quantum communications and photonic quantum computing."
as an alternative, the researchers here created an entangled- light- emitting diode (eled) with an indium arsenide quantum dot to generate pairs of entangled photons one at a time.
using the eleds, the researchers demonstrated an average teleportation fidelity that exceeds the maximum that can be achieved using only classical correlations, proving the quantum nature of the teleportation.
a key difference with this set- up compared with laser- based set- ups is that here the photons are generated electrically rather than optically.
one benefit of electric generation is that the emission wavelength of the quantum dot can be easily tuned using electric fields, which could make it compatible with a wide range of input photons of different wavelengths.
"it is actually quite straightforward to embed a quantum dot in an led," shields said.
"the quantum dot is formed by a self- organizing method during the growth of the semiconductor layers.
the quantum dot is comprised of indium arsenide, which has a larger lattice constant than the gallium arsenide substrate.
this results in a very thin layer (about 2 monolayers thick) self- assembling into quantum dots.
after growing the rest of semiconductor structure, we are left with a layer of quantum dots embedded inside the led."
the researchers hope that the ability of the eleds to generate single pairs of entangled photons, along with future improvements in light collection efficiency and entanglement fidelity, will lead to the realization of a variety of teleportation- based quantum information applications.
"quantum teleportation is an important primitive in quantum information processing," shields said.
"we are planning to apply it to quantum communications and deterministic photonic quantum logic gates."
the realization of quantum networks is one of the major challenges of modern physics.
now, new research shows how high- quality photons can be generated from 'solid- state' chips, bringing us closer to the quantum 'internet'.
the number of transistors on a microprocessor continues to double every two years, amazingly holding firm to a prediction by intel co- founder gordon moore almost 50 years ago.
if this is to continue, conceptual and technical advances harnessing the power of quantum mechanics in microchips will need to be investigated within the next decade.
developing a distributed quantum network is one promising direction pursued by many researchers today.
a variety of solid- state systems are currently being investigated as candidates for quantum bits of information, or qubits, as well as a number of approaches to quantum computing protocols, and the race is on for identifying the best combination.
one such qubit, a quantum dot, is made of semiconductor nanocrystals embedded in a chip and can be controlled electro- optically.
single photons will form an integral part of distributed quantum networks as flying qubits.
first, they are the natural choice for quantum communication, as they carry information quickly and reliably across long distances.
second, they can take part in quantum logic operations, provided all the photons taking part are identical.
unfortunately, the quality of photons generated from solid- state qubits, including quantum dots, can be low due to decoherence mechanisms within the materials.
with each emitted photon being distinct from the others, developing a quantum photonic network faces a major roadblock.
now, researchers from the cavendish laboratory at cambridge university have implemented a novel technique to generate single photons with tailored properties from solid- state devices that are identical in quality to lasers.
their research is published today in the journal nature communications.
as their photon source, the researchers built a semiconductor schottky diode device containing individually addressable quantum dots.
the transitions of quantum dots were used to generate single photons via resonance fluorescence - a technique demonstrated previously by the same team.
under weak excitation, also known as the heitler regime, the main contribution to photon generation is through elastic scattering.
by operating in this way, photon decoherence can be avoided altogether.
the researchers were able to quantify how similar these photons are to lasers in terms of coherence and waveform - it turned out they were identical.
"our research has added the concepts of coherent photon shaping and generation to the toolbox of solid- state quantum photonics," said dr mete atature from the department of physics, who led the research.
"we are now achieving a high- rate of single photons which are identical in quality to lasers with the further advantage of coherently programmable waveform - a significant paradigm shift to the conventional single photon generation via spontaneous decay."
there are already protocols proposed for quantum computing and communication which rely on this photon generation scheme, and this work can be extended to other single photon sources as well, such as single molecules, colour centres in diamond and nanowires.
"we are at the dawn of quantum- enabled technologies, and quantum computing is one of many thrilling possibilities," added atature.
"our results in particular suggest that multiple distant qubits in a distributed quantum network can share a highly coherent and programmable photonic interconnect that is liberated from the detrimental properties of the chips.
consequently, the ability to generate quantum entanglement and perform quantum teleportation between distant quantum- dot spin qubits with very high fidelity is now only a matter of time."
scientists developing a prototype optical quantum hard drive have improved storage time by a factor of over 100.
the team's record storage time of six hours is a major step towards a secure worldwide data encryption network based on quantum information.
scientists developing a prototype quantum hard drive have improved storage time by a factor of more than 100.
the team's record storage time of six hours is a major step towards a secure worldwide data encryption network based on quantum information, which could be used for banking transactions and personal emails.
"we believe it will soon be possible to distribute quantum information between any two points on the globe," said lead author manjin zhong, from the research school of physics and engineering (rspe) at the australian national university (anu).
"quantum states are very fragile and normally collapse in milliseconds.
our long storage times have the potential to revolutionise the transmission of quantum information."
quantum information promises unbreakable encryption because quantum particles such as photons of light can be created in a way that intrinsically links them.
interactions with either of these entangled particles affect the other, no matter how far they are separated.
the team of physicists at anu and the university of otago stored quantum information in atoms of the rare earth element europium embedded in a crystal.
their solid- state technique is a promising alternative to using laser beams in optical fibres, an approach which is currently used to create quantum networks around 100 kilometres long.
"our storage times are now so long that it means people need to rethink what is the best way to distribute quantum data," ms zhong said.
"even transporting our crystals at pedestrian speeds we have less loss than laser systems for a given distance."
"we can now imagine storing entangled light in separate crystals and then transporting them to different parts of the network thousands of kilometres apart.
so, we are thinking of our crystals as portable optical hard drives for quantum entanglement."
after writing a quantum state onto the nuclear spin of the europium using laser light, the team subjected the crystal to a combination of a fixed and oscillating magnetic fields to preserve the fragile quantum information.
"the two fields isolate the europium spins and prevent the quantum information leaking away," said dr jevon longdell of the university of otago.
the anu group is also excited about the fundamental tests of quantum mechanics that a quantum optical hard drive will enable.
"we have never before had the possibility to explore quantum entanglement over such long distances," said associate professor matthew sellars, leader of the research team.
"we should always be looking to test whether our theories match up with reality.
maybe in this new regime our theory of quantum mechanics breaks."
quantum information scientists have introduced a new method for machine learning classifications in quantum computing.
the non- linear quantum kernels in a quantum binary classifier provide new insights for improving the accuracy of quantum machine learning, deemed able to outperform the current ai technology.
the research team led by professor june- koo kevin rhee from the school of electrical engineering, proposed a quantum classifier based on quantum state fidelity by using a different initial state and replacing the hadamard classification with a swap test.
unlike the conventional approach, this method is expected to significantly enhance the classification tasks when the training dataset is small, by exploiting the quantum advantage in finding non- linear features in a large feature space.
quantum machine learning holds promise as one of the imperative applications for quantum computing.
in machine learning, one fundamental problem for a wide range of applications is classification, a task needed for recognizing patterns in labeled training data in order to assign a label to new, previously unseen data; and the kernel method has been an invaluable classification tool for identifying non- linear relationships in complex data.
more recently, the kernel method has been introduced in quantum machine learning with great success.
the ability of quantum computers to efficiently access and manipulate data in the quantum feature space can open opportunities for quantum techniques to enhance various existing machine learning methods.
the idea of the classification algorithm with a nonlinear kernel is that given a quantum test state, the protocol calculates the weighted power sum of the fidelities of quantum data in quantum parallel via a swap- test circuit followed by two single- qubit measurements.
this requires only a small number of quantum data operations regardless of the size of data.
the novelty of this approach lies in the fact that labeled training data can be densely packed into a quantum state and then compared to the test data.
the kaist team, in collaboration with researchers from the university of kwazulu- natal (ukzn) in south africa and data cybernetics in germany, has further advanced the rapidly evolving field of quantum machine learning by introducing quantum classifiers with tailored quantum kernels.
the input data is either represented by classical data via a quantum feature map or intrinsic quantum data, and the classification is based on the kernel function that measures the closeness of the test data to training data.
dr. daniel park at kaist, one of the lead authors of this research, said that the quantum kernel can be tailored systematically to an arbitrary power sum, which makes it an excellent candidate for real- world applications.
professor rhee said that quantum forking, a technique that was invented by the team previously, makes it possible to start the protocol from scratch, even when all the labeled training data and the test data are independently encoded in separate qubits.
professor francesco petruccione from ukzn explained, "the state fidelity of two quantum states includes the imaginary parts of the probability amplitudes, which enables use of the full quantum feature space."
to demonstrate the usefulness of the classification protocol, carsten blank from data cybernetics implemented the classifier and compared classical simulations using the five- qubit ibm quantum computer that is freely available to public users via cloud service.
"this is a promising sign that the field is progressing," blank noted.
story source:
materials provided by the korea advanced institute of science and technology (kaist).
note: content may be edited for style and length.
constructing quantum computers and other quantum devices requires the ability to leverage quantum properties such as superposition and entanglement - but these effects are fragile and therefore hard to maintain.
recently, scientists at ecole normale superieure in paris demonstrated a novel method for controlling the quantum properties of light by probing a superconducting circuit in a cavity with microwave photons to control the energy levels that photon quanta can occupy.
specifically, the scientists prevented access to a single energy level corresponding to a number of photons n, and thereby confined the dynamics of the field to levels 0 to n - 1.
in so doing, the intracavity field changed from a classical wave to a schrodinger cat of light - a superposition between two waves of opposite phases instead of a single one.
as a result, this new technique could apply to the development of quantum computers by protecting qubits from decoherence as well as enhancing quantum error correction and quantum systems measurement.
prof. benjamin huard discussed the paper that he and his colleagues published in science.
"the primary difficulty in developing our method for manipulating electromagnetic modes by effectively controlling their phase space was to find a proper way of preventing any access to one or few energy levels," huard tells phys.org.
"we did that using another quantum system - a superconducting qubit - which allowed us to change the energy of any level we chose by simply turning a microwave signal on or off.
in this context, the main challenge was designing a cavity and a qubit with the right properties to realize and observe this new method of control."
the scientists also had an issue in finding that level occupation oscillated in time when using the same light mode and qubit for several operations.
"the basic experiment consists in driving the light mode while the qubit controls its phase space," huard explains.
"however, in order to measure the level occupation in time, we had to use the same qubit as a photocounter, and the same light mode to measure the qubit state."
(photocounters tally photon distributions by counting photoelectric electrons, or photoelectrons - that is, electrons emitted by various metals when illuminated by photons.)
a third obstacle was using circuit quantum electrodynamics, or circuit- qed, architecture - the implementation of quantum electrodynamics (a quantum field theory of electromagnetic force) for circuits - to apply quantum zeno dynamics (qzd) to light.
qzd is based on the quantum zeno effect (qze) - named after the zeno arrow paradox - in which if observed continuously an unstable particle will never decay, meaning that an unstable quantum system measured with sufficient frequency will not evolve.
however, in certain circumstances, quantum zeno dynamics - in which the quantum system changes over time - can occur.
"with its large number of energy levels and ease of control," huard points out, "a single electromagnetic mode offers a wider and more controllable phase space than atoms and two- level systems.
nevertheless, in order to make it work, we had to identify the several constraints on the parameters we had access to.
using superconducting circuits helped since it is fairly easy to tune their parameters and establish coupling to microwave light.
using the same systems twice for various operations required many careful calibrations, and we had to find the optimal temporal sequences to realize the experiment."
huard addresses the implications of the finding stated in the paper that under a resonant drive, or cavity, the level occupation was found to oscillate in time, similarly to an n- level system.
"by preventing any access to one energy level of the light mode, it indeed acts exactly as an n- level system - but here, since n can be chosen and modified in time, it's as if we had engineered an atom with a spin (n- 1)/2 that can change at will in time by simply turning on or off microwave signals.
it would be interesting to observe the dynamics of such a spin whose number is changing in time."
huard next discusses how fine control of the field in its phase space may enable applications in quantum information and metrology.
"our method is a new technique that can produce exotic quantum states of light similar to schrodinger cat states or vacuum squeezed states that are well- suited for quantum information or metrology purposes by increasing the precision of field or position measurements."
(a vacuum squeezed state is a nonclassical state of light in which quantum noise is no longer independent of the phase of the light wave, and is below the standard quantum limit.)
"our method can also be used to generate and protect entanglement, which is a fundamental resource of quantum information, as well as to perform quantum error correction on qubits encoded using schrodinger cat- like states."
in fact, the paper states that the new method allows the possibility of manipulating schrodinger cat states in a unique way.
"our method is the essential brick that enables the creation of phase space light tweezers, as proposed by jean- michel raimond and coworkers1," huard says.
"these tweezers can displace parts of the wigner function in its phase space one at a time."
(the wigner function is a so- called quasiprobability distribution that links the schrodinger wavefunction to a probability distribution in phase space, and counterintuitively can have regions of negative probability density.)
"it therefore becomes possible to enlarge or rotate a schrodinger cat state directly in its phase space."
this leads to the ability to effect quantum error correction of cat- qubits (quantum information encoded in logical bases composed of schrodinger cat states) as a quantum computing paradigm.
"in fact," huard points out, "a way to encode quantum information with superpositions of cat- like states was recently proposed in the context of circuit- qed by mazyar mirrahimi and coworkers2," adding that finding ways to perform quantum error correction on these states is essential for their potential use in a quantum computing architecture.
"we believe that our technique could be used to perform this quantum error correction in a unique way.
indeed, decoherence leads to an exponential relaxation of the cat size, which needs to be overcome - and by displacing the cat 'legs' one by one using qzd, this relaxation can be canceled without losing any quantum information."
looking ahead, huard tells phys.org that the researchers are "pursuing the investigation of the fascinating effect of measurement on quantum systems.
for instance," he illustrates, "we now have an experiment where we intercept the signal that leaks towards the environment of a qubit and usually leads to decoherence.
however, using this signal we can now infer what the environment knows about the qubit state and preserve the purity of the quantum state - and we've recently managed to use that signal for preserving any state by feedback."
in addition, he adds, they are interested in applying their technique to systems with longer coherence times and to schrodinger cat preservations.
regarding other areas of research that might benefit from their study, huard concludes that "it's hard to tell right now - but in the long run, if our technique helps build quantum simulators or computers, it could have an impact in many areas requiring intense computation, such as machine learning or chemistry."
more information: quantum dynamics of an electromagnetic mode that cannot contain n photons, science (2015) 348:6236 776- 779, doi:10.1126/science.1259345
related:
1phase space tweezers for tailoring cavity fields by quantum zeno dynamics, physical review letters (2010) 105:213601, doi:10.1103/physrevlett.105.213601
2dynamically protected cat- qubits: a new paradigm for universal quantum computation, new journal of physics (2014) 16:045014, doi:10.1088/1367- 2630/16/4/045014
oxford instruments superconductivity has joined a collaboration in quantum nanotechnology that will drive research and development of quantum scale computers and ultimately leading to true quantum computers.
oxford instruments will supply the cryogenic consultancy and an ultra low temperature (ult) system to the cryogenic instrumentation for quantum electronics collaboration.
this will enable extremely low temperature environments, essential for research into quantum electronics, quantum computing and quantum nanotechnology.
the overall aim of the collaboration, which is headed by professor briggs of oxford university, is to provide the technological infrastructure that will give scientists and industries a head start in the development of new kinds of nanotechnology and its commercial exploitation.
'this project brings together a unique combination of uk talent and expertise in quantum information theory,' said professor briggs.
'together we can exploit advances in solid state quantum- scale computing, putting us at the forefront if emerging quantum electronics technologies.
furthermore, the control circuitry will benefit anyone performing cryogenic electric measurements, from physics experiments to space applications.'
the collaboration also includes cambridge university, hitachi europe ltd and the cclrc rutherford appleton laboratory it is supported by a ps2 million fund from the engineering and physical sciences research council (epsrc).
posted april 2003
physicists at eth zurich have demonstrated a five-  meter- long microwave quantum link, the longest of its kind to date.
it can be used both for future quantum computer networks and for experiments in basic quantum physics research.
collaboration is everything - also in the quantum world.
to build powerful quantum computers in the future, it will be necessary to connect several smaller computers to form a kind of cluster or local network (lan).
since those computers work with quantum mechanical superposition states, which contain the logical values "0" and "1" at the same time, the links between them should also be "quantum links".
the eth quantum link in andreas wallraff's laboratory.
the tube at the centre contains the strongly cooled waveguide that connects the two quantum chips in their cryostats via microwave photons.
image credit: eth zurich / heidi hostettler
the longest such link to date based on microwaves, at five metres long, was recently built in the laboratory of andreas wallraff, professor at the quantum device lab at eth zurich.
the researchers were scheduled to present their results on it at the annual meeting of the american physical society in denver.
because of the current epidemic situation this conference was canceled at short notice.
instead, the scientists now report their results at a virtual substitute conference.
"that's really a milestone for us", wallraff explains, "since now we can show that quantum-  lans are possible in principle.
in the next 10 to 20 years, quantum computers will probably increasingly rely on them."
currently, there are computers with a few dozen quantum bits or qubits, but several hundreds of thousands of them are almost impossible to accommodate in existing devices.
one reason for this is that qubits based on superconducting electrical oscillators, such as those used in the quantum chips in wallraff's lab (and also by ibm and google), need to be cooled down to temperatures close to the absolute zero of -  273,15 degrees celsius.
this suppresses thermal perturbations that would cause the quantum states to lose their superposition property - this is known as decoherence - and hence errors in the quantum calculations to occur.
extreme cold against decoherence
"the challenge was to connect two of those superconducting quantum chips in such a way as to be able to exchange superposition states between them with minimal decoherence", says philipp kurpiers, a former phd student in wallraff's group.
this happens by means of microwave photons that are emitted by one superconducting oscillator and received by another.
in between, they fly through a waveguide, which is a metal cavity a few centimetres in width, which also needs to be strongly cooled so that the quantum states of the photons are not influenced.
each of the quantum chips is cooled down over several days in a cryostat (an extremely powerful refrigerator), using compressed and also liquid helium, to a few hundredths of a degree above absolute zero.
to that end, the five-  metre waveguide that creates the quantum link was equipped with a shell consisting of several layers of copper sheet.
each of those sheets acts as a heat shield for the different temperature stages of the cryostat: -  223 degrees, -  269 degrees, -  272 degrees and finally -  273,1 degrees.
altogether, those heat shields alone weigh around a quarter of a tonne.
no "table-  top" experiment
"so, this is definitely not a "table-  top" experiment anymore that one can put together on a small workbench", wallraff says.
"a lot of development work has gone into this, and eth is an ideal place for building such an ambitious apparatus.
it's a kind of mini-  cern that we first had to build over several years in order to be able to do interesting things with it now."
apart from the three phd students who carried out the experiments, several engineers and technicians, also in the workshops at eth and at the paul scherrer institute (psi), were involved in producing and constructing the quantum link.
the physicists at eth not only showed that the quantum link can be sufficiently cooled down, but also that it can actually be used to reliably transmit quantum information between two quantum chips.
to demonstrate this, they created an entangled state between the two chips via the quantum link.
such entangled states, in which measuring one qubit instantaneously influences the result of a measurement on the other qubit, can also be used for tests in basic quantum research.
in those "bell tests", the qubits must be far enough apart from each other, so that any information transfer at the speed of light can be ruled out.
while wallraff and his collaborators are performing experiments with the new link, they have already started working on even longer quantum links.
already a year ago they were able to sufficiently cool down a ten-  metre link, but without doing any quantum experiments with it.
now they are working on a 30-  metre quantum link, for which a room at eth has been specially prepared.
you can offer your link to a page which is relevant to the topic of this post.
does the idea of playing about with a quantum computer please you?
if so, you can check out one fresh alternative route, thanks to a group of google engineers.
how about a gpu- accelerated quantum computer?
you can take advantage of something called the quantum computing playground which has launched as a browser- based webgl chrome experiment.
it features a gpu- accelerated quantum computer with a simple ide interface.
it has its own scripting language, with debugging and 3d quantum state visualization features.
quantum computing playground can simulate quantum registers up to 22 qubits, run grover's and shor's algorithms, and has quantum gates built into the scripting language itself.
commenting, extremetech said the programs are written in a language called qscript,- which "looks a lot like any other simple bash- like scripting language."
the 'playground' web page provides some background to quantum computers and how they are unique.
"a classic computer processes bits, which at any given time can be in one of two states: 0 or 1.
quantum computers use qubits, which can exist in any superposition of states 0 and 1, and are represented by a complex number.
when n qubits are in superposition, a combination of 2^n states is created.
a classic computer can only hold one of these states at a time, while quantum computers can perform meaningful operations on superpositions of states.
this basic property of quantum computers opens a way to multiple interesting algorithms."
the comments expressed by technology sites describing the new quantum computing playground mentioned the lack of any detailed tutorial.
they note that one needs some sort of programming experience to dig in and enjoy.
said i programmer, for example: "one of the problems with using it is that it doesn't provide a course in quantum computing or quantum principles and to make much sense of it you need to know something about quantum mechanics and have a rough idea if what quantum gates are all about."
a step by step demo is provided along with some very useful information on the help page but the need for previous exposure to the principles of quantum computing would be helpful.
extremetech said, "the help/about page has a few details about the inner workings of the simulator and qscript, but you'll still need a pretty solid grounding in computer science or quantum computer theory."
all the same, the quantum computing playground needs little coaxing to draw the interest of those who are genuinely curious about working with quantum algorithms.
"if you have ever wanted to try your hand at quantum algorithms, there is no longer an excuse," i programmer said thursday.
after all, according to the playground site, quantum computers that perform operations on sequences of qubits are not available commercially.
"the proof- of- concepts for capabilities of quantum computing have been demonstrated in multiple laboratories around the world, so there is a chance that quantum computers will become one day everyday's reality.
for now, you can experience the technology of tomorrow today, inside our playground."
ibm scientists today unveiled two critical advances towards the realization of a practical quantum computer.
for the first time, they showed the ability to detect and measure both kinds of quantum errors simultaneously, as well as demonstrated a new, square quantum bit circuit design that is the only physical architecture that could successfully scale to larger dimensions.
with moore's law expected to run out of steam, quantum computing will be among the inventions that could usher in a new era of innovation across industries.
quantum computers promise to open up new capabilities in the fields of optimization and simulation simply not possible using today's computers.
if a quantum computer could be built with just 50 quantum bits (qubits), no combination of today's top500 supercomputers could successfully outperform it.
the ibm breakthroughs, described in the april 29 issue of the journal nature communications, show for the first time the ability to detect and measure the two types of quantum errors (bit- flip and phase- flip) that will occur in any real quantum computer.
until now, it was only possible to address one type of quantum error or the other, but never both at the same time.
this is a necessary step toward quantum error correction, which is a critical requirement for building a practical and reliable large- scale quantum computer.
ibm's novel and complex quantum bit circuit, based on a square lattice of four superconducting qubits on a chip roughly one- quarter- inch square, enables both types of quantum errors to be detected at the same time.
by opting for a square- shaped design versus a linear array - which prevents the detection of both kinds of quantum errors simultaneously - ibm's design shows the best potential to scale by adding more qubits to arrive at a working quantum system.
"quantum computing could be potentially transformative, enabling us to solve problems that are impossible or impractical to solve today," said arvind krishna, senior vice president and director of ibm research.
"while quantum computers have traditionally been explored for cryptography, one area we find very compelling is the potential for practical quantum systems to solve problems in physics and quantum chemistry that are unsolvable today.
this could have enormous potential in materials or drug design, opening up a new realm of applications."
for instance, in physics and chemistry, quantum computing could allow scientists to design new materials and drug compounds without expensive trial and error experiments in the lab, potentially speeding up the rate and pace of innovation across many industries.
for a world consumed by big data, quantum computers could quickly sort and curate ever larger databases as well as massive stores of diverse, unstructured data.
this could transform how people make decisions and how researchers across industries make critical discoveries.
one of the great challenges for scientists seeking to harness the power of quantum computing is controlling or removing quantum decoherence - the creation of errors in calculations caused by interference from factors such as heat, electromagnetic radiation, and material defects.
the errors are especially acute in quantum machines, since quantum information is so fragile.
"up until now, researchers have been able to detect bit- flip or phase- flip quantum errors, but never the two together.
previous work in this area, using linear arrangements, only looked at bit- flip errors offering incomplete information on the quantum state of a system and making them inadequate for a quantum computer," said jay gambetta, a manager in the ibm quantum computing group.
"our four qubit results take us past this hurdle by detecting both types of quantum errors and can be scalable to larger systems, as the qubits are arranged in a square lattice as opposed to a linear array."
the work at ibm was funded in part by the iarpa (intelligence advanced research projects activity) multi- qubit- coherent- operations program.
detecting quantum errors
the most basic piece of information that a typical computer understands is a bit.
much like a beam of light that can be switched on or off, a bit can have only one of two values: "1" or "0".
however, a quantum bit (qubit) can hold a value of 1 or 0 as well as both values at the same time, described as superposition and simply denoted as "0+1".
the sign of this superposition is important because both states 0 and 1 have a phase relationship to each other.
this superposition property is what allows quantum computers to choose the correct solution amongst millions of possibilities in a time much faster than a conventional computer.
two types of errors can occur on such a superposition state.
one is called a bit- flip error, which simply flips a 0 to a 1 and vice versa.
this is similar to classical bit- flip errors and previous work has showed how to detect these errors on qubits.
however, this is not sufficient for quantum error correction because phase- flip errors can also be present, which flip the sign of the phase relationship between 0 and 1 in a superposition state.
both types of errors must be detected in order for quantum error correction to function properly.
quantum information is very fragile because all existing qubit technologies lose their information when interacting with matter and electromagnetic radiation.
theorists have found ways to preserve the information much longer by spreading information across many physical qubits.
"surface code" is the technical name for a specific error correction scheme which spreads quantum information across many qubits.
it allows for only nearest neighbor interactions to encode one logical qubit, making it sufficiently stable to perform error- free operations.
the ibm research team used a variety of techniques to measure the states of two independent syndrome (measurement) qubits.
each reveals one aspect of the quantum information stored on two other qubits (called code, or data qubits).
specifically, one syndrome qubit revealed whether a bit- flip error occurred to either of the code qubits, while the other syndrome qubit revealed whether a phase- flip error occurred.
determining the joint quantum information in the code qubits is an essential step for quantum error correction because directly measuring the code qubits destroys the information contained within them.
because these qubits can be designed and manufactured using standard silicon fabrication techniques, ibm anticipates that once a handful of superconducting qubits can be manufactured reliably and repeatedly, and controlled with low error rates, there will be no fundamental obstacle to demonstrating error correction in larger lattices of qubits.
physicists at uc santa barbara have made an important advance in electrically controlling quantum states of electrons, a step that could help in the development of quantum computing.
the work is published online today on the science express web site.
the researchers have demonstrated the ability to electrically manipulate, at gigahertz rates, the quantum states of electrons trapped on individual defects in diamond crystals.
this could aid in the development of quantum computers that could use electron spins to perform computations at unprecedented speed.
using electromagnetic waveguides on diamond- based chips, the researchers were able to generate magnetic fields large enough to change the quantum state of an atomic- scale defect in less than one billionth of a second.
the microwave techniques used in the experiment are analogous to those that underlie magnetic resonance imaging (mri) technology.
the key achievement in the current work is that it gives a new perspective on how such resonant manipulation can be performed.
"we set out to see if there is a practical limit to how fast we can manipulate these quantum states in diamond," said lead author greg fuchs, a postdoctoral researcher at ucsb.
"eventually, we reached the point where the standard assumptions of magnetic resonance no longer hold, but to our surprise we found that we actually gained an increase in operation speed by breaking the conventional assumptions."
while these results are unlikely to change mri technology, they do offer hope for the nascent field of quantum computing.
in this field, individual quantum states take on the role that transistors perform in classical computing.
"from an information technology standpoint, there is still a lot to learn about controlling quantum systems," said david awschalom, principal investigator and professor of physics, electrical and computer engineering at ucsb.
"still, it's exciting to stand back and realize that we can already electrically control the quantum state of just a few atoms at gigahertz rates -  speeds comparable to what you might find in your computer at home."
more information: gigahertz dynamics of a strongly driven single quantum spin, http://www.sciencemag.org/cgi/ ... ract/science.1181193
a team at the university of sydney and microsoft, in collaboration with stanford university in the us, has miniaturised a component that is essential for the scale- up of quantum computing.
the work constitutes the first practical application of a new phase of matter, first discovered in 2006, the so- called topological insulators.
beyond the familiar phases of matter - solid, liquid, or gas - topological insulators are materials that operate as insulators in the bulk of their structures but have surfaces that act as conductors.
manipulation of these materials provide a pathway to construct the circuitry needed for the interaction between quantum and classical systems, vital for building a practical quantum computer.
theoretical work underpinning the discovery of this phase of matter was awarded the 2016 nobel prize in physics.
the sydney team's component, coined a microwave circulator, acts like a traffic roundabout, ensuring that electrical signals only propagate in one direction, clockwise or anti- clockwise, as required.
similar devices are found in mobile phone base- stations and radar systems, and will be required in large quantities in the construction of quantum computers.
a major limitation, until now, is that typical circulators are bulky objects the size of your hand.
this invention, reported by the sydney team in the journal nature communications, represents the miniaturisation of the common circulator device by a factor of 1000.
this has been done by exploiting the properties of topological insulators to slow the speed of light in the material.
this minaturisation paves the way for many circulators to be integrated on a chip and manufactured in the large quantities that will be needed to build quantum computers.
the leader of the sydney team, professor david reilly, explained that the work to scale- up quantum computing is driving breakthroughs in related areas of electronics and nanoscience.
"it is not just about qubits, the fundamental building blocks for quantum machines.
building a large- scale quantum computer will also need a revolution in classical computing and device engineering," professor reilly said.
"even if we had millions of qubits today, it is not clear that we have the classical technology to control them.
realising a scaled- up quantum computer will require the invention of new devices and techniques at the quantum- classical interface."
lead author of the paper and phd candidate alice mahoney said: "such compact circulators could be implemented in a variety of quantum hardware platforms, irrespective of the particular quantum system used."
a practical quantum computer is still some years away.
scientists expect to be able to carry out currently unsolveable computations with quantum computers that will have applications in fields such as chemistry and drug design, climate and economic modelling, and cryptography.
the portfolio includes 57 patent filings, across 17 patent families, and covers novel technological approaches in quantum science.
one of the focus technologies in the portfolio allows qkd to be used across long distances.
qkd has a potential to significantly advance data security, cybersecurity, and end- to- end encryption.
qinetiq's patent portfolio encompasses a way to scale up qkd from a local area to a planet- wide distribution, without having to wait for a quantum repeater or a satellite to be built.
qubitekk, based in california, usa, is the world's first company dedicated to commercialising quantum entanglement sources required to speed the adoption of quantum computing and cryptography applications, making the portfolio purchase a logical fit.
what this portfolio effectively offers is a way to accelerate the introduction of global qkd compared to waiting for a quantum repeater estimated to be 10 years from actualization.
communications networks and infrastructure will be secured in before quantum computers and repeaters become reality
dr. duncan earl, cto and president of qubitekk, added: "the patent portfolio we acquired from qinetiq along with our current holdings of quantum patents will make qubitekk one of the largest quantum patent holders in the united states.
we are encouraged with the opportunities this will give qubitekk in the marketplace."
thierry le gall, qinetiq ip exploitation manager, said: "we are pleased to have agreed the assignment of our qkd patent portfolio to qubitekk, an ideal and innovative partner to further enhance and exploit our quantum cryptography inventions"
us broker adapt ip ventures forged a relationship between qinetiq and qubitekk, to enable the sale of the entire qkd portfolio.
about qubitekk, inc.
qubitekk puts quantum technology into the hands or scientists, engineers, and developers.
our quantum- enabled entangled photon solutions provide capabilities and products never before thought possible.
qubitekk is a leader in the field of entangled photon quantum key distribution and is actively working with the federal government on active contracts and the promotion of quantum science.
our quantum education kit and entanglement academy are key to increasing the knowledge of the united states workforce in needed quantum skill sets.
more information can be found at www.qubitekk.com
about qinetiq
listed on the london stock exchange (lse: qq.l), qinetiq is a leading science and engineering company operating primarily in the defence, security and aerospace markets.
our customers are predominantly government organisations including defence departments, as well as international customers in other targeted sectors.
real- world experiment in chicago suburbs achieves quantum entanglement across 52- mile fiber network.
scientists from the u.s. department of energy's (doe) argonne national laboratory and the university of chicago entangled photons across a 52- mile network in the chicago suburbs, an important step in developing a national quantum internet.
department of energy under secretary for science paul dabbar and argonne and uchicago scientists and leaders discuss quantum entanglement along argonne's quantum loop, a 52- mile fiber optic testbed for quantum communication in the chicago suburbs.
(image by argonne national laboratory.)
the quantum loop, spearheaded by argonne senior scientist and uchicago professor david awschalom, ran its first successful entanglement experiments on february 11.
headquartered at argonne, the loop is amongst the longest land- based quantum networks in the nation.
"by integrating communities with our national lab system, we can look forward to a future filled with innovation and collaboration."
- under secretary for science paul dabbar
the experiment, funded by doe's office of science basic energy sciences, is seen as a foundational building block in the development of a quantum internet - potentially a highly secure and far- reaching network of quantum computers and other quantum devices.
a quantum internet could catalyze technologies that greatly accelerate today's internet, significantly improve the security of communications and support dramatic advances in computing and sensing.
scientists say quantum technology could revolutionize national and financial security, patient privacy, drug discovery and the design and manufacturing of new materials, while increasing our scientific understanding of the universe.
scientists entangle light particles across the quantum loop in the chicago suburbs.
animation shows the general path, which winds circuitously across a pair of 26- mile loops from argonne national laboratory to the i- 355 boughton rd.
toll plaza in bolingbrook, il.
(image by argonne national laboratory.)
"this is an important step forward in harnessing entanglement and building a network to help form the basis of future quantum communication systems," said awschalom, the liew family professor in the pritzker school of molecular engineering at uchicago, senior scientist in the materials science division at argonne and director of the chicago quantum exchange.
"we are excited by these initial demonstrations of distributing entanglement outside of a laboratory, as well as having a flexible communications platform that allows us to identify the challenges of translating quantum phenomena to the real world."
in the subatomic quantum world, particles can become entangled, sharing their states even though they're in different locations - a phenomenon which could be used to transfer information.
the network, which originates at argonne in lemont, illinois, and winds circuitously in a pair of 26- mile loops through several of chicago's western suburbs, taps the unique properties of quantum mechanics to eventually  "teleport" information virtually instantaneously across a distance.
as a bonus, scientists believe the information would be extremely difficult to hack - quantum states change when observed, so the presence of an outside listener would actually change the signal itself.
"by integrating communities with our national lab system, we can look forward to a future filled with innovation and collaboration," said under secretary for science paul dabbar.
"the department of energy is proud to bring businesses closer to the technology built at argonne.
by developing the quantum loop, we will remain globally competitive around the world."
the white house on feb. 11 announced funding and a strategic vision for quantum networks in the united states.
the plan envisions companies and national laboratories over the next five years working together to demonstrate the foundational science and key technologies to enable quantum networks, and over the next 20 years, quantum internet links that enable new capabilities not possible with classical technology.
argonne plans to scale this network by developing a two- way quantum link network with fermi national accelerator laboratory.
such a link could help to lay the foundation for a national laboratory- led, cross- country quantum internet.
"both locally and nationally, argonne fosters important industry partnerships that accelerate technology innovation and commercialization," said argonne director paul kearns.
as part of the chicago quantum exchange, we create pioneering collaborations with industry, which is key to maintaining u.s. leadership in this important emerging field."
though quantum technology holds a great deal of promise, it's mostly theoretical right now; quantum systems are extremely sensitive to interference and to date have been mainly tested in clean, controlled lab environments.
this experiment instead runs through an existing underground network of optical fiber, built decades ago for conventional telecommunications.
"in the real world, the fiber cables are expanding and contracting as the temperature changes.
there is also vibration and noise from the environment such as local traffics," said tian zhong, argonne scientist in the nanoscience and technology division and assistant professor of molecular engineering at uchicago .
"these are all factors that can affect the quantum signal transmission, and that we can only find out by performing an experiment of this magnitude under real- world operating conditions."
"many tests of quantum technologies are confined to a research environment," said alan dibos, argonne assistant scientist in the nanoscience and technology division.
"one of the exciting aspects of this project is the expansion of our laboratory into the greater chicago area."
in achieving this milestone, awschalom and team worked closely with companies in the emerging quantum industry.
in partnership with qubitekk, a new company developing quantum technologies, the team created entangled photon pairs and distributed them across two 26- mile fiber loops.
the returning photon pairs were detected, and their correlation was verified with a high signal- to- noise ratio.
the result is the latest from members of the chicago quantum exchange, a national leader in quantum information science.
the exchange includes more than 130 members from universities and national laboratories, including founding members uchicago, argonne, fermilab and the university of illinois at urbana- champaign.
the exchange also includes several nonprofit and international partners and seven corporate partners, all of which have expertise in varying areas of quantum information technology.
the quantum loop is supported by doe's office of science.
additional support for this experiment was provided by the joint task force initiative, a university of chicago program dedicated to helping argonne and fermilab achieve mission success.
classical digital computers use transistors to process information in various sequences of zeros and ones, but have a limited ability to carry out calculations.
quantum computers use the laws of quantum mechanics.
because particles such as electrons and photons can be in multiple states- one, zero, or both at the same time- they offer many more calculation possibilities than traditional machines with only two options- on or off.
this could allow very complex calculations in areas such as genome modeling, drug research and weather forecasting.
quantum computers can operate 100 million times faster than traditional computers.
a 500 quantum bit (or qubit) computer could perform more calculations in a single step than the number of atoms in the universe.
and it's in the cloud that quantum computers will typically operate to make massive calculations.
cue quantum internet.
a quantum platform in the cloud
dr. matthias keller is senior lecturer in atomic, molecular and optical physics at the university of sussex.
he stated that "a quantum network would basically work similarly to a classical fiber network.
but instead of using strong optical signals, the signal is carried by a single photon."
these are the individual particles of light that transmit information between nodes.
ibm has developed the world's first quantum computing platform at the ibm watson research center in new york.
online since last may, it has a five- qubit quantum processor and is accessible to everyone via the ibm cloud.
scientists hope that such powerful computers will enable them to model genomes and ecosystems.
the human genome could be unraveled to expand drug development, while a model of earth's weather systems would make forecasting much more accurate.
quantum computers also could search massive databases instantaneously, and handle large amounts of data from sensors in industrial plants and on connected machinery.
this makes them perfect for the swiftly developing industrial internet of things.
possible threats?
however, quantum computers also constitute a threat to today's internet, warns andersen cheng, ceo of post- quantum:
quantum computers will be able to crack the most commonly used encryption protocol today, which will make the internet as we know it totally unusable.
we we won't be able to tell if the information came from, or will go to the right person, which will completely destroy the trust we have in the internet.
this is because current online cryptographic systems- such as messaging services, email and cloud sync software- will be very simple for quantum computers to crack, while covering their tracks.
cheng therefore thinks we need to investigate quantum safe identity authentication, which his company is developing.
quantum key distribution is one solution.
it sends a secure key across a network - impossible to copy - to decipher a conventional message or file.
facing challenges
another complication is the strict environmental conditions required.
such supercomputers must be kept super cold- at absolute zero.
they also require shielding from any electromagnetic interference (emi) to control the unstable quantum states.
only then are huge calculations possible.
so far, a universal quantum computer does not exist, but there are already projects that go beyond mere prototypes.
canada's d- wave systems recently released its third generation d- wave 2x, which uses a 1,098- qubit processor.
the one located at nasa's ames research center in california is shielded against emi 50,000 times weaker than earth's magnetic field and is housed in a vacuum.
it's also cryogenically cooled to - 460 degrees fahrenheit, about 180 times colder than interstellar space.
ibm predicts the appearance of medium- sized quantum processors of 50- 100 qubits in the next decade.
universal quantum computers could be one of the greatest milestones in the history of it.
researchers working in singapore and the united states have discovered that all entangled states of two particles have a classical 'fingerprint'.
this breakthrough could help engineers guard against errors and devices that don't do what they promise in quantum computing and quantum cryptography.
goh koon tong and valerio scarani at the centre for quantum technologies at the national university of singapore, with andrea coladangelo at the california institute of technology, reported in nature communications on 26 may that a simple set of measurements can act as an identity check for any two- particle entangled state.
the presence of this fingerprint could help certify quantum computers or quantum encryption devices purchased from third parties.
an entangled quantum state is made of two or more particles held in a multitude of undecided outcomes.
such states are fuel for quantum computing and bring security to quantum communication.
the problem is, it is difficult to check that these states have the properties expected of them.
that leaves the door open for poorly- functioning devices.
"i like to see our work as bringing the power of testing quantum devices to the consumers who use them.
currently, only those who build the devices or understand the engineering aspect of them can perform the test," says goh.
quantum physicists could also use this 'self- testing' tool as a check step in lab experiments.
the work builds on results by other groups, extending findings for qubits to the more exotic qudits.
qudits are higher- dimensional quantum bits.
rather than just storing a binary bit of information -  a 0 or 1 -  a qudit has bigger information density, storing a 0, 1, 2, 3, 4, etc.
such states, though hard to make, are interesting because they could accelerate some computing or communication tasks.
the idea of self- testing is significant because it is generally difficult to gain a lot of information about the quantum state of a particle.
a particle's state is described by a 'wave function' that encodes the probabilities for the particle's various properties, such as polarisation or momentum.
to be sure about a quantum state, you need to know the whole wave function.
however, there is a problem here.
measuring the quantum state reveals just one value -  not the full set of possibilities.
the traditional way to try to learn the full quantum state involves a technique called tomography.
this requires measuring many copies of the quantum state in different ways, counting up all the outcomes of the various measurements to give a full set of probabilities.
it also involves a laborious process of characterising the measurement devices and aligning them with the source of the quantum particles.
self- testing is more efficient, requiring fewer measurements.
it is also 'device- independent', or like blind tomography -  needing no characterisation of the measurement device, as long as the device is guaranteed to detect most of the particles.
this is because the fingerprint is a pattern of results across measurements of the two particles that could only be consistently created by the weird correlations in the quantum state, not by any classical process or by chance.
seeing this pattern then means the quantum state must be present.
the famous 'chsh experiment' in quantum physics is an example of fingerprinting for a quantum state of two qubits.
to prove that fingerprint tests exist for all two- qudit states, the authors showed that these states can be considered as composed of blocks of two- level systems, akin to qubits.
even better, this mathematical equivalence points to what measurements are needed -  although it's not clear yet if they are experimentally- friendly to make.
the team hope that this discovery will motivate a new wave of research to find straightforward ways to incorporate this check in experiments or devices.
so far, the signs are good.
"of all my work in the past five years, this has attracted the most attention," says scarani.
as well as hearing from colleagues interested in the result, he has been invited to give a talk on self- testing at qcrypt, an annual conference on quantum cryptography being held this year in the uk in september.
story source:
materials provided by national university of singapore.
note: content may be edited for style and length.
quantum corp. (nasdaq: qmco) today announced it will host virtualq | nab, a virtual video event featuring live keynotes, educational sessions, technology demonstrations, roundtable discussions and more, from may 12- 14, 2020.
over the course of three days, quantum will unveil its latest product innovations as well as host virtual sessions featuring media and entertainment subject matter experts and quantum technology partners including adobe, dalet, ipv and teradici.
the event will serve to highlight the addition of technology used in quantum's activescale(tm) object storage system into the company's product portfolio, and introduce new developments to its stornext(r) software, designed to make cloud content more accessible, with improved retrieval speeds.
virtualq | nab attendees will learn about:
solutions for remote online editing, including hybrid and multi- cloud technologies.
the latest developments in quantum's stornext(r) file system to improve cloud content accessibility and accelerate retrieval speeds.
how nvme high performance storage solutions for post- production can speed multi- node render times by orders of magnitude and reduce infrastructure costs and complexity.
the benefits of object storage for long- term content management, and how quantum's activescale object store system is enabling customers to create online, extremely durable archives for indexing and content monetization.
attendees can sign up to meet with quantum experts including executives, engineers and product leadership teams virtually.
"we support the difficult decision to conduct nab remotely this year as nab express, although i miss the one- on- one time we spend with our long- term friends, many of whom are customers and industry colleagues," said ed fiore, vice president and general manager, primary storage, quantum.
"virtualq i nab is our opportunity to connect and collaborate remotely with customers and partners in a meaningful way and highlight our latest innovations.
we look forward to when we can all safely gather again in person."
the full virtualq | nab agenda and registration are available at: https://www.quantum.com/en/resources/events/virtualqnab/.
about quantum
quantum technology and services help customers capture, create and share digital content - and preserve and protect it for decades.
with solutions built for every stage of the data lifecycle, quantum's platforms provide the fastest performance for high- resolution video, images, and industrial iot.
that's why the world's leading entertainment companies, sports franchises, researchers, government agencies, enterprises, and cloud providers are making the world happier, safer, and smarter on quantum.
see how at www.quantum.com.
quantum, the quantum logo and stornext, are either registered trademarks of quantum corporation, and activescale is a trademark of quantum corporation and its affiliates in the united states and/or other countries.
all other trademarks are the property of their respective owners.
scientists at the university of sydney have demonstrated the ability to "see" the future of quantum systems, and used that knowledge to preempt their demise, in a major achievement that could help bring the strange and powerful world of quantum technology closer to reality.
the applications of quantum- enabled technologies are compelling and already demonstrating significant impacts - especially in the realm of sensing and metrology.
and the potential to build exceptionally powerful quantum computers using quantum bits, or qubits, is driving investment from the world's largest companies.
however a significant obstacle to building reliable quantum technologies has been the randomisation of quantum systems by their environments, or decoherence, which effectively destroys the useful quantum character.
the physicists have taken a technical quantum leap in addressing this, using techniques from big data to predict how quantum systems will change and then preventing the system's breakdown from occurring.
the research is published today in nature communications.
"much the way the individual components in mobile phones will eventually fail, so too do quantum systems," said the paper's senior author professor michael j. biercuk.
"but in quantum technology the lifetime is generally measured in fractions of a second, rather than years."
professor biercuk, from the university of sydney's school of physics and a chief investigator at the australian research council's centre for engineered quantum systems, said his group had demonstrated it was possible to suppress decoherence in a preventive manner.
the key was to develop a technique to predict how the system would disintegrate.
professor biercuk highlighted the challenges of making predictions in a quantum world: "humans routinely employ predictive techniques in our daily experience; for instance, when we play tennis we predict where the ball will end up based on observations of the airborne ball," he said.
"this works because the rules that govern how the ball will move, like gravity, are regular and known.
but what if the rules changed randomly while the ball was on its way to you?
in that case it's next to impossible to predict the future behavior of that ball.
"and yet this situation is exactly what we had to deal with because the disintegration of quantum systems is random.
moreover, in the quantum realm observation erases quantumness, so our team needed to be able to guess how and when the system would randomly break.
"we effectively needed to swing at the randomly moving tennis ball while blindfolded."
the team turned to machine learning for help in keeping their quantum systems - qubits realised in trapped atoms - from breaking.
what might look like random behavior actually contained enough information for a computer program to guess how the system would change in the future.
it could then predict the future without direct observation, which would otherwise erase the system's useful characteristics.
the predictions were remarkably accurate, allowing the team to use their guesses preemptively to compensate for the anticipated changes.
doing this in real time allowed the team to prevent the disintegration of the quantum character, extending the useful lifetime of the qubits.
"we know that building real quantum technologies will require major advances in our ability to control and stabilise qubits - to make them useful in applications," professor biercuk said.
our techniques apply to any qubit, built in any technology, including the special superconducting circuits being used by major corporations.
"we're excited to be developing new capabilities that turn quantum systems from novelties into useful technologies.
the quantum future is looking better all the time," professor biercuk said.
practically every new technology gets caught in a hype cycle- many would argue that 5g networks are at the forefront of this, presently, as promises about what 5g will be able to do are far off from what initial network rollouts can actually provide.
the overabundance of hype in technology sets an unrealistic expectation for its performance, inevitably leading to disappointment when the technology finally reaches consumers.
the hype cycle effect is slightly more indirect in enterprise technology.
immeasurable amounts of money have been invested in technologies that never quite made it- this, in part, is a failure of planning, as not every piece of technology can be directly and equally applied to every business use case, despite overzealous attempts to do so, as is increasingly the case with blockchain.
see: quantum computing: an insider's guide (free pdf) (techrepublic)
hewlett packard enterprise sees quantum computing in this light.
though hpe is not building quantum computers, the company is not betting against the technology.
"if you're trying to find the ground state of a molecule that you think might be a great drug, then a quantum computer would be your go- to machine to do that," hewlett packard enterprise senior fellow ray beausoleil told techrepublic.
"i'm a big booster of quantum computing.
i think the applications are going to be incredibly interesting and important.
i just don't think that the enterprise is going to be one of those places where those applications are found, unless you're a pharmaceutical or materials company."
typical office work is not going to be improved by quantum computers; this is not a technology that an average accountant can utilize to improve their work.
"quantum computers are not very good at the three rs- reading, writing and arithmetic.
they don't like to read big databases, they can't give you very big complicated readouts, and they don't do regular arithmetic as well as a regular old classical computer," beausoleil said.
classical applications are poor fits for quantum systems
the difficulty of attempting classic calculations on quantum computers is not widely understood, leading to optimistic predictions about their applicability for general- purpose use cases.
"suppose that you have a 200- layer deep neural network, 50 nodes per layer.
that's 200 by 50 by 50 weights that you need after you're done training... let's pretend that each of those weights as 50 bits," beausoleil said, setting up an example of processing a petabyte of data using a machine learning algorithm.
"in in principle we could store [one petabyte] in 50 cubits.
however, those have to be 50 perfect cubits."
the current generation of quantum computers- noisy intermediate- scale quantum (nisq) systems- use imperfect qubits which are subject to environmental noise, and are operable for a short time before reaching decoherence.
it is possible for a quantum computer to combine noisy qubits to simulate a perfect qubit, with john preskill estimating this conversion around 1,000 noisy qubits for 1 good qubit, while ibm researchers have seen some success by amplifying and measuring noise to extrapolate what a noiseless state would be.
beausoleil notes, however, that "the big problem is that no one has any idea how to efficiently take that petabit of information and encode it in 50 cubits.
there is no algorithm for that," adding that "there's no such thing as a quantum hard drive."
every time the neural network is run, the data set must be re- loaded- which beausoleil contends is not yet possible, and poses a large encumbrance for adoption of quantum computers.
"i can only get one classical bit of information out of every cubit.
i have to re- run this 200 by 50 by 50 times, and then carefully plan my measurements so that each time i extract a unique weight," he said.
"each time i'm doing that run, i'm re- encoding that petabit of data into the quantum register."
there's no quantum roadmap
unlike industry resources like the international technology roadmap for semiconductors, and industry anecdotes like moore's law, there is no shared wisdom about when higher- performance quantum computers will be available.
after nisq, the next step is to establish "quantum supremacy," a threshold at which a quantum computer is demonstrably capable of performing a calculation that a traditional computer.
when this occurs, it will be a remarkable technical achievement; however, it will still fall short of being transformative.
"the questions that people are trying to answer to demonstrate quantum supremacy, no one cares about.
they're not important, groundbreaking questions," beausoleil said.
"when a quantum computer can be used to answer a question that cannot be answered on a classical computer, and it is of real significance, that is when quantum computing is a thing."
for more on quantum computing, learn how a new manufacturing technique could create scalable quantum computers, and how helium shortages will impact quantum computer research.
also see
university of adelaide- led research has moved the world one step closer to reliable, high- performance quantum computing.
an international team has developed a ground- breaking single- electron "pump".
the electron pump device developed by the researchers can produce one billion electrons per second and uses quantum mechanics to control them one- by- one.
and it's so precise they have been able to use this device to measure the limitations of current electronics equipment.
this paves the way for future quantum information processing applications, including in defence, cybersecurity and encryption, and big data analysis.
"this research puts us one step closer to the holy grail- reliable, high- performance quantum computing," says project leader dr. giuseppe c. tettamanzi, senior research fellow, at the university of adelaide's institute for photonics and advanced sensing.
published in the journal nano letters, the researchers also report observations of electron behaviour that's never been seen before - a key finding for those around the world working on quantum computing.
"quantum computing, or more broadly quantum information processing, will allow us to solve problems that just won't be possible under classical computing systems," says dr. tettamanzi.
"it operates at a scale that's close to an atom and, at this scale, normal physics goes out the window and quantum mechanics comes into play.
"to indicate its potential computational power, conventional computing works on instructions and data written in a series of 1s and 0s - think about it as a series of on and off switches; in quantum computing every possible value between 0 and 1 is available.
we can then increase exponentially the number of calculations that can be done simultaneously."
this university of adelaide team, in collaboration with the university of cambridge, aalto university in finland, university of new south wales, and the university of latvia, is working in an emerging field called electron quantum optics.
this involves controlled preparation, manipulation and measurement of single electrons.
although a considerable amount of work has been devoted world- wide to understand electronic quantum transport, there is much still to be understood and achieved.
"achieving full control of electrons in these nano- systems will be highly beneficial for realistic implementation of a scalable quantum computer.
we, of course, have been controlling electrons for the past 150 years, ever since electricity was discovered.
but, at this small scale, the old physics rules can be thrown out," says dr. tettamanzi.
"our final goal is to provide a flow of electrons that's reliable, continuous and consistent - and in this research, we've managed to move a big step towards realistic quantum computing.
"and, maybe equally exciting, along the way we have discovered new quantum effects never observed before, where, at specific frequencies, there is competition between different states for the capture of the same electrons.
this observation will help advances in this game- changing field."
measuring the computational ability of quantum computers is- as is anything involving quantum systems- a complex problem.
counting the number of qubits in a quantum computer to determine computational power is too simplistic to be functionally useful- differences in how individual qubits are connected, how the qubits themselves are designed, and environmental factors make this type of comparison inequitable.
for example, d- wave is planning to launch a 5,000- qubit system for cloud- based access in mid- 2020.
google, for contrast, has a 72- qubit quantum computer called "bristlecone" and ibm's q system one is a 20- qubit design.
differences in how these qubits are designed and connected make cross- vendor comparisons unreliable- while d- wave's upcoming 5,000- qubit system will undoubtedly be more capable than its current- generation 2,000- qubit system, it is not necessarily better than ibm's designs, or google's prototypes.
further, d- wave's design is a quantum annealer, useful for a single type of calculation called "quadratic unconstrained binary optimization (qubo)."
in contrast, the ibm and google designs are general- purpose quantum computers, and can be used for a wider variety of calculations, including integer factorization- a type of operation necessary to break rsa encryption.
various types of qubit designs exist in general- purpose quantum computers, including superconducting qubits, ion- trap systems, semiconductor- based and spin qubits.
a standard for measuring the computational ability of quantum computers was proposed by ibm in 2017, called "quantum volume."
quantum volume is measured by calculating the number of physical qubits, connectivity between qubits, and time to decoherence, as well as the available hardware gate set, and number of operations that can be run in parallel.
according to the researchers who defined quantum volume, that metric "enables the comparison of hardware with widely different performance characteristics and quantifies the complexity of algorithms that can be run."
likewise, the researchers noted that quantum volume can only increase if the number of qubits, and the error rate of those qubits, increase in parallel.
on monday, at gartner's catalyst conference in san diego, the research and advisory firm embraced the quantum volume benchmark as an important way of measuring progress toward quantum advantage- the point at which quantum computers are capable of performing a calculation demonstrably faster than traditional computers- and noted the importance of quantum volume in planning for adoption of quantum computers.
while it is presently unclear when quantum advantage will be achieved, gartner projects evaluation of quantum use cases in the enterprise by 2022, with early quantum applications in deployment by 2026, and commercial use of quantum computing by 2030.
for more on quantum computing, check out "why post- quantum encryption will be critical to protect current classical computers," "ibm reduces noise in quantum computing, increasing accuracy of calculations," "d- wave's 2000q variant reduces noise for cloud- based quantum computing," and "quantum computing is not a cure- all for business computing challenges" on techrepublic.
also see
engineering researchers have demonstrated proof- of- principle for a device that could serve as the backbone of a future quantum internet.
university of toronto engineering professor hoi- kwong lo and his collaborators have developed a prototype for a key element for all- photonic quantum repeaters, a critical step in long- distance quantum communication.
a quantum internet is the 'holy grail' of quantum information processing, enabling many novel applications including information- theoretic secure communication.
today's internet was not specifically designed for security, and it shows: hacking, break- ins and computer espionage are common challenges.
nefarious hackers are constantly poking holes in sophisticated layers of defence erected by individuals, corporations and governments.
in light of this, researchers have proposed other ways of transmitting data that would leverage key features of quantum physics to provide virtually unbreakable encryption.
one of the most promising technologies involves a technique known as quantum key distribution (qkd).
qkd exploits the fact that the simple act of sensing or measuring the state of a quantum system disturbs that system.
because of this, any third- party eavesdropping would leave behind a clearly detectable trace, and the communication can be aborted before any sensitive information is lost.
until now, this type of quantum security has been demonstrated in small- scale systems.
lo and his team are among a group of researchers around the world who are laying the groundwork for a future quantum internet by working to address some of the challenges in transmitting quantum information over great distances, using optical fibre communication.
because light signals lose potency as they travel long distances through fibre- optic cables, devices called repeaters are inserted at regular intervals along the line.
these repeaters boost and amplify the signals to help transmit the information along the line.
but quantum information is different, and existing repeaters for quantum information are highly problematic.
they require storage of the quantum state at the repeater sites, making the repeaters much more error prone, difficult to build, and very expensive because they often operate at cryogenic temperatures.
lo and his team have proposed a different approach.
they are working on the development of the next generation of repeaters, called all- photonic quantum repeaters, that would eliminate or reduce many of the shortcomings of standard quantum repeaters.
with collaborators at osaka university, toyama university and ntt corporation in japan, lo and his team have demonstrated proof- of- concept of their work in a paper recently published in nature communications.
"we have developed all- photonic repeaters that allow time- reversed adaptive bell measurement," says lo.
"because these repeaters are all- optical, they offer advantages that traditional -  quantum- memory- based matter -  repeaters do not.
for example, this method could work at room temperature."
a quantum internet could offer applications that are impossible to implement in the conventional internet, such as impenetrable security and quantum teleportation.
"an all- optical network is a promising form of infrastructure for fast and energy- efficient communication that is required for a future quantum internet," says lo.
"our work helps pave the way toward this future."
dr. s. s. verma, department of physics, s.l.i.e.t., longowal, distt.- sangrur (punjab)- 148106
in comparison to phenomenal certainty of macro world, quantum world is full of probabilities of unexpected phenomena.
these quantum world probabilities of unexpected outcomes can be observed and exploited to the level best in various applications of engineering and technology.
quantum electronics is the area of physics dealing with the effects of quantum mechanics on the behavior of electrons in matter.
quantum mechanics is not only fascinating in its own right, but also offers the possibility of revolutionary applications.
we create electronic devices that exploit quantum behaviours of superposition and entanglement.
we aim both to develop new technology and to explore fundamental science in quantum engineered devices.
we work in semiconducting and molecular materials using techniques of nanofabrication, quantum transport, and spin resonance.
the field concerned with the interaction of radiation and matter, and on the effects of quantum mechanics on the behavior of electrons.
the quantum behavior of electrons and light, as well as the interaction mechanism between them, are responsible for a wide variety of complex physical phenomena.
studies are carried out on optical communications - fiber optics, optical devices and research on ultrafast phenomena.
the field deals with methods for the amplification and generation of electromagnetic oscillations based on the use of the effect of stimulated emission, and also with the properties of quantum mechanical amplifiers and generators and with their use.
scientists and engineers are exploring all ways to exploit quantum world phenomena based electronic devices with great advantages of speed, storage, reduced size, efficiency etc.
there are significant advances in the understanding of quantum electronics phenomena or the demonstration of new devices, systems, or applications.
the coherence of quantum systems is the foundation upon which hardware for future information technologies is based.
quantum information is carried by units called quantum bits, or qubits.
they can be used to secure electronic communications - and they enable very fast searches of databases.
researchers have developed a new electronic component which will help to process, transfer and store superposition states such as the overlapping of the binary digits zero and one.
the atoms are trapped in a magnetic field above the surface of the microchip.
because superconductors allow an electric current to flow without resistance, the current does not become weaker in a superconducting ring.
researchers have made use of this to construct a complex superconducting ring- circuit and a particularly stable storage space for atoms.
and the researchers can test how long atoms remain in the quantum superposition states within the system - by using the atoms themselves as a clock.
the researchers are now planning experiments on atoms in superconducting microwave resonators - which could serve as a shuttle for data between integrated circuits and atoms.
advantages
the study of quantum electronics is enabling the miniaturization of technology and the development of the transistors and integrated circuits of tomorrow.
as the size of devices shrinks and reaches the nanometer scale, the effects of quantum mechanics on electrons becomes more pronounced and, eventually, determines their electronic properties.
quantum generators of radio waves differ from other radio apparatus in that the frequency of the oscillations generated is very stable; quantum magnetic amplifiers of radio- frequency waves are distinguished by their extremely low noise level.
developmental status
interconnect can often take up most of the space on silicon chip and the limits of the interconnect often form the limits of a computing system's performance.
scientists have found a way to connect quantum devices together, transmitting entanglement - and crucially the quantum properties that could deliver the next- generation of electronics.
a team of scientists of around the world have managed to build and test a quantum interconnect that links two chips and carries both photons (and that entanglement) between them.
entanglement is where quantum particles share the same existence, even while apart: preserving this state is the hard part, and the scientists managed it using optical fiver and a quantum quirk where photons traveling along two channels overlap, entangling, and then carrying on.
a team of scientists has discovered a new way of using light to draw and erase quantum- mechanical circuits in a unique class of materials called topological insulators.
in contrast to using advanced nanofabrication facilities based on chemical processing of materials, this flexible technique allows for rewritable optical fabrication of devices.
this finding is likely to spawn new developments in emerging technologies such as low- power electronics based on the spin of electrons or ultrafast quantum computers.
the electrons in topological insulators have unique quantum properties that many scientists believe will be useful for developing spin- based electronics and quantum computers.
however, making even the simplest experimental circuits with these materials has proved difficult because traditional semiconductor engineering techniques tend to destroy their fragile quantum properties.
even a brief exposure to air can reduce their quality.
the researchers report the discovery of an optical effect that allows them to "tune" the energy of electrons in these materials using light, and without ever having to touch the material itself.
they have used it to draw and erasep- n junctions- one of the central components of a transistor- in a topological insulator for the first time.
the researchers found that the surface of strontium titanate, the substrate material on which they had grown their samples, becomes electrically polarized when exposed to ultraviolet light, and their room lights happened to emit at just the right wavelength.
the electric field from the polarized strontium titanate was leaking into the topological insulator layer, changing its electronic properties.
researchers found that by intentionally focusing beams of light on their samples, they could draw electronic structures that persisted long after the light was removed.
since the electrical polarization occurs in an adjacent material, and the effect persists in the dark, the topological insulator remains relatively undisturbed.
this effect could allow electrical tuning of materials in a wide range of optical, magnetic and spectroscopic experiments where electrical contacts are extremely difficult or simply impossible.
researchers at the university of chicago published a novel technique for improving the reliability of quantum computers by accessing higher energy levels than traditionally considered.
most prior work in quantum computation deals with "qubits," the quantum analogue of binary bits that encode either zero or one.
the new work instead leverages "qutrits," quantum analogues of three- level trits capable of representing zero, one or two.
the uchicago group worked alongside researchers based at duke university.
both groups are part of the epiqc (enabling practical- scale quantum computation) collaboration, an nsf expedition in computing.
epiqc's interdisciplinary research spans from algorithm and software development to architecture and hardware design, with the ultimate goal of more quickly realizing the enormous potential of quantum computing for scientific discovery and computing innovation.
accessing higher energy levels
the work can be viewed in the context of a fundamental space- time trade- off that is common in computer science: programs can be sped up by using more memory, or alternatively, programs can reduce memory requirements by incurring longer runtimes.
but in the context of quantum computing, where near- term machines are severely constrained in both memory and runtimes supported, neither of these tradeoffs are acceptable.
the solution the epiqc team discovered was to break the abstraction of using binary qubits.
"while binary logic makes sense for the on- off physics underlying conventional computers, quantum hardware is not inherently binary," explains researcher pranav gokhale, a graduate student at the university of chicago.
in fact, states on a quantum computer belong to an infinite spectrum, so the qubit is merely an artificially engineered choice of using only two of the states.
the team found that by allowing the use of three states via qutrits, one of the fundamental operations in quantum computation is exponentially faster without requiring additional memory.
the team verified their discovery with simulations run under realistic noise conditions.
"qutrits do come at a cost, since the presence of an additional state implies more possible sources of error," said gokhale.
"nonetheless, our simulations demonstrate that qutrits have a convincing advantage with two to ten times higher reliability than qubit- only algorithms for near- term benchmarks."
bridging the gap between hardware and software
the team's discovery is well matched to epiqc's interdisciplinary focus on bridging the gap between quantum hardware and software.
an early stage of this work was presented at the quantum information processing conference this january, where it won the award for best poster.
since then, the research has been fine- tuned to match sophisticated hardware models developed in conjunction with experts working on superconducting and trapped ion quantum computers.
"by tailoring algorithms to take advantage of the unique capabilities of quantum hardware, we realize efficiency gains that are otherwise hidden behind the abstraction barriers between hardware and software," notes fred chong, seymour goodman professor of computer science at uchicago and lead pi for epiqc.
"in this case, our hardware modeling led us to revisit and challenge the conventional wisdom that binary operation is best for computation."
the full paper, "asymptotic improvements to quantum circuits via qutrits," is now published on arxiv.
more information: pranav gokhale et al.
asymptotic improvements to quantum circuits via qutrits, proceedings of the 46th international symposium on computer architecture - isca '19 (2019).
doi: 10.1145/3307650.3322253
with the aim of empowering the quantum research community, zurich instruments has come up with two new products: the uhfqa quantum analyzer and the pqsc programmable quantum system controller.
in combination with its hdawg arbitrary waveform generators and the labone(r) control software, the new products constitute the first commercial quantum computing control system (qccs).
the qccs from zurich instruments is a comprehensive and integrated system of individually certified instruments and software developed to deal with a quantum computing setup's high complexity.
it offers the advanced control electronics crucial for initializing, manipulating, and reading out quantum bits with the fidelity needed for quantum computers, as well as providing an effective interface to higher level components of the full quantum computing stack.
first commercial quantum analyzer
the uhfqa quantum analyzer from zurich instruments is the first commercial instrument designed exclusively to read out spin and superconducting qubits with high speed and fidelity.
the uhfqa will assist quantum researchers to upgrade their systems to 100 or more qubits and enhance the performance of the calculations, while minimizing the related costs and complexity.
first quantum system controller
the pqsc programmable quantum system controller from zurich instruments is the first commercial instrument dedicated to the precise synchronization of all the electronic components needed to control a quantum computer.
thanks to its low- latency real- time communication links, the pqsc can solve the practical drawbacks of conventional control strategies, rendering it feasible to perform very fast and automated qubit calibration routines.
a user can program the pqsc programmable quantum system controller to optimize for quick tune- up and error correction, and it is possible to adapt the processing to the particular algorithm and computer architecture used.
europe's open quantum computer - opensuperq
zurich instruments' proficiency in quantum research instrumentation, and the two products launched recently, will be put to use in the recently announced eu project opensuperq.
the aim of this international attempt is to create an open superconducting quantum computer of up to 100 qubits, which includes all the features of the full quantum computing stack.
collaborating with nine other collaborators, zurich instruments will share a budget of eur10.33 million over an initial period of three years.
being part of the opensuperq project, the role of zurich instruments is to develop the instrumentation that implements the quantum algorithms and reads out the qubit states.
the entire team at zurich instruments is excited to continue the intense collaboration with the top researchers in the field.
the new pqsc and the new quantum analyzer will contribute to a fast and sustainable advance of the opensuperq project and foster the global leadership of zurich instruments in quantum computing control systems.
jan benhelm, head of marketing, zurich instruments
zurich instrument's core quantum team (from top left): bruno kung (application scientist), jurg schwizer (head of software), sadik hafizovic (ceo), niels haandbaek (senior engineer), jan benhelm (head of marketing), and adrian messmer (vp r&d).
this information has been sourced, reviewed and adapted from materials provided by zurich instruments.
quantum communication, which ensures absolute data security, is one of the most advanced branches of the "second quantum revolution."
in quantum communication, the participating parties can detect any attempt at eavesdropping by resorting to the fundamental principle of quantum mechanics -  a measurement affects the measured quantity.
thus, the mere existence of an eavesdropper can be detected by identifying the traces that his measurements of the communication channel leave behind.
the major drawback of quantum communication today is the slow speed of data transfer, which is limited by the speed at which the parties can perform quantum measurements.
researchers at bar- ilan university have devised a method that overcomes this "speed limit," and enables an increase in the rate of data transfer by more than 5 orders of magnitude!
their findings were published today in the journal nature communications.
homodyne detection is a cornerstone of quantum optics, acting as a fundamental tool for processing quantum information.
however, the standard homodyne method suffers from a strong bandwidth limitation.
while quantum optical phenomena, exploited for quantum communication, can easily span a bandwidth of many thz, the standard processing methods of this information are inherently limited to the electronically accessible mhz- to- ghz range, leaving a dramatic gap between the relevant optical phenomena that is used for carrying the quantum information, and the capability to measure it.
thus, the rate at which quantum information can be processed is strongly limited.
in their work, the researchers replace the electrical nonlinearity that serves as the heart of homodyne detection, which transforms the optical quantum information into a classical electrical signal, with a direct optical nonlinearity, transforming the quantum information into a classical optical signal.
thus, the output signal of the measurement remains in the optical regime, and preserves the enormous bandwidth optical phenomena offers.
"we offer a direct optical measurement that conserves the information bandwidth, instead of an electrical measurement that compromises the bandwidth of the quantum optical information," says dr. yaakov shaked, who conducted the research during his ph.d. studies in the lab of prof. avi pe'er.
to demonstrate this idea, the researchers perform a simultaneous measurement of an ultra- broadband quantum optical state, spanning 55thz, presenting non- classical behavior across the entire spectrum.
such a measurement, using standard method, would be practically impossible.
the research was accomplished through a collaboration between the quantum optics labs of prof. avi pe'er and prof. michael rosenbluh, together with yoad michael, dr. rafi z. vered and leon bello at the department of physics and institute for nanotechnology and advanced materials at bar- ilan university.
this new form of quantum measurement is relevant also to other branches of the "second quantum revolution," such as quantum computing with super powers, quantum sensing with super sensitivity, and quantum imaging with super resolution.
story source:
materials provided by bar- ilan university.
note: content may be edited for style and length.
last month, a team of russian and american scientists unveiled a quantum computer array with 51 qubits at the international conference on quantum technologies in moscow.
here's a look at how they accomplished this new milestone with the use of cold atoms and lasers.
what is quantum computing?
if you're already familiar with quantum computing, i recommend skipping to the next section.
if you're not familiar with quantum computing, it is aptly named for its quantum properties.
in quantum physics, particles do not have a defined location until they are observed.
in classical computing, digital data is read in bits, which are 1s and 0s, or on and off states, which we know as binary, which can be manipulated into different arrangements using various logic gates.
quantum computing combines concepts from classical computing and quantum mechanics to make qubits (a shortened nickname for "quantum bits").
unlike classical bits, qubits can be a 1 or 0 at the same time, much like schrodinger's cat, which is in a state of flux until observed.
so, four bits have 16 possible combinations (24), whereas four qubits can be in every possible combination at the same time until they are observed.
this allows a quantum computer to perform every possible calculation at the same time.
a quantum algorithm reduces the time required for large calculations by the square root of the number of entries being searched.
quantum computers are not practical for most tasks handled by personal computers, but they excel at large- scale calculations such as searching databases, running simulations, and even breaking encryptions.
the video below is the simplest explanation of quantum computing i have seen so far.
the 51 qubit quantum computer and cold atom physics
it seems like every few months, quantum computing reaches a new milestone.
last month, at the international conference on quantum technologies in moscow, attendees and reporters gathered in mass for professor john martinis' presentation of a chip embedded with 49 qubits.
instead, in a fashion that reminds me of steve harvey announcing the miss universe pageant, mikhail lukin, a harvard professor and co- founder of the russian quantum center made his own announcement and stole the show.
lukin's team had successfully created the world's most powerful, functional quantum computer to date, which runs on 51 qubits.
the device was tested successfully at harvard, where it solved physics problems that silicon chip- based supercomputers were struggling with.
most quantum computers have been designed using superconductors and even semiconductors.
martinis' 49- qubit chip was constructed in this fashion.
since traditional semiconductor materials are reaching their limits, lukin's team took a different approach.
the 51- qubit machine uses "cold atoms" in place of qubits that are locked onto laser cells.
cold atom physics is the discipline of studying atoms at incredibly low temperatures (.0000001 degrees kelvin) in order to recreate quantum conditions.
cooling atoms to temperatures near absolute zero slows their movement down and makes them easier to observe.
the video below gives an introduction to cold atom physics (starting at 1:35).
after that, we'll get into the biggest question i had about all of this:
how the heck do super- cooled atoms with lasers shining through them make a computer?
how do cold atoms make a functional computer?
lukin's team wrote a research paper (pdf) explaining the experiment they set up.
after sifting through the equations, i arrived at the data- reading mechanism.
the setup consists of a linear array101 evenly spaced "optical tweezers", which are generated by feeding a multi- tone rf signal into an into an acousto- optic deflector.
in simpler terms, they shine a laser beam through a vacuum tube and take fluorescence images (a type of laser scanning microscopy) of the atoms as they change between positions.
the "traps" that control the position of the atoms are programmable, which allows this super cooled vacuum tube with a laser shooting through it to function like a quantum computer.
as computing devices become ever smaller, engineers have been teaming up with scientists from other disciplines like physics and biology to make some outside- the box computing devices.
although it's unlikely that any of these will end up in personal devices anytime soon (or ever), it always reminds me that a computer is just a device that calculates problems, and what our concept of a "computer" will look like in 100 years might just be beyond our current levels of comprehension.
if you'd like to learn more about quantum computing, i've compiled some resources below along with some of my favorite outlandish non- silicon computers!
quantum computing resources
some "out there" computers and logic- based devices
featured image used courtesy of kurzgesagt
as part of the collaboration, an ibm q system one quantum computer will be installed in a ibm computer center near stuttgart.
the system is scheduled to go into operation in early 2021 and will be the first of its kind in europe.
fraunhofer plans to bring together established partners from research and industry under the umbrella of a research infrastructure of fraunhofer institutes, which will work together in a centrally coordinated national fraunhofer competence network for quantum computing.
this network has set itself the goal of further developing and transferring application- oriented quantum computing strategies under complete data sovereignty of european law and will initially be represented by competence centers in expected six german states - baden- wurttemberg, bavaria, rhineland- palatinate, berlin, hesse and north rhine- westphalia.
currently, more than ten fraunhofer institutes are already working on various fields of quantum technology.
as early as april 1, 2020, interested companies and research institutions will have access to the world's largest group of quantum computers in the us- based ibm quantum computation center under the umbrella of the nationwide fraunhofer competence network.
the ibm quantum computation center currently comprises 15 systems and is located in the us state of new york.
under the terms of the agreement, ibm will offer fraunhofer technical support and assistance in using the ibm quantum systems.
pioneering initiative on applied quantum computing
the signing of the cooperation follows the joint announcement of september 2019, a pioneering initiative on applied quantum computing for germany's research institutions and companies.
the cooperation partners support the german federal government's goal of investing almost one billion euros over the next two years in order to develop quantum technology from basic research to marketable applications.
this is to be made possible by the development of a research infrastructure that strategically promotes the further development and dissemination of quantum computing in germany.
the participating federal states baden- wurttemberg and bavaria contribute the largest financial share.
"a central research question is which concrete application scenarios are suitable for calculation with a quantum computer, how algorithms for this can be developed and translated into simple applications.
quantum computing has the potential to analyze the complex systems in business and industry, to unravel molecular and chemical interactions, to solve complicated optimization problems and to make artificial intelligence significantly more powerful," explains fraunhofer president prof. reimund neugebauer.
"such advances could open the door to new scientific knowledge and enormous improvements, for example in supply chains, logistics and the modelling of financial data, as well as problems from the classical engineering sciences."
"this agreement opens up another opportunity for europe to become a pioneer in the further development of a promising technology with germany playing a leading role as we grow the scientific, academic, public and private ecosystem which seeks to solve complex problems, such as climate change and healthcare issues.
a memorable milestone for our region", says martin jetter, senior vice president & chairman ibm europe.
"the installation of the first physical quantum computer on european soil sends out a strong signal in support of germany as a research location.
it marks an important step toward the further establishment of an internationally recognizable ecosystem in the field of quantum technology," says helge braun, head of the federal chancellery.
"the quantum computer puts us in a strong position to help shape, at an early stage and in a decisive way, the key technologies of the future.
germany in general, and baden- wurttemberg in particular, will become the center of quantum technology in europe.
this will provide both industry and science with immense research and experimental opportunities in the fields of transportation, machine tools, communications, health care, and the finance and energy industries.
naturally, as minister- president, i am delighted that baden- wurttemberg has been chosen as the location for such pioneering technologies," says minister- president winfried kretschmann.
hubert aiwanger, bavarian state minister of economic affairs: "quantum computing has enormous potential in many fields of application, including logistics, materials research, artificial intelligence and it security.
bavaria will therefore support this initiative of the fraunhofer- gesellschaft with a high- performance competence center, to be led by fraunhofer institute for applied and integrated security aisec in garching.
one area of focus will be the interaction between quantum computing and it security.
we want to push ahead with the development of quantum computing as a new key technology for bavarian companies."
nicole hoffmeister- kraut, baden- wurttemberg state minister of economic affairs, says: "the installation of a quantum computer in baden- wurttemberg is a great achievement and an enormous opportunity for science and industry in our region.
as a 'state of hidden champions' and one of europe's most innovative regions, we offer ideal conditions for this new technology as well as a host of application fields in which quantum computing can be of benefit.
the availability of such immense computing power will make it possible to solve, in the shortest possible time, fundamental economic problems that currently take years to process.
the state government is therefore providing 40 million euros in funding to build up the necessary expertise in industry, together with ibm and the fraunhofer- gesellschaft.
the establishment of a competence center in baden- wurttemberg marks an important step, since it will enable us to make the most of the opportunities offered by quantum computing in the future."
ibm system q one is optimized to ensure the quality, stability, reliability and reproducibility of multi- qubit applications.
due to these factors and the resulting high quantum volume (a measure of the performance of a quantum computer), the ibm system q one enables state- of- the- art research for concrete application scenarios in science and industry.
in 2016, ibm was the first company to make universal quantum computers accessible via the cloud.
an active community of more than 200,000 users have run hundreds of billions of executions on real ibm quantum hardware, and have published more than 200 research papers based on these experiments.
ibm is also the first company to have commercial clients via the ibm q network, a community of more than 100 businesses, start- ups, research labs, education institutions and governments working with ibm to advance quantum computing.
recently, at their 2019 ignite conference, microsoft announced a new service known as "azure quantum," a partnership between microsoft and honeywell that provides users with cloud access to the quantum computers of honeywell, ionq, and qci.
alongside google, ibm, and a handful of others, microsoft is considered one of the leaders in the field of quantum computing, and this new service includes access to their open- source quantum development kit (utilizing q#, microsoft's quantum computer programming language), marking the service as a potentially attractive choice for potential developers.
what is quantum computing?
at the very core of quantum computing lies the qubit.
where traditional computing methods store and quantify basic information using bits in binary values of 0 or 1, quantum computing utilizes qubits.
qubits, like binary bits, still possess the two possible outcomes of "0" or "1".
by the principle of quantum superposition, however, quantum computing states that these two quantum states of "0" or "1" can be superposed on one- another and result in another valid quantum state.
additionally, the reverse is also true, meaning that any quantum state can also be represented as the sum of any two (or more) quantum states.
as a result, qubits can exist in this superposition, and send two bits worth of information as a single unit of quantum information, or one qubit.
because of this, the parallel nature of quantum computers allows them to work on potentially millions of computations at once instead of operating on the binary level of our modern computers.
this has caused quantum computing to become perhaps the most promising emergent computational model for large- scale information processing.
quantum computing in industry
quantum computers are far more suited to industrial and commercial applications over small- scale use for, say, personal devices.
the promise of quantum computing in industry is an unparalleled ability to run calculations millions of times faster than standard computers, providing industry solutions in cybersecurity, database analytics, trade optimization, artificial intelligence, and even code- breaking.
as mentioned earlier in the article, there is a select group of leaders in the quantum computing space.
giants such as google, microsoft, ibm, hewlett- packard, and intel are racing to be the first to break the barrier and achieve "quantum supremacy."
additionally, ibm has already begun selling the use of their quantum supercomputer to commercial and research clients, showing us that commercial quantum computing is becoming more of a reality by the day.
honeywell and quantum computing
according to their quantum solutions page, honeywelll has been active in the quantum computing space for more than a decade.
as tony uttley, president of honeywell quantum solutions, explained last year, the company began investigating the technology because they already had many of the proficiencies needed: "state- of- the- art hardware and software control systems, advanced electronics, optics and photonics..." honeywell researchers proposed that combining some of honeywell's existing technologies would allow them to build a quantum computer of their own.
uttley also explained his view on a more sweeping reason it makes sense for honeywell to develop quantum computers: "...at its heart, quantum computing is a controls problem."
"...at its heart, quantum computing is a controls problem."
honeywell's trapped ion technology
the company refers to themselves as "future shapers" in this space, which can be seen in their trapped- ion quantum technology.
essentially, honeywell's quantum computers use trapped ions as qubits to complete computations and transfer information.
their industry- leading solution has led to record- setting fidelity, and their newly- developed parallel quantum computing has further increased the speed of qubit operations.
they've accomplished this feat by incorporating multiple parallel, independent operating zones for the trapped ions via multiple optical beams.
the future of quantum computing
the recent partnership with microsoft is a step for honeywell towards the commercialization of this technology.
as far back as 2016, ibm has allowed engineers access to their quantum computer via the cloud.
honeywell will do the same in early 2020, utilizing microsoft's cloud technology to give access to their own computers to allow public use for program development.
uttley said last year that among the first industries that could benefit from this technology is pharmaceuticals, where quantum computing could allow for much faster r&d.
it could also, however, affect many other industries, including the optimization of manufacturing and oil and gas.
honeywell
while quantum computing is still in its infancy, it is becoming more of a reality with each passing day.
it may only be a matter of time before automation functions via quantum algorithms in a facility near you.
how familiar are you with quantum computing?
will you be checking out honeywell's program to try your hand at developing quantum algorithms?
share your thoughts in the comments below.
scientists at linkoping university have shown how a quantum computer really works and have managed to simulate quantum computer properties in a classical computer.
"our results should be highly significant in determining how to build quantum computers," says professor jan- ake larsson.
the dream of superfast and powerful quantum computers has again been brought into focus, and large resources have been invested in research in sweden, europe and the world.
a swedish quantum computer is to be built within ten years, and the eu has designated quantum technology one of its flagship projects.
at the moment, few useful algorithms are available for quantum computers, but it is expected that the technology will be hugely significant in simulations of biological, chemical and physical systems that are far too complicated for even the most powerful computers currently available.
a bit in a computer can take only the value one or zero, but a quantum bit can take all values in between.
simply put, this means that quantum computers do not need to take as many operations for each calculation they carry out.
professor jan- ake larsson and his doctoral student niklas johansson, in the division for information coding at the department of electrical engineering, linkoping university, have come to grips with what happens in a quantum computer and why it is more powerful than a classical computer.
their results have been published in the scientific journal entropy.
"we have shown that the major difference is that quantum computers have two degrees of freedom for each bit.
by simulating an additional degree of freedom in a classical computer, we can run some of the algorithms at the same speed as they would achieve in a quantum computer," says jan- ake larsson.
they have constructed a simulation tool, quantum simulation logic, qsl, that enables them to simulate the operation of a quantum computer in a classical computer.
the simulation tool contains one, and only one, property that a quantum computer has that a classical computer does not: one extra degree of freedom for each bit that is part of the calculation.
"thus, each bit has two degrees of freedom: it can be compared with a mechanical system in which each part has two degrees of freedom -  position and speed.
in this case, we deal with computation bits -  which carry information about the result of the function, and phase bits -  which carry information about the structure of the function," jan- ake larsson explains.
they have used the simulation tool to study some of the quantum algorithms that manage the structure of the function.
several of the algorithms run as fast in the simulation as they would in a quantum computer.
"the result shows that the higher speed in quantum computers comes from their ability to store, process and retrieve information in one additional information- carrying degree of freedom.
this enables us to better understand how quantum computers work.
also, this knowledge should make it easier to build quantum computers, since we know which property is most important for the quantum computer to work as expected," says jan- ake larsson.
jan- ake larsson and his co- workers have also supplemented their theoretical simulations with a physical version built with electronic components.
the gates are similar to those used in quantum computers, and the toolkit simulates how a quantum computer works.
with its help students, for example, can simulate and understand how quantum cryptography and quantum teleportation works, and also some of the most common quantum computing algorithms, such as shor's algorithm for factorisation.
(the algorithm works in the current version of the simulation but is equally fast -  or slow -  as in classical computers).
"it's a long way to the top if you wanna rock 'n' roll."
- ac/dc
you'll never lose your job for buying ibm.
that was the conventional wisdom in the days of large mainframe computers, the heavy metal that ran your uncle's downtown business.
ibm was considered the safe choice, the blue- chip, reliable, businesslike ("it's in their name!")
supplier of computers, typewriters, and assorted office technology.
big blue.
now we have big weird.
but weird is good.
more to the point, weird is where we're heading, ready or not.
someday our great- grandchildren will look back on today's digital computers as quaint curiosities, right after they post snapchat updates on their quantum cellphones.
ibm put a proverbial stake in the ground this week, announcing that it was officially entering the race to supply "the first universal quantum computer for business and science."
after commercializing mainframe computers in the 1970s and revolutionizing personal computers in the 1980s, ibm is now setting its sights on the next big thing.
and it's plenty weird.
if you've never used, seen, or heard of a quantum computer, you're not alone.
i imagine most folks back in, say, 1954 would've said the same thing about computers of any sort (apart from the human "computers," often women, who tallied sums, of course).
quantum computers are like today's normal digital computers, inasmuch as they're boxes with blinking lights that calculate difficult math problems... but that's about where the similarities end.
quantum computers are deeply strange because quantum physics is strange.
they rely on the profoundly counterintuitive properties of certain superconducting materials operating at extraordinarily low temperatures.
in that realm, where schrodinger's cat, heisenberg's uncertainty principle, and pauli operators rule, data bits are not what they seem.
a quantum bit is called a qubit, and it can retain one of two states, which we arbitrarily call 1 or 0.
no problem, right?
except that qubits can be both a 1 and a 0 simultaneously, a condition known as superposition.
there is no apparent contradiction here; that's just how quantum physics works.
moreover, qubit states are more probabilistic than actual (the born rule), so you can never really know what your qubit is doing - which would seem to make it completely pointless as a data- storage element.
except that an eerie characteristic known as quantum entanglement means that the state of one qubit is (somehow) related to the state of another, across any distance and regardless of time (we think).
so you can monitor the state of one qubit by observing a different qubit.
it goes farther down the rabbit hole from there, but the point is, making real working quantum computers is hard.
so far, ibm has a 5- qubit machine working reliably, and the company has produced systems with 15 or more qubits that it rents out to researchers.
the goal is to have a 50- qubit machine, at which point the computers probably won't need us anymore.
paradoxically, quantum computers are huge machines.
ibm's current models are vast, steampunk- esque calliopes of brass tubes and mechanical connectors.
you almost expect to find steam hissing from fittings among the clanking gears.
presumably, the ibm inventors with top hats and goggles are pulling on huge levers just out of the picture.
the victorian- era plumbing is there to keep the little qubits cool.
ibm's qubits are fabricated in silicon, just like "normal" computer hardware, but with lashings of niobium, aluminum, and other superconducting metals.
these are all cooled ("frozen" would be more accurate) to just 15 millikelvins, or 0.015 degrees above absolute zero, the temperature at which all physical energy stops.
that's more than 100 times colder than outer space.
but it's only at these unbelievably low temperatures that quantum materials perform their party tricks.
quantum machines aren't just remarkably fast, they're fundamentally different.
a few companies have built conventional computers using quantum elements, but ibm dismisses these as half- hearted "science fair experiments."
to truly reap the benefits of quantum computing, you need to embrace the weird.
quantum machines don't run serially, like conventional computers do.
complex problems like factoring integers don't become exponentially harder for a quantum computer, as they do for today's digital machines.
instead, quantum computers can automatically, almost naturally, parallelize complex problems that would be unfeasible with a conventional machine.
as an example, ibm points to shor's algorithm, the current state of the art in finding the prime factors in large integers.
for large factors, such as those commonly used in cryptography, a conventional computer would take an unreasonably long time, which is the whole basis of the crypto algorithm's security.
it's not that the numbers can't be factored; it would just take hundreds of years using current technology.
a quantum machine, however, could crack the code in a matter of seconds.
big difference.
quantum machines also lend themselves to real- world problems in weather prediction, pharmaceutical research, and physics experiments - all areas where simulating multiple simultaneous agents is paramount.
these are all variations of the "knapsack problem," and are essentially unsolvable using current methods.
ibm believes that the proverbial knee on the curve comes at around 50 qubits.
once you have a 50- qubit quantum computer running reliably, you've passed the point of no return.
at that level of performance, you can no longer simulate or duplicate the results of a quantum machine using a conventional machine.
that means, for instance, that you can't design a 50- qubit computer using conventional eda tools - it simply can't be simulated in any reasonable amount of time.
nor can you verify quantum results with a digital machine.
you've passed beyond the capabilities of today's architectures and you're in uncharted territory.
want a taste of quantum computing for yourself?
ibm will let you play with one of their machines, for free.
the company launched its quantum experience website last year as a browser- based portal into a real 5- qubit quantum computer.
like any microcontroller company, ibm allows users to upload programs and watch the results, a kind of "try before you buy" sampler of quantum computing.
given that ibm is out in left field, are they alone in left field?
is this the only company building quantum computers?
in a word, no.
there are others, most notably d- wave, which is arguably out in front of ibm in terms of commercialization.
d- wave's d2000q machine is a real, working quantum computer, and the company has paying customers, mostly in government and aerospace, as you might expect.
for its part, ibm feels that its approach will be more general- purpose, more mainstream (if that word can be applied to quantum computing).
the company acknowledges d- wave's lead in commercialization but also dismisses that company's quantum architecture as simply a form of "annealing" that's not appropriate for "business and scientific" applications like ibm's.
with multiple companies spending lavish amounts to develop quantum computing, it's fair to ask, why do it at all?
sure, it's cool and high- tech, but will it ever become... normal?
can it be commercially successful, or will quantum computing always be a specialized niche, like carbon- fiber bicycles or molecular gastronomy?
at least publicly, ibm believes that conventional computing and moore's law scaling are both nearing the end of the road.
"there's no such thing as a zero- nanometer transistor," says dave turk, ibm's vp of high- performance computing.
zeno's paradox notwithstanding, we have to stop shrinking transistors eventually.
it's time for a complete rethink of how we design computers.
personally, i'm always skeptical of claims that the sky is falling and that technology x is moribund.
"moore's law is dead; email is dead; retail is dead; x86 is dead; etc."
we show a remarkable tendency to cling to our old technologies (qwerty keyboard or ascii encoding, anyone?)
in spite of all the "better" alternatives.
i don't see digital von neumann architectures disappearing anytime soon.
we'll all have jetpacks and flying cars before that happens.
still...
i can't help thinking that ibm is onto something.
there are some truly intractable problems that conventional computer architectures can't solve.
crypto technology depends on those problems being unsolvable.
others, we don't even attempt to model, like complex molecular interactions in physics or medicine.
quantum technology appears to be ideally suited to this class of problems.
conveniently, quantum mechanics is very good at modeling the effects of quantum mechanics.
ibm changed our idea of personal computers in 1981 with the model 5150.
maybe the company can do it again in a few decades.
i wonder if it will still run windows.
related
los alamos national laboratory scientists have developed a new quantum computing algorithm that offers a clearer understanding of the quantum- to- classical transition, which could help model systems on the cusp of quantum and classical worlds, such as biological proteins, and also resolve questions about how quantum mechanics applies to large- scale objects.
"the quantum- to- classical transition occurs when you add more and more particles to a quantum system," said patrick coles of the physics of condensed matter and complex systems group at los alamos national laboratory, "such that the weird quantum effects go away and the system starts to behave more classically.
for these systems, it's essentially impossible to use a classical computer to study the quantum- to- classical transition.
we could study this with our algorithm and a quantum computer consisting of several hundred qubits, which we anticipate will be available in the next few years based on the current progress in the field."
answering questions about the quantum- to- classical transition is notoriously difficult.
for systems of more than a few atoms, the problem rapidly becomes intractable.
the number of equations grows exponentially with each added atom.
proteins, for example, consist of long strings of molecules that may become important biological components or sources of disease, depending on how they fold up.
although proteins can be comparatively large molecules, they are small enough that the quantum- to- classical transition, and algorithms that can handle it, become important when trying to understand and predict how proteins fold.
in order to study aspects of the quantum- to- classical transition on a quantum computer, researchers first need a means to characterize how close a quantum system is to behaving classically.
quantum objects have characteristics of both particles and waves.
in some cases, they interact like tiny billiard balls, in others they interfere with each other in much the same way that waves on the ocean combine to make larger waves or cancel each other out.
the wave- like interference is a quantum effect.
fortunately, a quantum system can be described using intuitive classical probabilities rather than the more challenging methods of quantum mechanics, when there is no interference.
the lanl group's algorithm determines how close a quantum system is to behaving classically.
the result is a tool they can use to search for classicality in quantum systems and understand how quantum systems, in the end, seem classical to us in our everyday life.
fiber optics has made communication faster than ever, but the next step involves a quantum leap -  literally.
in order to improve the security of the transfer of information, scientists are working on how to translate electrical quantum states to optical quantum states in a way that would enable ultrafast, quantum- encrypted communications.
a uc santa barbara research team has demonstrated the first and arguably most challenging step in the process.
the paper, published in nature physics, describes a nanomechanical transducer that provides strong and coherent coupling between microwave signals and optical photons.
in other words, the transducer is an effective conduit for translating electrical signals (microwaves) into light (photons).
today's high- speed internet converts electrical signals to light and sends it through optical fibers, but accomplishing this with quantum information is one of the great challenges in quantum physics.
if realized, this would enable secure communication and even quantum teleportation, a process by which quantum information can be transmitted from one location to another.
"there's this big effort going on in science now to construct computers and networks that work on the principles of quantum physics," says lead author jorg bochmann, a postdoctoral scholar in ucsb's department of physics.
"and we have found that there actually is a way to translate electrical quantum states to optical quantum states."
the new paper outlines the concept and presents a prototype device, which uses an optomechanical crystal implemented in a piezoelectric material in a way that is compatible with superconducting qubits, quantum analogs of classical bits.
operating the device at the single phonon limit, the scientists were able generate coherent interactions between electrical signals, very high frequency mechanical vibrations, and optical signals.
although the first prototype of the transducer has not been operated in the quantum realm, that is, in fact, the next step for the research effort.
"in this paper, we're characterizing the system using classical electrical and optical signals and find that the essential parameters look very promising," says bochmann.
"in the next step, we would have to actually input quantum signals from the electrical side and then check whether the quantum properties are preserved in the light."
according to the authors, their prototype transducer is fully compatible with superconducting quantum circuits and is well suited for cryogenic operation.
"the coupled dynamics of the system should be the same at low temperatures as in our room temperature measurements, albeit with a lower thermal background," said co- author andrew cleland, a professor of physics and associate director of the california nanosystems institute at ucsb.
"genuine quantum features and non- classical mechanical states will emerge when we couple a superconducting qubit to the transducer.
"we believe that combining optomechanics with superconducting quantum devices will enable a new generation of on- chip quantum devices with unique capabilities, as well as opening an exciting pathway for realizing entangled networks of electronic and photonic quantum systems," cleland said.
scientists from the doe's brookhaven national laboratory and energy sciences network (esnet) and stony brook university have been working on an experiment that keeps u.s. quantum networking research on the international map.
they have built a quantum- network testbed that connects several buildings on the brookhaven lab campus using portable quantum entanglement sources and an existing esnet communications fiber network- a significant step toward building a large- scale quantum network that can transmit information over long distances.
"in quantum mechanics, the physical properties of entangled particles remain associated, even when separated by vast distances," says kerstin kleese van dam, director of brookhaven lab's computational science initiative.
"thus, when measurements are performed on one side, it also affects the other.
to date, this work has been successfully demonstrated with entangled photons separated by approximately 11 miles.
this is one of the largest quantum entanglement distribution networks in the world, and the longest- distance entanglement experiment in the u.s."
one distinct aspect of the team's work that sets it apart from quantum networks being run in china and europe is that the entanglement sources are portable and can be easily mounted in standard data center computer server racks connected to regular fiber- distribution panels.
the team successfully installed a portable quantum- entangled photon source in a server rack housed within the bnl scientific data and computing center, where the lab's central networking hub is located.
with this connectivity, entangled photons now can be distributed to every building on the lab's campus over brookhaven and esnet fiber infrastructure.
esnet's fibers have been placed along paths between buildings to enable the distribution and study of entanglement over increasingly longer distances.
the portable entanglement sources also are compatible with existing quantum memories (atom- filled glass cells that can store quantum information).
normally kept at super- cold temperatures, these cells can be stimulated using lasers to control the atomic states within them.
the brookhaven- testbed features portable quantum memories that operate at room temperature.
such memories, engineered for quantum networking on a large scale, have been a longtime "pet project" for eden figueroa, a joint appointee with brookhaven's csi and instrumentation division and a stony brook university professor who leads its quantum information technology group.
he serves as lead investigator of the quantum networking testbed project.
"the demonstration aims to combine entanglement with compatible atomic quantum memories," figueroa says "our quantum memories have the advantage of operating at room temperature rather than requiring subfreezing cold.
this makes it natural to expand the test to principles of quantum repeaters, which are the technological keys to quantum communication over hundreds of kilometers."
quantum networks send light pulses (photons) through the fiber, which requires the light to be periodically amplified as it travels through the lines.
however, unlike digital transmissions in communication networks, quantum entanglement is limited by decoherence, where entangled photons, for example, revert to classical states because interactions with the environment make them lose the ability to remain entangled.
this limits these fragile quantum states from being sent over large distances.
viable quantum repeaters will let figueroa and his team scale up ongoing experiments within "local- area" quantum networks to a distributed, or "wide- area," version.
in anticipation of this, the team is building the necessary optical connections to link brookhaven lab's quantum network to ones at stony brook and yale universities.
"the quantum network with entangled photon sources mounted in server racks, portable quantum memories, and operable repeaters will mark the first real quantum communication network in the world that truly connects quantum computing processors and memories using photonic quantum entanglement," figueroa says.
"it will mark a sea change in communications that can impact the world."
the realisation of quantum networks is one of the major challenges of modern physics.
now, new research shows how high- quality photons can be generated from 'solid- state' chips, bringing us closer to the quantum 'internet'.
the number of transistors on a microprocessor continues to double every two years, amazingly holding firm to a prediction by intel co- founder gordon moore almost 50 years ago.
if this is to continue, conceptual and technical advances harnessing the power of quantum mechanics in microchips will need to be investigated within the next decade.
developing a distributed quantum network is one promising direction pursued by many researchers today.
a variety of solid- state systems are currently being investigated as candidates for quantum bits of information, or qubits, as well as a number of approaches to quantum computing protocols, and the race is on for identifying the best combination.
one such qubit, a quantum dot, is made of semiconductor nanocrystals embedded in a chip and can be controlled electro- optically.
single photons will form an integral part of distributed quantum networks as flying qubits.
first, they are the natural choice for quantum communication, as they carry information quickly and reliably across long distances.
second, they can take part in quantum logic operations, provided all the photons taking part are identical.
unfortunately, the quality of photons generated from solid- state qubits, including quantum dots, can be low due to decoherence mechanisms within the materials.
with each emitted photon being distinct from the others, developing a quantum photonic network faces a major roadblock.
now, researchers from the cavendish laboratory at cambridge university have implemented a novel technique to generate single photons with tailored properties from solid- state devices that are identical in quality to lasers.
their research is published today in the journal nature communications.
as their photon source, the researchers built a semiconductor schottky diode device containing individually addressable quantum dots.
the transitions of quantum dots were used to generate single photons via resonance fluorescence - a technique demonstrated previously by the same team.
under weak excitation, also known as the heitler regime, the main contribution to photon generation is through elastic scattering.
by operating in this way, photon decoherence can be avoided altogether.
the researchers were able to quantify how similar these photons are to lasers in terms of coherence and waveform - it turned out they were identical.
"our research has added the concepts of coherent photon shaping and generation to the toolbox of solid- state quantum photonics," said dr mete atature from the department of physics, who led the research.
"we are now achieving a high- rate of single photons which are identical in quality to lasers with the further advantage of coherently programmable waveform - a significant paradigm shift to the conventional single photon generation via spontaneous decay."
there are already protocols proposed for quantum computing and communication which rely on this photon generation scheme, and this work can be extended to other single photon sources as well, such as single molecules, colour centres in diamond and nanowires.
"we are at the dawn of quantum- enabled technologies, and quantum computing is one of many thrilling possibilities," added atature.
"our results in particular suggest that multiple distant qubits in a distributed quantum network can share a highly coherent and programmable photonic interconnect that is liberated from the detrimental properties of the chips.
consequently, the ability to generate quantum entanglement and perform quantum teleportation between distant quantum- dot spin qubits with very high fidelity is now only a matter of time."
scientists developing a prototype quantum hard drive have improved storage time by a factor of more than 100.
the team's record storage time of six hours is a major step towards a secure worldwide data encryption network based on quantum information, which could be used for banking transactions and personal emails.
"we believe it will soon be possible to distribute quantum information between any two points on the globe," said lead author manjin zhong, from the research school of physics and engineering (rspe) at the australian national university (anu).
"quantum states are very fragile and normally collapse in milliseconds.
our long storage times have the potential to revolutionise the transmission of quantum information."
quantum information promises unbreakable encryption because quantum particles such as photons of light can be created in a way that intrinsically links them.
interactions with either of these entangled particles affect the other, no matter how far they are separated.
the team of physicists at anu and the university of otago stored quantum information in atoms of the rare earth element europium embedded in a crystal.
their solid- state technique is a promising alternative to using laser beams in optical fibres, an approach which is currently used to create quantum networks around 100 kilometres long.
"our storage times are now so long that it means people need to rethink what is the best way to distribute quantum data," ms zhong said.
"even transporting our crystals at pedestrian speeds we have less loss than laser systems for a given distance."
"we can now imagine storing entangled light in separate crystals and then transporting them to different parts of the network thousands of kilometres apart.
so, we are thinking of our crystals as portable optical hard drives for quantum entanglement."
after writing a quantum state onto the nuclear spin of the europium using laser light, the team subjected the crystal to a combination of a fixed and oscillating magnetic fields to preserve the fragile quantum information.
"the two fields isolate the europium spins and prevent the quantum information leaking away," said dr jevon longdell of the university of otago.
the anu group is also excited about the fundamental tests of quantum mechanics that a quantum optical hard drive will enable.
"we have never before had the possibility to explore quantum entanglement over such long distances," said associate professor matthew sellars, leader of the research team.
"we should always be looking to test whether our theories match up with reality.
maybe in this new regime our theory of quantum mechanics breaks."
google announced monday that it is making available an open- source library for quantum machine- learning applications.
tensorflow quantum, a free library of applications, is an add- on to the widely- used tensorflow toolkit, which has helped to bring the world of machine learning to developers across the globe.
"we hope this framework provides the necessary tools for the quantum computing and machine learning research communities to explore models of both natural and artificial quantum systems, and ultimately discover new quantum algorithms which could potentially yield a quantum advantage," a report posted by members of google's x unit on the ai blog states.
they note that advances in recent years in quantum computing "could have a profound impact on the world's biggest problems, leading to breakthroughs in the areas of medicine, materials, sensing and communications."
improvements derived through quantum research have been seen in cancer detection, earthquake forecasting, extreme weather predictions and discoveries of new exoplanets.
but there has been a lack of analytical tools that can help developers fully utilize the enormous inroads quantum analysis can provide when dealing with massive volumes of data.
google researchers' tensorflow quantum is a major step towards broadening the reach of quantum computing to research communities.
the project was undertaken by the google ai quantum team in conjunction with students of the university of waterloo, alphabet's x and volkswagen.
google is not alone in undertaking a major new project in quantum computing.
microsoft recently launched its azure quantum project, and xanadu in toronto recently introduced a platform similar to tensorflow called pennylane.
and honeywell has also recently announced that it is developing a new quantum computing system.
but much attention will be focused on google's tensorflow project because of the company's popularity among massive numbers of developer communities and its sound track record in the quantum field.
quantum computing is increasingly being seen as a critical component for businesses seeking to grab the lead in an ever- increasingly high- tech and competitive business atmosphere.
a study by fujitsu last year found 70 percent of businesses want to apply quantum computing to their business models so they can more rapidly analyze data and develop solutions.
some 89 percent in the study said they believe inferior computing power is making them less competitive.
the tensorflow quantum announcement comes the same week as the annual meeting of machine learning enthusiasts at google's headquarters in mountain view, california.
instead of a live presentation, a printed report was distributed to members because of concerns over the coronavirus.
more information: announcing tensorflow quantum: an open source library for quantum machine learning, https://ai.googleblog.com/2020 ... ow- quantum- open.html
tensorflow quantum: a software framework for quantum machine learning, arxiv:2003.02989 [quant- ph] https://arxiv.org/abs/2003.02989
scientists at the rdecom research laboratory, the army's corporate research laboratory (arl) have found a novel way to safeguard quantum information during transmission, opening the door for more secure and reliable communication for warfighters on the battlefield.
recent advancements of cutting- edge technologies in lasers and nanophysics, quantum optics and photonics have given researchers the necessary tools to control and manipulate miniature quantum systems, such as individual atoms or photons - the smallest particles of light.
these developments have given rise to a new area of science - quantum information science, or qis, that studies information encoded in quantum systems and encompasses quantum computing, quantum communication and quantum sensing among other subfields.
quantum information science is believed to have the potential to shape the way information is processed in the future.
the army's corporate research laboratory invests in qis research to guarantee continuous technological superiority in this rapidly developing field, which in turn will bring about multiple new technologies in computation, encryption, secure communication and precise measurements.
however, to utilize quantum information, scientists need to figure out robust ways to process and transmit it - a task being tackled by drs.
daniel jones, brian kirby, and michael brodsky from the laboratory's computational and information sciences directorate.
"in our classical world, information is often corrupted during manipulation and transmission - everyone is familiar with noisy cell phone connections in poor reception areas," brodsky said.
"thus, communication engineers have been working on a variety of techniques to filter out the noise."
in classical communications, the filtering is rather straightforward as it is done locally, that is in the very place the information is received, such as directly in your phone or internet router.
in the quantum world, things become much more intricate.
the lab's research team has been looking into ways of filtering noise from little bits of quantum information - quantum bits or qubits sent across fiber- optic telecom links.
they discovered that the filtering does not necessarily need to be done by the receiving party.
"the nature of the quantum states in which the information is encoded is such that the filtering could be more easily done at a different location in the network," kirby said.
that's right, to fix a qubit sent over a certain route, one could actually apply a filter to other qubits that traverse a different route.
over the last year, the researchers have been looking into the problem of transmission of entangled pairs of photons over optical fibers.
"we started with developing an understanding of how physical properties of real telecom fibers, such as inherent residual birefringence and polarization dependent loss, or pdl, affect the quality of quantum communications," jones said.
"we exploited a novel mathematical approach, which has led to the development of a simple and elegant geometrical model of the pdl effects on polarization entanglement," kirby added.
the developed model predicts both the quality of transmitted quantum states as well as the rate at which quantum information could be transmitted.
furthermore, the lab's team invented a new technique that helps reduce the deleterious effects of the noise.
the developed models were experimentally validated using the recently built quantum networking testbed at the lab, which simulates the practical telecom fiber infrastructure.
"we believe that this research has a potential to revolutionize cybersecurity and to enable secure secret sharing and authentication for the warfighter of the future," brodsky said.
"in addition, it will have an impact on developing better sensors for position navigation and timing as well as quantum computers that might result in the synthesis of novel special materials with on demand properties."
according to the researchers, in order to make quantum technology a reality, a large- scale field- deployed testbed must be built, thus guiding the development of both quantum hardware and software.
a journal paper documenting the research titled "tuning quantum channels to maximize polarization entanglement for telecom photon pairs" is featured in the prestigious nature partner journal quantum information.
a way to speed up quantum computer tech progress has arrived from intel.
if you are interested in following the waves and advances in quantum computing, then get familiar with this word trio: cryogenic wafer prober.
before their design, the electrical characterization of qubits was slower than with traditional transistors.
even small subsets of data might take days to collect.
drug development.
chemistry.
climate change.
financial modeling.
scientists in all areas look forward to more advancements to push quantum computers to the frontlines.
speeding progress could also mean speeding up advancements in science and industry.
"quantum computing, in essence, is the ultimate in parallel computing, with the potential to tackle problems conventional computers can't handle," said intel.
in turn, interest is high over the quantum computing testing tool: intel, bluefors and afore introduced a cryoprober, a quantum testing device, and it is named the cryogenic wafer prober.
intel developed the prober with the other two companies.
an intel- bred description: cryogenic wafer prober is a cryoprober tool.
it has been designed to test and validate qubits needed for quantum computing.
the quantum chips are called qubits.
as far as mit technology review's the download can see it, intel's prober is a big deal because it means speeding up the testing of quantum bit, or qubits.
the latter, after all, are the key to "the potential power" of quantum computers.
how quantum computers and their chips are usually tested: tweaktown's anthony garreffa walked it through.
think "super- low temperature dilution refrigerator," to see what works out and what does not.
each quantum processor is tested for months in a low- temperature dilution refrigerator.
tweaks can be sent to manufacturing before the chips are made.
with the prober, researchers can test qubits on 300mm wafers down to temperatures of a few kelvins.
there is a timing advantage.
intel can characterize a large subset of these transistors on a 300mm wafer in about an hour and inform the feedback loop back to the fabrication line.
what can the cryoprober tell intel?
lucian armasu in tom's hardware said the prober allows intel to automate and collect information on spin qubits, such as the sources of quantum noise, the quality of quantum dots and the materials that can be used to create the spin qubits."
the feb. 28 news release explained the temperature factor: for quantum computing, turn- on characteristics of qubits must be measured at low temperatures of less than a few kelvins above absolute zero.
dean pennington in techspot explained: "because the characteristics of qubits must be measured at extremely low temperatures, the equipment and technological limitations of existing testing environments meant small subsets of data often took days to collect in conventional dilution refrigerators.
the cryoprober tool will allow intel to automate and collect information on qubits in a matter of minutes."
the intel site carried a graphic showing the tool's ability to create a statistical correlation of the increase in turn- on voltage between (1) room temperature and (2) cryogenic temperature.
why the tool matters: the tool can scale up the manufacture of silicon quantum computers, said garreffa, with less issues.
as for the scaling issue, the download, in the post by martin giles, reflected on "scaling."
it's a question.
for quantum machines to be effective, the requirements are "vast numbers of qubits" for quantum machines.
there are challenges to that requirement.
the tiniest vibration.
changes in temperature.
then, qubits may lose their fragile quantum state.
while thinking about how to scale the manufacturing makes sense, said the download, some experts think this "noise" issue "could prevent the computers from ever going mainstream."
in order to build the cryogenic wafer prober, intel approached bluefors and afore.
why bluefors?
they are noted for building cryogen- free dilution refrigerator systems with a focus on quantum computing, and afore is noted for micro- electro- mechanical systems (mems) test solutions.
moving forward, the first cryogenic wafer prober will be located at intel's oregon campus.
yale's latest work expanding the reach of quantum information science is actually a game of quantum pitch and catch.
in a new study published april 23 in the journal nature physics, yale researchers "pitch" a qubit- a tiny bit of quantum data- from one physical point in a microwave cavity to a separate point in a different cavity.
it is the first time an end- to- end quantum transmission has been done on demand and represents the first of two yale experiments involving "pitch- and- catch" technologies that will be published this year.
quantum computing offers the possibility of computation speeds that are orders of magnitude faster than today's supercomputers.
yale researchers are at the forefront of efforts to develop the first fully useful quantum computers, and have done pioneering work in quantum computing with superconducting circuits.
but in order for a quantum computer to run more complex algorithms, it will need more processing power, just as a classical computer does.
to do that, qubits must be interfaced with each other- which is why a "pitch and catch" capability would come in handy.
"our approach is to use a quantum network to connect many qubits together in independent modules," said christopher axline, a yale graduate student and co- lead author of the new study.
"the strategy is similar to clustering computers together on a local area network."
axline works in the yale lab of robert schoelkopf, the study's principal investigator.
the other co- lead authors of the study are yale graduate student luke burkhart and former yale postdoctoral associate wolfgang pfaff, who is now at microsoft.
previous work by the researchers enabled them to pitch a qubit, while preserving its information.
now they're able to catch the information, as well.
"you might think catching our flying qubit would be a straightforward extension of our other work, but it actually requires some careful treatment," burkhart said.
"it meant varying how quickly, and at what frequency, the information is released.
if we open the floodgates and let energy flow out as quickly as possible, it will overwhelm the catcher."
instead, the researchers carefully shape their pitch- and- catch over time, so that both ends of the transaction are in sync.
another first for the experiment is the use of the cavities- in addition to the qubit itself- as the memory for the system.
"much of the research in our lab and at the yale quantum institute focuses on how to take advantage of cavity modes for quantum information processing," axline said.
"superconducting cavities are the most secure places we can store quantum information, and even more important, cavities are flexible as to the form of the stored information."
this quantum game of pitch and catch also includes quantum entanglement, a key concept in quantum physics and a requirement in any quantum algorithm.
in this instance, it means the pitcher is pitching and not pitching, simultaneously.
"we entangle the states between the pitcher and the catcher," burkhart said.
"this remote entanglement will be crucial in quantum networks."
xanadu, a canadian quantum hardware and technology company has received a $4.4m investment from sustainable development technology canada (sdtc).
the investment will expedite the development of xanadu's photonic quantum computers and make them available over the cloud.
this project will also further the company's overall progress towards the construction of energy- efficient universal quantum computers.
"canadian cleantech entrepreneurs are tackling problems across canada and in every sector.
i have never been more positive about the future.
the quantum hardware technology that xanadu is building will develop quantum computers with the ability to solve extremely challenging computational problems, completing chemical calculations in minutes - which would otherwise require a million cpus in a data center," said leah lawrence, president and ceo, sustainable development technology canada.
despite efforts to improve the power efficiency of traditional computing methods, the rapid growth of data centres and cloud computing presents a major source of new electricity consumption.
in comparison to classical computing, quantum computing systems have the benefit of performing certain tasks and algorithms at an unprecedented rate.
this will ultimately reduce the requirements for electrical power and the accompanying air and water emissions associated with electricity production.
xanadu is developing a unique type of quantum computer, based on photonic technology, which is inherently more power- efficient than electronics.
xanadu's photonic approach uses laser light to carry information through optical chips, rather than the electrons or ions used by their competitors.
by using photonic technology, xanadu's quantum computers will one day have the ability to perform calculations at room temperature, and eliminate the bulky and power- hungry cooling systems required by most other types of quantum computers.
the project will be undertaken by xanadu's team of in- house scientists, with collaboration from the university of toronto and swiftride.
the project will be carried out over three years and will encompass the development of xanadu's architecture, hardware, software and client interfaces with the overall goal of expediting the development of the company's technology, and demonstrating the practical benefits of quantum computing for users and customers by the end of 2022.
"we are thrilled by the recognition and support that we are receiving from sdtc for the development of our technology.
we firmly believe that our unique, photonic- based approach to quantum computing will deliver both valuable insights and tangible environmental benefits for our customers and partners," said christian weedbrook, ceo of xanadu.
about xanadu
xanadu is a photonic quantum hardware company.
we build integrated photonic chips that can be used in quantum computing, communication and sensing systems.
the company's mission is to build quantum computers that are useful and available to people everywhere, visit www.xanadu.ai or follow us on twitter @xanaduai.
about sdtc
sustainable development technology canada (sdtc) is a foundation created by the government of canada to advance clean technology innovation in canada by funding and supporting small and medium- sized enterprises developing and demonstrating clean technology solutions.
follow sustainable development technology canada on twitter: @sdtc
when two researchers from the swiss federal institute of technology (eth zurich) announced in april that they had successfully simulated a 45- qubit quantum circuit, the science community took notice: it was the largest ever simulation of a quantum computer, and another step closer to simulating "quantum supremacy"- the point at which quantum computers become more powerful than ordinary computers.
the computations were performed at the national energy research scientific computing center (nersc), a doe office of science user facility at the u.s. department of energy's lawrence berkeley national laboratory.
researchers thomas haner and damien steiger, both ph.d. students at eth, used 8,192 of 9,688 intel xeon phi processors on nersc's newest supercomputer, cori, to support this simulation, the largest in a series they ran at nersc for the project.
"quantum computing" has been the subject of dedicated research for decades, and with good reason: quantum computers have the potential to break common cryptography techniques and simulate quantum systems in a fraction of the time it would take on current "classical" computers.
they do this by leveraging the quantum states of particles to store information in qubits (quantum bits), a unit of quantum information akin to a regular bit in classical computing.
better yet, qubits have a secret power: they can perform more than one calculation at a time.
one qubit can perform two calculations in a quantum superposition, two can perform four, three eight, and so forth, with a corresponding exponential increase in quantum parallelism.
yet harnessing this quantum parallelism is difficult, as observing the quantum state causes the system to collapse to just one answer.
so how close are we to realizing a true working prototype?
it is generally thought that a quantum computer deploying 49 qubits- a unit of quantum information- will be able to match the computing power of today's most powerful supercomputers.
toward this end, haner and steiger's simulations will aid in benchmarking and calibrating near- term quantum computers by carrying out quantum supremacy experiments with these early devices and comparing them to their simulation results.
in the mean time, we are seeing a surge in investments in quantum computing technology from the likes of google, ibm and other leading tech companies- even volkswagen- which could dramatically accelerate the development process.
simulation and emulation of quantum computers
both emulation and simulation are important for calibrating, validating and benchmarking emerging quantum computing hardware and architectures.
in a paper presented at sc16, haner and steiger wrote: "while large- scale quantum computers are not yet available, their performance can be inferred using quantum compilation frameworks and estimates of potential hardware specifications.
however, without testing and debugging quantum programs on small scale problems, their correctness cannot be taken for granted.
simulators and emulators ... are essential to address this need."
that paper discussed emulating quantum circuits- a common representation of quantum programs- while the 45- qubit paper focuses on simulating quantum circuits.
emulation is only possible for certain types of quantum subroutines, while the simulation of quantum circuits is a general method that also allows the inclusion of the effects of noise.
such simulations can be very challenging even on today's fastest supercomputers, haner and steiger explained.
for the 45- qubit simulation, for example, they used most of the available memory on each of the 8,192 nodes.
"this increases the probability of node failure significantly, and we could not expect to run on the full system for more than an hour without failure," they said.
"we thus had to reduce time- to- solution at all scales (node- level as well as cluster- level) to achieve this simulation."
optimizing the quantum circuit simulator was key.
haner and steiger employed automatic code generation, optimized the compute kernels and applied a scheduling algorithm to the quantum supremacy circuits, thus reducing the required node- to- node communication.
during the optimization process they worked with nersc staff and used berkeley lab's roofline model to identify potential areas where performance could be boosted.
in addition to the 45- qubit simulation, which used 0.5 petabytes of memory on cori and achieved a performance of 0.428 petaflops, they also simulated 30- , 36- and 42- qubit quantum circuits.
when they compared the results with simulations of 30- and 36- qubit circuits run on nersc's edison system, they found that the edison simulations also ran faster.
"our optimizations improved the performance - the number of floating- point operations per time - by 10x for edison and between 10x and 20x for cori (depending on the circuit to simulate and the size per node)," haner and steiger said.
"the time- to- solution decreased by over 12x when compared to the times of a similar simulation reported in a recent paper on quantum supremacy by boixo and collaborators, which made the 45- qubit simulation possible."
looking ahead, the duo is interested in performing more quantum circuit simulations at nersc to determine the performance of near- term quantum computers solving quantum chemistry problems.
they are also hoping to use solid- state drives to store larger wave functions and thus try to simulate even more qubits.
members of the department of theoretical physics and history of science of the upv/ehu's faculty of science and technology together with researchers from the university of hannover have achieved quantum entanglement between two spatially separated bose- einstein condensates, ultra- cold atomic ensembles.
the team lead by geza toth, ikerbasque research professor, concentrated on verifying the presence of entanglement by measurements, while the experiment at hannover was carried out in the group of carsten klempt.
the study is published in science.
quantum entanglement was discovered by schrodinger and later studied by einstein and other scientists in the 20th century.
it is a quantum phenomenon with no counterparts in classical physics.
the groups of entangled particles lose their individuality and behave as a single entity.
any change in one of the particles leads to an immediate response in the other, even if they are spatially separated.
"quantum entanglement is essential in applications such as quantum computing, since it enables certain tasks to be performed much faster than in classical computing," explained toth.
unlike previous methods of quantum entanglement involving incoherent and thermal clouds of particles, in this experiment, the researchers used a cloud of atoms in the bose- einstein condensate state.
toth said, "bose- einstein condensates are achieved by cooling down the atoms to very low temperatures, close to absolute zero.
at that temperature, all the atoms are in a highly coherent quantum state; in a sense, they all occupy the same position in space.
in that state, quantum entanglement exists between the atoms of the ensemble."
subsequently, the ensemble was split into two atomic clouds.
"we separated the two clouds from each other by a distance, and we were able to demonstrate that the two parts remained entangled with each other," he continued.
the demonstration that entanglement can be created between two ensembles in the bose- einstein condensate state could lead to an improvement in many fields in which quantum technology is used, such as quantum computing, quantum simulation and quantum metrology, since these require the creation and control of large ensembles of entangled particles.
"the advantage of cold atoms is that it is possible to create highly entangled states containing quantities of particles outnumbering any other physical systems by several orders of magnitude, which could provide a basis for large scale quantum computing," said the researcher.
one material that scientists have experimented with for making thermoelectric energy harvesters is quantum dots, nano- sized crystals with semiconducting properties.
due to their sharp, discrete energy levels, quantum dots are good energy filters, which is an important property for thermoelectric devices.
in a new study published in the new journal of physics, a team of researchers from switzerland, spain, and the us has investigated a thermoelectric energy harvester design based on quantum wells.
although quantum wells are also made of semiconducting materials, they have different structures and energy- filtering properties than quantum dots.
"we have shown that quantum wells can be used as powerful and efficient energy harvesters," said coauthor bjorn sothmann, a physicist at the university of geneva in switzerland.
"compared to our previous proposal based on quantum dots, quantum wells are easier to fabricate and offer the potential to be operated at room temperature."
the energy harvester design that the researchers investigated here consists of a central cavity connected via quantum wells to two electronic reservoirs.
the central cavity is kept at a hotter temperature than the two electronic reservoirs, and the quantum wells act as filters that allow electrons of certain energies to pass through.
in general, the greater the temperature difference between the central cavity and the reservoirs, the greater the electron flow and output power.
in their analysis, the researchers found that the quantum well energy harvester delivers an output power of about 0.18 w/cm2 for a temperature difference of 1 k, which is nearly double the power of a quantum dot energy harvester.
this increased power is due to the ability of quantum wells to deliver larger currents compared to quantum dots as a result of their extra degrees of freedom.
although the quantum well energy harvester has a good efficiency, the efficiency is slightly lower than that of energy harvesters based on quantum dots.
the researchers explain that this difference occurs because of the difference in energy filtering: quantum wells transmit electrons of any energy above a certain level, while quantum dots are more selective and let only electrons of a specific energy pass through.
as a result, quantum wells are less efficient energy filters.
quantum well energy harvesters appear promising for applications.
for one thing, they may be easier to fabricate than energy harvesters that use quantum dots, since quantum dots are required to have similar properties in order to achieve good performance, and there is no such requirement for quantum wells.
in addition, the fact that they can operate at room temperature may make quantum well energy harvesters suitable for a variety of applications, such as electric circuits.
"the energy harvester can be used to convert waste heat from electric circuits, e.g.
in computer chips, back into electricity," sothmann said.
"this way, one can reduce both the consumed power as well as the need for cooling the chip."
the researchers hope that their work encourages experimental groups to build and test the device.
standard chartered bank and universities space research association (usra) have signed a collaborative research agreement to partner on quantum computing research and developing quantum computing applications.
in finance, the most promising use cases with real- world applications include quantum machine learning models (generating synthetic data and data anonymisation) and discriminative models (building strong classifiers and predictors) with multiple potential uses such as credit scoring and generating trading signals.
as quantum computing technology matures, clients should benefit from higher quality services such as faster execution, better risk management and the development of new financial products.
kahina van dyke, global head of digital channels and client data analytics at standard chartered, said: "similar to other major technological advancements, quantum computing is set to bring widespread benefits as well as disrupt many existing business processes.
this is why it's important for companies to future- proof themselves by adopting this new technology from an early stage.
the partnership with usra gives us access to world- class academic researchers and provides us with a unique opportunity to explore a wide range of models and algorithms with the potential to establish quantum advantage for the real- world use cases."
bernie seery, senior vp of technology at usra noted that "this partnership with the private sector enables a diversity of research through a competitively selected portfolio of quantum computing research projects involving academic institutions and non- profits, growing an ecosystem for quantum artificial intelligence that has already involved over 150 researchers from more than 40 organizations that produced over 50 peer- reviewed publications over the last seven years."
alex manson, global head of sc ventures, standard chartered's innovation, fintech investment and ventures arm, stated: "the world is currently in the process of identifying commercial use cases where quantum computer capabilities will surpass classical computers.
we have a conviction that some of these use cases will transform the way we manage risks in financial services, for example by simulating portfolios and exponentially speeding up the generation of market data.
we will work with usra to identify such use cases in financial services, with a view to implementing them within our bank, as well as potentially offering this service to other market participants over time."
mark johnson, vice president, processor design, development and quantum products at d- wave said: "quantum computing research and development are poised to have a profound impact on the industries responsible for solving today's most complex problems.
that's why researchers and businesses alike are looking to quantum computing today to start demonstrating tangible value.
we're proud to work with usra and standard chartered bank as they improve global access to quantum systems and undertake essential research and development."
at usra's research institute for advanced computer science, dr. davide venturelli, associate director for quantum computing, notes that quantum annealing is implementing a powerful approach to computing, featuring unique advantages with respect to other traditional and novel approaches, that should be studied, theoretically and experimentally, to advance the state of art of computing technologies for the benefit of nearly all disciplines.
standard chartered's team, led by dr. alexei kondratyev, global head of data science and innovation, and usra have collaborated in quantum computing research since 2017.
an earlier success in investigating the quantum annealing approach to computational problems in portfolio optimisation use cases led to this strategic partnership, where usra will continue to support fundamental academic research in quantum physics and artificial intelligence and standard chartered will focus on future commercial applications.
in 2012, usra partnered with nasa to found the quantum artificial intelligence laboratory (quail): the space agency's hub to evaluate the near- term impact of quantum technologies.
with quail, the usra team has investigated the physics, the engineering and the performance of multiple generations of quantum annealing processors built by d- wave systems.
it has also participated in u.s. government research programs that looked into application of quantum annealing for combinatorial optimization, aviation, earth science and machine learning.
nasa ames research center is currently hosting a d- wave 2000q annealing system, thanks to the support of this partnership.
standard chartered and usra intend to develop this initial collaboration beyond quantum annealing to all unconventional computing systems that could provide an advantage to applications of interest, such as gate- model noisy- intermediate scale quantum (nisq) processors and coherent ising machines.
for more information, contact:
standard chartered: group media relations contact: shaun gamble, shaun.gamble@sc.com
tel: +44 2078855934
usra: pr contact: suraiya farukhi, sfarukhi@usra.edu
technical contact: david bell, dbell@usra.edu
about usra
founded in 1969, under the auspices of the national academy of sciences at the request of the u.s. government, the universities space research association (usra) is a nonprofit corporation chartered to advance space- related science, technology and engineering.
usra operates scientific institutes and facilities, and conducts other major research and educational programs, under federal funding.
usra engages the university community and employs in- house scientific leadership, innovative research and development, and project management expertise.
riacs is a usra department for research in fundamental and applied information sciences, leading projects on quantum computing funded by nasa, darpa, the us airforce and nsf.
more info at: https://riacs.usra.edu/quantum/ and www.usra.edu.
about standard chartered
we are a leading international banking group, with a presence in 59 of the world's most dynamic markets, and serving clients in a further 85.
our purpose is to drive commerce and prosperity through our unique diversity, and our heritage and values are expressed in our brand promise, here for good.
standard chartered plc is listed on the london and hong kong stock exchanges as well as the bombay and national stock exchanges in india.
for more stories and expert opinions please visit insights at sc.com.
follow standard chartered on twitter, linkedin and facebook.
a new test to check if a quantum computer is giving correct answers to questions beyond the scope of traditional computing could help the first quantum computer that can outperform a classical computer to be realized.
by creating a protocol that allows a quantum computer to check its own answers to difficult problems, the scientists from the university of warwick have provided a means to confirm that a quantum computer is working correctly without excessive use of resources.
samuele ferracin, theodoros kapourniotis and dr. animesh datta from the university's department of physics have recently tackled this problem in a paper for the new journal of physics, published today.
the researchers have developed a protocol to quantify the effects of noise on the outputs of quantum computers.
noise is defined as anything that affects a quantum machine's hardware but is beyond the user's control, such as fluctuations in temperature or flaws in the fabrication.
this can affect the accuracy of a quantum computer's results.
when applied, the researchers' test produces two percentages: how close it estimates the quantum computer is to the correct result and how confident a user can be of that closeness.
the test will help the builders of quantum computers to determine whether their machine is performing correctly to help refine their performance, a key step in establishing the usefulness of quantum computing in the future.
dr. animesh datta from the university of warwick department of physics said: "a quantum computer is only useful if it does two things: first, that it solves a difficult problem; the second, which i think is less appreciated, is that it solves the hard problem correctly.
if it solves it incorrectly, we had no way of finding out.
so what our paper provides is a way of deciding how close the outcome of a computation is to being correct."
determining whether a quantum computer has produced a correct answer to a difficult problem is a significant challenge as, by definition, these problems are beyond the scope of an existing classical computer.
checking that the answer it has produced is correct typically involves using a large number of classical computers to tackle the problem, something that is not feasible to do as they tackle ever more challenging problems.
instead, the researchers have proposed an alternative method that involves using the quantum computer to run a number of easy calculations that we already know the answer to and establishing the accuracy of those results.
based on this, the researchers can put a statistical boundary on how far the quantum computer can be from the correct answer in the difficult problem that we want it to answer, known as the target computation.
it is a similar process to that which computer programmers use to check large computer programs, by putting in small functions with known answers.
if the program answers enough of these correctly then they can be confident that the whole program is correct.
dr. datta adds: "the whole point of having a quantum computer is to not spend an exponential amount of time solving problems, so taking an exponential amount of time to check whether it's correct or not defeats the point of it.
so our method is efficient in that it doesn't require an exponential amount of resources.
"we do not need a classical computer to check our quantum computer.
our method is self- contained within a quantum system that can be used independently of large servers."
lead author samuele ferracin has been developing ways for scientists working on quantum computers to incorporate the test into their work.
he said: "we have spent the last few years thinking about new methods to check the answers of quantum computers and proposing them to experimentalists.
the first methods turned out to be too demanding for the existing quantum computers, which can only implement 'small' computations and perform restricted tasks.
with our latest work we have successfully developed a method that suits existing quantum computers and encompasses all their main limitations.
we are now collaborating with experimentalists to understand how it performs on a real machine."
quantum computing harnesses the unusual properties of quantum physics to process information in a wholly different way to conventional computers.
taking advantage of the behavior of quantum systems, such as existing in multiple different states at the same time, this radical form of computing is designed to process data in all of those states simultaneously, lending it a huge advantage over classical computing.
certain kinds of problems, like those found in codebreaking and in chemistry, are particularly suited to exploiting this property.
the last few years have seen unprecedented experimental advances.
the largest quantum computers are doubling in size every six months and seem now very close to achieve quantum supremacy.
quantum supremacy refers to a milestone in the development of quantum computers, where a quantum computer first performs a function that would require an unreasonably large amount of time using a classical computer.
dr. datta adds: "what we are interested in is designing or identifying ways of using these quantum machines to solve hard problems in physics and chemistry, to design new chemicals and materials, or identify materials with interesting or exotic properties.
and that is why we are particularly interested in the correctness of the computation."
quantum machines, creator of the first complete hardware and software solution for the control and operation of quantum computers, announced today that it has secured $17.5m in funding to accelerate the already rapid adoption of the company's quantum orchestration platform, which is driving the development of tomorrow's quantum breakthroughs.
the series a round was led by avigdor willenz and harel with the participation of previous backers tlv partners and battery ventures.
in the race to bring general- purpose quantum computers to fruition, quantum machines (qm) announced a major breakthrough earlier this year with the launch of its quantum orchestration platform and its adoption by major players.
its complete set of features works with all quantum technologies, giving researchers and development teams everything they need to run the most complex quantum algorithms and experiments.
looking to the future, quantum orchestration lays the ground for tackling some of the most challenging hurdles facing quantum computing, such as complex multi- qubit calibrations, quantum- error- correction, and scaling up to many hundreds of qubits.
israeli entrepreneur avigdor willenz, who recently sold habana labs to intel for $2 billion, is backing qm after the massive enthusiasm he's witnessed from across the quantum computing industry.
"the race to commercial quantum computers is one of the most exciting technological challenges of our generation," said willenz, who has also sold companies to intel, marvell, amazon and cisco.
"our goal at qm is to make this happen faster than anticipated, and establish ourselves as an essential player in this emerging industry."
qm was founded in 2018 by drs.
itamar sivan, yonatan cohen and nissim ofek, three physics ph.ds.
they met at israel's weizmann institute after each had studied at the world's top universities, including yale university, university of washington, oxford university and the ecole normale superieure.
the qm team has since grown to nearly 30 - more than half of them physicists.
qm's orchestration platform has already been adopted by multinational corporations and startups at the forefront of quantum computing, with many new paying customers joining us every month.
"we have been fortunate to assemble a team of all- star researchers and scientists working on the greatest challenges of quantum computing," said dr. itamar sivan, co- founder and ceo of qm.
"their prowess is evident in the lead that qm has taken as the only company to develop quantum orchestration.
quantum technology will decisively shape our future and this new investment will ensure that qm remains at the center of these exciting advancements."
about quantum machines
qm's full- stack quantum orchestration platform enables an entirely new approach to controlling and operating quantum processors.
capable of running even the most complex algorithms - from near- term applications of quantum computers to challenges of quantum- error- correction - the quantum orchestration platform allows users to realize the potential of all quantum processors right out of the box via its powerful, yet intuitive, programming language qua.
"right now, classical computers are faster than quantum computers," rene stock tells physorg.com.
"the goal of quantum computing is to eventually speed up the time scale of solving certain important problems, such as factoring and data search, so that quantum computing can not only compete with, but far outperform, classical computing on large scale problems.
one of the most promising ways to possibly do this is with ion traps."
stock, a post- doc at the university of toronto, points out that ion trap quantum computing has made a lot of progress in the last 10 years.
"ions in traps have been one of most successful physical implementation of quantum computing in physical systems."
stock believes that it is possible to use ion- trap quantum computing to create measurement- based quantum computers that could compete with classical computers for very large and complex problems - and even on smaller scale problems.
his work on the subject, done with daniel james, appears in physical review letters: "scalable, high- speed measurement- based quantum computer using trapped ions."
"one of the most important considerations in quantum computing is the fact that quantum computing scales polynomially, rather than exponentially, as classical computing does."
this polynomial scaling is what makes quantum computing so useful for breaking data encryption.
in order to make data encryption more secure, one usually increases the number of bits used.
"because of the exponential scaling, breaking data encryptions quickly becomes impossible using standard classical computers or even networks of computers," stock explains.
"the improved scaling with quantum computers could be one a biggest threads to data encryption and security."
while this sounds promising, stock points this out that there are still problems with quantum information processing: "while scaling would be better with quantum computing, current operation of quantum information processing is too slow to even compete with classical computers on large factoring problems that take 5 months to solve."
the way ion- trap quantum computing works now - or at least is envisioned to work - requires that ions be shuttled back and forth around the trap architecture.
stock explains that this takes time.
"as the complexity of problems and the size of the quantum computing to be implemented increases, the time issue becomes even more important.
we wanted to figure out how we could change the time scale," stock explains.
"we found that we could speed up the processing by using an array of trapped ions and by parallelizing entangling operations."
"instead of moving ions around," stock continues, "you apply a two- ion operation between all neighboring ions at the same time.
the created multipartite 'entangled' array of ions is a resource for quantum computing."
actual computing is then based on measurement of ions in the array in a prescribed order and using a slightly different measurement basis for each ion.
"in this scheme, it is the time required to read out information from the ions that critically determines the operational time scale of the quantum computer," stock says.
stock describes the measurement component as vital to this model of quantum computing.
instead of exciting the ions and getting them to emit a photon and measuring the photon, stock and his colleague instead devised a different way in which they were able to measure the quantum bit encoded in a calcium ion.
"you can use an ionization process to speed up measurement, since the electron can be extracted faster from the atom than you can get a photon out of an atom.
the extracted electron is then guided onto a detector by the ion trap itself."
all of this takes place on a nanosecond time scale.
"by speeding up the measurement," stock insists, "we can speed up the operation capability of the quantum computer."
stock points out that this quantum computing scheme would be impractical as far as taking over common use from classical computers.
"the lattice would have thousands of ions, which would need to be controlled, and carefully stored and protected.
it means that the computer would be relatively large and impractical."
uses for such a quantum computer are not limited to breaking data encryption.
"this process would allow us to take problems of great complexity and still solve them on a humanly possible timescale.
this could provide the key to modeling complex systems - especially perhaps in biology - that we can't solve now.
this would be a tremendous advantage over classical computing."
more information: stock, rene and james, daniel.
"scalable, high- speed measurement- based quantum computer using trapped ions."
physical review letters (2009).
available online: http://link.aps.org/doi/10.110 ... ysrevlett.102.170501 .
on august 11th, the national security agency (nsa) announced their preliminary plans for transitioning to quantum resistant algorithms and away from the suite b cryptographic algorithms specified by the national institute of standards and technology (nist).
they even said that vendors that haven't yet switched to suite b algorithms should not do so at this point and use their resources "to prepare for the upcoming quantum resistant algorithm transition."
they then went on to say "unfortunately, the growth of elliptic curve use has bumped up against the fact of continued progress in the research on quantum computing, necessitating a re- evaluation of our cryptographic strategy."
i laud the nsa for this public recognition of the quantum computing threat.
quantum computing is a real threat to the public key infrastructure (pki) that is in place today and needs to be addressed.
for those of you not familiar with the threat, quantum computers are able to execute shor's algorithm, which has been proven to be able to break rsa and ecc, the two most popular crypto algorithms used in pki and digital signatures.
quantum computers of sufficient strength to run shor's algorithm do not yet exist, but there has been significant progress in academic and corporate research (and probably government as well), including:
researchers move quantum computing to silicon
google launches quantum artificial intelligence lab
snowden documents state the nsa is running a $79.7 million research program with the aim of developing a quantum computer capable of breaking encryption
ibm scientists achieve critical steps to building first practical quantum computer
the impact of breaking rsa and ecc is significant, as they are the backbone of most of the world's secure network communications, including https websites, software updates, ecommerce, etc.
with broken rsa and ecc, we will no longer be sure that our information isn't be intercepted, changed or that anyone we exchange information with is who they say they are.
but as the nsa mentions, there are a few cryptographic algorithms that are resistant to all known quantum computing attacks, including ntru, which is owned by my employer, security innovation.
in a report titled quantum resistant public key cryptography: a survey, nist wrote, "of the various lattice based cryptographic schemes that have been developed, the ntru family of cryptographic algorithms appears to be the most practical...smallest key size...highest performance."
replacing rsa and ecc with a quantum safe algorithm is not an easy task and will take time.
this is likely the reason that the nsa is pushing for this transition now, before powerful quantum computers exist.
there is some reluctance by those in charge of a company's crypto infrastructure to change, as it is perceived as a costly and time- consuming effort - it's better to leave the problem for their successor.
but we have recently submitted an internet draft to the internet engineering task force (ietf) for a quantum safe hybrid (qsh) that allows users to protect their network traffic with any conventional algorithm (like rsa) and a quantum safe algorithm (like ntru).
this allows for an easier transition to post- quantum security while still maintaining current crypto schemes until they gain trust in the new post- quantum crypto solutions.
it could also protect internet traffic from being collected and stored now and being harvested once quantum computers are available.
and in most cases, the post quantum crypto is so fast relative to rsa that the performance penalty is negligible.
the nsa is right to get in front of the issue and boost the public's awareness of the quantum computing threat.
an unprepared company could be devastated once quantum computers arrive.
i hope that the public heeds this warning and pushes for companies and governments to take appropriate counter- measures.
gene carter has been the director of product management for the embedded security business unit at security innovation for the past two years.
carter has spent the past 20 years in embedded and automotive product management roles for nxp semiconductors, philips semiconductors, and coto technology.
he holds an mba from the university of southern california's marshall school of business and a bsc in electrical engineering from tufts university.
a team of researchers led by the department of energy's oak ridge national laboratory has demonstrated a new method for splitting light beams into their frequency modes.
the scientists can then choose the frequencies they want to work with and encode photons with quantum information.
their work could spur advancements in quantum information processing and distributed quantum computing.
the team's findings were published in physical review letters.
the frequency of light determines its color.
when the frequencies are separated, as in a rainbow, each color photon can be encoded with quantum information, delivered in units known as qubits.
qubits are analogous to but different from classical bits, which have a value of either 0 or 1, because qubits are encoded with values of both 0 and 1 at the same time.
the researchers liken quantum information processing to stepping into a hallway and being able to go both ways, whereas in classical computing just one path is possible.
the team's novel approach- featuring the first demonstration of a frequency tritter, an instrument that splits light into three frequencies- returned experimental results that matched their predictions and showed that many quantum information processing operations can be run simultaneously without increasing error.
the quantum system performed as expected under increasingly complex conditions without degrading the encoded information.
"under our experimental conditions, we got a factor 10 better than typical error rates," said nicholas peters, quantum communications team lead for ornl's quantum information science group.
"this establishes our method as a frontrunner for high- dimensional frequency- based quantum information processing."
photons can carry quantum information in superpositions- where photons simultaneously have multiple bit values- and the presence of two quantum systems in superposition can lead to entanglement, a key resource in quantum computing.
entanglement boosts the number of calculations a quantum computer could run, and the team's focus on creating more complex frequency states aims to make quantum simulations more powerful and efficient.
the researchers' method is also notable because it demonstrates the hadamard gate, one of the elemental circuits required for universal quantum computing.
"we were able to demonstrate extremely high- fidelity results right off the bat, which is very impressive for the optics approach," said pavel lougovski, the project's principal investigator.
"we are carving out a subfield here at ornl with our frequency- based encoding work."
the method leverages widely available telecommunications technology with off- the- shelf components while yielding high- fidelity results.
efforts to develop quantum repeaters, which extend the distance quantum information can be transmitted between physically separated computers, will benefit from this work.
"the fact that our method is telecom network- compatible is a big advantage," lougovski said.
"we could perform quantum operations on telecom networks if needed."
peters added that their project demonstrates that unused fiber- optic bandwidth could be harnessed to reduce computational time by running operations in parallel.
"our work uses frequency's main advantage- stability- to get very high fidelity and then do controlled frequency jumping when we want it," said wigner fellow joseph lukens, who led the ornl experiment.
the researchers have experimentally shown that quantum systems can be transformed to yield desired outputs.
the researchers suggest their method could be paired with existing beam- splitting technology, taking advantage of the strengths of both and bringing the scientific community closer to full use of frequency- based photonic quantum information processing.
peters, lougovski and lukens, all physicists with ornl's quantum information science group, collaborated with graduate student hsuan- hao lu, professor andrew weiner, and colleagues at purdue university.
the team published the theory for their experiments in optica in january 2017.
researchers at the university of oxford have achieved a quantum logic gate with record- breaking 99.9% precision, reaching the benchmark required theoretically to build a quantum computer.
quantum computers, which function according to the laws of quantum physics, have the potential to dwarf the processing power of today's computers, able to process huge amounts of information all at once.
the team achieved the logic gate, which places two atoms in a state of quantum entanglement and is the fundamental building block of quantum computing, with a precision (or fidelity) substantially greater than the previous world record.
quantum entanglement- a phenomenon described by einstein as 'spooky' but which is at the heart of quantum technologies- occurs when two particles stay connected, such that an action on one affects the other, even when they are separated by great distances.
the research, carried out by scientists from the engineering and physical sciences research council (epsrc)- funded networked quantum information technologies hub (nqit), which is led by oxford university, is reported in the journal physical review letters.
dr chris ballance, a research fellow at magdalen college, oxford and lead author of the paper, said: 'the development of a "quantum computer" is one of the outstanding technological challenges of the 21st century.
a quantum computer is a machine that processes information according to the rules of quantum physics, which govern the behaviour of microscopic particles at the scale of atoms and smaller.
'an important point is that it is not merely a different technology for computing in the same way our everyday computers work; it is at a very fundamental level a different way of processing information.
it turns out that this quantum- mechanical way of manipulating information gives quantum computers the ability to solve certain problems far more efficiently than any conceivable conventional computer.
one such problem is related to breaking secure codes, while another is searching large data sets.
quantum computers are naturally well- suited to simulating other quantum systems, which may help, for example, our understanding of complex molecules relevant to chemistry and biology.'
quantum technology is a complex area, but one analogy that has been used to explain the concept of quantum computing is that it is like being able to read all of the books in a library at the same time, whereas conventional computing is like having to read them one after another.
this may be over- simplistic, but it is useful in conveying the way in which quantum computing has the potential to revolutionise the field.
professor david lucas, of oxford university's department of physics and balliol college, oxford, a co- author of the paper, said: 'the concept of "quantum entanglement" is fundamental to quantum computing and describes a situation where two quantum objects- in our case, two individual atoms- share a joint quantum state.
that means, for example, that measuring a property of one of the atoms tells you something about the other.
'a quantum logic gate is an operation which can take two independent atoms and put them into this special entangled state.
the precision of the gate is a measure of how well this works: in our case, 99.9% precision means that, on average, 999 times out of 1,000 we will have generated the entangled state correctly, and one time out of 1,000 something went wrong.
'to put this in context, quantum theory says that - as far as anyone has found so far - you simply can't build a quantum computer at all if the precision drops below about 99%.
at the 99.9% level you can build a quantum computer in theory, but in practice it could very difficult and thus enormously expensive.
if, in the future, a precision of 99.99% can be attained, the prospects look a lot more favourable.'
professor lucas added: 'achieving a logic gate with 99.9% precision is another important milestone on the road to developing a quantum computer.
a quantum logic gate on its own does not constitute a quantum computer, but you can't build the computer without them.
'an analogy from conventional computing hardware would be that we have finally worked out how to build a transistor with good enough performance to make logic circuits, but the technology for wiring thousands of those transistors together to build an electronic computer is still in its infancy.'
scientists at the california institute of technology have laid the groundwork for a crucial step in quantum information science.
they show how entanglement, an essential property of quantum mechanics, can be generated between beams of light, stored in a quantum memory, and mapped back into light with the push of a button.
in the march 6 issue of the journal nature, caltech valentine professor of physics h. jeff kimble and his colleagues demonstrate for the first time an important capability required for the control of quantum information and quantum networks, namely the coherent conversion of photonic entanglement into and out of separated quantum memories.
entanglement lies at the heart of quantum physics, and is a state where parts of a composite system are more strongly correlated than is possible for any classical counterparts regardless of the distance separating them.
entanglement is a critical resource for diverse applications in quantum information science, such as for quantum metrology, computation, and communication.
quantum networks rely on entanglement for the teleportation of quantum states from place to place.
in a quest to turn these abstract ideas into real laboratory systems and to distribute entanglement to remote locations (even on a continental scale), kimble explains that quantum physicists have studied ways to propagate photonic information into and out of quantum memory using a system called a quantum repeater, invented in 1998 by h. briegel, j.i.
cirac, and p. zoller at the university of innsbruck.
until now, work in kimble's group on the realization of a quantum repeater with atomic ensembles relied upon the probabilistic creation of entanglement.
in this setting entanglement between two clouds of atoms was generated probabilistically but with an unambiguous heralding event.
while such systems hold the potential for scalable quantum networks, it has been difficult for kimble's quantum optics group to apply such schemes to certain protocols necessary for quantum networks, such as entanglement connection.
now, with the new protocol and future improvements, "we can push a button and generate entanglement," says physics graduate student kyung soo choi, one of four authors of the caltech experiment.
while entanglement has been traditionally carried out with photons in attempt to connect two distant systems, these particles of light are difficult to store because of their small interactions with matter when taken one by one.
a quantum memory for light is an essential ingredient for achieving scalable quantum networks with photons.
choi says.
"the question is now, 'how do you change the entangled state of light into an entanglement of matter and back into light?'"
this was not possible for any physical system until now.
the new work, choi says, "is a proof- of- principle demonstration that entanglement between material systems can be generated deterministically by mapping the entanglement of light to and from two spatially separated quantum memories."
the caltech team separated the processes for generating and storing the entanglement, thereby breaking a previous inherent link between the quality and probability of state preparation.
"in a general context, our work represents an important step in laboratory capabilities for the creation and manipulation of entangled states of light and matter.
we hope that our results will be useful as a tool in the effort to realize quantum repeaters and thereby scalable quantum networks over long distances," remarks kimble.
in the caltech experiment, a single photon is first split, generating an entangled state of light with quantum amplitudes for the photon to propagate two distinct paths, taking both at once.
the caltech team in turn transcribed, or mapped, the entanglement onto distinct atomic ensembles separated by one millimeter.
to create the interface between the light and matter, the team employed laser- cooled cesium atoms whose atomic states interact with a control laser to create destructive quantum interference, making the atomic ensembles either invisible or highly opaque to the input light.
called electromagnetically induced transparency and pioneered by s. harris at stanford university, the mechanism manipulates the speed of the light for the incoming entangled photon and that kicks off the entire procedure.
"we can reduce the speed of light to the speed of a train, and then in fact stop the light inside the matter by slowly turning off the control laser, where now the quantum information- the entangled state of light- is stored inside the atomic ensembles," choi describes.
"by turning on the control laser again, we can reversibly accelerate the 'stopped' light back to the speed of light and restore the quantum entanglement as propagating beams of light."
in this experiment, the photonic entanglement was mapped into the atomic ensembles in a time ~ 20 nanoseconds and then stored in the atomic ensembles for one microsecond, with storage times extendable up to 10 microseconds.
the photonic entanglements of the input and output of the quantum interface were explicitly quantified with a conversion efficiency of 20 percent.
however, the researchers emphasize, real- world realization of a quantum network remains far out of reach even with these parameters and the state- of- the- art of quantum controls.
choi comments, "further improvements in quantum control and storage capabilities in matter- light interfaces will lead to fruitful and exciting discoveries in quantum information science, including for the realization of quantum networks."
engineers from unsw sydney have successfully measured the accuracy of two- qubit logic operations in silicon for the very first time.
scientists believe the results could one day allow them to scale up to a quantum processor.
this has the potential to be a big step forward in the field of quantum computing.
"all quantum computations can be made up of one- qubit operations and two- qubit operations - they're the central building blocks of quantum computing.
once you've got those, you can perform any computation you want - but the accuracy of both operations needs to be very high."
back in 2015, the team built the first quantum logic gate in silicon.
this made it possible to make a calculation between two qubits of information.
significantly, this was a big step forward in turning the dream of silicon quantum computers into a reality.
since then, several groups have successfully demonstrated two- qubit gates in silicon.
however, none of the teams were able to determine the accuracy of the two- qubit gates in any consistent or accurate way - until now.
to achieve this, scientists used a technique to assess qubit accuracy across a variety of technology platforms.
the scientists successfully demonstrated an average two- qubit gate of fidelity of 98%.
they managed to achieve this accuracy thanks to identifying and mitigating primary error sources.
this allowed scientists to improve gate fidelities significantly.
scientists were able to perform more than 50 gate operations on the two- qubit device, even after randomizing the benchmarking sequences.
however, this is just the beginning.
millions of qubits will be needed to achieve even the basic goals set for quantum computers.
plus, even the smallest of quantum errors could lead to big problems - which makes accuracy in these small devices more important than ever.
if the qubits themselves are proven to be extremely accurate, it will make the scientist's future endeavors more likely, and pave the way for greater innovations regarding quantum computing.
scientists have high hopes for their work, and for the future of quantum computing.
they intend to continue with their research in this field until they can finally take the theory of quantum computing and turn it into a reality.
just over a year ago, the uk government announced an investment of ps270m over five years to help get quantum technology out of laboratories and into the marketplace.
oxford was chosen to lead one of four epsrc- funded 'hubs' looking at different aspects of quantum technology - in oxford's case, shaping the future of quantum networking and computing, towards the ultimate goal of developing a functioning quantum computer.
since then, the networked quantum information technologies (nqit - pronounced 'n- kit') hub, based at oxford but involving nearly 30 academic and industrial partners, has been focusing on developing quantum technologies that could dwarf the processing power of today's supercomputers.
a new paper by oxford researchers, published in the journal nature, demonstrates how the work of the hub is progressing.
professor david lucas of oxford's department of physics, co- leader, with professor andrew steane, of the ion trap quantum computing group, explains: 'the development of a "quantum computer" is one of the outstanding technological challenges of the 21st century.
a quantum computer is a machine that processes information according to the rules of quantum physics, which govern the behaviour of microscopic particles at the scale of atoms and smaller.
'an important point is that it is not merely a different technology for computing in the same way our everyday computers work; it is at a very fundamental level a different way of processing information.
it turns out that this quantum- mechanical way of manipulating information gives quantum computers the ability to solve certain problems far more efficiently than any conceivable conventional computer.
one such problem is related to breaking secure codes, while another is searching large data sets.
quantum computers are naturally well- suited to simulating other quantum systems, which may help, for example, our understanding of complex molecules relevant to chemistry and biology.'
one of the leading technologies for building a quantum computer is trapped atomic ions, and a principal goal of the nqit project is to develop the constituent elements of a quantum computer based on these ions.
professor lucas says: 'each trapped ion (a single atom, with one electron removed) is used to represent one "quantum bit" of information.
the quantum states of the ions are controlled with laser pulses of precise frequency and duration.
two different species of ion are needed in the computer: one to store information (a "memory qubit") and one to link different parts of the computer together via photons (an "interface qubit").'
the nature paper, whose lead author is magdalen college junior research fellow chris ballance, demonstrates the all- important quantum 'logic gate' between two different species of ion - in this case two isotopes of calcium, the abundant isotope calcium- 40 and the rare isotope calcium- 43.
professor lucas says: 'the oxford team has previously shown that calcium- 43 makes the best single- qubit memory ever demonstrated, across all physical systems, while the calcium- 40 ion has a simpler structure which is well- suited for use as an "interface qubit".
the logic gate, which was first demonstrated for same- species ions at nist boulder (usa) in 2003, allows quantum information to be transferred from one qubit to another; in the present work, the qubits reside in the two different isotopes, stored in the same ion trap.
the oxford work was the first to demonstrate that this type of logic gate is possible with the demanding precision necessary to build a quantum computer.
'in a nice piece of "spin- off science" from this technological achievement, we were able to perform a "bell test", by first using the high- precision logic gate to generate an entangled state of the two different- species ions, then manipulating and measuring them independently.
this is a test which probes the non- local nature of quantum mechanics; that is, the fact that an entangled state of two separated particles has properties that cannot be mimicked by a classical system.
this was the first time such a test had been performed on two different species of atom separated by many times the atomic size.'
while professor lucas cautions that the so- called 'locality loophole' is still present in this experiment, there is no doubt the work is an important contribution to the growing body of research exploring the physics of entanglement.
he says: 'the significance of the work for trapped- ion quantum computing is that we show that quantum logic gates between different isotopic species are possible, can be driven by a relatively simple laser system, and can work with precision beyond the so- called "fault- tolerant threshold" precision of approximately 99% - the precision necessary to implement the techniques of quantum error correction, without which a quantum computer of useful size cannot be built.'
in the long term, it is likely that different atomic elements will be required, rather than different isotopes.
in closely related work published in the same issue of nature, by ting rei tan et al, the nist ion storage group has demonstrated a different type of quantum logic gate using ions of two different elements (beryllium and magnesium).
things are getting real for researchers in the uc santa barbara john martinis/google group.
they are making good on their intentions to declare supremacy in a tight global race to build the first quantum machine to outperform the world's best classical supercomputers.
but what is quantum supremacy in a field where horizons are being widened on a regular basis, in which teams of the brightest quantum computing minds in the world routinely up the ante on the number and type of quantum bits ("qubits") they can build, each with their own range of qualities?
"let's define that, because it's kind of vague," said google researcher charles neill.
simply put, he continued, "we would like to perform an algorithm or computation that couldn't be done otherwise.
that's what we actually mean."
neill is lead author of the group's new paper, "a blueprint for demonstrating quantum supremacy with superconducting qubits," now published in the journal science.
fortunately, nature offers up many such complex situations, in which the variables are so numerous and interdependent that classical computers can't hold all the values and perform the operations.
think chemical reactions, fluid interactions, even quantum phase changes in solids and a host of other problems that have daunted researchers in the past.
something on the order of at least 49 qubits- roughly equivalent to a petabyte (one million gigabytes) of classical random access memory- could put a quantum computer on equal footing with the world's supercomputers.
just recently, neill's google/martinis colleagues announced an effort toward quantum supremacy with a 72- qubit chip possessing a "bristlecone" architecture that has yet to be put through its paces.
but according to neill, it's more than the number of qubits on hand.
"you have to generate some sort of evolution in the system which leads you to use every state that has a name associated with it," he said.
the power of quantum computing lies in, among other things, the superpositioning of states.
in classical computers, each bit can exist in one of two states- zero or one, off or on, true or false- but qubits can exist in a third state that is a superposition of both zero and one, raising exponentially the number of possible states a quantum system can explore.
additionally, say the researchers, fidelity is important, because massive processing power is not worth much if it's not accurate.
decoherence is a major challenge for anyone building a quantum computer- perturb the system, the information changes.
wait a few hundredths of a second too long, the information changes again.
"people might build 50 qubit systems, but you have to ask how well it computed what you wanted it to compute," neill said.
"that's a critical question.
it's the hardest part of the field."
experiments with their superconducting qubits have demonstrated an error rate of one percent per qubit with three- and nine- qubit systems, which, they say, can be reduced as they scale up, via improvements in hardware, calibration, materials, architecture and machine learning.
building a qubit system complete with error correction components- the researchers estimate a range of 100,000 to a million qubits- is doable and part of the plan.
and still years away.
but that doesn't mean their system isn't already capable of doing some heavy lifting.
just recently it was deployed, with spectroscopy, on the issue of many- body localization in a quantum phase change- a quantum computer solving a quantum statistical mechanics problem.
in that experiment, the nine- qubit system became a quantum simulator, using photons bouncing around in their array to map the evolution of electrons in a system of increasing, yet highly controlled, disorder.
"a good reason why our fidelity was so high is because we're able to reach complex states in very little time," neill explained.
the more quickly a system can explore all possible states, the better the prediction of how a system will evolve, he said.
if all goes smoothly, the world should be seeing a practicable ucsb/google quantum computer soon.
the researchers are eager to put it through its paces, gaining answers to questions that were once accessible only through theory, extrapolation and highly educated guessing- and opening up a whole new level of experiments and research.
"it's definitely very exciting," said google researcher pedram roushan, who led the many- body quantum simulation work published in science in 2017.
they expect their early work to stay close to home, such as research in condensed matter physics and quantum statistical mechanics, but they plan to branch out to other areas, including chemistry and materials, as the technology becomes more refined and accessible.
"for instance, knowing whether or not a molecule would form a bond or react in some other way with another molecule for some new technology... there are some important problems that you can't roughly estimate; they really depend on details and very strong computational power," roushan said, hinting that a few years down the line they may be able to provide wider access to this computing power.
"so you can get an account, log in and explore the quantum world."
nasa's jet propulsion laboratory, pasadena, california
work was carried out in determination of the mathematical origin of randomness in quantum mechanics and creating a hidden statistics of schrodinger equation; i.e., to expose the transitional stochastic process as a "bridge" to the quantum world.
the governing equations of hidden statistics would preserve such properties of quantum physics as superposition, entanglement, and direct- product decomposability while allowing one to measure its state variables using classical methods.
in other words, such a system would reinforce the advantages and minimize the limitations of both quantum and classical aspects, and therefore, it will be useful for implementation of quantum computing.
recent advances in quantum information theory have inspired an explosion of interest in new quantum algorithms for solving hard computational problems.
three basic "non- classical" properties of quantum mechanics - superposition, entanglement, and direct tensor- product decomposability - were main reasons for optimism about capabilities of quantum computers and quantum communications as well as for a new approach to cryptography.
however, one major problem is keeping the components of a quantum computer in a coherent state, as the slightest interaction with the external world would cause the system to decohere.
another problem is measurement: by the laws of quantum mechanics, a measurement yields a random and incomplete answer, and it destroys the stored state.
this proposed reinterpretation of quantum formalism opens up new advantages of quantum computers: if the madelung equations are implemented on a classical scale (using, for instance, electrical circuits or optical devices), all the quantum effects important for computations would be preserved; at the same time, the problems associated with decoherence and measurement would be removed.
this work was done by michail zak of caltech for nasa's jet propulsion laboratory.
for more information, contact iaoffice@jpl.nasa.gov.
npo- 46731
a team of cambridge researchers have found a way to control the sea of nuclei in semiconductor quantum dots so they can operate as a quantum memory device.
quantum dots are crystals made up of thousands of atoms, and each of these atoms interacts magnetically with the trapped electron.
if left alone to its own devices, this interaction of the electron with the nuclear spins, limits the usefulness of the electron as a quantum bit -  a qubit.
led by professor mete atature, a fellow at st john's college, university of cambridge, the research group, located at the cavendish laboratory, exploit the laws of quantum physics and optics to investigate computing, sensing or communication applications.
atature said: "quantum dots offer an ideal interface, as mediated by light, to a system where the dynamics of individual interacting spins could be controlled and exploited.
because the nuclei randomly 'steal' information from the electron they have traditionally been an annoyance, but we have shown we can harness them as a resource."
the cambridge team found a way to exploit the interaction between the electron and the thousands of nuclei using lasers to 'cool' the nuclei to less than 1 millikelvin, or a thousandth of a degree above the absolute zero temperature.
they then showed they can control and manipulate the thousands of nuclei as if they form a single body in unison, like a second qubit.
this proves the nuclei in the quantum dot can exchange information with the electron qubit and can be used to store quantum information as a memory device.
the findings have been published in science today.
quantum computing aims to harness fundamental concepts of quantum physics, such as entanglement and superposition principle, to outperform current approaches to computing and could revolutionise technology, business and research.
just like classical computers, quantum computers need a processor, memory, and a bus to transport the information backwards and forwards.
the processor is a qubit which can be an electron trapped in a quantum dot, the bus is a single photon that these quantum dots generate and are ideal for exchanging information.
but the missing link for quantum dots is quantum memory.
atature said: "instead of talking to individual nuclear spins, we worked on accessing collective spin waves by lasers.
this is like a stadium where you don't need to worry about who raises their hands in the mexican wave going round, as long as there is one collective wave because they all dance in unison.
"we then went on to show that these spin waves have quantum coherence.
this was the missing piece of the jigsaw and we now have everything needed to build a dedicated quantum memory for every qubit."
in quantum technologies, the photon, the qubit and the memory need to interact with each other in a controlled way.
this is mostly realised by interfacing different physical systems to form a single hybrid unit which can be inefficient.
the researchers have been able to show that in quantum dots, the memory element is automatically there with every single qubit.
dr dorian gangloff, one of the first authors of the paper and a fellow at st john's, said the discovery will renew interest in these types of semiconductor quantum dots.
dr gangloff explained: "this is a holy grail breakthrough for quantum dot research -  both for quantum memory and fundamental research; we now have the tools to study dynamics of complex systems in the spirit of quantum simulation."
the long term opportunities of this work could be seen in the field of quantum computing.
last month, ibm launched the world's first commercial quantum computer, and the chief executive of microsoft has said quantum computing has the potential to 'radically reshape the world'.
gangloff said: "the impact of the qubit could be half a century away but the power of disruptive technology is that it is hard to conceive of the problems we might open up -  you can try to think of it as known unknowns but at some point you get into new territory.
we don't yet know the kind of problems it will help to solve which is very exciting."
ibm's new 53- qubit quantum computer will be available in the newly opened quantum computation center in armonk, ny on wednesday, as part of an initiative to make quantum computers more accessible for enterprises, startups, and academic researchers.
likely in a bid to counteract impressions that quantum computers are theoretical or restricted to academia, the company has been increasingly outspoken about ibm q, in part with the publication of a free quantum textbook earlier this month.
the 53- qubit system, which will be available by the end of october, is purported to be "the single largest universal approximate quantum computing system made available for external access in the industry, to date," per ibm's press release.
this is a contrast to designs from other systems as google's 72- qubit bristlecone design, which is not available for public access, as well as d- wave's 2000- qubit quantum annealer, which is designed around solving quadratic unconstrained binary optimization (qubo) problems.
ibm q, however, is capable of general- purpose calculation, including integer factorization, which is a requirement to break rsa encryption.
(quantum computers from google, rigetti, and others, likewise, are capable of integer factorization.)
while increasing the number of qubits extends the computational ability of a quantum computer, the 53- qubit design incorporates newer design methods including compact custom circuitry used to reduce error rates and cross- talk between qubits, as well as increasing design scalability.
the currently available noisy intermediate- scale quantum (nisq) computers, including ibm q, are limited by short coherence times- the amount of useful operational time in a calculation before information is lost- and circuit depth, which measures the number of sequential operations that can be performed.
ibm's "zero- noise extrapolation" method, published in march, serves to extend the computational capabilities of quantum computers.
ibm's quantum computation center was created "to significantly broaden ibm's existing quantum computing program, which provides cloud- accessible hardware and open source software for research and the exploration of commercial use cases," according to a press release, adding that "ibm's quantum computing systems are optimized for the reliability and reproducibility of multi- qubit operations.
due to these factors, ibm's systems enable state- of- the- art quantum computational research for science with a 95% availability."
ibm notes that the 20- qubit systems available in the quantum computation center have a quantum volume of 16, with first- generation 20- qubit systems having a quantum volume of eight.
ibm is still performing benchmarks of the 53- qubit system.
the concept of quantum volume was proposed by ibm as a metric that "enables the comparison of hardware with widely different performance characteristics and quantifies the complexity of algorithms that can be run," and has been advanced by nist and gartner as a benchmark of quantum computers.
for more, check out "why post- quantum encryption will be critical to protect current classical computers" and "ibm opens q network hub in tokyo to help businesses explore quantum computing" on techrepublic.
also see
classical digital computers use transistors to process information in various sequences of zeros and ones, but have a limited ability to carry out calculations.
quantum computers use the laws of quantum mechanics.
because particles such as electrons and photons can be in multiple states- one, zero, or both at the same time- they offer many more calculation possibilities than traditional machines with only two options- on or off.
this could allow very complex calculations in areas such as genome modeling, drug research and weather forecasting.
quantum computers can operate 100 million times faster than traditional computers.
a 500 quantum bit (or qubit) computer could perform more calculations in a single step than the number of atoms in the universe.
and it's in the cloud that quantum computers will typically operate to make massive calculations.
cue quantum internet.
a quantum platform in the cloud
dr. matthias keller is senior lecturer in atomic, molecular and optical physics at the university of sussex.
he stated that "a quantum network would basically work similarly to a classical fiber network.
but instead of using strong optical signals, the signal is carried by a single photon."
these are the individual particles of light that transmit information between nodes.
ibm has developed the world's first quantum computing platform at the ibm watson research center in new york.
online since last may, it has a five- qubit quantum processor and is accessible to everyone via the ibm cloud.
scientists hope that such powerful computers will enable them to model genomes and ecosystems.
the human genome could be unraveled to expand drug development, while a model of earth's weather systems would make forecasting much more accurate.
quantum computers also could search massive databases instantaneously, and handle large amounts of data from sensors in industrial plants and on connected machinery.
this makes them perfect for the swiftly developing industrial internet of things.
possible threats?
however, quantum computers also constitute a threat to today's internet, warns andersen cheng, ceo of post- quantum:
quantum computers will be able to crack the most commonly used encryption protocol today, which will make the internet as we know it totally unusable.
we we won't be able to tell if the information came from, or will go to the right person, which will completely destroy the trust we have in the internet.
this is because current online cryptographic systems- such as messaging services, email and cloud sync software- will be very simple for quantum computers to crack, while covering their tracks.
cheng therefore thinks we need to investigate quantum safe identity authentication, which his company is developing.
quantum key distribution is one solution.
it sends a secure key across a network - impossible to copy - to decipher a conventional message or file.
facing challenges
another complication is the strict environmental conditions required.
such supercomputers must be kept super cold- at absolute zero.
they also require shielding from any electromagnetic interference (emi) to control the unstable quantum states.
only then are huge calculations possible.
so far, a universal quantum computer does not exist, but there are already projects that go beyond mere prototypes.
canada's d- wave systems recently released its third generation d- wave 2x, which uses a 1,098- qubit processor.
the one located at nasa's ames research center in california is shielded against emi 50,000 times weaker than earth's magnetic field and is housed in a vacuum.
it's also cryogenically cooled to - 460 degrees fahrenheit, about 180 times colder than interstellar space.
ibm predicts the appearance of medium- sized quantum processors of 50- 100 qubits in the next decade.
universal quantum computers could be one of the greatest milestones in the history of it.
an international team of quantum scientists and engineers led by the university of bristol and involving groups from china, denmark, spain, germany and poland, have realised an advanced large- scale silicon quantum photonic device that can entangle photons to incredible levels of precision.
while standard quantum hardware entangles particles in two states, the team has found a way to generate and entangle pairs of particles that each has 15 states.
the integrated photonic chip sets a new standard for complexity and precision of quantum photonics, with immediate applications for quantum technologies.
integrated quantum photonics allows the routing and control of single particles of light with intrinsically high stability and precision, however to date it has been limited to small- scale demonstrations in which only a small number of components are integrated on a chip.
scaling up these quantum circuits is of paramount importance to increasing the complexity and computational power of modern quantum information processing technologies, opening- up the possibility of many revolutionary applications.
the team, led by scientists from the university of bristol's quantum engineering technology laboratories (qet labs) has demonstrated the first ever large- scale integrated quantum photonic circuit, which integrating hundreds of essential components, can generate, control and analyse high- dimensional entanglement with an unprecedented level of precision.
the quantum chip was realised using a scalable silicon photonics technology, similar to today's electronic circuits, which would provide a path to manufacture massive components for the realization of a optical quantum computer.
the work, in collaboration with peking university, technical university of denmark (dtu), institut de ciencies fotoniques (icfo), max planck institute, center for theoretical physics of the polish academy of sciences, and university of copenhagen, has been published today in the journal science.
the coherent and precise control of large quantum devices and complex multidimensional entanglement systems has been a challenging task owing to the complex interactions of correlated particles in large quantum systems.
significant progress towards the realization of large- scale quantum devices has been recently reported in a variety of platforms including photons, superconductors, ions, dots and defects.
in particular, photonics represents a promising approach to naturally encode and process multidimensional qudit states in the photon's different degrees of freedom.
in this work, a programmable path- encoded multidimensional entangled system with dimension up to 15x15 is demonstrated, where two photon exists over 15 optical paths at the same time and are entangled with each other.
this multidimensional entanglement is realised by exploiting silicon- photonics quantum circuits, integrating in a single chip, 550 optical components, including 16 identical photon- pair sources, 93 optical phase- shifters, 122 beam- splitters.
lead author, dr. jianwei wang, said: "it is the maturity of today's silicon- photonics that allows us to scale up the technology and reach a large- scale integration of quantum circuits.
"this is the most beautiful thing of quantum photonics on silicon.
our quantum chip allows us to reach unprecedented levels of precision and control of multidimensional entanglement, a key factor in many quantum information tasks of computing and communication."
senior researcher, corresponding author yunhong ding from dtu, centre for silicon photonics for optical communication (spoc), added: "new technologies always enable new applications.
"the capabilities of our silicon photonics integrated technologies at dtu allow large scale, highly stable quantum information processing chips, which enable us to observe high- quality multidimensional quantum correlations including generalized bell and epr steering violations, and also to implement experimentally unexplored multidimensional quantum protocols: multidimensional randomness expansion and state self- testing."
dr. anthony laing, a lead academic in bristol's qetlabs and corresponding author, said: "entanglement is a fascinating feature of quantum mechanics and one that we do not yet fully understand.
this device and future generations of chips of increasing complexity and sophistication will allow us to explore this realm of quantum science and make new discoveries."
professor mark thompson, leader of the bristol team, added: "we have used the same manufacturing tools and techniques that are exploited in today's microelectronics industry to realise our silicon quantum photonic microchip.
however, unlike conventional electronic circuits that utilise the classical behaviour of electrons, our circuits exploit the quantum properties of single particle of light.
this silicon photonics approach to quantum technologies provides a clear path to scaling up to the many millions of components that are ultimately needed for large- scale quantum computing applications."
berlin - the fraunhofer- gesellschaft, europe's leading organization for applied research, and ibm (nyse: ibm) announced today the signing of the agreement aimed at advancing quantum computing in germany.
the collaboration gives companies and research institutions access to ibm quantum computers in germany and the usa under the umbrella of a nationwide fraunhofer competence network.
the aim is to research the technology, application scenarios and algorithms while generating competence development and competitive advantages for the local economy and science.
as part of the collaboration, an ibm q system one quantum computer will be installed in a ibm computer center near stuttgart.
the system is scheduled to go into operation in early 2021 and will be the first of its kind in europe.
fraunhofer plans to bring together established partners from research and industry under the umbrella of a research infrastructure of fraunhofer institutes, which will work together in a centrally coordinated national fraunhofer competence network for quantum computing.
this network has set itself the goal of further developing and transferring application- oriented quantum computing strategies under complete data sovereignty of european law and will initially be represented by competence centers in expected six german states - baden- wurttemberg, bavaria, rhineland- palatinate, berlin, hesse and north rhine- westphalia.
currently, more than ten fraunhofer institutes are already working on various fields of quantum technology.
as early as april 1, 2020, interested companies and research institutions will have access to the world's largest group of quantum computers in the us- based ibm quantum computation center under the umbrella of the nationwide fraunhofer competence network.
the ibm quantum computation center currently comprises 15 systems and is located in the us state of new york.
under the terms of the agreement, ibm will offer fraunhofer technical support and assistance in using the ibm quantum systems.
pioneering initiative on applied quantum computing
the signing of the cooperation follows the joint announcement of september 2019, a pioneering initiative on applied quantum computing for germany's research institutions and companies.
the cooperation partners support the german federal government's goal of investing almost one billion euros over the next two years in order to develop quantum technology from basic research to marketable applications.
this is to be made possible by the development of a research infrastructure that strategically promotes the further development and dissemination of quantum computing in germany.
the participating federal states baden- wurttemberg and bavaria contribute the largest financial share.
"a central research question is which concrete application scenarios are suitable for calculation with a quantum computer, how algorithms for this can be developed and translated into simple applications.
quantum computing has the potential to analyze the complex systems in business and industry, to unravel molecular and chemical interactions, to solve complicated optimization problems and to make artificial intelligence significantly more powerful," explains fraunhofer president prof. reimund neugebauer.
"such advances could open the door to new scientific knowledge and enormous improvements, for example in supply chains, logistics and the modelling of financial data, as well as problems from the classical engineering sciences."
"this agreement opens up another opportunity for europe to become a pioneer in the further development of a promising technology with germany playing a leading role as we grow the scientific, academic, public and private ecosystem which seeks to solve complex problems, such as climate change and healthcare issues.
a memorable milestone for our region", says martin jetter, senior vice president & chairman ibm europe.
"the installation of the first physical quantum computer on european soil sends out a strong signal in support of germany as a research location.
it marks an important step toward the further establishment of an internationally recognizable ecosystem in the field of quantum technology," says helge braun, head of the federal chancellery.
"the quantum computer puts us in a strong position to help shape, at an early stage and in a decisive way, the key technologies of the future.
germany in general, and baden- wurttemberg in particular, will become the center of quantum technology in europe.
this will provide both industry and science with immense research and experimental opportunities in the fields of transportation, machine tools, communications, health care, and the finance and energy industries.
naturally, as minister- president, i am delighted that baden- wurttemberg has been chosen as the location for such pioneering technologies," says minister- president winfried kretschmann.
hubert aiwanger, bavarian state minister of economic affairs: "quantum computing has enormous potential in many fields of application, including logistics, materials research, artificial intelligence and it security.
bavaria will therefore support this initiative of the fraunhofer- gesellschaft with a high- performance competence center, to be led by fraunhofer institute for applied and integrated security aisec in garching.
one area of focus will be the interaction between quantum computing and it security.
we want to push ahead with the development of quantum computing as a new key technology for bavarian companies."
nicole hoffmeister- kraut, baden- wurttemberg state minister of economic affairs, says: "the installation of a quantum computer in baden- wurttemberg is a great achievement and an enormous opportunity for science and industry in our region.
as a 'state of hidden champions' and one of europe's most innovative regions, we offer ideal conditions for this new technology as well as a host of application fields in which quantum computing can be of benefit.
the availability of such immense computing power will make it possible to solve, in the shortest possible time, fundamental economic problems that currently take years to process.
the state government is therefore providing 40 million euros in funding to build up the necessary expertise in industry, together with ibm and the fraunhofer- gesellschaft.
the establishment of a competence center in baden- wurttemberg marks an important step, since it will enable us to make the most of the opportunities offered by quantum computing in the future."
ibm system q one
ibm system q one is optimized to ensure the quality, stability, reliability and reproducibility of multi- qubit applications.
due to these factors and the resulting high quantum volume (a measure of the performance of a quantum computer), the ibm system q one enables state- of- the- art research for concrete application scenarios in science and industry.
in 2016, ibm was the first company to make universal quantum computers accessible via the cloud.
an active community of more than 200,000 users have run hundreds of billions of executions on real ibm quantum hardware, and have published more than 200 research papers based on these experiments.
ibm is also the first company to have commercial clients via the ibm q network, a community of more than 100 businesses, start- ups, research labs, education institutions and governments working with ibm to advance quantum computing.
using a compact optical platform that exploits the quantum characteristics of light, professor roberto morandotti and his team are one step closer to realizing the first powerful photonic quantum computer.
in the journal nature physics, the inrs researchers revealed to have generated a particular class of quantum states- d- level cluster states- , as well as to have used them to implement novel quantum operations.
the demonstrated states exhibit unique properties that make them more robust and powerful than any other such states demonstrated thus far.
for nearly ten years, professor roberto morandotti has been building an ambitious system piece by piece by developing chips that use light particles (photons) as the data medium.
on these coin- sized chip structures, photons are generated and transformed so they can be assigned unique quantum properties.
his team was the first to successfully create high- dimensional (i.e.
qudit) optical cluster states, one of the elements that can enable the ongoing quest to harness the power of quantum computing.
electronic computer systems are nearing the limit of their capabilities, yet demand for greater computing power is constantly growing.
this is why scientists are turning to quantum computing, investigating how to encode a significant amount of information in light particles and perform calculations of unprecedented complexity.
to get there, the data medium has to be shifted to quantum bits (or qubits), the non- classical computing equivalent of conventional bits.
by judiciously designing the quantum state of the photons, it is possible to increase the information storage capacity of qubits and boost them to obtain so- called qudits.
then, by grouping the qudits into clusters, quantum computing operations based on the so called 'one- way' scheme become possible.
other approaches to quantum computing use ions, atoms, or other quantum resources, but the efforts to manipulate them towards a higher- dimensional encoding have been inefficient.
according to professor jose azana (inrs), an expert of telecommunications who contributed to this research, photons also present another advantage: "they are used to transmit information via optical fibers in existing telecommunications systems.
that means photons with controlled quantum properties can also travel through these same channels without losing their attributes."
the complexity and richness of the cluster states described in the article in nature physics is unprecedented.
moreover, the team of researchers also achieved another first by performing high- dimensional quantum computing operations harnessing the realized cluster states.
they demonstrated that light has all the necessary features to power the superfast computers of the future.
in a significant leap forward, this was done with a compact system compatible with existing technologies.
the platform developed by the inrs team is capable of generating quantum states with complexities sufficient to achieve quantum computing objectives, thus paving the way to one- way quantum computers.
story source:
materials provided by institut national de la recherche scientifique - inrs.
note: content may be edited for style and length.
journal reference:
christian reimer, stefania sciara, piotr roztocki, mehedi islam, luis romero cortes, yanbing zhang, bennet fischer, sebastien loranger, raman kashyap, alfonso cino, sai t. chu, brent e. little, david j. moss, lucia caspani, william j. munro, jose azana, michael kues, roberto morandotti.
high- dimensional one- way quantum processing implemented on d- level cluster states.
nature physics, 2018; doi: 10.1038/s41567- 018- 0347- x
quantum computing has the potential to change the world, transforming fields such as artificial intelligence, medicine, and cybersecurity.
despite its growing importance, quantum is rarely taught to university students, let alone high school students.
mit researchers and the coding school are changing that by offering a first- of- its- kind virtual quantum computing camp this summer to high school and first- year university students.
continue reading
the goal of the camp is for students to develop foundational knowledge of quantum physics and practical skills in quantum computation.
by the end of the camp, students learn how to program a quantum computer and run quantum circuits - such as teleporting quantum information.
students globally can apply here.
the camp is led by amir karamlou, a graduate research fellow and instructor for mit's introduction to quantum computing.
his research focuses on experimental quantum computation using superconducting qubits.
other instructors include bharath kannan, a phd student researching microwave quantum optics, and grecia castelazo, studying physics and math at mit.
"today, we're at the dawn of a new era in computing technology.
you don't need an advanced degree in physics to explore quantum computing.
over the next decade, quantum is likely to revolutionize the world in the same way the modern computer did in the mid- 20th century.
students who develop knowledge in quantum now will be prepared for this world- altering technological movement," explained karamlou.
the camp is part of a larger quantum initiative by the coding school's codeconnects program, a leading tech education nonprofit.
fall 2020, the coding school will offer an unprecedented year- long quantum course for high school students.
the virtual course is being led by francisca vasconcelos, a rhodes scholar and mit graduate.
the coding school is dedicated to ensuring computer science education is accessible, supportive, and empowering.
pioneering high- quality online, live coding education since 2017, they've taught over 70,000 hours of coding instruction to students nationwide and across 40 countries.
"to ensure long- term employability and social mobility, it's critical to look forward to the tech skills of the future and prepare students with those now.
quantum computation is one of those skills.
we're proud at the coding school to be paving the way in equipping the next generation by making quantum education accessible for all," remarked kiera peltz, founder of the coding school.
to ensure accessibility, scholarships are available to students with financial need and who have been significantly affected by covid- 19, including if a parent has lost a job or is an essential worker.
besides quantum computing, the coding school offers other virtual camps for students grades 3- 12 including a techtaster, music+tech, and creativetech.
for more personalized instruction, they offer one- on- one coding lessons in 18 specialized curriculums, including ai and cybersecurity.
learn more: www.codeconnects.org/summercamps + programs@the- cs.org.
media contact: abeer dhanani
(323) 790- 9992
amazon unveiled braket, a quantum computing service that represents a largely perfunctory entry in the quantum computing market.- itit brings choice, certainly, as amazon provides cloud access to d- wave, ionq, and rigetti quantum computers- but practically extracting any value from any of these remains the task of the user.
to start, d- wave, ionq, and rigetti represent three different strategies toward quantum computing: d- wave offers a quantum annealer, ionq uses trapped- ion qubits, and rigetti is advancing a superconducting qubit co- processor.
d- wave's quantum annealer is a frequent target of criticism in the industry for being less robust than other approaches- their offering relies on all problems being expressed as a quadratic unconstrained binary optimization (qubo) problem.
relatively, this is skim milk compared to the full- fat designs of other computers.
while d- wave contends that qubo has business value- and there are businesses using it- few circumstances appear in which a d- wave annealer performs well in head- to- head comparisons with anything else, for the practical reason that programmers essentially need to write specifically for d- wave.
as an aside, d- wave also garners criticism for characterizing their quantum annealer as containing up to 2,000 qubits.
amazon's announcement blog post, published monday, indicates that "as i write this, the largest quantum computers contain about 50 qubits."
ionq has the distinction of having partnerships with aws, and with azure quantum, announced at ignite 2019 in orlando last month.
this likely bodes well for ionq in terms of availability to developers at a minimum, considering the size and market position of aws and azure.
likewise, rigetti's partnership is likely to help in mindshare.
while there is no guarantee that the hosted platform model will win out for quantum- particularly keeping in mind differences between hardware developers- that delivery model has worked out for practically everything else up to this point.
alongside the unveiling of bracket, the company is rolling out a quantum solutions lab, that the company touts as allowing organizations "to tap into our own expertise and that of our consulting partners.
our goal is to work with you to find those practical uses, and to help you to build up your own 'bench' of qualified quantum developers."
similarly, the aws center for quantum computing, starting at caltech, aims to "bring together researchers and engineers from amazon with leading academic institutions in quantum computing, to develop more powerful quantum computing hardware and identify novel quantum applications with the goal of boosting innovation in science and industry."
update (december 4, 2019):
a previous version of this article indicated that d- wave systems possess up to 5,000 qubits.
d- wave's currently commercialized system - the 2000q - has 2,000 qubits.
the referenced 5,000 qubit system is slated for availability in 2020.
further, a d- wave spokesperson told techrepublic that "a universal quantum system is on our roadmap."
on programming, d- wave contends that "for any system, you need to formulate the problem in a way that the qpu understands.
d- wave, among others, has tools to make this easier by handling high- and low- level abstractions."
after the publication of this article, the amazon blog post language was altered to state "the largest gated- based quantum computers contain about 50 qubits."
also see
when a tiny, quantum- scale, hypothetical balloon is popped in a vacuum, do the particles inside spread out all over the place as predicted by classical mechanics?
the question is deceptively complex, since quantum particles do not look or act like air molecules in a real balloon.
matter at the infinitesimally small quantum scale is both a wave and a particle, and its location cannot be fixed precisely because measurement alters the system.
now, theoretical physicists at the university of southern california and the university of massachusetts boston have proven a long- standing hypothesis that quantum- scale chaos exists ... sort of.
writing in the april 17 edition of nature, senior author maxim olshanii reported that when an observer attempts to measure the energies of particles coming out of a quantum balloon, the interference caused by the attempt throws the system into a final, "relaxed" state analogous to the chaotic scattering of air molecules.
the result is the same for any starting arrangement of particles, olshanii added, since the act of measuring wipes out the differences between varying initial states.
"it's enough to know the properties of a single stationary state of definite energy of the system to predict the properties of the thermal equilibrium (the end state)," olshanii said.
the measurement - which must involve interaction between observer and observed, such as light traveling between the two - disrupts the "coherent" state of the system, olshanii said.
in mathematical terms, the resulting interference reveals the final state, which had been hidden in the equations describing the initial state of the system.
"the thermal equilibrium is already encoded in an initial state," olshanii said.
"you can see some signatures for the future equilibrium.
they were already there but more masked by quantum coherences."
the finding holds implications for the emerging fields of quantum computing and quantum information theory, said paolo zanardi, an associate professor of physics studying quantum information at usc.
in zanardi's world, researchers want to prevent coherent systems from falling into the chaos of thermal equilibrium.
"finding such 'unthermalizable' states of matter and manipulating them is exactly one of those things that quantum information/computation folks like me would love to do," zanardi wrote.
"such states would be immune from 'decoherence' (loss of quantum coherence induced by the coupling with environment) that's still the most serious, both conceptually and practically, obstacle between us and viable quantum information processing."
zanardi and a collaborator introduced the notion of "decoherence- free" quantum states in 1997.
researchers such as zanardi and daniel lidar, associate professor of chemistry, among others, have helped make usc a major center for the study of quantum computing.
the future of secure communication will be in quantum encryption, and kth will lead research in this area under the auspices of a new national research centre financed by the knut and alice wallenberg foundation.
gunnar bjork, professor of photonics at kth, says that the wallenberg centre for quantum technology is considered to be one of sweden's largest individual research efforts in recent years.
with nodes at kth, chalmers university in gothenburg, and lund university, the centre will operate on a total budget of about eur 95.3 million over 10 years.
the knut and alice wallenberg foundation provides the majority of financing, with a grant of eur 60 million.
matching funds from the universities and industry comprise the balance.
"the centre will have four orientations," bjork says, "quantum computers and simulators, quantum sensors and quantum communication."
while kth coordinates quantum communication research and academic excellence, chalmers is responsible for overall direction of the centre and the core project of developing what may be the world's biggest quantum computer, with a capacity of 100 qubits.
chalmers will coordinate research in quantum computing and simulators, and lund is responsible for research with sensors.
the research at kth mainly involves the secure transmission of encrypted data.
quantum encryption secures data from eavesdropping, but transmission of quantum signals is presently limited to 100km.
the research led by kth will focus on amplifying signals with "quantum repeaters" in a worldwide quantum network, developing photon sources for carrying the data, perfecting single photon detectors and developing new encryption protocols.
quantum communication security draws its unique impenetrability from werner heisenberg's uncertainty principle, which holds that there is no way to make a measurement of a quantum system's characteristics without changing the system itself.
"physical quantum systems can neither be measured nor copied without back action on the system," bjork says.
"trying to access the information will change it."
today, quantum computers represent one of the greatest possible breakthroughs in data processing and problem solving.
unlike traditional computers which process information in a binary state (a bit is either represented by a "1" or a "0"), quantum computers operate using the principles of quantum mechanics and storing information in mind- bendingly complex qubits.
unlike the traditional bit, a qubit can store information as a 1, a 0 or any other quantum superposition of those two states - simultaneously.
by combining multiple qubits into a single system, engineers can begin to exponentially increase the computing power in a quantum system by using the idea of quantum entanglement, which is an idea einstein panned as "spooky action at a distance," to make massive correlations between the qubits in a system.
these correlations will eventually create a quantum state that's different from its original state based on the instructions issued to the machine during its computational process.
while the theory underpinning quantum computing is mind bending on its own (how do you observe a final result without changing it?
), designing the hardware that will run these systems is equally challenging.
think about it: just as quantum computing's operations will be radically different from digital computing, so too will its hardware.
in fact, there are numerous challenges still facing the field, such as how do you develop a transistor, a reprogrammable chip or other core components of a quantum computing system?
well, a group of finnish researchers working at aalto university in finland has made a breakthrough in quantum computing architecture, creating a "quantum circuit refrigerator" that can reduce errors in quantum computing processes.
in their original state, just before they're set loose on a problem, qubits must be kept cool to ensure their delicate quantum state computes correctly.
but the process of keeping these bits cool hasn't been simple.
according to an aalto release, "just like ordinary processors, a quantum computer also needs a cooling mechanism.
in the future, thousands or even millions of logical qubits may be simultaneously used in computation, and in order to obtain the correct result, every qubit has to be reset in the beginning of the computation.
if the qubits are too hot, they cannot be initialized because they are switching between different states too much."
using a two- nanometer thick insulators, the aalto team excited a tunneling electron with slightly less energy than it would need for its usual tunneling behavior.
to compensate and complete its usual course, the electron grabs a bit of energy from a nearby quantum device, in this case the qubit, cooling it down and keeping it suspended in the state needed to complete a calculation and state in order.
if all of this is a bit confusing, don't be concerned.
even richard feynman opined that if you say you understand quantum mechanics, you're lying.
but to help out, here's a video that further explains this breakthrough:
while initial tests of the aalto quantum fridge used simulated qubits to confirm their design, the researchers are ready to move forward with the real thing.
if their system works, quantum computing architecture may have just made a huge leap forward.
scientists and engineers from the universities of bristol and western australia have developed how to efficiently simulate a "quantum walk" on a new design for a primitive quantum computer.
quantum computers have significant potential to open entirely new directions for processing information and to overhaul the way that we think about and use the science of computation.
modern computers already play a huge role in society- they routinely handle and process vast amounts of data and solve calculations at an incredible rate.
however, there are some problems that they just cannot solve in a useful amount of time, no matter how fast they become.
the concept of a quantum computer aims to address this, exploring uncharted computation and solving at least some of these problems that classical computers cannot.
the study published today in nature communications, reports strong evidence that with this method something meaningful can already be seen with a primitive quantum computer that cannot be seen with a classical computer.
the very first steps towards this have been implemented in the lab in bristol.
dr ashley montanaro, lecturer in applied mathematics and epsrc fellow from the university of bristol's school of mathematics, said: "a quantum computer is a machine designed to use quantum mechanics to solve problems more efficiently than any possible classical computer.
"we know some algorithms that can run on such machines and it's an open and exciting challenge to find more.
but most of the quantum algorithms we know need to be run on a large- scale quantum computer to see a speed up."
building a large- scale quantum computer is one of the biggest engineering challenges today.
there's a growing worldwide effort to develop one and it needs substantial effort from a wide range of expertise - including as part of the uk national quantum technologies programme (uknqt).
the results could be tremendous, offering fast and cheap ways to design new materials and new pharmaceuticals.
but there is a field of research emerging now that can help accelerate understanding how quantum computers will work and how users can apply them.
examining the power of smaller, more primitive designs for quantum computers indicates that sooner than we thought, quantum machines could outperform the capabilities of classical computing for very specific tasks- "boson sampling" is a recent example that is driven by what is experimentally available very soon.
big questions researchers face include what can these primitive quantum processors do that is useful to someone and how sophisticated do they need to be.
the results published in today's paper help to answer this question, by looking at how to simulate particular kinds of a phenomenon called the quantum walk.
the quantum walk at first glance is abstract.
but it is the quantum mechanical version of very useful models such as brownian motion and the "drunken sailor's random walk".
the key difference is the particle in the quantum walk is endowed with the principle of quantum superposition.
this has enabled other researchers to show they are a new way to think about how full- scale quantum computers might operate and to create useful quantum algorithms.
xiaogang qiang, phd student in the school of physics who implemented the experiment, said: "it's like the particle can explore space in parallel.
this parallelism is key to quantum algorithms, based on quantum walks that search huge databases more efficiently than we can currently."
dr jonathan matthews, epsrc early career fellow and lecturer in the school of physics and the centre for quantum photonics, explained: "an exciting outcome of our work is that we may have found a new example of quantum walk physics that we can observe with a primitive quantum computer, that otherwise a classical computer could not see.
"these otherwise hidden properties have practical use, perhaps in helping to design more sophisticated quantum computers."
scientists at the university of sydney have adapted techniques from autonomous vehicles and robotics to efficiently assess the performance of quantum devices, an important process to help stabilise the emerging technologies.
the innovative approach has been shown experimentally to outperform simplistic characterisation of these environments by a factor of three, with a much higher result for more complex simulated environments.
"using this approach, we can map the 'noise' causing performance variations across quantum devices at least three times as quickly as a brute- force approach," said lead author riddhi gupta, a phd student in the school of physics.
"rapidly assessing the noise environment can help us improve the overall stability of quantum devices."
the research has been published in nature partner journal quantum information.
quantum computing is still in its early stages of development yet promises to revolutionise technology by solving problems beyond the scope of classical computing.
one of the barriers to develop these systems to practical scale is overcoming the imperfections of hardware.
the basic units of quantum technology -  quantum bits, or qubits -  are highly sensitive to disturbance from their environments, such as electromagnetic 'noise', and exhibit performance variations that reduce their usefulness.
ms gupta, also part of the arc centre of excellence for engineered quantum systems, has taken techniques from classical estimation used in robotics and adapted them to improve hardware performance.
this is achieved through the efficient automation of processes that map both the environment of and performance variations across large quantum devices.
"our idea was to adapt algorithms used in robotics that map the environment and place an object relative to other objects in their estimated terrain," she said.
"we effectively use some qubits in the device as sensors to help understand the classical terrain in which other qubits are processing information."
in robotics, machines rely on simultaneous localisation and mapping, or slam, algorithms.
devices like robotic vacuum cleaners are continuously mapping their environments then estimating their location within that environment in order to move.
the difficulty with adapting slam algorithms to quantum systems is that if you measure, or characterise, the performance of a single qubit, you destroy its quantum information.
what ms gupta has done is develop an adaptive algorithm that measures the performance of one qubit and uses that information to estimate the capabilities of nearby qubits.
"we have called this 'noise mapping for quantum architectures'.
rather than estimate the classical environment for each and every qubit, we are able to automate the process, reducing the number of measurements and qubits required, which speeds up the whole process," ms gupta said.
dr cornelius hempel, whose experimental team provided ms gupta with data from experiments on a one- dimensional string of trapped ions, said he was pleased to see a threefold improvement even in the mapping of such a small quantum system.
"however, when riddhi modelled this process in a larger and more complex system, the improvement in speed was as high as twentyfold.
this is a great result given the future of quantum processing is in larger devices," he said.
ms gupta's supervisor is professor michael j. biercuk, founder of quantum technology company q- ctrl and director of the university of sydney quantum control laboratory in the sydney nanoscience hub.
he said: "this work is an exciting demonstration that state- of- the- art knowledge in robotics can directly shape the future of quantum computing.
this was a first step to unify concepts from these two fields, and we see a very bright future for the continued development of quantum control engineering."
compact quantum devices could be incorporated into laptops and mobile phones, thanks in part to small devices called quantum optical micro- combs.
quantum optical micro- combs are devices that generate very sharp precise frequencies of light an equal distance apart - a bit like the teeth of a comb.
they can enable ultrafast processes and could be an important component of quantum computer systems.
in a review paper covering the development of these devices, professor david moss, director of the centre for micro- photonics (cmp) at swinburne describes the advances that have been made in making these devices smaller and portable enough to be included on a chip.
"these devices will enable an unprecedented level of sophistication in generating entangled photons on a chip - a key breakthrough that, in my opinion, could very well accelerate the quest of achieving so- called 'quantum supremacy' - quantum devices that have the ability to perform functions beyond the capability of conventional electronic computers", says professor moss.
a key challenge for quantum science and technology is to develop practical large- scale, systems that can be precisely controlled.
quantum optical micro- combs provide a unique, practical and scalable framework for quantum signal and information processing to help crack the code to ultra- secure telecommunications and greatly advance quantum computing.
quantum optical micro- combs have achieved record complexity and sophistication in the photon quantum version of a classical computer bit, a qudit,that can be generated and controlled in the tiny space of a computer chip.
these breakthroughs have shown that compact, highly complex quantum can exist outside of large laboratories, opening the possibility that ultimately- quantum devices could be used in laptops and mobile phones, bringing the vision of powerful optical quantum computers for everyday use closer than ever before.
one of the biggest challenges in quantum science is to build a functioning quantum bit, the basic element for the quantum computer.
an important theoretical candidate for such a quantum bit is using a bent carbon nanotube.
scientists at the delft university of technology and the foundation for fundamental research on matter (fom), led by professor leo kouwenhoven, have succeeded for the first time to create a working quantum bit using a carbon nanotube.
on july 28 they published their results in nature nanotechnology.
quantum computer
quantum computers have the advantage that they can be much faster than "ordinary" supercomputers for various calculations.
an ordinary computer bit has the value '1 'or '0', but a quantum bit uses the state of a single electron, which due to the extraordinary properties of the quantum world can take both values at the same time.
"we call that a 'superposition''' says phd student fei pei who carried out part of the measurements.
"the main problem that quantum physicists have to deal with is that the superposition can be disturbed quickly by external factors, for instance by other particles around the quantum bit."
bent tubes
carbon nanotubes of a few nanometers in diameter have exceptional properties that make them a very suitable material for hosting quantum bits, but so far no one could manipulate and 'read' a single electron in a nanotube.
the scientists from delft have achieved that.
they were inspired by the work of colleagues from copenhagen who showed in 2010 theoretically that the quantum state of an electron can be controlled in a bent nanotube, and carried on their own work from 2012.
back then the delft scientists showed how to read the electron quantum states in a nanotube.
putting both ideas together created a 'functioning' quantum bit.
"the next challenge would be to improve the stability of this quantum bit as currently we can not maintain the superposition state long enough, "said pei.
"we will now focus on reducing the disturbance, so to extend the lifetime of the superposition".
ibm collaborates with professor sabrina maniscalco and her group to develop new transformative tools for teaching and outreach about quantum technologies.
a university collaboration between ibm and aalto and turku universities aims to build easy- to- use open- source content to improve knowledge and intuition of quantum phenomena, like quantum computing and quantum technologies.
the goal is to develop videos, games, and written material that will lower barriers for anyone wanting to take a look at what quantum technologies actually are.
the playful content will provide a native platform for the younger generations to explore the complex and counter- intuitive concepts in quantum physics and quantum computing.
the project is part of the centre for quantum engineering's (cqe) activities at aalto university.
'we're creating an interactive online mnemonic medium, intended to augment human memory, which incorporates and combines videos, short games, text, simple math, and computer programs, all with the aim of building the knowledge (and intuition even!)
of quantum phenomena.'
says professor sabrina maniscalco, professor of theoretical physics at the university of turku, adjunct professor at aalto university and the vice- director of the national centre of excellence, quantum technology finland.
strengthening the quantum ecosystem in finland
the collaboration between ibm, aalto university and the university of turku realizes the objectives of ibms q network.
the q network aims to accelerate joint research in quantum computing and help train students for careers in quantum technology.
examples of future direction of quantum computing application research and exploration by these universities include:
aalto university plans to work with ibm researchers to extend the quantum technology ecosystem in finland.
this intended collaboration in education, outreach and science will strengthen aalto's capabilities in quantum computing.
university of turku plans to investigate quantum computation and simulation research, as well as use the ibm q experience for outreach and specialized education focused on quantum algorithms, quantum and classical programming, and fundamental quantum physics.
'we are really enthusiastic about this new university collaboration initiative related to quantum here in finland.
collaboration with academic community is vital to the growth of a 'quantum ready' ecosystem, that involves scientists, professors, developers, students, and even younger generations.
developing quantum computing skills and expertise will lead to discovery of new innovations and increase the knowledge and skills of all age groups in this new arena' says maarit palo, director of ibm university programs in nordics and corporate citizenship, governmental affairs in finland.
featured news from related categories:
researchers have studied how a 'drumstick' made of light could make a microscopic 'drum' vibrate and stand still at the same time.
a team of researchers from the uk and australia have made a key step towards understanding the boundary between the quantum world and our everyday classical world.
quantum mechanics is truly weird.
objects can behave like both particles and waves, and can be both here and there at the same time, defying our common sense.
such counterintuitive behaviour is typically confined to the microscopic realm and the question "why don't we see such behaviour in everyday objects?"
challenges many scientists today.
now, a team of researchers have developed a new technique to generate this type of quantum behaviour in the motion of a tiny drum just visible to the naked eye.
the details of their research are published today in new journal of physics.
project principal investigator, dr michael vanner from the quantum measurement lab at imperial college london, said: "such systems offer significant potential for the development of powerful new quantum- enhanced technologies, such as ultra- precise sensors, and new types of transducers.
"excitingly, this research direction will also enable us to test the fundamental limits of quantum mechanics by observing how quantum superpositions behave at a large scale."
mechanical vibrations, such as those that create the sound from a drum, are an important part of our everyday experience.
hitting a drum with a drumstick causes it to rapidly move up and down, producing the sound we hear.
in the quantum world, a drum can vibrate and stand still at the same time.
however, generating such quantum motion is very challenging.
lead author of the project dr martin ringbauer from the university of queensland node of the australian research council centre for engineered quantum systems, said: "you need a special kind of drumstick to make such a quantum vibration with our tiny drum."
in recent years, the emerging field of quantum optomechanics has made great progress towards the goal of a quantum drum using laser light as a type of drumstick.
however, many challenges remain, so the authors' present study takes an unconventional approach.
dr ringbauer continues: "we adapted a trick from optical quantum computing to help us play the quantum drum.
we used a measurement with single particles of light -  photons -  to tailor the properties of the drumstick.
"this provides a promising route to making a mechanical version of schrodinger's cat, where the drum vibrates and stands still at the same time."
these experiments have made the first observation of mechanical interferences fringes, which is a crucial step forward for the field.
in the experiment, the fringes were at a classical level due to thermal noise, but motivated by this success, the team are now working hard to improve their technique and operate the experiments at temperatures close to absolute zero where quantum mechanics is expected to dominate.
these future experiments may reveal new intricacies of quantum mechanics and may even help light the path to a theory that links the quantum world and the physics of gravity.
it's always been assumed that quantum computing is better - at least to the layperson.
but ibm research scientists have now actually proven mathematically that quantum computing is faster than a classical computer for certain problems.
the critical word, however, is "certain."
in a telephone interview with ee times, bob sutor, vice president of ibm q ecosystem and strategy, said this mathematical proof demonstrates concretely the difference between certain types of computations that can be done with a quantum computer versus a classical computer.
"and it says that quantum computers will be faster - that the behavior or what you're trying to do on a quantum computer is different enough because of the innate properties of quantum mechanics and quantum computing that it will have this type of significant advantage."
sutor said this proof is an extremely important milestone, as it will be a foundation for building the rest of the formal structure of quantum computers - how they are coded, built, and what choices are made around algorithms and how they are applied.
but it will also provide guidance on when quantum computing might be the option, or whether classical computing will still be sufficient, he said.
the mathematical proof is outlined in the science article as "quantum advantage with shallow circuits" by drs.
sergey bravyi of ibm research, david gosset of the university of waterloo's institute for quantum computing, and robert konig of the institute for advanced study and zentrum mathematik, technische universitat munchen.
to understand the significance of the proof, it's important to understand the basic computational unit in quantum computing is a quantum bit - a qubit, which unlike a classical bit that is always 0 or 1, can take on many other additional values.
the potential computational power doubles every time a qubit is added through entanglement, and the qubits together with the operations applied to them are called a circuit.
qubits aren't perfect in that they have small error rates and only exist for a certain length of time before they become chaotic.
this is known as the coherence time, and it means there only so many operations that can be done before the time limit is reached.
the number of operations performed is depth, and the overall depth of a quantum circuit is the minimum of all the depths per qubit.
the math proves that certain problems need only a fixed circuit depth when done on a quantum computer, no matter how the number of inputs increases, whereas a classical computer requires the circuit depth to grow larger as inputs increase for the same problem.
this limited depth means ibm scientists are most interested in what can be done with short- depth circuits because they are practical for implementing quantum algorithms and demonstrating quantum computing has an advantage over a classical approach.
the mathematical proof shows that fault tolerant quantum computers will do some things better than classical computers can - not all.
"it's a little subtle i think sometimes," said sutor.
"but that's a very important distinction."
the proof is a first foundational step, and it's important to manage expectations - we're in the very early days of quantum computing, he said.
"in practical terms, we have 50 qubits.
that's our largest prototype right now."
and one major myth sutor wants to dispel is the "quantum crypto apocalypse" that would break encryption on the web, which would in fact require on the order 100 million cubits.
"there's several miracles between here and there," sutor said.
- gary hilson is a general contributing editor with a focus on memory and flash technologies for ee times.
related articles:
partner content: how wireless connectivity and interoperability are transforming smart homes and smart living
gary hilson is a freelance writer and editor who has written thousands of words for print and pixel publications across north america.
his areas of interest include software, enterprise and networking technology, research and education, sustainable transportation, and community news.
his articles have been published by network computing, informationweek, computing canada, computer dealer news, toronto business times, strategy magazine, and the ottawa citizen.
nature published on wednesday the finalized version of the google paper claiming quantum supremacy that leaked in september, leading to widespread criticism among the quantum computing industry.
google's claim, at its core, is that their 53- qubit "sycamore" computer is capable of performing a test calculation in 200 seconds "that would have taken the best known algorithms in the most powerful supercomputers thousands of years to accomplish," ceo sundar pichai said in a blog post.
the industry objection to this claim is that the calculation in question is of no practical use outside of research laboratories- even inside labs, the utility of it does not extend meaningfully beyond the synthetic benchmark scenario google pursued for this paper.
pichai likens the benchmark to the wright brothers, in an interview with mit technology review.
"the first plane flew only for 12 seconds, and so there is no practical application of that.
but it showed the possibility that a plane could fly."
he also dismisses criticism of their use of "quantum supremacy" as implying quantum computers will eventually outperform classical computers on all fronts, though quantum computers are likely to need to work with classical computers as an accelerator, like gpus and in- memory computing solutions.
"it is a technical term of art.
people in the community understand exactly what the milestone means," pichai said.
john preskill, professor of theoretical physics at the california institute of technology, coined the term "quantum supremacy" in 2012.
preskill downplayed the meaningfulness of google's claim, noting that "the problem their machine solved with astounding speed was carefully chosen just for the purpose of demonstrating the quantum computer's superiority," in a column in quanta magazine.
"this quantum computation has very little structure, which makes it harder for the classical computer to keep up, but also means that the answer is not very informative."
despite this, preskill recognizes the demonstration as significant.
"now that we know the hardware is working, we can begin the search for more useful applications," he added.
ibm, which rolled out its own 53- qubit system in september, takes a significantly cooler stance toward google's claims.
in a post written by edwin pednault, john gunnels, and jay gambetta, ibm argues "an ideal simulation of the same task can be performed on a classical system in 2.5 days and with far greater fidelity," claiming this is a "a conservative, worst- case estimate," and arguing that google's synthetic benchmark does not meet the threshold of quantum supremacy as defined by preskill.
further, ibm notes the hazard of printing headlines that exclaim variations of "quantum supremacy achieved" as "inevitably [misleading] the general public," though notes that "google's experiment is an excellent demonstration of the progress in superconducting- based quantum computing, showing state- of- the- art gate fidelities on a 53- qubit device, but it should not be viewed as proof that quantum computers are 'supreme' over classical computers."
scott aaronson, author of quantum computing since democritus and professor of computer science at the university of texas ataustin, takes a middle ground approach to the issue, criticizing the overhyping and dismissal of google's claim.
"have a little respect for the immensity of what we're talking about here, and for the terrifying engineering that's needed to make it reality," aaronson wrote.
"before quantum supremacy, by definition, the [quantum computing] skeptics can all laugh to each other that, for all the billions of dollars spent over 20+ years, still no quantum computer has even once been used to solve any problem faster than your laptop could solve it, or at least not in any way that depended on its being a quantum computer.
in a post- quantum- supremacy world, that's no longer the case."
"the dismissiveness i'm seeing in some corners of the internet is kind of breathtaking to me.
it's like, if you believed that useful air travel was fundamentally impossible, then seeing a dinky wooden propeller plane keep itself aloft wouldn't refute your belief ... but it sure as hell shouldn't reassure you either."
for more on quantum computing, check out "why quantum volume is vital for plotting the path to quantum advantage" and "aliro aims to make quantum computers usable by traditional programmers" at techrepublic.
also see
prof. manfred hauswirth, director of the fraunhofer institute for open communication systems fokus in berlin, talks about the opportunities, applications and potential roadblocks for quantum computing.
when will quantum computing become relevant for industry?
prof. hauswirth: we're only just starting to investigate possible application scenarios, but it would be wrong to think that it's all a long time in the future.
although we have only few practical results at the moment, the technology and the application know- how are developing very quickly.
every euro invested into developing practically usable quantum computers will pay off in many ways.
it will enable new projects here in germany, help to create new jobs and help to innovate new forms of value creation.
therefore it's important that industry gets ready right now.
and, of course, this must be supported by strategic investments by the state.
google, ibm and alibaba are all building quantum computers.
china launched the first quantum satellite back in 2016.
what can europe and germany do to fight the competition from china and the usa?
prof. hauswirth: europe has a lot to offer here.
we have a lot of expertise in the actual quantum processes that underly quantum computing.
and more importantly still, we have an in- depth understanding of the potential applications of quantum computing.
in other words, we're familiar with the production processes, and we have the necessary know- how in material sciences and logistics.
that's what counts in the current phase, where research is now focusing on the potential applications of quantum technology.
there's a reason why ibm and google now have locations in germany.
it's because they both need this knowledge.
fraunhofer and ibm are getting the first quantum computer to germany.
why is this so relevant?
prof. hauswirth: firstly, it's to investigate technical feasibility and applications potential.
you can't just run any algorithm on a quantum computer.
we need to test which algorithms are suitable and then find an easy way to make them usable for industrial applications.
the translation process is a prerequisite for the practical applications that we want to develop with partners from industry.
another key issue is to ensure our digital sovereignty.
having a location in germany will mean that all the data is processed according to european law and our european data- protection standards.
what are your hopes for current developments in quantum computing and the impacts it will have on the economy and society?
prof. hauswirth: many areas today already depend on massively complex computations - take simulations in materials research, for example, or traffic flow models, or algorithmic trading and other complex stock- market transactions.
this requires huge computing resources and consumes a lot of processing time.
quantum computers will make it much more efficient to carry out many of these simulations - meaning that we can develop new materials more quickly, increase the predictive accuracy of forecasts, control traffic more efficiently, or model the chemical processes required for battery development and acceleration of the transition to a sustainable energy system.
are there any problems that quantum computers cannot solve?
prof. hauswirth: quantum computing is not really suitable for processes in which the next step depends on the result of the previous one - i.e., where computations build on one another - which is the case in classic business processes.
day- to- day data- processing applications require computation on a much smaller scale for which traditional computers will always be more efficient and more economical.
quantum computing will complement conventional computing for some types of problems but will not replace all computers.
in many areas, it will also make sense to use a hybrid solution of conventional and quantum computing.
are there any concrete hurdles that still prevent a breakthrough of quantum computing?
prof. hauswirth: the stability of qubits and the stability of entanglement are vital for the performance of a quantum computer.
today's quantum computer hardware is still extremely sensitive, and the technical requirements are very high.
for example, a quantum processor must be cooled to a temperature approaching absolute zero; and it must be protected against any form of radiation and vibration.
at present, a lot of research in this field is focusing on how to reduce the technical efforts required to achieve this.
how long do we have to wait until we see the beginnings of a widespread commercial use of quantum computing?
prof. hauswirth: it's going to take at least 10 years, if not 20, before quantum computers emerge from the lab and find widespread applications.
what's more important, however, is that when we reach this point, we are ready with the necessary software and expertise.
we already know where the journey's heading, because we're helping to draw the map.
fraunhofer has a key role to play here.
our job is to help bring industry and applied research into the best position and ensure they are ready.
the interview was conducted by mandy bartel.
in 1975, swedish physicist goran lindblad developed a theorem that describes the change in entropy that occurs during a quantum measurement.
today, this theorem is a foundational component of quantum information theory, underlying such important concepts as the uncertainty principle, the second law of thermodynamics, and data transmission in quantum communication systems.
now, 40 years later, physicist mark m. wilde, assistant professor at louisiana state university, has improved this theorem in a way that allows for understanding how quantum measurements can be approximately reversed under certain circumstances.
the new results allow for understanding how quantum information that has been lost during a measurement can be nearly recovered, which has potential implications for a variety of quantum technologies.
quantum relative entropy never increases
most people are familiar with entropy as a measure of disorder and the law that "entropy never decreases"- it either increases or stays the same during a thermodynamic process, according to the second law of thermodynamics.
however, here the focus is on "quantum relative entropy," which in some sense is the negative of entropy, so the reverse is true: quantum relative entropy never increases, but instead only decreases or stays the same.
in fact, this was the entropy inequality theorem that lindblad proved in 1975: that the quantum relative entropy cannot increase after a measurement.
in this context, quantum relative entropy is interpreted as a measure of how well one can distinguish between two quantum states, so it's this distinguishability that can never increase.
(wilde describes a proof of lindblad's result in greater detail in his textbook quantum information theory, published by cambridge university press.)
one thing that lindblad's proof doesn't address, however, is whether it makes any difference if the quantum relative entropy decreases by a little or by a lot after a measurement.
in the new paper, wilde has shown that, if the quantum relative entropy decreases by only a little, then the quantum measurement (or any other type of so- called "quantum physical evolution") can be approximately reversed.
"when looking at lindblad's entropy inequality, a natural question is to wonder what we could say if the quantum relative entropy goes down only by a little when the quantum physical evolution is applied," wilde told phys.org.
"it is quite reasonable to suspect that we might be able to approximately reverse the evolution.
this was arguably open since the work of lindblad in 1975, addressed in an important way by denes petz in the late 1980s (for the case in which the quantum relative entropy stays the same under the action of the evolution), and finally formulated as a conjecture around 2008 by andreas winter.
what my work did was to prove this result as a theorem: if the quantum relative entropy goes down only by a little under a quantum physical evolution, then we can approximately reverse its action."
wide implications
wilde's improvements to lindblad's theorem have a variety of implications, but the main one that wilde discusses in his paper is how the new results allow for recovering quantum information.
"if the decrease in quantum relative entropy between two quantum states after a quantum physical evolution is relatively small," he said, "then it is possible to perform a recovery operation, such that one can perfectly recover one state while approximately recovering the other.
this can be interpreted as quantifying how well one can reverse a quantum physical evolution."
so the smaller the relative entropy decrease, the better the reversal process.
the ability to recover quantum information could prove useful for quantum error correction, which aims to protect quantum information from damaging external effects.
wilde plans to address this application more in the future with his colleagues.
as wilde explained, lindblad's original theorem can also be used to prove the uncertainty principle of quantum mechanics in terms of entropies, as well as the second law of thermodynamics for quantum systems, so the new results have implications in these areas, as well.
"lindblad's entropy inequality underlies many limiting statements, in some cases said to be physical laws or principles," wilde said.
"examples are the uncertainty principle and the second law of thermodynamics.
another example is that this entropy inequality is the core step in determining limitations on how much data we can communicate over quantum communication channels.
we could go as far as to say that the above entropy inequality constitutes a fundamental law of quantum information theory, which is a direct mathematical consequence of the postulates of quantum mechanics."
regarding the uncertainty principle, wilde and two coauthors, mario berta and stephanie wehner, discuss this angle in a forthcoming paper.
they explain that the uncertainty principle involves quantum measurements, which are a type of quantum physical evolution and therefore subject to lindblad's theorem.
in one formulation of the uncertainty principle, two experiments are performed on different copies of the same quantum state, with both experimental outcomes having some uncertainty.
"the uncertainty principle is the statement that you cannot generally make the uncertainties of both experiments arbitrarily small, i.e., there is generally a limitation," wilde said.
"it is now known that a statement of the uncertainty principle in terms of entropies can be proved by using the 'decrease of quantum relative entropy inequality.'
so what the new theorem allows for doing is relating the uncertainties of the measurement outcomes to how well we could try to reverse the action of one of the measurements.
that is, there is now a single mathematical inequality which captures all of these notions."
in terms of the second law of thermodynamics, wilde explains how the new results have implications for reversing thermodynamic processes in both classical and quantum systems.
"the new theorem allows for quantifying how well we can approximately reverse a thermodynamic transition from one state to another without using any energy at all," he said.
he explained that this is possible due to the connection between entropy, energy, and work.
according to the second law of thermodynamics, a thermodynamic transition from one quantum state to another is allowed only if the free energy decreases from the original state to the final state.
during this process, one can gain work and store energy.
this law can be rewritten as a statement involving relative entropies and can be proved as a consequence of the decrease of quantum relative entropy.
"what my new work with stephanie wehner and mischa woods allows for is a refinement of this statement," wilde said.
"we can say that if the free energy does not go down by very much under a thermodynamic transition (i.e., if there is not too much work gained in the process), then it is possible to go back approximately to the original state from the final state, without investing any work at all.
the key word here is that you can go back only approximately, so we are not in violation of the second law, only providing a refinement of it."
in addition to these implications, the new theorem can also be applied to other research topics in quantum information theory, including the holevo bound, quantum discord, and multipartite information measures.
wilde's work was funded in part by the darpa quiness program (ending now), which focused on quantum key distribution, or using quantum mechanics to ensure secret communication between two parties.
he describes more about this application, in particular how alice and bob might use a quantum state to share secrets that can be kept private from an eavesdropper eve (and help them survive being attacked by a bear), in a recent blog post.
more information: goran lindblad.
"completely positive maps and ... tropy inequalities."
communications in mathematical physics.
june 1975, volume 40, issue 2, pp 147- 151
mark m. wilde.
"recoverability in quantum information theory."
proceedings of the royal society a. doi: 10.1098/rspa.2015.0338
also at arxiv:1505.04661 [quant- ph]
stephanie wehner, mark m. wilde, and mischa p. woods.
"work and reversibility in quantum thermodynamics."
arxiv:1506.08145 [quant- ph]
in a very recent preprint at arxiv:1509.07127 [quant- ph], wilde and his colleagues have further improved upon the result from the paper in the proceedings of the roayl society a, which should allow for it to be applied even more widely and potentially allow for experimental tests.
quantum computing is heralded as the next revolution in terms of global computing.
google, intel and ibm are just some of the big names investing millions currently in the field of quantum computing which will enable faster, more efficient computing required to power the requirements of our future computing needs.
now a researcher and his team at tyndall national institute in cork have made a 'quantum leap' by developing a technical step that could enable the use of quantum computers sooner than expected.
conventional digital computing uses 'on- off' switches, but quantum computing looks to harness quantum state of matters- such as entangled photons of light or multiple states of atoms- to encode information.
in theory, this can lead to much faster and more powerful computer processing, but the technology to underpin quantum computing is currently difficult to develop at scale.
researchers at tyndall have taken a step forward by making quantum dot light- emitting diodes (leds) that can produce entangled photons (whose actions are linked), theoretically enabling their use to encode information in quantum computing.
this is not the first time that leds have been made that can produce entangled photons, but the methods and materials described in the new paper have important implications for the future of quantum technologies, explains researcher dr emanuele pelucchi, head of epitaxy and physics of nanostructures and a member of the science foundation ireland- funded irish photonic integration centre (ipic) at tyndall national institute in cork.
"the new development here is that we have engineered a scalable array of electrically driven quantum dots using easily- sourced materials and conventional semiconductor fabrication technologies, and our method allows you to direct the position of these sources of entangled photons," he says.
"being able to control the positions of the quantum dots and to build them at scale are key factors to underpin more widespread use of quantum computing technologies as they develop."
the tyndall technology uses nanotechnology to electrify arrays of the pyramid- shaped quantum dots so they produce entangled photons.
"we exploit intrinsic nanoscale properties of the whole "pyramidal" structure, in particular, an engineered self- assembled vertical quantum wire, which selectively injects current into the vicinity of a quantum dot," explains dr pelucchi.
"the reported results are an important step towards the realisation of integrated quantum photonic circuits designed for quantum information processing tasks, where thousands or more sources would function in unison."
"it is exciting to see how research at tyndall continues to break new ground, particularly in relation to this development in quantum computing.
the significant breakthrough by dr pelucchi advances our understanding of how to harness the opportunity and power of quantum computing and undoubtedly accelerates progress in this field internationally.
photonics innovations by the ipic team at tyndall are being commercialised across a number sectors and as a result, we are directly driving global innovation through our investment, talent and research in this area," said dr kieran drain, ceo at tyndall national institute.
an international research group of scientists and engineers led by the university of bristol, uk, has made an important advance towards a quantum computer by shrinking down key components and integrating them onto a silicon microchip.
engineers from an international collaboration led by dr mark thompson from the university of bristol have, for the first time, generated and manipulated single particles of light (photons) on a silicon chip - a major step forward in the race to build a quantum computer.
quantum computers and quantum technologies in general are widely anticipated as the next major technology advancement, and are poised to replace conventional information and computing devices in applications ranging from ultra- secure communications and high- precision sensing to immensely powerful computers.
quantum computers themselves will likely lead to breakthroughs in the design of new materials and in the discovery of new medical drugs.
whilst still in their infancy, quantum technologies are making rapid process, and a revolutionary new approach pioneered by the university of bristol is exploiting state- of- the- art engineering processes and principles to make leaps and bounds in a field previously dominated by scientists.
featuring on the front cover of nature photonics, this latest advancement is one of the important pieces in the jigsaw needed in order to realise a quantum computer.
while previous attempts have required external light sources to generate the photons, this new chip integrates components that can generate photons inside the chip.
"we were surprised by how well the integrated sources performed together," admits joshua silverstone, lead author of the paper.
"they produced high- quality identical photons in a reproducible way, confirming that we could one day manufacture a silicon chip with hundreds of similar sources on it, all working together.
this could eventually lead to an optical quantum computer capable of performing enormously complex calculations."
group leader mark thompson explained: "single- photon detectors, sources and circuits have all been developed separately in silicon but putting them all together and integrating them on a chip is a huge challenge.
our device is the most functionally complex photonic quantum circuit to date, and was fabricated by toshiba using exactly the same manufacturing techniques used to make conventional electronic devices.
we can generate and manipulate quantum entanglement all within a single mm- sized micro- chip."
the group, which, includes researchers from toshiba corporation (japan), stanford university (us), university of glasgow (uk) and tu delft (the netherlands), now plans to integrate the remaining necessary components onto a chip, and show that large- scale quantum devices using photons are possible.
"our group has been making steady progress towards a functioning quantum computer over the last five years," said thompson.
"we hope to have within the next couple of years, photon- based devices complex enough to rival modern computing hardware for highly- specialised tasks."
however, these are just the first steps.
to realise useful quantum machines will required a new breed of engineering - quantum engineers, individuals capable of understanding the fundamentals of quantum mechanics and applying this knowledge to real world problems.
bristol's newly established centre for doctoral training in quantum engineering will train a new generation of engineers, scientists and entrepreneurs to harness the power of quantum mechanics and lead the quantum technology revolution.
this innovative centre bridges the gaps between physics, engineering, mathematics and computer science, working closely with chemists and biologists while interacting strongly with industry.
when two parties use a quantum system to share information, the amount of quantum information that can be communicated is fundamentally limited by quantum properties.
now in a new paper, damian pitalua- garcia, a scientist in the university of cambridge's centre for quantum information and foundations in the department of applied mathematics and theoretical physics, has proposed a principle that can determine the maximum amount of quantum information that a quantum system can communicate.
according to this principle, the maximum amount of information is limited only by the quantum system's dimension, and does not depend on any physical resources previously shared by the communicating parties.
pitalua- garcia's paper, called "quantum information causality," is published in a recent issue of physical review letters.
specifically, pitalua- garcia's principle of quantum information causality says that, after a quantum system of m qubits is transmitted from one party to another, the quantum information shared between the two parties cannot increase by more than 2m.
as pitalua- garcia explains, this limit is the maximum amount of information that a quantum system can fundamentally communicate, regardless of how technologically advanced it may be and how much quantum entanglement the communicating parties share.
"the principle of information causality states that m classical bits can transmit m's worth of information," pitalua- garcia told phys.org.
"on the other hand, quantum information causality states that m qubits can transmit 2m's worth of information.
in this sense, a qubit can communicate twice the amount of information that a classical bit can communicate.
this might seem strange because, after being measured, a qubit reduces to a classical bit.
however, a qubit can be entangled with another qubit, while a classical bit cannot.
it is entanglement that allows a qubit to communicate more information than a classical bit."
in his paper, he showed that quantum information causality follows from three mathematical properties satisfied by quantum information.
while the maximum amount of information is independent of any previously shared quantum physical resources, it does depend on the quantum system's dimension.
"the dimension of a quantum system can be understood as the number of different possible outcomes that are obtained when the system is subject to a measurement," pitalua- garcia said.
"for example, a qubit has dimension two, because it gives one of two possible measurement results.
similarly, a system of m qubits has dimension 2^m.
it is thus natural to expect that a system with bigger dimension can communicate more quantum information.
this is proved mathematically by quantum information causality."
in order to illustrate the limit imposed by quantum information causality and come as close as possible to reaching this limit, pitalua- garcia presented a new quantum game.
he found that an optimal strategy in this game is a quantum teleportation strategy.
although this method is not the first for determining the maximum of quantum information that can be communicated by a quantum system, it is different because it does not involve any classical components.
"other methods can determine the maximum amount of information that can be transmitted by a quantum system, but in different scenarios," pitalua- garcia said.
"for example, [in some scenarios,] the communicated information is classical, as stated in a theorem by holevo in 1973, or the transmitted system is classical, as published in 2009 in the principle of information causality.
our approach considers the scenario in which the transmitted and the communicated information are both quantum and the communicating parties share any quantum physical resources.
this scenario is more general because, fundamentally, every system is quantum, and a classical system is a special class of quantum system."
overall, the principle of quantum information causality may have implications for the broad field of quantum information, which deals with how information can be fundamentally encoded, processed, and communicated using quantum systems.
the world's first entanglement of quantum bits (qubits) in a solid- state superconducting josephson junction circuit has been reported by university of maryland researchers.
though far from a quantum logic circuit and even farther from a quantum computer, the demonstration holds hope that engineering improvements could someday produce such a computer.
"our findings indicate that you could use josephson junctions to build a quantum computer," said professor fred wellstood, leader of the project and director of the university's center for superconductivity research.
others contributing to the project include andrew berkley, mark gubrud, joseph foley, matt kenyon, jan olaf gaudestad, huizhong xu and roberto ramos.
unlike the usual physical states such as voltage or current used to encode information in electronic circuits, quantum states have only a probable existence until they are measured.
that counters intuition, but it also offers a high degree of parallelism since all possible states of an elementary particle have a probable existence simultaneously.
there are many different quantum statesfrom electron spin direction to photon polarization anglesbut all share the common ability to exist in a nebulous state that is not resolved until an observation of them is made.
by encoding "1s" and "0s" in these unresolved nebulous states, quantum computers promise to perform parallel operations on both values in a single step.
quantum entanglement is important because it enables the final result to be read out without disturbing the sequence of parallel operations.
wellstood said entanglement blurs the distinction between individual particles so that it is impossible to describe the particles separately no matter how far away they are physically separated.
"we view entanglement as essential to quantum computing because it packs more information into quantum bits than is possible with classical computing bits.
six quantum bits, for instance, can represent 64 pieces of information," said berkley.
wellstood claims his approach "scales up" more easily than competing methods because it is based on solid- state electronic devices, not free- floating subatomic particles.
by demonstrating entanglement between two josephson junctions, wellstood said he has provided important evidence that quantum computers are possible.
the josephson junction device used by the team is composed of two superconductors separated by an insulating layer so thin that electrons can tunnel through it.
otherwise he uses the same techniques used to make conventional ics, concluding that his approach is well suited for scaling up.
"we can go to the thousands of devices you need to build a real working quantum computer," said wellstood.
wellstood's work builds on that of at&t's albert chan, whose nearest- neighbor qubits were shown to be capable of interbit coupling that potentially performs calculations without resolving the qubits nebulous quantum state.
the space race of the cold war saw the us and russia develop a wide range of aerospace technologies.
but a new race is about to begin, mostly among corporations, and the winner will unlock the power of the quantum computing realm.
the security rapture
in the past 30 years, computing has given rise to many key technologies, most importantly the internet, which have had a massive impact on the global economy.
banks can be accessed remotely, shopping can be done using a phone, and stocks can be traded at the push of a button.
but sending messages to perform such tasks must be encrypted to prevent attackers from obtaining sensitive information.
as a result, many different forms of cryptography have been developed to encrypt messages that would take billions of years for a computer to attack.
prime numbers are key to modern cryptography systems due to their difficulty in calculation and so prime numbers have to be found as opposed to being calculated (i.e., there is no general formula for predicting prime numbers).
while there have always been vulnerabilities in computing, we're about to enter a new era of dealing with cryptological threats.
practical quantum computers are very close to becoming a reality.
quantum computers, unlike traditional computers, utilize quantum effects to perform calculations on quantum states which can perform multiple calculations simultaneously.
in fact, 30 qubits alone can perform 1 billion calculations simultaneously.
with the ability to perform such calculations, quantum computers could trivially solve modern cryptographic systems, partly because finding very large prime numbers would be much easier.
quantum computers are already in existence but they are not yet practical machines- each utilizes a relatively small number of qubits and they're mainly for scientific research.
but when quantum computers become more common, emphasis on quantum- level security will be of paramount importance.
this is why the race among corporations to build practical quantum computers (and security measures against them) has begun.
here's a look at some recent initiatives surrounding the issue of quantum computing.
programming quantum computers
even though current quantum computer systems are not yet practical, many are already trying to tackle the issue of how to appropriately program them.
traditional computers (those based on good old binary), are linear systems and run instructions one by one which makes creating and following programs easy.
a quantum computer, however, is more of a mathematical machine whereby complex quantum algorithms are configured using quantum gates and then quantum data fed into the system n number of times to produce a result.
however, tech companies have started to think about how quantum computers can actually be used.
microsoft's new language, q#, is a quantum computer language designed to run on classical computers so that users can develop quantum projects in the visual studio environment.
microsoft has also released four tutorials, called "katas", that teach the basics of quantum computing.
each tutorial has examples and outputs that allow the user to test their quantum programs to see if they are correct.
the four tutorials cover basic quantum gates, superposition, measurements, and the deutsch- jozsa algorithm.
meanwhile, google has recently announced cirq, an open- source framework for developing quantum algorithms and programs on nisq computers (noisy intermediate scale quantum).
while currently, quantum algorithms developed do not run on a real quantum computer, they do provide a framework for development.
like q#, cirq can be downloaded via github but is coded in python, which allows development of quantum algorithms on most platforms.
interestingly, cirq allows the creation of quantum circuits and gates by simulating molecules and properties of materials- but, as they are simulations, performing large quantum calculations is impossible (which is to say that security systems are theoretically safe for now).
an incubator for quantum development
ibm, one of the companies on the quantum frontlines, has created a network called the "ibm q network".
its main aim is to bring leading companies together to help push the boundaries of quantum computing technologies and explore practical applications for business and science.
in their own words, the goal of the program is to "cut through the hype and focus on the present state of quantum computing and how organizations, and developers can prepare for the future."
who are the companies in this program and where are they from?
a quick bit of research shows that they are companies mainly situated in the west such as zapta computing (cambridge), strangeworks (austin), and qc ware (palo alto).
legislators attempt to keep up
this quantum race is also seeing legislation in law.
last month, texas congressman will hurd warned of a computing threat from competitors such as china and russia.
in response to this threat, hurd has highlighted a proposal in congress, the national quantum initiative act, that would put position the us as a leader in quantum information science.
introduced by fellow texan, u.s. house science, space, and technology committee chairman lamar smith, the national quantum initiative act aims to do things like support quantum development on us soil, identify and address "research gaps" and employment issues for american quantum research, and establish a national quantum coordination office to organize these efforts.
conclusion
the era of quantum computing is approaching.
when practical quantum computers arrive, current cryptographic methods will not be able to protect sensitive data.
but who will get the technology first?
will the power be used to exploit banks and the stock market?
or will it be used to immediately create new cryptographic solutions?
the reality is probably a combination of the two- quantum computing can and will be used for good and for ill.
what would you like to learn about when it comes to quantum computers?
let us know in the comments below.
featured image courtesy of ibm.
read more
a significant step towards ultra- high speed quantum computers
the core circuits of quantum teleportation, which generate and detect quantum entanglement, have been successfully integrated into a photonic chip by an international team of scientists from the universities of bristol, tokyo, southampton and ntt device technology laboratories.
these results pave the way to developing ultra- high- speed quantum computers and strengthening the security of communication.
qubits (quantum bits) are sensitive quantum versions of today's computer 0's and 1's (bits) and are the foundation of quantum computers.
photons are particles of light and they are a promising way to implement excellent qubits.
one of the most important tasks is to successfully enable quantum teleportation, which transfers qubits from one photon to another.
however, the conventional experimental implementation of quantum teleportation fills a laboratory and requires hundreds of optical instruments painstakingly aligned, a far cry from the scale and robustness of device required in a modern day computer or handheld device.
in 2013, professor furusawa and his colleagues succeeded in realising perfect quantum teleportation, however, this required a set- up covering several square metres; took many months to build, and reached the limit in terms of scalability.
new research at the university of bristol led by professor jeremy o'brien has taken those optical circuits and implemented them on to a silicon microchip measuring just a few millimetres (0.0001 square metres) using state- of- the- art nano- fabrication methods.
this is the first time quantum teleportation has been demonstrated on a silicon chip and the result has radically solved the problem of scalability.
the team of researchers have taken a significant step closer towards their ultimate goal of integrating a quantum computer into a photonic chip.
while there has been significant progress in current computing technology, its performance is now reaching the fundamental limit of classical physics.
on the other hand, it has been predicted that principles of quantum mechanics will enable the development of ultra- secure quantum communication and ultra- powerful quantum computers, overcoming the limit of current technologies.
one of the most important steps in achieving this is to establish technologies for quantum teleportation (transferring signals of quantum bits in photons from a sender to a receiver at a distance).
the implementation of teleportation on to a micro- chip is an important building block unlocking the potential for practical quantum technologies.
professor akira furusawa from the university of tokyo said: "this latest achievement enables us to perform the perfect quantum teleportation with a photonic chip.
the next step is to integrate whole the system of quantum teleportation."
professor jeremy o'brien, director of the centre for quantum photonics at the university of bristol, who led the bristol elements of the research, said: "being able to replicate an optical circuit which would normally require a room sized optical table on a photonic chip is a hugely significant achievement.
in effect, we have reduced a very complex quantum optical system by ten thousand in size."
the research is published this week in nature photonics.
in recent years, quantum devices have become available that enable researchers- for the first time- to use real quantum hardware to begin to solve scientific problems.
however, in the near term, the number and quality of qubits (the basic unit of quantum information) for quantum computers are expected to remain limited, making it difficult to use these machines for practical applications.
a hybrid quantum and classical approach may be the answer to tackling this problem with existing quantum hardware.
researchers at the u.s. department of energy's (doe) argonne national laboratory and los alamos national laboratory, along with researchers at clemson university and fujitsu laboratories of america, have developed hybrid algorithms to run on quantum machines and have demonstrated them for practical applications using ibm quantum computers (see below for description of argonne's role in the ibm q hub at oak ridge national laboratory [ornl]) and a d- wave quantum computer.
"this approach will enable researchers to use near- term quantum computers to solve applications that support the doe mission.
for example, it can be applied to find community structures in metabolic networks or a microbiome," says yuri alexeev, principal project specialist, computational science division
the team's work is presented in an article entitled "a hybrid approach for solving optimization problems on small quantum computers" that appears in the june 2019 issue of the institute of electrical and electronics engineers (ieee) computer magazine.
concerns about qubit connectivity, high noise levels, the effort required to correct errors, and the scalability of quantum hardware have limited researchers' ability to deliver the solutions that future quantum computing promises.
the hybrid algorithms that the team developed employ the best features and capabilities of both classical and quantum computers to address these limitations.
for example, classical computers have large memories capable of storing huge datasets- a challenge for quantum devices that have only a small number of qubits.
on the other hand, quantum algorithms perform better for certain problems than classical algorithms.
to distinguish between the types of computation performed on two completely different types of hardware, the team referred to the classical and quantum stages of hybrid algorithms as central processing units (cpus) for classical computers and quantum processing units (qpus) for quantum computers.
the team seized on graph partitioning and clustering as examples of practical and important optimization problems that can already be solved using quantum computers: a small graph problem can be solved directly on a qpu, while larger graph problems require hybrid quantum- classical approaches.
as a problem became too large to run directly on quantum computers, the researchers used decomposition methods to break the problem down into smaller pieces that the qpu could manage- an idea they borrowed from high- performance computing and classical numerical methods.
all the pieces were then assembled into a final solution on the cpu, which not only found better parameters, but also identified the best sub- problem size to solve on a quantum computer.
such hybrid approaches are not a silver bullet; they do not allow for quantum speedup because using decomposition schemes limits speed as the size of the problem increases.
in the next 10 years, though, expected improvements in qubits (quality, count, and connectivity), error correction, and quantum algorithms will decrease runtime and enable more advanced computation.
"in the meantime," according to yuri alexeev, principal project specialist in the computational science division, "this approach will enable researchers to use near- term quantum computers to solve applications that support the doe mission.
for example, it can be applied to find community structures in metabolic networks or a microbiome."
more information: ruslan shaydulin et al, a hybrid approach for solving optimization problems on small quantum computers, computer (2019).
doi: 10.1109/mc.2019.2908942
quantum computers promise to perform certain types of operations much more quickly than conventional digital computers.
but many challenges must be addressed before these ultra- fast machines become available, among them, the loss of order in the systems - a problem known as quantum decoherence - which worsens as the number of bits in a quantum computer increases.
one proposed solution is to divide the computing among multiple small quantum computers that would work together much as today's multi- core supercomputers team up to tackle big digital operations.
the individual computers in such a system could communicate quantum information using bose- einstein condensates (becs) - clouds of ultra- cold atoms that all exist in exactly the same quantum state.
the approach could address the decoherence problem by reducing the number of bits necessary for a single computer.
now, a team of physicists at the georgia institute of technology has examined how this bose- einstein communication might work.
the researchers determined the amount of time needed for quantum information to propagate across their bec, essentially establishing the top speed at which such quantum computers could communicate.
"what we did in this study was look at how this kind of quantum information would propagate," said chandra raman, an associate professor in georgia tech's school of physics.
"we are interested in the dynamics of this quantum information flow not just for quantum information systems, but also more generally for fundamental problems in physics."
the research is scheduled to be published in the april 19 online edition of the journal physical review letters.
the research was funded by the u.s. department of energy (doe) and the national science foundation (nsf).
the work involved both an experimental physics group headed by raman and a theoretical physics group headed by associate professor carlos sa de melo, also in the georgia tech school of physics.
the researchers first assembled a gaseous bose- einstein condensate that consisted of as many as three million sodium atoms cooled to nearly absolute zero.
to begin the experiment, they switched on a magnetic field applied to the bec that instantly placed the system out of equilibrium.
that triggered spin- exchange collisions as the atoms attempted to transition from one ground state to a new one.
atoms near one another became entangled, pairing up with one atom's spin pointing up, and the other's pointing down.
this pairing of opposite spins created a correlation between pairs of atoms that moved through the entire bec as it established a new equilibrium.
the researchers, who included graduate student anshuman vinit and former postdoctoral fellow eva bookjans, measured the correlations as they spread through the cloud of cold atoms.
at first, the quantum entanglement was concentrated in space, but over time, it spread outward like drop of dye diffuses through water.
"you can imagine having a drop of dye that is concentrated at one point in space," raman said.
"through diffusion, the dye molecules move throughout the water, slowly spreading throughout the entire system."
the research could help scientists anticipate the operating speed for a quantum computing system composed of many cores communicating through a bec.
"this propagation takes place on the time scale of ten to a hundred milliseconds," raman said.
"this is the speed at which quantum information naturally flows through this kind of system.
if you were to use this medium for quantum communication, that would be its natural time scale, and that would set the timing for other processes."
though relevant to communication of quantum information, the process also showed how a large system undergoing a phase transition does so in localized patches that expand to attempt to incorporate the entire system.
"an extended system doesn't move from one phase to another in a uniform way," said raman.
"it does this locally.
things happen locally that are not connected to one another initially, so you see this inhomogeneity."
beyond quantum computing, the results may also have implications for quantum sensing - and for the study of other physical systems that undergo phase transitions.
"phase transitions have universal properties," raman noted.
"you can take the phase transitions that happen in a variety of systems and find that they are described by the same physics.
it is a unifying principle."
raman hopes the work will lead to new ways of thinking about quantum computing, regardless of its immediate practical use.
"one paradigm of quantum computing is to build a linear chain of as many trapped ions as possible and to simultaneously engineer away as many challenges as possible," he said.
"but perhaps what may be successful is to build these smaller quantum systems that can communicate with one another.
it's important to try as many things as possible and to keep an open mind.
we need to try to understand these systems as well as we can."
the co- founder of the first successful quantum technology company claimed this week to have demonstrated macroscopic quantum entanglement, a breakthrough that could be help to develop quantum communications networks.
in a presentation at international conference on quantum technologies (icqt) here this week, nicolas gisin, a board member at geneva- based quantum key distribution system vendor id quantique, showed quantum entanglement involving two macroscopic optically distinct states that render a quantum phenomenon visible to the naked eye.
he also showed how the technique can be automated with simple non- quantum detectors which are sensitive only to the number of photons present as opposed to the exotic quantum measurement techniques necessary today.
in his talk, he began by describing a new quantum memory technology, developed at id quantique to facilitate long- distance communications of quantum- bits (q- bits), which make use of quantum- entanglement and - teleportation in order to transfer q- bits from photons to ultra- cool atoms trapped inside a photonic crystal.
"to make a quantum communications network, you need the quantum equivalent of repeater nodes, which require coherent and reversibly mapped quantum entanglement with a quantum memory," said gisin.
gisin's quantum memory works using an ensemble of up to 10 billion super cooled rare- earth ions inside a photonic crystal, which have been forced into a huge superposition state.
the technique was able to demonstrate an entanglement- reserving quantum memory transferred the q- bits to the ions using light- to- crystal quantum teleportation that transferred the quantum state with a fidelity of 93 percent.
the technique enabled what gisin called "large entanglment" of thousands of photons trapped inside a photonic crystal large enough to hold in your hand.
the entangled bits (e- bits) were thus large enough to be viewed with the naked eye, and could be automatically detecting using non- quantum "classical" detectors to determine their state.
in experiments, gisin's showed how single- photon entanglement could be transferred to the large ensemble of photons using an optical fiber coupler, which was verified by separating out a single photon from the ensemble and using an analyzer with an interferometer that measured their entanglement.
and by combining with the quantum memory at telecommunications wavelengths, gisim's set- up allowed the temporary storage of thousands of e- bits in a solid state quantum memory.
r. colin johnson has been a technology editor at ee times since 1986, covering next- generation electronics technologies.
he's the author of the book, cognizers - neural networks and machines that think, is acontributing editor on slashdot.org, and is a kyoto prize journalism fellow for his coverage of advanced technologies and international issues.
quantum computing has the potential to tackle problems conventional computers can't handle, such as discovering how diseases develop and creating more effective drugs to treat them.
it exploits fundamental laws of physics to solve complex computing problems in new ways that are not well served by classical computers.
the potential of its massive, parallel- computing power has driven academia, government and private companies alike in a race to invent it.
although we're in the early phases, quantum computing research has advanced from an academic discussion of physics to the production of small systems.
that's why intel is encouraged by bipartisan progress on the national quantum initiative act, versions of which are making their way through the house and senate.
the bill aims to ensure u.s. leadership in quantum computing by supporting research and development, the creation of international standards, interagency planning and coordination and public- private partnerships.
it represents a fundamental step in advancing u.s. competitiveness and shines a light on three fundamental needs.
first, the bill launches a new moonshot.when president kennedy told americans, "we choose to go to the moon," he set the agenda for years of innovation.
he realized the need to bring research communities together to work toward a shared objective.
with this common direction, we'll be able to concentrate funding for quantum research projects to solve the critical and universal challenges to advance the field.
we cannot spread ourselves too thin by investigating so many vectors that we are unable to support our big goals.
second, we need to unify public, private and academic interests.this opportunity will only succeed if we can bring together the different parties working to advance quantum science.
from academics and researchers to technology industry leaders to policymakers, it's imperative that all parties work together if we are going to move one step closer to technology's next giant leap.
finally, we need to build a workforce for what's next.
we'll need to train and empower a new generation of computer scientists, engineers, designers and technicians for the quantum era of computing.
the experts we need include device physicists and process engineers who understand circuit fabrication at the single electron or photon limit, computer system designers and architects who can co- design a quantum system and a new set of thinkers who can take advantage of quantum platforms in ways we have yet to consider.
the u.s. has long been at the cutting edge of technology.
the national quantum initiative act is clear sign that, as a nation, we have no intention of letting up.
intel applauds the bipartisan efforts of congress and will continue to partner with the research community worldwide to advance our collective understanding of quantum information sciences and help deliver this exciting, future era of computing.
- jim clarke is director of quantum hardware at intel labs and launched intel's quantum computing effort in 2015 with a primary focus on using intel's process expertise to develop scalable qubit arrays.
professor michelle simmons' team at unsw sydney has demonstrated a compact sensor for accessing information stored in the electrons of individual atoms- a breakthrough that brings us one step closer to scalable quantum computing in silicon.
the research, conducted within the simmons group at the centre of excellence for quantum computation and communication technology (cqc2t) with ph.d. student prasanna pakkiam as lead author, was published today in the prestigious journal physical review x.
quantum bits (or qubits) made from electrons hosted on single atoms in semiconductors is a promising platform for large- scale quantum computers, thanks to their long- lasting stability.
creating qubits by precisely positioning and encapsulating individual phosphorus atoms within a silicon chip is a unique australian approach that simmons' team has been leading globally.
but adding in all the connections and gates required for scale up of the phosphorus atom architecture was going to be a challenge- until now.
"to monitor even one qubit, you have to build multiple connections and gates around individual atoms, where there is not a lot of room," says professor simmons.
"what's more, you need high- quality qubits in close proximity so they can talk to each other- which is only achievable if you've got as little gate infrastructure around them as possible."
compared with other approaches for making a quantum computer, simmons' system already had a relatively low gate density.
yet conventional measurement still required at least 4 gates per qubit: 1 to control it and 3 to read it.
by integrating the read- out sensor into one of the control gates the team at unsw has been able to drop this to just two gates: 1 for control and 1 for reading.
"not only is our system more compact, but by integrating a superconducting circuit attached to the gate we now have the sensitivity to determine the quantum state of the qubit by measuring whether an electron moves between two neighbouring atoms," lead author pakkiam states.
"and we've shown that we can do this real- time with just one measurement- single shot- without the need to repeat the experiment and average the outcomes."
"this represents a major advance in how we read information embedded in our qubits," concludes simmons.
"the result confirms that single- gate reading of qubits is now reaching the sensitivity needed to perform the necessary quantum error correction for a scalable quantum computer."
australia's first quantum computing company
since may 2017, australia's first quantum computing company, silicon quantum computing pty limited (sqc), has been working to create and commercialise a quantum computer based on a suite of intellectual property developed at the australian centre of excellence for quantum computation and communication technology (cqc2t).
co- located with cqc2t on the unsw campus in sydney, sqc is investing in a portfolio of parallel technology development projects led by world- leading quantum researchers, including australian of the year and laureate professor michelle simmons.
its goal is to produce a 10- qubit demonstration device in silicon by 2022 as the forerunner to a commercial scale silicon- based quantum computer.
sqc believes that quantum computing will ultimately have a significant impact across the global economy, with possible applications in software design, machine learning, scheduling and logistical planning, financial analysis, stock market modelling, software and hardware verification, climate modelling, rapid drug design and testing, and early disease detection and prevention.
created via a unique coalition of governments, corporations and universities, sqc is competing with some of the largest tech multinationals and foreign research laboratories.
as well as developing its own proprietary technology and intellectual property, sqc will continue to work with cqc2t and other participants in the australian and international quantum computing ecosystems, to build and develop a silicon quantum computing industry in australia and, ultimately, to bring its products and services to global markets.
physicists at eth zurich have demonstrated a five- meter- long microwave quantum link, the longest of its kind to date.
it can be used both for future quantum computer networks and for experiments in basic quantum physics research.
collaboration is everything- also in the quantum world.
to build powerful quantum computers in the future, it will be necessary to connect several smaller computers to form a kind of cluster or local network (lan).
since those computers work with quantum mechanical superposition states, which contain the logical values "0" and "1" at the same time, the links between them should also be "quantum links."
the longest such link to date based on microwaves, at five meters long, was recently built in the laboratory of andreas wallraff, professor at the quantum device lab at eth zurich.
the researchers were scheduled to present their results on it at the annual meeting of the american physical society in denver.
because of the covid- 19 epidemic situation, this conference was canceled at short notice.
instead, the scientists now report their results at a virtual substitute conference.
"that's really a milestone for us," wallraff explains, "since now we can show that quantum- lans are possible in principle.
in the next 10 to 20 years, quantum computers will probably increasingly rely on them."
currently there are computers with a few dozen quantum bits or qubits, but several hundreds of thousands of them are almost impossible to accommodate in existing devices.
one reason for this is that qubits based on superconducting electrical oscillators, such as those used in the quantum chips in wallraff's lab (and also by ibm and google), need to be cooled down to temperatures close to the absolute zero of - 273,15 degrees celsius.
this supresses thermal perturbations that would cause the quantum states to lose their superposition property- this is known as decoherence- and hence errors in the quantum calculations to occur.
extreme cold against decoherence
"the challenge was to connect two of those superconducting quantum chips in such a way as to be able to exchange superposition states between them with minimal decoherence," says philipp kurpiers, a former ph.d. student in wallraff's group.
this happens by means of microwave photons that are emitted by one superconducting oscillator and received by another.
in between, they fly through a waveguide, which is a metal cavity a few centimeters in width, which also needs to be strongly cooled so that the quantum states of the photons are not influenced.
each of the quantum chips is cooled down over several days in a cryostat (an extremely powerful refrigerator), using compressed and also liquid helium, to a few hundredths of a degree above absolute zero.
to that end, the five- meter waveguide that creates the quantum link was equipped with a shell consisting of several layers of copper sheet.
each of those sheets acts as a heat shield for the different temperature stages of the cryostat: - 223 degrees, - 269 degrees, - 272 degrees and finally - 273,1 degrees.
altogether, those heat shields alone weigh around a quarter of a tonne.
no "table- top" experiment
"so, this is definitely not a "table- top" experiment anymore that one can put together on a small workbench," wallraff says.
"a lot of development work has gone into this, and eth is an ideal place for building such an ambitious apparatus.
it's a kind of mini- cern that we first had to build over several years in order to be able to do interesting things with it now."
apart from the three ph.d. students who carried out the experiments, several engineers and technicians, also in the workshops at eth and at the paul scherrer institute (psi), were involved in producing and constructing the quantum link.
the physicists at eth not only showed that the quantum link can be sufficiently cooled down, but also that it can actually be used to reliably transmit quantum information between two quantum chips.
to demonstrate this, they created an entangled state between the two chips via the quantum link.
such entangled states, in which measuring one qubit instantaneously influences the result of a measurement on the other qubit, can also be used for tests in basic quantum research.
in those "bell tests," the qubits must be far enough apart from each other, so that any information transfer at the speed of light can be ruled out.
while wallraff and his collaborators are performing experiments with the new link, they have already started working on even longer quantum links.
already a year ago they were able to sufficiently cool down a ten- meter link, but without doing any quantum experiments with it.
now they are working on a 30- meter quantum link, for which a room at eth has been specially prepared.
quantum coherence and quantum entanglement are two landmark features of quantum physics, and now physicists have demonstrated that the two phenomena are "operationally equivalent"- that is, equivalent for all practical purposes, though still conceptually distinct.
this finding allows physicists to apply decades of research on entanglement to the more fundamental but less- well- researched concept of coherence, offering the possibility of advancing a wide range of quantum technologies.
close relatives with the same roots
although physicists have known that coherence and entanglement are close relatives, the exact relationship between the two resources has not been clear.
it's well- known that quantum coherence and quantum entanglement are both rooted in the superposition principle- the phenomenon in which a single quantum state simultaneously consists of multiple states- but in different ways.
quantum coherence deals with the idea that all objects have wave- like properties.
if an object's wave- like nature is split in two, then the two waves may coherently interfere with each other in such a way as to form a single state that is a superposition of the two states.
this concept of superposition is famously represented by schrodinger's cat, which is both dead and alive at the same time when in its coherent state inside a closed box.
coherence also lies at the heart of quantum computing, in which a qubit is in a superposition of the "0" and "1" states, resulting in a speed- up over various classical algorithms.
when such a state experiences decoherence, however, all of its quantumness is typically lost and the advantage vanishes.
the second phenomenon, quantum entanglement, also involves superposition.
but in this case, the states in a superposition are the shared states of two entangled particles rather than those of the two split waves of a single particle.
the intrigue of entanglement lies in the fact that the two entangled particles are so intimately correlated that a measurement on one particle instantly affects the other particle, even when separated by a large distance.
like coherence, quantum entanglement also plays an essential role in quantum technologies, such as quantum teleportation, quantum cryptography, and super dense coding.
converting one to the other
in a paper to be published in physical review letters, physicists led by gerardo adesso, associate professor at the university of nottingham in the uk, with coauthors from spain and india, have provided a simple yet powerful answer to the question of how these two resources are related: the scientists show that coherence and entanglement are quantitatively, or operationally, equivalent, based on their behavior arising from their respective resource theories.
the physicists arrived at this result by showing that, in general, any nonzero amount of coherence in a system can be converted into an equal amount of entanglement between that system and another initially incoherent one.
this discovery of the conversion between coherence and entanglement has several important implications.
for one, it means that quantum coherence can be measured through entanglement.
consequently, all of the comprehensive knowledge that researchers have obtained about entanglement can now be directly applied to coherence, which in general is not nearly as well- researched (outside of the area of quantum optics).
for example, the new knowledge has already allowed the physicists to settle an important open question concerning the geometric measure of coherence: since the geometric measure of entanglement is a "full convex monotone," the same can be said of the associated coherence measure.
as the scientists explained, this is possible because the new results allowed them to define and quantify one resource in terms of the other.
"the significance of our work lies in the fact that we prove the close relation between entanglement and coherence not only qualitatively, but on a quantitative level," coauthor alex streltsov, of icfo- the institute of photonic sciences in barcelona, told phys.org.
"more precisely, we show that any quantifier of entanglement gives rise to a quantifier of coherence.
this concept allowed us to prove that the geometric measure of coherence is a valid coherence quantifier, thus answering a question left open in several previous works."
while the results show that coherence and entanglement are operationally equivalent, the physicists explain that this doesn't mean that are the exact same thing, as they are still conceptually different ideas.
"despite having the same roots of origin, namely quantum superposition, coherence and entanglement are conceptually different," said coauthors uttam singh, himadri dhar, and manabendra bera at the harish- chandra research institute in allahabad, india.
"for example, coherence can be present in single quantum systems, where entanglement is not well- defined.
also, coherence is defined with respect to a given basis, while entanglement is invariant under local basis changes.
in all, we believe coherence and entanglement are operationally equivalent but conceptually different."
future quantum connections
the operational equivalence of coherence and entanglement will likely have a far- reaching impact on areas ranging from quantum information theory to more nascent fields such as quantum biology and nanoscale thermodynamics.
in the future, the physicists plan to investigate whether coherence and entanglement might also be interconverted into a third resource- that of quantum discord, which, like entanglement, is another type of quantum correlation between two systems.
"our future plans are diverse," adesso said.
"on the theoretical side, we are working to construct a unified framework to interpret, classify and quantify all different forms of quantum resources, including and beyond entanglement and coherence, and highlight the interlinks among them from an operational perspective.
this will allow us to navigate the hierarchy of quantumness indicators in composite systems with a common pilot, and to appreciate which particular ingredients are needed in various informational tasks.
"on the practical side, we are investigating experimentally friendly schemes to detect, quantify, and preserve coherence, entanglement and other quantum correlations in noisy environments.
more fundamentally, we hope these results will inspire us to devise scalable and efficient methods to convert between different quantum resources for technological applications, and bring us closer to understanding where the boundaries of the quantum world ultimately lie in realistic scenarios."
researchers of the university of vienna used a quantum mechanical system in the laboratory to simulate complex many- body systems.
this experiment promises future quantum simulators enormous potential insights into unknown quantum phenomena.
researchers from the vienna center for quantum science and technology at the university of vienna and the institute of quantum optics and quantum information at the austrian academy of sciences used a quantum mechanical system in the laboratory to simulate complex many- body systems.
this experiment, which is published in nature physics, promises future quantum simulators enormous potential insights into unknown quantum phenomena.
already the behavior of relatively small quantum systems cannot be calculated because quantum states contain much more information than their classical counter- parts.
however, if another quantum system is used to simulate the quantum system of interest, then answers about the properties of the complex quantum system can be obtained.
when is a quantum system frustrated?
currently, many international groups are focusing their research on frustrated quantum systems, which have been conjectured to explain high- temperature superconductivity.
a quantum system is frustrated if competing requirements cannot be satisfied simultaneously.
the viennese research group realized for the first time an experimental quantum simulation, where the frustration regarding the "pairing" of correlations was closely investigated.
using two pairs of entangled photons, a frustrated quantum system could be simulated that consists of four particles.
"just the recent development of our quantum technology allows us to not only rebuild other quantum systems, but also to simulate its dynamics" says philip walther (university of vienna).
"now we can prepare quantum states of individual photons to gain insights into other quantum systems", explains xiao- song ma (austrian academy of sciences).therefore, two in polarization entangled photons exhibit in many ways the same quantum physical properties as for example electrons in matter.
conflict over partnerships
the research team of international scientists from china, serbia, new zeeland and austria prepared single photons that were facing the conflict over partnerships between each other.
each photon can establish a single bond to only one partner exclusively, but wants to get correlated with several partners - obviously this leads to frustration.
as a result, the quantum system uses "tricks" that allow quantum fluctuations that different pairings can coexist as superposition.
the work of the viennese group underlines that quantum simulations are a very good tool for calculating quantum states of matter and are thus opening the path for the investigation of more complex systems.
more information: xiao- song ma, borivoje dakic, william naylor, anton zeilinger & philip walther, quantum simulation of the wavefunction to probe frustrated heisenberg spin systems, nature physics 7, 399- 405 (2011) doi:10.1038/nphys1919
two independent teams of scientists, including one from the university of maryland (umd) and the national institute of standards and technology (nist), have used more than 50 interacting atomic qubits to mimic magnetic quantum matter, blowing past the complexity of previous demonstrations.
the results appear in this week's issue of nature.
as the basis for its quantum simulation, the umd- nist team deploys up to 53 individual ytterbium ions- charged atoms trapped in place by gold- coated and razor- sharp electrodes.
a complementary design by harvard and mit researchers uses 51 uncharged rubidium atoms confined by an array of laser beams.
with so many qubits these quantum simulators are on the cusp of exploring physics that is unreachable by even the fastest modern supercomputers.
and adding even more qubits is just a matter of lassoing more atoms into the mix.
"each ion qubit is a stable atomic clock that can be perfectly replicated," says umd team lead christopher monroe, who is also the co- founder and chief scientist at the startup ionq inc. "they are effectively wired together with external laser beams.
this means that the same device can be reprogrammed and reconfigured, from the outside, to adapt to any type of quantum simulation or future quantum computer application that comes up."
monroe has been one of the early pioneers in quantum computing and his research group's quantum simulator is part of a blueprint for a general- purpose quantum computer.
quantum hardware for a quantum problem
while modern, transistor- driven computers are great for crunching their way through many problems, they can screech to a halt when dealing with more than 20 interacting quantum objects.
that's certainly the case for quantum magnetism, in which the interactions can lead to magnetic alignment or to a jumble of competing interests at the quantum scale.
"what makes this problem hard is that each magnet interacts with all the other magnets," says umd research scientist zhexuan gong, lead theorist and co- author on the study.
"with the 53 interacting quantum magnets in this experiment, there are over a quadrillion possible magnet configurations, and this number doubles with each additional magnet.
simulating this large- scale problem on a conventional computer is extremely challenging, if at all possible."
when these calculations hit a wall, a quantum simulator may help scientists push the envelope on difficult problems.
this is a restricted type of quantum computer that uses qubits to mimic complex quantum matter.
qubits are isolated and well- controlled quantum systems that can be in a combination of two or more states at once.
qubits come in different forms, and atoms- the versatile building blocks of everything- are one of the leading choices for making qubits.
in recent years, scientists have controlled 10 to 20 atomic qubits in small- scale quantum simulations.
currently, tech industry behemoths, startups and university researchers are in a fierce race to build prototype quantum computers that can control even more qubits.
but qubits are delicate and must stay isolated from the environment to protect the device's quantum nature.
with each added qubit this protection becomes more difficult, especially if qubits are not identical from the start, as is the case with fabricated circuits.
this is one reason that atoms are an attractive choice that can dramatically simplify the process of scaling up to large- scale quantum machinery.
an atomic advantage
unlike the integrated circuitry of modern computers, atomic qubits reside inside of a room- temperature vacuum chamber that maintains a pressure similar to outer space.
this isolation is necessary to keep the destructive environment at bay, and it allows the scientists to precisely control the atomic qubits with a highly engineered network of lasers, lenses, mirrors, optical fibers and electrical circuitry.
"the principles of quantum computing differ radically from those of conventional computing, so there's no reason to expect that these two technologies will look anything alike," says monroe.
in the 53- qubit simulator, the ion qubits are made from atoms that all have the same electrical charge and therefore repel one another.
but as they push each other away, an electric field generated by a trap forces them back together.
the two effects balance each other, and the ions line up single file.
physicists leverage the inherent repulsion to create deliberate ion- to- ion interactions, which are necessary for simulating of interacting quantum matter.
the quantum simulation begins with a laser pulse that commands all the qubits into the same state.
then, a second set of laser beams interacts with the ion qubits, forcing them to act like tiny magnets, each having a north and south pole.
the team does this second step suddenly, which jars the qubits into action.
they feel torn between two choices, or phases, of quantum matter.
as magnets, they can either align their poles with their neighbors to form a ferromagnet or point in random directions yielding no magnetization.
the physicists can change the relative strengths of the laser beams and observe which phase wins out under different laser conditions.
the entire simulation takes only a few milliseconds.
by repeating the process many times and measuring the resulting states at different points during the simulation, the team can see the process as it unfolds from start to finish.
the researchers observe how the qubit magnets organize as different phases form, dynamics that the authors say are nearly impossible to calculate using conventional means when there are so many interactions.
this quantum simulator is suitable for probing magnetic matter and related problems.
but other kinds of calculations may need a more general quantum computer with arbitrarily programmable interactions in order to get a boost.
"quantum simulations are widely believed to be one of the first useful applications of quantum computers," says alexey gorshkov, nist theoretical physicist and co- author of the study.
"after perfecting these quantum simulators, we can then implement quantum circuits and eventually quantum- connect many such ion chains together to build a full- scale quantum computer with a much wider domain of applications."
as they look to add even more qubits, the team believes that its simulator will embark on more computationally challenging terrain, beyond magnetism.
"we are continuing to refine our system, and we think that soon, we will be able to control 100 ion qubits, or more," says jiehang zhang, the study's lead author and umd postdoctoral researcher.
"at that point, we can potentially explore difficult problems in quantum chemistry or materials design."
more information: j. zhang et al, observation of a many- body dynamical phase transition with a 53- qubit quantum simulator, nature (2017).
doi: 10.1038/nature24654
related: probing many- body dynamics on a 51- atom quantum simulator, nature (2017).
http://nature.com/articles/doi:10.1038/nature24622 (read: scientists demonstrate one of largest quantum simulators: http://physorg.com/news431171700.html)
experimental physicists have put a lot of effort in isolating sensitive measurements from the disruptive influences of the environment.
in an international first, austrian quantum physicists have realized a toolbox of elementary building blocks for an open- system quantum simulator, where a controlled coupling to an environment is used in a beneficial way.
this offers novel prospects for studying the behavior of highly complex quantum systems.
the researchers have published their work in the scientific journal nature.
many phenomena in our world are based on the nature of quantum physics: the structure of atoms and molecules, chemical reactions, material properties, magnetism and possibly also certain biological processes.
since the complexity of phenomena increases exponentially with more quantum particles involved, a detailed study of these complex systems reaches its limits quickly; and conventional computers fail when calculating these problems.
to overcome these difficulties, physicists have been developing quantum simulators on various platforms, such as neutral atoms, ions or solid- state systems, which, similar to quantum computers, utilize the particular nature of quantum physics to control this complexity.
in a special issue at the end of 2010, the scientific journal science chose the progress made in this field as one of the scientific breakthroughs of the year 2010.
in another breakthrough in this field, a team of young scientists in the research groups of rainer blatt and peter zoller at the institute for experimental physics and theoretical physics of the university of innsbruck and the institute of quantum optics and quantum information (iqoqi) of the austrian academy of sciences have been the first to engineer a comprehensive toolbox for an open- system quantum computer, which will enable researchers to construct more sophisticated quantum simulators for investigating complex problems in quantum physics.
using controlled dissipation
the physicists use a natural phenomenon in their experiments that they usually try to minimize as much as possible: environmental disturbances.
such disturbances usually cause information loss in quantum systems and destroy fragile quantum effects such as entanglement or interference.
in physics this deleterious process is called dissipation.
innsbruck researchers, led by experimental physicists julio barreiro and philipp schindler as well as the theorist markus muller, have now been first in using dissipation in a quantum simulator with trapped ions in a beneficial way and engineered system- environment coupling experimentally.
"we not only control all internal states of the quantum system consisting of up to four ions but also the coupling to the environment," explains julio barreiro.
"in our experiment we use an additional ion that interacts with the quantum system and, at the same time, establishes a controlled contact to the environment," explains philipp schindler.
the surprising result is that by using dissipation, the researchers are able to generate and intensify quantum effects, such as entanglement, in the system.
"we achieved this by controlling the disruptive environment," says an excited markus muller.
putting the quantum world into order
in one of their experiments the researchers demonstrate the control of dissipative dynamics by entangling four ions using the environment ion.
"contrary to other common procedures this also works irrespective of the initial state of each particle," explains muller.
"through a collective cooling process, the particles are driven to a common state."
this procedure can be used to prepare many- body states, which otherwise could only be created and observed in an extremely well isolated quantum system.
the beneficial use of an environment allows for the realization of new types of quantum dynamics and the investigation of systems that have scarcely been accessible for experiments until now.
in the last few years there has been continuous thinking about how dissipation, instead of suppressing it, could be actively used as a resource for building quantum computers and quantum memories.
innsbruck theoretical and experimental physicists cooperated closely and they have now been the first to successfully implement these dissipative effects in a quantum simulator.
more information: an open- system quantum simulator with trapped ions.
julio t. barreiro, markus muller, philipp schindler, daniel nigg, thomas monz, michael chwalla, markus hennrich, christian f. roos, peter zoller und rainer blatt.
nature 2011.
doi: 10.1038/nature09801
since 2013, google and nasa have worked on code designed for a quantum machine bought from d- wave.
google research blog said tuesday that quantum ai lab researchers report in a new paper that has yet to be peer- reviewed that the machine can perform quantum computing operations.
the machine beat- resoundingly- a conventional computer in tests, said tom simonite in mit technology review.
puiu said the paper was published online by google but has yet to pass peer- review.
"what is the computational value of finite range tunneling?"
is on the arxiv and authors are vasil s. denchev, sergio boixo, sergei v. isakov, nan ding, ryan babbush, vadim smelyanskiy, john martinis and hartmut neven.
the d- wave machine is capable of performing a limited set of tasks; it's not a universal quantum computer, according to zme science.
the machine can run code 100 million times faster than a conventional single- core processor, he said, but only for specific problems.
"google and nasa announced that the d- wave computer can actually walk the talk for some highly specialized workloads known as annealing," he said in zme science.
"in a face to face stand- off, the researchers used simulated annealing on a conventional processor, and quantum annealing on the d- wave machine.
for this particular workload, the d- wave computer was able to solve the problem 100 million times faster."
simonite said the machine "really can use quantum physics to work through a type of math that's crucial to artificial intelligence much faster than a conventional computer."
simonite said that d- wave's chips are controversial among quantum physicists; researchers have been unable to prove conclusively that the devices can tap into quantum physics to beat out conventional computers.
nonetheless, "for a specific, carefully crafted proof- of- concept problem we achieve a 100- million- fold speed- up," said neven, one of the co- authors of the paper, in mit technology review.
neven, director of engineering, provided some background for the news; he also wrote about the work on the google research blog on tuesday.
"during the last two years, the google quantum ai team has made progress in understanding the physics governing quantum annealers.
we recently applied these new insights to construct proof- of- principle optimization problems and programmed these into the d- wave 2x quantum annealer that google operates jointly with nasa."
what they hoped to demonstrate: quantum annealing, wrote neven, can offer runtime advantages for hard optimization problems characterized by rugged energy landscapes.
simonite talked about what quantum annealer means.
the computer installed at the ames center in mountain view operates on data using a superconducting chip called a quantum annealer and the annealer is hard- coded with an algorithm suited to optimization problems, common in machine- learning and artificial- intelligence software.
jacob aron in new scientist explained it further:
"d- wave's computer is a specialized device called a quantum annealer, which works by exploring an energy landscape of hills and valleys that corresponds to the problem it is trying to solve.
the goal is to reach the lowest point on this landscape, which corresponds to the best solution.
a property called quantum tunneling allows the d- wave to traverse this landscape more quickly by "tunneling" through the hills, thus theoretically letting it reach an answer faster."
neven wrote in the blog: "we found that for problem instances involving nearly 1000 binary variables, quantum annealing significantly outperforms its classical counterpart, simulated annealing.
it is more than 108 times faster than simulated annealing running on a single core.
we also compared the quantum hardware to another algorithm called quantum monte carlo.
this is a method designed to emulate the behavior of quantum systems, but it runs on conventional processors.
while the scaling with size between these two methods is comparable, they are again separated by a large factor sometimes as high as 108."
in the bigger picture, simonite remarked that "computing giants believe quantum computers could make their artificial- intelligence software much more powerful and unlock scientific leaps in areas like materials science.
nasa hopes quantum computers could help schedule rocket launches and simulate future missions and spacecraft."
he quoted rupak biswas, director of exploration technology at nasa's ames research center.
"it is a truly disruptive technology that could change how we do everything," said biswas.
in that context, it is understandable that the paper on arxiv will draw not only interest but conversation.
new scientist's headline on wednesday was "experts doubt google's claim about its quantum computer's speed."
"this is certainly the most impressive demonstration so far of the d- wave machine's capabilities," said scott aaronson at mit.
"and yet, it remains totally unclear whether you can get to what i'd consider 'true quantum speedup' using d- wave's architecture."
aaronson, the author of quantum computing since democritus, wrote in his own blog, "while there's been genuine, interesting progress, it remains uncertain whether d- wave's approach will lead to speedups over the best known classical algorithms, let alone to speedups over the best known classical algorithms that are also asymptotic or also of practical importance."
more information: 1.
google research blog: when can quantum annealing win?
http://googleresearch.blogspot ... m- annealing- win.html
2.
what is the computational value of finite range tunneling?
arxiv:1512.02206 [quant- ph] http://arxiv.org/abs/1512.02206
abstract
quantum annealing (qa) has been proposed as a quantum enhanced optimization heuristic exploiting tunneling.
here, we demonstrate how finite range tunneling can provide considerable computational advantage.
for a crafted problem designed to have tall and narrow energy barriers separating local minima, the d- wave 2x quantum annealer achieves significant runtime advantages relative to simulated annealing (sa).
for instances with 945 variables this results in a time- to- 99%- success- probability that is ~108 times faster than sa running on a single processor core.
we also compared physical qa with quantum monte carlo (qmc), an algorithm that emulates quantum tunneling on classical processors.
we observe a substantial constant overhead against physical qa: d- wave 2x runs up to ~108 times faster than an optimized implementation of qmc on a single core.
to investigate whether finite range tunneling will also confer an advantage for problems of practical interest, we conduct numerical studies on binary optimization problems that cannot yet be represented on quantum hardware.
for random instances of the number partitioning problem, we find numerically that qmc, as well as other algorithms designed to simulate qa, scale better than sa and better than the best known classical algorithms for this problem.
we discuss the implications of these findings for the design of next generation quantum annealers.
moore's law famously states that the number of transistors on a microchip doubles every year.
this has roughly proven true, with computing power doubling on that approximate schedule.
catherine mcgeoch, a professor of computer science at amherst college, recently conducted an experiment to test the speed of a quantum computing chip versus conventional massively parallel supercomputers.
in the world of high- performance computing, quantum computers have been the holy grail ever since they were first proposed by yuri mann and richard feynman in the 1980s.
a quantum computer is, shockingly, a computer that leverages quantum phenomena to make super- fast calculations.
using the properties of the adiabatic theorem, d- waves' vesuvius prototype is "the world's first commercially available quantum computer" - a 512- qubit chip capable of making all of the calculation that a traditional, transistor- based chip can make.
to test how the vesuvius chip stacked up again traditional computers, mcgeoch focused on the classic "travelling salesperson" problem (tsp), a cornerstone of theoretical computing.
the tsp is categorized as an optimization problem and asks "given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?"
the results of her test, which will be discussed later today, found that while the traditional chip took thirty minutes to solve the problem, the vesuvius was able to complete the same task in less than half a second.
though the vesuvius does have incredible speed, mcgeoch hedged on whether quantum computer are likely to be commercial products, saying "this type of computer is not intended for surfing the internet, but it does solve this narrow, but important type of problem really, really fast".
its need for a cryogenic cooling system to keep it at a constant twenty millikelvin may also limit its mass- market appeal.
but while solving complex yet narrow problems will likely be the domain of giants like governments and google, mcgeoch remembers back to one erroneous prediction made in the early days of computing.
"the founder of ibm famously predicted that only about five of his company's first computers would be sold, because he just didn't see the need for that much computing power."
in the coming years, mcgeoch predicts that "these quantum computing methods will solve more and bigger problems significantly faster than the best conventional computing options out there".
maybe, in the coming decades, quantum computing will allow us to immediately diagnose cancer, or have deep understanding of our neuro- chemistry.
at that point, it might behoove us to have this technology at our disposal, so we can diagnose and understand our body's ever changing make- up in real- time.
regardless of whether any of my sci- fi fantasies come to pass, it does seem that quantum computing might not be up- ending moore's law so much as making it obsolete.
watch a video detailing the development of d- wave's quantum computers:
if successfully implemented, quantum communication could be an extremely secure method of transmitting information - but there are major roadblocks to pass.
recently, physicists suggested a way, at least in theory, to overcome perhaps the biggest of these problems: making quantum communication possible over "real life" networks with serious imperfections, such as leakage, and across distances greater than 10 kilometers.
all of the issues slowing the progress of quantum communication have to do with the foundation of quantum communication, a phenomenon called "quantum entanglement."
quantum entanglement occurs when two quantum- information carriers, such as photons, are aware of each other's existence and know each other's particular quantum state despite never having previously interacted and being physically separate.
it is one peculiar effect of the strange, mysterious world of quantum physics.
currently, photon channels, such as fiber- optic cables, are the only realistic choice for quantum communication.
however, creating high- fidelity quantum entanglement between photons at two distant locations becomes exponentially more difficult as the distance between them increases, seriously impeding the real- life implementation of quantum communication.
extending the range to practical distances remains a challenge on many levels.
but, as they discuss in a recent paper in physical review a, physicists from nanjing university in china propose a quantum- communications network in which producing entanglement over a long distance is conceptually possible.
the basic network they suggest is made of a sending node and receiving node coupled to a quantum channel (such as a fiber optic cable) that contains an optical circulator, a fiber- optic component that allows signals to simultaneously travel in both directions down a fiber.
inside the sending and receiving nodes are a quantum dot (typically a very tiny cluster of atoms that behaves as a single atom in the quantum sense) in a microcavity.
each dot can be in one of three quantum states: a ground state, an excited state, and an intermediate state.
each state is a qubit, or quantum bit, the most basic piece of quantum information, like how a "0" or "1" form a bit of computer storage.
these qubits are stationary.
the scheme also includes a "flying qubit," a mobile piece of quantum information, that moves between them.
the flying qubit in this case is a pulse of light with a specific shape.
the pulse acts as something like a middle man, initially being entangled with the sending qubit but swapping its entanglement with the receiving qubit, thus leaving the sending and receiving qubits entangled.
this scheme, when its parameters are properly and meticulously tweaked, avoids some of the issues that arise in other methods that have been proposed and, the scientists say, can yield fidelities that are almost perfect.
citation: physical review a 76, 052302 (2007)
physicists have shown that superconducting circuits- circuits that have zero electrical resistance- can function as piston- like mechanical quantum engines.
the new perspective may help researchers design quantum computers and other devices with improved efficiencies.
the physicists, kewin sachtleben, kahio t. mazon, and luis g. c. rego at the federal university of santa catarina in florianopolis, brazil, have published a paper on their work on superconducting qubits in a recent issue of physical review letters.
in their study, the physicists explain that superconducting circuits are functionally equivalent to quantum systems in which quantum particles tunnel in a double- quantum well.
these wells have the ability to oscillate, meaning the width of the well changes repeatedly.
when this happens, the system behaves somewhat like a piston that moves up and down in a cylinder, which changes the volume of the cylinder.
this oscillatory behavior allows work to be performed on the system.
the researchers show that, in the double- quantum well, part of this work comes from quantum coherent dynamics, which creates friction that decreases the work output.
these results provide a better understanding of the connection between quantum and classical thermodynamic work.
"the distinction between 'classical' thermodynamic work, responsible for population transfer, and a quantum component, responsible for creating coherences, is an important result," mazon told phys.org.
"the creation of coherences, in turn, generates a similar effect to friction, causing a not- completely- reversible operation of the engine.
in our work we have been able to calculate the reaction force caused on the quantum piston wall due to the creation of coherences.
in principle this force can be measured, thus constituting the experimental possibility of observing the emergence of coherences during the operation of the quantum engine."
one of the potential benefits of viewing superconducting qubits as quantum engines is that it may allow researchers to incorporate quantum coherent dynamics into future technologies, in particular quantum computers.
the physicists explain that a similar behavior can be seen in nature, where quantum coherences improve the efficiency of processes such as photosynthesis, light sensing, and other natural processes.
"quantum machines may have applications in the field of quantum information, where the energy of quantum coherences is used to perform information manipulation in the quantum regime," mazon said.
"it is worth remembering that even photosynthesis can be described according to the working principles of a quantum machine, so unraveling the mysteries of quantum thermodynamics can help us to better understand and interpret various natural processes."
some math problems are so complicated that they can bog down even the world's most powerful supercomputers.
but a wild new frontier in computing that applies the rules of the quantum realm offers a different approach.
a new study led by a physicist at lawrence berkeley national laboratory (berkeley lab), published in the journal scientific reports, details how a quantum computing technique called "quantum annealing" can be used to solve problems relevant to fundamental questions in nuclear physics about the subatomic building blocks of all matter.
it could also help answer other vexing questions in science and industry, too.
seeking a quantum solution to really big problems
"no quantum annealing algorithm exists for the problems that we are trying to solve," said chia cheng "jason" chang, a riken ithems fellow in berkeley lab's nuclear science division and a research scientist at riken, a scientific institute in japan.
"the problems we are looking at are really, really big," said chang, who led the international team behind the study.
"the idea here is that the quantum annealer can evaluate a large number of variables at the same time and return the right solution in the end."
the same problem- solving algorithm that chang devised for the latest study, and that is available to the public via open- source code, could potentially be adapted and scaled for use in systems engineering and operations research, for example, or in other industry applications.
classical algebra with a quantum computer
"we are cooking up small 'toy' examples just to develop how an algorithm works.
the simplicity of current quantum annealers is that the solution is classical -  akin to doing algebra with a quantum computer.
you can check and understand what you are doing with a quantum annealer in a straightforward manner, without the massive overhead of verifying the solution classically."
chang's team used a commercial quantum annealer located in burnaby, canada, called the d- wave 2000q that features superconducting electronic elements chilled to extreme temperatures to carry out its calculations.
access to the d- wave annealer was provided via the oak ridge leadership computing facility at oak ridge national laboratory (ornl).
"these methods will help us test the promise of quantum computers to solve problems in applied mathematics that are important to the u.s. department of energy's scientific computing mission," said travis humble, director of ornl's quantum computing institute.
quantum data: a one, a zero, or both at the same time
there are currently two of these machines in operation that are available to the public.
they work by applying a common rule in physics: systems in physics tend to seek out their lowest- energy state.
for example, in a series of steep hills and deep valleys, a person traversing this terrain would tend to end up in the deepest valley, as it takes a lot of energy to climb out of it and the least amount of energy to settle in this valley.
the annealer applies this rule to calculations.
in a typical computer, memory is stored in a series of bits that are occupied by either one or a zero.
but quantum computing introduces a new paradigm in calculations: quantum bits, or qubits.
with qubits, information can exist as either a one, a zero, or both at the same time.
this trait makes quantum computers better suited to solving some problems with a very large number of possible variables that must be considered for a solution.
each of the qubits used in the latest study ultimately produces a result of either a one or a zero by applying the lowest- energy- state rule, and researchers tested the algorithm using up to 30 logical qubits.
the algorithm that chang developed to run on the quantum annealer can solve polynomial equations, which are equations that can have both numbers and variables and are set to add up to zero.
a variable can represent any number in a large range of numbers.
when there are 'fewer but very dense calculations'
berkeley lab and neighboring uc berkeley have become a hotbed for r&d in the emerging field of quantum information science, and last year announced the formation of a partnership called berkeley quantum to advance this field.
chang said that the quantum annealing approach used in the study, also known as adiabatic quantum computing, "works well for fewer but very dense calculations," and that the technique appealed to him because the rules of quantum mechanics are familiar to him as a physicist.
the data output from the annealer was a series of solutions for the equations sorted into columns and rows.
this data was then mapped into a representation of the annealer's qubits, chang explained, and the bulk of the algorithm was designed to properly account for the strength of the interaction between the annealer's qubits.
"we repeated the process thousands of times" to help validate the results, he said.
"solving the system classically using this approach would take an exponentially long time to complete, but verifying the solution was very quick" with the annealer, he said, because it was solving a classical problem with a single solution.
if the problem was quantum in nature, the solution would be expected to be different every time you measure it.
real- world applications for a quantum algorithm
as quantum computers are equipped with more qubits that allow them to solve more complex problems more quickly, they can also potentially lead to energy savings by reducing the use of far larger supercomputers that could take far longer to solve the same problems.
the quantum approach brings within reach direct and verifiable solutions to problems involving "nonlinear" systems -  in which the outcome of an equation does not match up proportionately to the input values.
nonlinear equations are problematic because they may appear more unpredictable or chaotic than other "linear" problems that are far more straightforward and solvable.
chang sought the help of quantum- computing experts in quantum computing both in the u.s. and in japan to develop the successfully tested algorithm.
he said he is hopeful the algorithm will ultimately prove useful to calculations that can test how subatomic quarks behave and interact with other subatomic particles in the nuclei of atoms.
while it will be an exciting next step to work to apply the algorithm to solve nuclear physics problems, "this algorithm is much more general than just for nuclear science," chang noted.
"it would be exciting to find new ways to use these new computers."
the oak ridge leadership computing facility is a doe office of science user facility.
quantum computing will change lives, society and the economy and a working system is expected to be developed by 2020 according to a leading figure in the world of quantum computing, who will talk tomorrow jan. 21, 2016 at the world economic forum (wef) in davos, switzerland.
professor o'brien, director of the centre for quantum photonics at the university of bristol and visiting fellow at stanford university, is part of a european research council (erc) ideas lab delegation who have been invited to talk at the annual meeting to industrial and political leaders of the world, including prime minister david cameron.
the session will discuss the future of computing and how new fields of computer sciences are paving the way for the next digital revolution.
quantum computing has the capability to unlock answers to some of humanity's most pressing questions that are presently unsolvable with current computing technologies.
in 2014, the uk government invested over ps270 million in the development of quantum technologies, ensuring that the uk becomes the epicentre of a technology revolution and professor o'brien has been leading the development of quantum computing using light in its quantum state - the photon - as the key ingredient.
professor o'brien said: "in less than ten years quantum computers will begin to outperform everyday computers, leading to breakthroughs in artificial intelligence, the discovery of new pharmaceuticals and beyond.
"the very fast computing power given by quantum computers has the potential to disrupt traditional businesses and challenge our cyber- security.
businesses need to be ready for a quantum future because it's coming."
in his talk, professor o'brien will outline the current status of quantum computing and its potential applications and he will reveal his architectural blue- print for a manufacturable photonic quantum computer, showing all the components and a roadmap toward building a practical machine.
quantum technologies offer ultra- secure communications, sensors of unprecedented precision and computers that are exponentially more powerful than any supercomputer for a given task.
these technologies are destined to fundamentally change our lives and the first commercially available quantum devices are only now beginning to emerge.
as the holder of a prestigious royal academy of engineering chair in quantum engineering and an epsrc rise leader, professor o'brien has a ten year vision to engineer new quantum technologies that will inevitably disrupt todays ict models, creating new businesses and valuable new markets.
the world economic forum (wef) annual meeting of business and political leaders will take place from jan. 20- 23, 2016 in davos, switzerland.
it is impossible to obtain all information about a large quantum system consisting of hundreds or thousands of particles.
a new technique allows to describe such systems in terms of 'continuous matrix product states.'
with this approximation, the relevant information about a quantum system can be obtained by only a few measurements.
for a long time, quantum experiments were only carried out with a small number of particles.
even the behaviour of single atoms or molecules can be very hard to describe.
today, it has become possible to control several thousand atoms in an experiment, but for theoretical calculations this entails serious problems.
the quantum state of such a large system is so complicated that all matter on earth would not be enough to store it in a classical way.
in the journal nature communications, scientists from the tu wien (vienna) and the free university of berlin now present a quantum tomography method, which makes it possible to measure and describe the state of a large quantum system very precisely with just a few measurements.
the basic idea behind this new technique is simple: even though the system can be in one of unimaginably many quantum states, it is a very good approximation to ignore most of them.
many particles, many states
the result of a coin toss is either heads or tails.
the behaviour of quantum particles, however, is much more complicated.
when a quantum system can be in two different states, any mixture of these states is also a physically allowed state.
therefore it is much more complicated to describe the state of a quantum particle than it is to describe the state of a coin lying on the table.
"the larger the number of particles, the more complicated the description of the systems becomes", says professor jorg schmiedmayer from the vienna center for quantum science and technology (vcq) at tu wien.
"the storage capacity required to describe a quantum state grows exponentially with the number of particles.
for a system of several hundred quantum particles, there are more possible quantum states than there are atoms in the universe.
it is absolutely impossible to write down such a state or to do calculations with it."
but exactly knowing the quantum state is not always necessary.
the new theoretical method, developed in berlin by professor jens eisert's research group, uses a special kind of description for the quantum states - the so called "continuous matrix product states" (cmps).
this special class of states only represents a vanishingly small fraction of all possible states, but from a physical point of view they are particularly important.
"this class contains states with realistic quantum entanglement", says jens eisert.
"exotic, complicated entanglement patterns between many quantum particles may in principle be possible, but in practice they do not show up in physical systems.
that is why we can limit ourselves to the cmps in our calculations."
for any possible quantum state, there is a cmps arbitrarily close to the true quantum state.
no matter which state is really occupied by the system - the error that occurs by only taking into account the cmps can be made arbitrarily small.
"it is like fractions in mathematics", says eisert.
"the rational numbers, which can be written as fractions, only represent a tiny part of all real numbers.
but for any real number, a fractional number can be found which comes arbitrarily close."
the number pi is not a fractional number - but the approximation for pi used by a pocket calculator is.
for all practical purposes, this is good enough.
measurements yielding a quantum picture
by restricting oneself to the cmps, it becomes possible to read out the state of a large quantum system in an experiment.
"we cannot gain complete knowledge about the system from a finite number of measurements, but that is also not what we need", says tim langen, who led the experiments in schmiedmayer's research group.
"with our new method, we can reconstruct the quantum state from only a few measurements.
the precision is so high that we can use this approximate state to predict the result of further measurements."
this technique is called "quantum tomography" - much like computer tomography in a hospital, where several pictures are used to calculate a 3d model, quantum tomography uses several measurements to calculate a picture of the quantum state.
the new method does not only open up new possibilities for many- body quantum physics.
it could also path the way to new quantum simulators - quantum systems, which are prepared in such a way that they can be used to simulate other quantum systems that cannot be controlled by standard methods.
"when two different quantum systems can be described with the same basic formulas, then we can learn a lot about one system by studying the other", says schmiedmayer.
"we can control thousands of atoms on our quantum chip, this system is thus very well suited for future quantum simulations."
cambridge quantum computing (cqc) has announced it has become the first startup- based hub in the ibm q network.
as a hub, cqc will expand membership of the network with cloud- based access to the ibm quantum computation center, which now includes 20 of the most- advanced quantum computers commercially available to explore practical applications for business and science, including eight systems with a quantum volume of 32, and a 53- qubit system - the largest available for clients in the industry.
cqc and its hub member organizations will collaborate on chemistry, optimization, finance, and quantum machine learning and natural language processing using ibm quantum systems and the open source qiskit framework to advance and grow the industry's quantum computing ecosystem.
founded in 2014 and backed by some of the world's leading quantum computing companies, cqc is a global leader in quantum software and quantum algorithms that help clients get the best out of rapidly evolving quantum computing hardware.
cqc is partnering with ibm to offer new models of access to ibm's quantum systems on the cloud to enable more organizations to get started in quantum computing and join the ibm q network.
a unique and immediately available first offering to clients from cqc is a six- month access term along with a joint application research engagement.
denise ruffner, cqc's chief business officer, stated "we are excited to deepen our strong partnership with ibm quantum by becoming the first startup- based hub in the ibm q network.
our unique capability to offer shorter term access to ibm's quantum systems, along with a research engagement from our expert research team is an opportunity for corporations to get started on quantum computing by working with cqc and the ibm q network."
cqc was part of the founding group of startups in the ibm q network's startup program, announced in 2018.
ibm invested in cqc in january of 2020, and now have further deepened the relationship by becoming the first startup hub.
currently the ibm q network has over 100 members.
"to accelerate progress toward the first commercial applications, we must make it easier for corporations to get started working with the most advanced quantum computers, supported by quantum application experts working closely with them," said dr. anthony annunziata, director of the ibm q network.
"by partnering with cqc to create the first startup- based hub, researchers and application developers in enterprises will be able to develop the skills, expertise and use case- specific approaches that will be essential to harness the capability of quantum for business advantage in the coming years."
about cambridge quantum computing:
cambridge quantum computing (cqc) is a world- leading quantum computing software company with offices in cambridge (uk), london, san francisco, washington, dc and tokyo.
cqc builds tools for the commercialization of quantum technologies that will have a profound global impact.
about ibm quantum:
ibm quantum is an industry- first initiative to build commercial universal quantum computing systems for business and science applications.
ibm q network(tm) is a trademark of international business machines corporation.
for more information about ibm's quantum computing efforts, please visit ibm.com/quantum.
researchers at the university of waterloo have developed a method that could pave the way to establishing universal standards for measuring the performance of quantum computers.
the new method, called cycle benchmarking, allows researchers to assess the potential of scalability and to compare one quantum platform against another.
"this finding could go a long way toward establishing standards for performance and strengthen the effort to build a large- scale, practical quantum computer," said joel wallman, an assistant professor at waterloo's faculty of mathematics and institute for quantum computing.
"a consistent method for characterizing and correcting the errors in quantum systems provides standardization for the way a quantum processor is assessed, allowing progress in different architectures to be fairly compared."
cycle benchmarking provides a solution that helps quantum computing users to both determine the comparative value of competing hardware platforms and increase the capability of each platform to deliver robust solutions for their applications of interest.
the breakthrough comes as the quantum computing race is rapidly heating up, and the number of cloud quantum computing platforms and offerings is quickly expanding.
in the past month alone, there have been significant announcements from microsoft, ibm and google.
this method determines the total probability of error under any given quantum computing applications when the application is implemented through randomized compiling.
this means that cycle benchmarking provides the first cross- platform means of measuring and comparing the capabilities of quantum processors that is customized to users' applications of interest.
"thanks to google's recent achievement of quantum supremacy, we are now at the dawn of what i call the `quantum discovery era', said joseph emerson, a faculty member at iqc.
"this means that error- prone quantum computers will deliver solutions to interesting computational problems, but the quality of their solutions can no longer be verified by high- performance computers.
"we are excited because cycle benchmarking provides a much- needed solution for improving and validating quantum computing solutions in this new era of quantum discovery."
emerson and wallman founded the iqc spin- off quantum benchmark inc., which has already licensed this technology to several world- leading quantum computing providers, including google's quantum ai effort.
quantum computers offer a fundamentally more powerful way of computing, thanks to quantum mechanics.
compared to a traditional or digital computer, quantum computers can solve certain types of problems more efficiently.
however, qubits -  the basic processing unit in a quantum computer -  are fragile; any imperfection or source of noise in the system can cause errors that lead to incorrect solutions under a quantum computation.
gaining control over a small- scale quantum computer with just one or two qubits is the first step in a larger, more ambitious endeavour.
a larger quantum computer may be able to perform increasingly complex tasks, like machine learning or simulating complex systems to discover new pharmaceutical drugs.
engineering a larger quantum computer is challenging; the spectrum of error pathways becomes more complicated as qubits are added and the quantum system scales.
characterizing a quantum system produces a profile of the noise and errors, indicating if the processor is performing the tasks or calculations, it is being asked to do.
to understand the performance of any existing quantum computer for a complex problem or to scale up a quantum computer by reducing errors, it's first necessary to characterize all significant errors affecting the system.
wallman, emerson and a group of researchers at the university of innsbruck identified a method to assess all error rates affecting a quantum computer.
they implemented this new technique for the ion trap quantum computer at the university innsbruck and found that error rates don't increase as the size of that quantum computer scales up a very promising result.
"cycle benchmarking is the first method for reliably checking if you are on the right track for scaling up the overall design of your quantum computer," said wallman.
"these results are significant because they provide a comprehensive way of characterizing errors across all quantum computing platforms."
ever heard of a tempest in a teacup?
sure.
but what about a tornado on a chip?
according to some rather bizarre research being undertaken by a group of scientists at the university of cambridge in the u.k., quantum fluid trapped on top of a semiconductor chip, which manifests itself as a sort of tornado- like vortex, can be used to measure movements.
apparently, a mass of these quantum twisters can line up like a regiment of tiny soldiers, allowing for the engineering of quantum circuits and chips that will allow super precise motion measurement.
if that sounds confusing, it's because it is.
we're talking quantum physics here, that branch of science that requires quantum leaps of imagination.
polariton quantum rivers flow from four hills creating quantum tornadoes in the valley.
the cambridge scientists, however, have explained it by saying the tiny twisters generated on the surface of a semiconductor chip can be controlled, something most big twisters of the wizard of oz variety cannot be.
by controlling where electrons move and how they interact with light, the scientists managed to merge electrons and photons into a new quantum particle they call "polariton."
these polaritons are half- light and half matter, making them "feather- light" and rather zippy, with researchers describing their movements similar to cascading water in a mountain river.
the quantum systems are also actually rather big, on a microscopic scale, spanning the width of a human hair, making them easy to observe.
the team, led by professor jeremy baumberg and the theoretical quantum fluids group of dr natalia berloff, has been able to do all kinds of unexpected things with the "liquid" on chips' surface, forcing it to flow in various directions, over bumps and letting it "settle" into "wildly raging quantum oceans."
the long and short of the matter is that playing with mini quantum tornadoes on a chip is actually allowing scientists to both create and control macroscopic quantum states.
what's that good for?
well, research and science, of course.
so much so, in fact, that the study has been granted funding from the engineering and physical sciences research council and the european union.
that being said, a tempest in a teacup now makes a lot more sense to me than quantum physics ever could.
related stories:
doubling performance every year is now the benchmark for quantum computers as designers look to eda vendors for new automation tools.
quantum computing tools
the latest ibm quantum volume benchmark shows that quantum computing is well on its way to simulate moore's law in the doubling of capabilities every year.
this realization has put pressure on semiconductor eda tools vendors to start developing sophisticated tools for quantum computing.
this "pressure" is not necessarily a bad thing as eda tool vendors are currently wrestling with a potential slowdown in moore's law for many market segments.
this slow- down at lower nodes is the of result increasing complexity and cost.
additionally, the demand for lower node designs is not as great as several markets - like iot - which are easily satisfied with higher node (not leading edge) tools and processes.
existing silicon chips and technology will not be replaced anytime soon by quantum computers.
rather, quantum computers will be but one of many computing technology options for the future.
it may well be that quantum computing chips will exist next to microprocessors, graphic processing units (gpu), accelerators and other processors within a single computer or server.
still, eda tool vendors are in the best position to develop automation tools for creating quantum computers.
but first they must figure out what type of quantum computers could be designed with existing chip tools and also fabricated in existing foundries.
a profitable business case must be developed that supports these two tasks.
aside from a profitable business case, automation tools won't really be needed until it becomes too difficult to design quantum computers by hand.
during a panel on future innovations at last year's design automation conference (dac), ibm's rasit onur topaloglu explained that this very issue arose early at ibm, namely, the point at which automation tools will be needed to design quantum computers.
"we've concluded that up until 200 qubits, maybe we can still do it manually," noted topaloglu.
"i am not going to project when we will reach 200 qubits, but we already have an 80 qubits architecture."
he went on to explain that, once quantum computing takes off, automation tools will be needed right away.
john blyler is a design news senior editor, covering the electronics and advanced manufacturing spaces.
with a bs in engineering physics and an ms in electrical engineering, he has years of hardware- software- network systems experience as an editor and engineer within the advanced manufacturing, iot and semiconductor industries.
john has co- authored books related to system engineering and electronics for ieee, wiley, and elsevier.
future quantum computers will be able to solve problems where conventional computers fail today.
we are still far away from any large- scale implementation, however, because quantum systems are very sensitive to environmental noise.
although systems can be protected from noise in principle, researchers have been able to build only small prototypes of quantum computers experimentally.
one way to reduce the error rate is by encoding quantum information not in one single quantum particle but in several quantum objects.
these logical quantum bits or qubits are more robust against noise.
in the last few years, theoretical physicists have developed a whole range of error correction codes and optimized them for specific tasks.
physicists hendrik poulsen nautrup and hans briegel from the institute of theoretical physics of the university of innsbruck and nicolai friis, now at the institute of quantum optics and quantum information in vienna, have found a technique to transfer quantum information between systems that are encoded differently.
interface between processor and memory
similar to classical computers, future quantum computers might be built with different components.
scientists have already built small- scale quantum processors and memories experimentally, and they have used different protocols to encode logical qubits: for example, for quantum processors they use so- called color codes and for quantum memories surface codes.
"for the two systems to interact with each other quantum mechanically, we have to connect them," says phd student hendrik poulsen nautrup.
"we have developed a protocol that allows us to merge quantum systems that are encoded differently."
the scientists suggest to locally modify specific elements of the encoded quantum bits.
this process is also called lattice surgery, which is used to couple systems such as quantum processors and memories.
once the systems are temporarily "sewed" together, quantum information can be teleported from the processor to the memory and vice versa.
"similar to a data bus in a conventional computer, scientists can use this technique to connect the components of a quantum computer," explains poulsen nautrup.
this new scheme is another step towards building a universal quantum computer and research for experimental realization is under way.
the research was conducted within the framework of the doctoral program atoms, light, and molecules offered at the university of innsbruck and was funded by the austrian science fund and the templeton world charity foundation.
so says alan woodward, cto at business and information technology consultancy charteris.
a chartered physicist, he points to the fundamentals of quantum mechanics, and the behaviour of energy as packets, or quanta, at an atomic level.
"but light doesn't only travel in quanta: it also travels as waves, and this gives rise to the phenomenon known as particle- wave duality," he explains.
"and an important implication is that, at certain sub- atomic conditions, photons, electrons and other types of energy do not behave only as particles or only as waves, but can exhibit properties of both."
so whereas in a classical computer, a 'bit' is a fundamental unit of information, in a quantum computer, it can exist not in one of two states but in one of four.
quantum bits, or 'qubits', are not binary but quaternary.
woodward explains the result as a blend, or superposition, of states founded on probabilities, and hence the sophistication.
"for example: computers that play chess - arguably one of the most successful current applications of artificial intelligence - use programs that make extensive use of the analysis of the probabilities of the opponent making a certain response," he explains.
"so, people who consider quantum computing to be the future of it argue that the qubit liberates conventional computing from simplistic on/off engineering.
quantum computing may facilitate new types of computer applications ... and enabling us to escape the limitations of moore's law," he adds.
he also believes that quantum computing may even be the route to providing powerful approximations to higher functions of the human mind, which conventional computers are notoriously bad at replicating.
"there is already clear evidence that quantum computing is likely to facilitate the creation of levels of computer security way beyond what is currently achievable," he asserts.
quantum computers offer another look at classic physics concepts.
during the computational science initiative's advisory board meeting in july 2019, raffaele miceli, who co- authored a study of thermo field quantum algorithms while working as a student at csi, described his quantum computing research to other summer interns at brookhaven lab.
image credit: bnl
"think what we can do if we teach a quantum computer to do statistical mechanics," posed michael mcguigan, a computational scientist with the computational science initiative at the u.s. department of energy's brookhaven national laboratory.
at the time, mcguigan was reflecting on ludwig boltzmann and how the renowned physicist had to vigorously defend his theories of statistical mechanics.
boltzmann, who proffered his ideas about how atomic properties determine physical properties of matter in the late 19th century, had one extraordinarily huge hurdle: atoms were not even proven to exist at the time.
fatigue and discouragement stemming from his peers not accepting his views on atoms and physics forever haunted boltzmann.
probability associated to the wave function of the universe calculated using qiskit.
the vertical axis denotes the probability of realizing a particular configuration in the simple model of early cosmology, while the other axes indicate scale factor of the universe and magnitude of the inflaton field (from kocher and mcguigan, 2018).
image credit: bnl
today, boltzmann's factor, which calculates the probability that a system of particles can be found in a specific energy state relative to zero energy, is widely used in physics.
for example, boltzmann's factor is used to perform calculations on the world's largest supercomputers to study the behavior of atoms, molecules, and the quark "soup" discovered using facilities such as the relativistic heavy ion collider located at brookhaven lab and the large hadron collider at cern.
while it took a sea change to show boltzmann was right, computer scientists now are at the precipice of a new computing wave, making the leap from supercomputers and bytes to quantum systems and quantum bits (or "qubits").
these quantum computers have the potential to unlock some of the most mysterious concepts in physics.
and, oddly, these so- called mysteries may seem a bit familiar to many.
time and temperature brought to you by...
although most people are well acquainted with the notions of time and temperature and check on them several times a day, it turns out these basic concepts remain enigmatic in physics.
boltzmann's factor helps model temperature effects that can be used to predict and control atomic behavior and physical properties, and they work great on classical computers.
however, on a quantum computer, the quantum logic gates used in the computation (akin to logic gates found in digital circuits) are represented by complex numbers, as opposed to boltzmann's factor, which by definition, is real.
this issue offered mcguigan and his student/coauthor raffaele miceli an interesting problem to tackle using a quantum computing testbed provided by way of brookhaven lab's access agreement to ibm's universal quantum computing systems, through the ibm q hub at oak ridge national laboratory.
the collaboration allows brookhaven (among others in network) access to ibm's commercial quantum systems, including 20- and 53- qubit systems for experiments.
"on a quantum computer, there is another way to simulate finite temperature called thermo field dynamics, which is able to compute quantities that are both time- and temperature- dependent," mcguigan explained.
"in this formalism, you construct a double of the system, called the thermo double, then proceed with the calculation on a quantum computer as the computation can be represented in terms of quantum logic gates with complex numbers.
"in the end, you can sum the double states and generate an effective boltzmann's factor for calculations at finite temperature," he continued.
"there also are certain advantages of the formalism.
for example, you can study the effects of finite temperature and how the system evolves in real time as time and temperature are separated using this quantum algorithm.
one disadvantage is that it requires twice as many qubits as a zero temperature calculation to handle the double states."
miceli and mcguigan demonstrated how to implement the quantum algorithm for thermo field dynamics for finite temperature on a simple system involving a few particles and found perfect agreement with the classical computation.
their work used resources from both classical and quantum computing.
according to mcguigan, they used qiskit open- source quantum computing software that allowed them to create their algorithm in the cloud.
qiskit then transpiled that code to pulses that communicate with a quantum computer in real time (in this case, an ibm q device).
optimizers that run classical algorithms further enable the back and forth between the traditional and quantum systems.
"our experiment shows quantum systems have an advantage of representing real- time calculations exactly rather than rotating from imaginary time to real time to find a result," mcguigan explained.
"it offers a truer picture of how a system evolves.
we can map the problem to a quantum simulation that lets it evolve."
into the cosmos
quantum cosmology is another area where mcguigan anticipates that new quantum computing options will have profound impact.
despite the multitude of advances in understanding the universe made possible by modern supercomputers, some physical systems remain beyond their reach.
the mathematical complexity, which usually includes accounting for full quantum gravity theory, is simply too great to obtain exact solutions.
however, a true quantum computer, complete with the ability to exploit entanglement and superposition, would expand the options for new, more precise algorithms.
"quantum systems can realize path integrals in real time, giving us access to large- scale simulations of the universe," mcguigan said.
"you can visualize the calculated wavefunction of the universe as it evolves forward without first formulating a full theory of quantum gravity."
again, using the qiskit package and access to ibm q hardware, mcguigan and his collaborator charles kocher, a student at brown university, employed a mix of classical computational methods and vqe to run varied experiments, including one that examined systems with gravity coupled to a boson field called an inflaton, a hypothetical particle that plays an important role in modern cosmology.
their work showed the hybrid vqe yielded wavefunctions consistent with the wheeler- dewitt equation, which mathematically combines quantum mechanics with albert einstein's theory of relativity.
inspiration on an expanding scale
while early quantum experiments are leading to different perspectives of the basics behind physics, quantum computing is expected to contribute major advances toward solving longstanding problems impacting doe's missions.
among them, it can be a tool for unveiling new materials, solving energy challenges, or adding to fundamental understandings (like time and temperature) in high energy physics and cosmology.
in turn, these changes could cascade into more readily recognizable areas.
for example, drug developers need more realized quantum mechanics to understand the structure of molecules.
quantum computers can enable discoveries by affording simulations of the full quantum mechanics that would provide a truly practical point of view.
"there seems to always be interest in the basics behind physics," mcguigan said.
"it has been of interest to the public for millennia.
right now, the combination of theoretical expertise and actual technology is converging with quantum computing.
yet, it still is a very human endeavor."
for now, using near- term quantum computers to solve small thermo field problems or to take a new look at an old universe is inspiring researchers to scale up their algorithms as they do bigger things in science.
"we get emboldened to do different things.
we all do," mcguigan said.
"other groups around the world, such as the perimeter institute in canada and universiteit van amsterdam in the netherlands, are already extending the thermo field double quantum algorithm to even bigger systems.
with the emergence of large near- term quantum computers of 50- 100 qubits, the goal is to run finite temperature simulations on realistic systems involving many particles.
it is exciting to have an actual quantum computer to test these ideas and problems that we once had no solutions for.
quantum mechanics with no tradeoffs- that is what science is all about."
recently, a team of australian researchers have discovered that a particular rare earth crystal can store quantum information for longer than one second.
so, what?
moore's law is the observation that the number of transistors per square inch on an integrated circuit has doubled every two years since their invention.
the law has held true for more than 50 years, but one day this rule is going to fail.
eventually, we will reach a minimum size, under which there simply aren't enough atoms to build working semiconductors.
where are we supposed to go from there?
the answer seems to be quantum computers.
in a nutshell, in classical computing, a bit shows the on/off state of a transistor.
in quantum computing, qubits show the spin state of a particle.
because of the quantum effects subatomic particles have on each other, the information storage potential of qubits is exponentially greater than that of classical bits.
there are many technical challenges to building the first quantum computer.
david divincenzo, of ibm, listed the following requirements for a practical quantum computer:
scalable physically to increase the number of qubits
qubits that can be initialized to arbitrary values
quantum gates that are faster than decoherence time
universal gate set
qubits that can be read easily
when these challenges are met, it will be possible to build a quantum computer.
the researchers team, led by anu professor matthew sellars, are addressing that third challenge: decoherence time.
coherence essentially refers to the stability of a particle's spin state.
for qubits to be useful for storing data, they need to be readable before their state changes.
prior to this crystal, quantum information could only be stored for fractions of a second.
sellars and his team have multiplied that by a factor of 10,000.
in short, it's quantum memory.
just as classical computers didn't reach their full potential until the advent of the internet, sellars believes that quantum computers need to be interconnected to reach their full potential.
this discovery enables the sending of qubit data over a long range, potentially around the globe.
furthermore, the erbium- doped crystal developed by sellars and his team operates in the same 1550- nanometer band as today's fiber- optic telecom networks.
this eliminates a complex conversion process and would enable quantum computer systems to easily connect to existing fiber- optic systems.
the discovery also opens the door for other practical devices, such as a quantum light source or photon emitter, or as an optical link to connect quantum computers to a quantum internet.
the material's versatility may make it possible to connect the many types of quantum computers being developed at google, ibm and other facilities, including superconducting and silicon- based qubits.
the published work is accessible here through the journal nature physics.
for more on quantum computing, click here.
a new kind of quantum computer is being proposed by scientists from the tu wien (vienna) and japan (national institute of informatics and ntt basic research labs).
the quantum computer is the holy grail of quantum technology.
its computing power would eclipse even the fastest classical computers we have today.
a team of researchers from tu wien (vienna) the national institute for informatics (tokyo) and ntt basic research labs in japan has now proposed a new architecture for quantum computing, based on microscopic defects in diamond.
a reliable quantum computer capable of solving complex problems would have to consist of billions of quantum systems, and such a device is still out of reach.
but the researchers are convinced that the basic elements of their newly proposed architecture are better suited to be miniaturized, mass- produced and integrated on a chip than previously suggested quantum computing concepts.
experiments towards the new quantum computing architecture are already being undertaken at tu wien.
fragile quantum superpositions
for decades, scientists have been trying to use quantum systems for logical calculations.
"in a classical computer, one bit can only store a number: zero or one.
quantum physics, however, allows superpositions of states.
a quantum bit can be in the state zero and the state one at the same time - and this opens up unbelievable possibilities for computing", says jorg schmiedmayer (tu wien).
such superposition states can be implemented in different kinds of quantum systems, such as ions, captured in electromagnetic traps, or in superconducting quantum bits.
the architecture which has now been published in the journal physical review x is different: nitrogen atoms which can occupy two different spin states are injected into a small diamond.
every nitrogen defect is trapped in an optical resonator made of two mirrors.
via glass fibres, photons are coupled to the quantum system consisting of the resonator, the diamond and the nitrogen atom.
this way, it is possible to read and manipulate the state of the quantum system without destroying the quantum properties of the spins in the diamond.
realistic quantum computers need error correction
each system - made up of mirrors, diamond and a nitrogen defect - can store one quantum bit of information: zero, one, or an arbitrary superposition of both.
but usually such a quantum bit is very unstable.
error correction procedures are needed to build a quantum computer that works reliably.
"if error correction is used, a quantum bit cannot be stored in one single quantum particle any more.
instead, a complex architecture of interconnected quantum systems is required", says michael trupke (tu wien).
the researchers calculated how the resonators, diamonds and nitrogen atoms can be assembled to create an error resistant two dimensional quantum system, a so- called "topologically protected quantum computer".
according to the calculations, about 4.5 billion such quantum systems would be sufficient to implement the algorithm "shor- 2048", which is able to calculate prime factors of a 2048- bit- number.
this huge number of quantum elements is required in any quantum computer architecture, no matter whether ion traps, superconducting quantum bits or nitrogen spins in diamonds are used.
"our approach has the big advantage that we know how to make the elements smaller.
this architecture has great potential for miniaturization and mass production", says michael trupke.
"whole industries are working with diamonds, materials science is progressing rapidly.
there are still many obstacles to overcome, but connecting nitrogen spins in solid materials opens up a path that could finally lead to a functioning quantum computer."
only the beginning - just like the transistor
trupke compares the current state of quantum computing with the early days of electronic computing: "when the first transistors were built, nobody could imagine placing them on a small chip by the billions.
today, we carry around such chips in our pockets.
these nitrogen spins in diamond could develop just like transistors did in classical computer science."
at tu wien, researchers have begun to create a small- scale realisation of this new architecture.
"we have the great advantage of being able to collaborate with a number of internationally renowned research teams in materials research and quantum technology right here at tu wien", says jorg schmiedmayer.
friedrich aumayr works on methods to inject the nitrogen atoms into the diamonds, peter mohn obtains numerical data in large- scale computer simulations.
the microcavity arrays are the result of an ongoing collaboration with ulrich schmid at the centre for micro- and nanostructures (zmns) within tu wien.
diamond chips are routinely analysed in the university's own x- ray centre.
there may still be a long way to go before algorithms like shor- 2048 run on a quantum computer.
but scientists believe that it should become possible to entangle quantum building blocks, creating larger cluster cells, within the next few years.
"once this happens, the scale- up will be fast", says kae nemoto of the national institute of informatics.
"in the end," schmiedmayer says, "it all depends on whether we manage to enter an era of mass production and miniaturization in quantum technology.
i do not see any physical laws that should keep us from doing that."
a new test to check if a quantum computer is giving correct answers to questions beyond the scope of traditional computing could help the first quantum computer that can outperform a classical computer to be realised.
by creating a protocol that allows a quantum computer to check its own answers to difficult problems, the scientists from the university of warwick have provided a means to confirm that a quantum computer is working correctly without excessive use of resources.
samuele ferracin, theodoros kapourniotis and dr animesh datta from the university's department of physics have recently tackled this problem in a paper for the new journal of physics, published today (18 november).
the researchers have developed a protocol to quantify the effects of noise on the outputs of quantum computers.
noise is defined as anything that affects a quantum machine's hardware but is beyond the user's control, such as fluctuations in temperature or flaws in the fabrication.
this can affect the accuracy of a quantum computer's results.
when applied, the researchers' test produces two percentages: how close it estimates the quantum computer is to the correct result and how confident a user can be of that closeness.
the test will help the builders of quantum computers to determine whether their machine is performing correctly to help refine their performance, a key step in establishing the usefulness of quantum computing in the future.
dr animesh datta from the university of warwick department of physics said: "a quantum computer is only useful if it does two things: first, that it solves a difficult problem; the second, which i think is less appreciated, is that it solves the hard problem correctly.
if it solves it incorrectly, we had no way of finding out.
so what our paper provides is a way of deciding how close the outcome of a computation is to being correct."
determining whether a quantum computer has produced a correct answer to a difficult problem is a significant challenge as, by definition, these problems are beyond the scope of an existing classical computer.
checking that the answer it has produced is correct typically involves using a large number of classical computers to tackle the problem, something that is not feasible to do as they tackle ever more challenging problems.
instead, the researchers have proposed an alternative method that involves using the quantum computer to run a number of easy calculations that we already know the answer to and establishing the accuracy of those results.
based on this, the researchers can put a statistical boundary on how far the quantum computer can be from the correct answer in the difficult problem that we want it to answer, known as the target computation.
it is a similar process to that which computer programmers use to check large computer programs, by putting in small functions with known answers.
if the program answers enough of these correctly then they can be confident that the whole program is correct.
dr datta adds: "the whole point of having a quantum computer is to not spend an exponential amount of time solving problems, so taking an exponential amount of time to check whether it's correct or not defeats the point of it.
so our method is efficient in that it doesn't require an exponential amount of resources.
"we do not need a classical computer to check our quantum computer.
our method is self- contained within a quantum system that can be used independently of large servers."
lead author samuele ferracin has been developing ways for scientists working on quantum computers to incorporate the test into their work.
he said: "we have spent the last few years thinking about new methods to check the answers of quantum computers and proposing them to experimentalists.
the first methods turned out to be too demanding for the existing quantum computers, which can only implement 'small' computations and perform restricted tasks.
with our latest work we have successfully developed a method that suits existing quantum computers and encompasses all their main limitations.
we are now collaborating with experimentalists to understand how it performs on a real machine."
quantum computing harnesses the unusual properties of quantum physics to process information in a wholly different way to conventional computers.
taking advantage of the behaviour of quantum systems, such as existing in multiple different states at the same time, this radical form of computing is designed to process data in all of those states simultaneously, lending it a huge advantage over classical computing.
certain kinds of problems, like those found in codebreaking and in chemistry, are particularly suited to exploiting this property.
the last few years have seen unprecedented experimental advances.
the largest quantum computers are doubling in size every six months and seem now very close to achieve quantum supremacy.
quantum supremacy refers to a milestone in the development of quantum computers, where a quantum computer first performs a function that would require an unreasonably large amount of time using a classical computer.
dr datta adds: "what we are interested in is designing or identifying ways of using these quantum machines to solve hard problems in physics and chemistry, to design new chemicals and materials, or identify materials with interesting or exotic properties.
and that is why we are particularly interested in the correctness of the computation."
for the first time, physicists have demonstrated that clients who possess only classical computers- and no quantum devices- can outsource computing tasks to quantum servers that perform blind quantum computing.
"blind" means the quantum servers do not have full information about the tasks they are computing, which ensures that the clients' computing tasks are kept secure.
until now, all blind quantum computing demonstrations have required that clients have their own quantum devices in order to delegate tasks for blind quantum computing.
the team of physicists, led by jian- wei pan and chao- yang lu at the university of science and technology of china, have published a paper on the demonstration of blind quantum computing for classical clients in a recent issue of physical review letters.
"we have demonstrated for the first time that a fully classical client can delegate a quantum computation to untrusted quantum servers while maintaining full privacy," lu told phys.org.
the idea behind blind quantum computing is that, while there are certain computing tasks that quantum computers can perform exponentially better than classical computers, quantum computing still involves expensive, complex hardware that will make it inaccessible for most clients.
so instead of everyone owning their own quantum computing devices, blind quantum computing makes it possible for clients to outsource their computing tasks to quantum servers that do the job for them.
ensuring that the quantum computing is performed blindly is important, since many of the potential applications of quantum computing will likely require a high degree of security.
although several blind quantum computing protocols have been performed in the past few years, they have all required that the clients have the ability to perform certain quantum tasks, such as prepare or measure qubit states.
eliminating this requirement will provide greater access to blind quantum computing, since most clients only have classical computing systems.
in the new study, the physicists experimentally demonstrated that a classical client can outsource a simple problem (factoring the number 15) to two quantum servers that do not fully know what problem they are solving.
this is because each server completes part of the task, and it is physically impossible for the servers to communicate with each other.
to ensure that the quantum servers are performing their tasks honestly, the client can give them "dummy tasks" that are indistinguishable from the real task to test their honesty and correctness.
the researchers expect that the new method can be scaled up for realizing secure, outsourced quantum computing, which could one day be implemented on quantum cloud servers and make the power of quantum computing widely available.
"blind quantum computing protocol is an important privacy- preserving technique for future secure quantum cloud computing and secure quantum networks," lu said.
"applying our implemented blind quantum computing protocol, classical clients could delegate computation tasks to servers 'in the cloud' blindly and correctly without directly owning quantum devices.
it saves resources and makes scalable quantum computing possible."
in the future, the physicists want to make blind quantum computing even easier for clients by further reducing the requirements.
"we plan to study more robust blind quantum computing protocols with fewer required resources and fewer constraints theoretically and experimentally," lu said.
"we will also explore blind quantum computing for more application scenarios, such as multi- user blind quantum computing, publicly verifiable quantum computing, and secure multi- party quantum computing."
research teams all over the world are exploring different ways to design a working computing chip that can integrate quantum interactions.
now, unsw engineers believe they have cracked the problem, reimagining the silicon microprocessors we know to create a complete design for a quantum computer chip that can be manufactured using mostly standard industry processes and components.
the new chip design, published in the journal nature communications, details a novel architecture that allows quantum calculations to be performed using existing semiconductor components, known as cmos (complementary metal- oxide- semiconductor) - the basis for all modern chips.
it was devised by andrew dzurak, director of the australian national fabrication facility at the university of new south wales (unsw), and dr menno veldhorst, lead author of the paper who was a research fellow at unsw when the conceptual work was done.
"we often think of landing on the moon as humanity's greatest technological marvel," said dzurak, who is also a program leader at australia's famed centre of excellence for quantum computation and communication technology (cqc2t).
"but creating a microprocessor chip with a billion operating devices integrated together to work like a symphony - that you can carry in your pocket!
- is an astounding technical achievement, and one that's revolutionised modern life.
"with quantum computing, we are on the verge of another technological leap that could be as deep and transformative.
but a complete engineering design to realise this on a single chip has been elusive.
i think what we have developed at unsw now makes that possible.
and most importantly, it can be made in a modern semiconductor manufacturing plant," he added.
veldhorst, now a team leader in quantum technology at qutech - a collaboration between delft university of technology and tno, the netherlands organisation for applied scientific research - said the power of the new design is that, for the first time, it charts a conceivable engineering pathway toward creating millions of quantum bits, or qubits.
"remarkable as they are, today's computer chips cannot harness the quantum effects needed to solve the really important problems that quantum computers will.
to solve problems that address major global challenges - like climate change or complex diseases like cancer - it's generally accepted we will need millions of qubits working in tandem.
to do that, we will need to pack qubits together and integrate them, like we do with modern microprocessor chips.
that's what this new design aims to achieve.
"our design incorporates conventional silicon transistor switches to 'turn on' operations between qubits in a vast two- dimensional array, using a grid- based 'word' and 'bit' select protocol similar to that used to select bits in a conventional computer memory chip," he added.
"by selecting electrodes above a qubit, we can control a qubit's spin, which stores the quantum binary code of a 0 or 1.
and by selecting electrodes between the qubits, two- qubit logic interactions, or calculations, can be performed between qubits."
a quantum computer exponentially expands the vocabulary of binary code used in modern computers by using two spooky principles of quantum physics - namely, 'entanglement' and 'superposition'.
qubits can store a 0, a 1, or an arbitrary combination of 0 and 1 at the same time.
and just as a quantum computer can store multiple values at once, so it can process them simultaneously, doing multiple operations at once.
this would allow a universal quantum computer to be millions of times faster than any conventional computer when solving a range of important problems.
but to solve complex problems, a useful universal quantum computer will need a large number of qubits, possibly millions, because all types of qubits we know are fragile, and even tiny errors can be quickly amplified into wrong answers.
"so we need to use error- correcting codes which employ multiple qubits to store a single piece of data," said dzurak.
"our chip blueprint incorporates a new type of error- correcting code designed specifically for spin qubits, and involves a sophisticated protocol of operations across the millions of qubits.
it's the first attempt to integrate into a single chip all of the conventional silicon circuitry needed to control and read the millions of qubits needed for quantum computing."
"we expect that there will still be modifications required to this design as we move towards manufacture, but all of the key components that are needed for quantum computing are here in one chip.
and that's what will be needed if we are to make quantum computers a workhorse for calculations that are well beyond today's computers," dzurak added.
"it shows how to integrate the millions of qubits needed to realise the true promise of quantum computing."
building such a universal quantum computer has been called the 'space race of the 21st century'.
for a range of calculations, they will be much faster than existing computers, and for some challenging problems they could find solutions in days, maybe even hours, when today's best supercomputers would take millions of years.
there are at least five major quantum computing approaches being explored worldwide: silicon spin qubits, ion traps, superconducting loops, diamond vacancies and topological qubits; unsw's design is based on silicon spin qubits.
the main problem with all of these approaches is that there is no clear pathway to scaling the number of quantum bits up to the millions needed without the computer becoming huge a system requiring bulky supporting equipment and costly infrastructure.
that's why unsw's new design is so exciting: relying on its silicon spin qubit approach - which already mimics much of the solid- state devices in silicon that are the heart of the us$380 billion global semiconductor industry - it shows how to dovetail spin qubit error correcting code into existing chip designs, enabling true universal quantum computation.
unlike almost every other major group elsewhere, cqc2t's quantum computing effort is obsessively focused on creating solid- state devices in silicon, from which all of the world's computer chips are made.
and they're not just creating ornate designs to show off how many qubits can be packed together, but aiming to build qubits that could one day be easily fabricated - and scaled up.
"it's kind of swept under the carpet a bit, but for large- scale quantum computing, we are going to need millions of qubits," said dzurak.
"here, we show a way that spin qubits can be scaled up massively.
and that's the key."
the design is a leap forward in silicon spin qubits; it was only two years ago, in a paper in nature, that dzurak and veldhorst showed, for the first time, how quantum logic calculations could be done in a real silicon device, with the creation of a two- qubit logic gate - the central building block of a quantum computer.
"those were the first baby steps, the first demonstrations of how to turn this radical quantum computing concept into a practical device using components that underpin all modern computing," said mark hoffman, unsw's dean of engineering.
"our team now has a blueprint for scaling that up dramatically.
"we've been testing elements of this design in the lab, with very positive results.
we just need to keep building on that - which is still a hell of a challenge, but the groundwork is there, and it's very encouraging.
it will still take great engineering to bring quantum computing to commercial reality, but clearly the work we see from this extraordinary team at cqc2t puts australia in the driver's seat," he added.
other cqc2t researchers involved in the design published in the nature communications paper were henry yang and gertjan eenink, the latter of whom has since joined veldhorst at qutech.
the unsw team has struck a a$83 million deal between unsw, telstra, commonwealth bank and the australian and new south wales governments to develop, by 2022, a 10- qubit prototype silicon quantum integrated circuit - the first step in building the world's first quantum computer in silicon.
in august, the partners launched silicon quantum computing pty ltd, australia's first quantum computing company, to advance the development and commercialisation of the team's unique technologies.
the nsw government pledged a$8.7 million, unsw a$25 million, the commonwealth bank a$14 million, telstra a$10 million and the australian government a$25 million.
scientists at ibm research have achieved major advances in quantum computing device performance that may accelerate the realization of a practical, full- scale quantum computer.
for specific applications, quantum computing, which exploits the underlying quantum mechanical behavior of matter, has the potential to deliver computational power that is unrivaled by any supercomputer today.
using a variety of techniques in the ibm labs, scientists have established three new records for reducing errors in elementary computations and retaining the integrity of quantum mechanical properties in quantum bits (qubits) - the basic units that carry information within quantum computing.
ibm has chosen to employ superconducting qubits, which use established microfabrication techniques developed for silicon technology, providing the potential to one day scale up to and manufacture thousands or millions of qubits.
ibm researchers will be presenting their latest results today at the annual american physical society meeting taking place february 27- march 2, 2012 in boston, mass.
the special properties of qubits will allow quantum computers to work on millions of computations at once, while desktop pcs can typically handle minimal simultaneous computations.
for example, a single 250- qubit state contains more bits of information than there are atoms in the universe.
these properties will have wide- spread implications foremost for the field of data encryption where quantum computers could factor very large numbers like those used to decode and encode sensitive information.
"the quantum computing work we are doing shows it is no longer just a brute force physics experiment.
it's time to start creating systems based on this science that will take computing to a new frontier," says ibm scientist matthias steffen, manager of the ibm research team that's focused on developing quantum computing systems to a point where it can be applied to real- world problems.
other potential applications for quantum computing may include searching databases of unstructured information, performing a range of optimization tasks and solving previously unsolvable mathematical problems.
the most basic piece of information that a typical computer understands is a bit.
much like a light that can be switched on or off, a bit can have only one of two values: "1" or "0".
for qubits, they can hold a value of "1" or "0" as well as both values at the same time.
described as superposition, this is what allows quantum computers to perform millions of calculations at once.
one of the great challenges for scientists seeking to harness the power of quantum computing is controlling or removing quantum decoherence - the creation of errors in calculations caused by interference from factors such as heat, electromagnetic radiation, and materials defects.
to deal with this problem, scientists have been experimenting for years to discover ways of reducing the number of errors and of lengthening the time periods over which the qubits retain their quantum mechanical properties.
when this time is sufficiently long, error correction schemes become effective making it possible to perform long and complex calculations.
there are many viable systems that can potentially lead to a functional quantum computer.
ibm is focusing on using superconducting qubits that will allow a more facile transition to scale up and manufacturing.
ibm has recently been experimenting with a unique "three dimensional" superconducting qubit (3d qubit), an approach that was initiated at yale university.
among the results, the ibm team has used a 3d qubit to extend the amount of time that the qubits retain their quantum states up to 100 microseconds - an improvement of 2 to 4 times upon previously reported records.
this value reaches just past the minimum threshold to enable effective error correction schemes and suggests that scientists can begin to focus on broader engineering aspects for scalability.
in separate experiments, the group at ibm also demonstrated a more traditional "two- dimensional" qubit (2d qubit) device and implemented a two- qubit logic operation - a controlled- not (cnot) operation, which is a fundamental building block of a larger quantum computing system.
their operation showed a 95 percent success rate, enabled in part due to the long coherence time of nearly 10 microseconds.
these numbers are on the cusp of effective error correction schemes and greatly facilitate future multi- qubit experiments.
the implementation of a practical quantum computer poses tremendous scientific and technological challenges, but all results taken together paint an optimistic picture of rapid progress in that direction.
core device technology and performance metrics at ibm have undergone a series of amazing advancements by a factor of 100 to 1,000 times since the middle of 2009, culminating in the recent results that are very close to the minimum requirements for a full- scale quantum computing system as determined by the world- wide research community.
in these advances, ibm stresses the importance and value of the ongoing exchange of information and learning with the quantum computing research community as well as direct university and industrial collaborations.
"the superconducting qubit research led by the ibm team has been progressing in a very focused way on the road to a reliable, scalable quantum computer.
the device performance that they have now reported brings them nearly to the tipping point; we can now see the building blocks that will be used to prove that error correction can be effective, and that reliable logical qubits can be realized," observes david divincenzo, professor at the institute of quantum information, aachen university and forschungszentrum juelich.
based on this progress, optimism about superconducting qubits and the possibilities for a future quantum computer are rapidly growing.
while most of the work in the field to date has focused on improvements in device performance, efforts in the community now must now include systems integration aspects, such as assessing the classical information processing demands for error correction, i/o issues, feasibility, and costs with scaling.
ibm envisions a practical quantum computing system as including a classical system intimately connected to the quantum computing hardware.
expertise in communications and packaging technology will be essential at and beyond the level presently practiced in the development of today's most sophisticated digital computers.
quantum computingaiblockchain
last week, a document posted to (and then immediately withdrawn from) nasa's website revealed that a team of google researchers at the tech giant's quantum artificial intelligence lab had reached a significant milestone: demonstrating "quantum supremacy."
in the race to create viable quantum computing technology, a development that is expected to have serious repercussions for finance, energy and cybersecurity, every step towards actionable breakthroughs sends shockwaves through the market.
meet sycamore
quantum supremacy is the development point at which a quantum computer is shown to be able to perform a task beyond the capabilities of even the most powerful supercomputer.
a calculation that reportedly would have taken ibm's summit - the world's most powerful commercially available computer - around 10,000 years to complete was allegedly cracked in under four minutes by google's 53- qubit sycamore machine.
for unknown reasons, the computation wasn't made with google's larger, more powerful 72- qubit bristlecone machine.
the implications of quantum computing are epoch- defining.
"in a discussion of quantum computing at mit technology review's emtech conference in cambridge, massachusetts this week before news of google's paper came out, will oliver, an mit professor and quantum specialist, likened the computing milestone to the first flight of the wright brothers at kitty hawk in aviation," according to the mit technology review, which goes on to explain that "unlike classical bits, which are either a 1 or a 0, qubits can be in a kind of combination of both at the same time.
thanks to other quantum phenomena... quantum computers can crunch large amounts of data in parallel that conventional machines have to work through sequentially."
the biggest disruption - and why the finance industry is most nervous about a quantum dawn before it's ready for one - will be in the field of encryption.
the increased power stemming from quantum computers' ability to process computations in parallel - rather than sequentially like conventional machines - is expected to render modern standards of encryption ineffective against a quantum cyber attack (single handedly the coolest three words i've ever strung together) potentially laying bare every aspect of the financial sector, the state secrets of world governments and the personal information of billions.
see also:
google's ai deepmind might be able to spot acute kidney injury 48 hours before doctors spot it
ibm hits quantum computing 'milestone' revealing its highest quantum volume to date
the truth and nature of this breakthrough is still hazy, however.
ongoing research into creating quantum- proof algorithms is proceeding at a breakneck pace (particularly in the cryptocurrency space) and the national institute of standards and technology (nist) has long been working on post- quantum cryptography.
the implications and shape of the next major technological leap still remain unknown, and aren't likely to affect the day to day lives of the general public for years to come.
today, though, gigabit takes a look at the engineer who reportedly led the team that demonstrated quantum supremacy: john m. martinis.
john martinis
an alumnus of the university of california, berkeley, martinis has been involved in the developing frontier of quantum computing, working for the nist, where he was involved in understanding the basic physics of the coulomb blockade, and worked to use this phenomenon to make a new fundamental electrical standard based on counting electrons.
since 2002, his research effort has focused on building a quantum computer using josephson junctions - the subject of his phd.
"he has pioneered many important demonstrations, including entangled states, bell state violation, fock and arbitrary photon generation, photon noon states, and the quantum von neumannn and rezqu architecture.
in 2010, he was awarded with collaborator andrew cleland the "science breakthrough of the year" for the first demonstration of the quantum ground state in a mechanical oscillator system.
in 2014 he was awarded the london prize for low- temperature physics research on superconducting quantum bits.
dr. martinis was a nist fellow, and is a fellow of the american physical society.
at the university of california, santa barbara he currently holds the wooster chair in experimental physics.
in 2014 he was awarded the london prize for his pioneering work on superconducting qubits.
in 2014, dr. martinis joined google to head up their quantum- hardware effort.
the aim of this research is to build the first useful quantum computer," explains his google bio.
it would seem that, after this week, martinis has - along with a cohort of some of the best engineers on the planet - potentially cracked that goal.
in recent years, quantum devices have become available that enable researchers -  for the first time -  to use real quantum hardware to begin to solve scientific problems.
however, in the near term, the number and quality of qubits (the basic unit of quantum information) for quantum computers are expected to remain limited, making it difficult to use these machines for practical applications.
a hybrid quantum and classical approach may be the answer to tackling this problem with existing quantum hardware.
researchers at the u.s. department of energy's (doe) argonne national laboratory and los alamos national laboratory, along with researchers at clemson university and fujitsu laboratories of america, have developed hybrid algorithms to run on quantum machines and have demonstrated them for practical applications using ibm quantum computers (see below for description of argonne's role in the ibm q hub at oak ridge national laboratory [ornl]) and a d- wave quantum computer.
the team's work is presented in an article entitled "a hybrid approach for solving optimization problems on small quantum computers" that appears in the june 2019 issue of the institute of electrical and electronics engineers (ieee) computer magazine.
concerns about qubit connectivity, high noise levels, the effort required to correct errors, and the scalability of quantum hardware have limited researchers' ability to deliver the solutions that future quantum computing promises.
the hybrid algorithms that the team developed employ the best features and capabilities of both classical and quantum computers to address these limitations.
for example, classical computers have large memories capable of storing huge datasets -  a challenge for quantum devices that have only a small number of qubits.
on the other hand, quantum algorithms perform better for certain problems than classical algorithms.
to distinguish between the types of computation performed on two completely different types of hardware, the team referred to the classical and quantum stages of hybrid algorithms as central processing units (cpus) for classical computers and quantum processing units (qpus) for quantum computers.
the team seized on graph partitioning and clustering as examples of practical and important optimization problems that can already be solved using quantum computers: a small graph problem can be solved directly on a qpu, while larger graph problems require hybrid quantum- classical approaches.
as a problem became too large to run directly on quantum computers, the researchers used decomposition methods to break the problem down into smaller pieces that the qpu could manage -  an idea they borrowed from high- performance computing and classical numerical methods.
all the pieces were then assembled into a final solution on the cpu, which not only found better parameters, but also identified the best sub- problem size to solve on a quantum computer.
such hybrid approaches are not a silver bullet; they do not allow for quantum speedup because using decomposition schemes limits speed as the size of the problem increases.
in the next 10 years, though, expected improvements in qubits (quality, count, and connectivity), error correction, and quantum algorithms will decrease runtime and enable more advanced computation.
"in the meantime," according to yuri alexeev, principal project specialist in the computational science division, "this approach will enable researchers to use near- term quantum computers to solve applications that support the doe mission.
for example, it can be applied to find community structures in metabolic networks or a microbiome."
for the first time, physicists have demonstrated that clients who possess only classical computers- and no quantum devices- can outsource computing tasks to quantum servers that perform blind quantum computing.
"blind" means the quantum servers do not have full information about the tasks they are computing, which ensures that the clients' computing tasks are kept secure.
until now, all blind quantum computing demonstrations have required that clients have their own quantum devices in order to delegate tasks for blind quantum computing.
the team of physicists, led by jian- wei pan and chao- yang lu at the university of science and technology of china, have published a paper on the demonstration of blind quantum computing for classical clients in a recent issue of physical review letters.
"we have demonstrated for the first time that a fully classical client can delegate a quantum computation to untrusted quantum servers while maintaining full privacy," lu told phys.org.
the idea behind blind quantum computing is that, while there are certain computing tasks that quantum computers can perform exponentially better than classical computers, quantum computing still involves expensive, complex hardware that will make it inaccessible for most clients.
so instead of everyone owning their own quantum computing devices, blind quantum computing makes it possible for clients to outsource their computing tasks to quantum servers that do the job for them.
ensuring that the quantum computing is performed blindly is important, since many of the potential applications of quantum computing will likely require a high degree of security.
although several blind quantum computing protocols have been performed in the past few years, they have all required that the clients have the ability to perform certain quantum tasks, such as prepare or measure qubit states.
eliminating this requirement will provide greater access to blind quantum computing, since most clients only have classical computing systems.
in the new study, the physicists experimentally demonstrated that a classical client can outsource a simple problem (factoring the number 15) to two quantum servers that do not fully know what problem they are solving.
this is because each server completes part of the task, and it is physically impossible for the servers to communicate with each other.
to ensure that the quantum servers are performing their tasks honestly, the client can give them "dummy tasks" that are indistinguishable from the real task to test their honesty and correctness.
the researchers expect that the new method can be scaled up for realizing secure, outsourced quantum computing, which could one day be implemented on quantum cloud servers and make the power of quantum computing widely available.
"blind quantum computing protocol is an important privacy- preserving technique for future secure quantum cloud computing and secure quantum networks," lu said.
"applying our implemented blind quantum computing protocol, classical clients could delegate computation tasks to servers 'in the cloud' blindly and correctly without directly owning quantum devices.
it saves resources and makes scalable quantum computing possible."
in the future, the physicists want to make blind quantum computing even easier for clients by further reducing the requirements.
"we plan to study more robust blind quantum computing protocols with fewer required resources and fewer constraints theoretically and experimentally," lu said.
"we will also explore blind quantum computing for more application scenarios, such as multi- user blind quantum computing, publicly verifiable quantum computing, and secure multi- party quantum computing."
a new paradigm in quantum information processing has been demonstrated by physicists at uc santa barbara.
their results are published in this week's issue of science express online.
ucsb physicists have demonstrated a quantum integrated circuit that implements the quantum von neumann architecture.
in this architecture, a long- lived quantum random access memory can be programmed using a quantum central processing unit, all constructed on a single chip, providing the key components for a quantum version of a classical computer.
the ucsb hardware is based on superconducting quantum circuits, and must be cooled to very low temperatures to display quantum behavior.
the architecture represents a new paradigm in quantum information processing, and shows that quantum large- scale- integration is within reach.
the quantum integrated circuit includes two quantum bits (qubits), a quantum communication bus, two bits of quantum memory, and a resetting register comprising a simple quantum computer.
"computational steps take a few billionths of a second, comparable to a classical computer, but the great power is that a quantum computer can perform a large number of calculations simultaneously," said matteo mariantoni, postdoctoral fellow in the department of physics.
"in our new ucsb architecture we have explored the possibility of writing quantum information to memory, while simultaneously performing other quantum calculations.
"on the quantum von neumann architecture, we were able to run the quantum fourier transform and a three- qubit toffoli gate -  key quantum logic circuits for the further development of quantum computing," said mariantoni.
microsoft announced azure quantum on monday at ignite 2019 in orlando, which the company positions as their "full- stack, open cloud ecosystem" aimed at bringing the capabilities of quantum computing to the mainstream.
microsoft is partnering with 1qbit, honeywell, ionq, and qci as part of azure quantum, for software and hardware integrations.
foremost among the offerings for developers is q#, and the quantum development kit (qdk), which can be tested on simulators as well as a variety of quantum hardware.
qdk, which is "interoperable" with python, aims to abstract differences that exist between different types of quantum computers.
microsoft is not a newcomer to the quantum space, starting research in 2000.
in 2018, the company announced the development of a noise- resistant topological qubit, which can be used to complete longer- lived, higher- complexity computations.
at ignite 2019, the company touted a collaboration with the university of sydney in controlling 50,000 qubits through three wires, as well as a cryogenic cmos design, and a one square- centimeter chip operating at near- absolute- zero temperatures.
for all of the excitement that follows the field of quantum computing, there has been to date few practical business uses of the technology, with naysayers complaining that the discipline is at best purely academic, or at worst, science fiction.
"the field is evolving.
i took my first quantum computing class in the late 90s, and it felt really far away then.
even in the last five, six years, things continue to accelerate," said julie love, senior director for quantum business development at microsoft.
"this is really a brand new technology.
we're building quantum systems that leverage the power of quantum physics, and it takes a new approach, innovations across the entire stack."
"we don't have that scalable quantum system now.
technology always happens this way, things always feel like they're really far away, and then we get exponentials, which are hard for humans to grok -  this pace of change, we're starting to see that impact now with quantum, we're seeing the work on the quantum- inspired side, we've had breakthroughs across the entire stack with our program, and each of those breakthroughs really gets us closer to that realization of true quantum impact," love said.
azure quantum is anticipated to launch in private preview "in the coming months," according to microsoft.
for more on quantum computing, check out "why quantum volume is vital for plotting the path to quantum advantage" and "google's quantum computing supremacy claim relies on a synthetic benchmark, researchers assert" at techrepublic.
also see
aisin group is taking digital transformation by the horns.
the automotive technology giant and qc ware today announced a research collaboration exploring the impact of quantum optimization and quantum machine learning algorithms on automotive applications.
the collaboration utilized commercially available quantum computers from d- wave systems and rigetti computing.
continue reading
aisin is a leading global supplier of components and systems for the automotive industry and the world's largest transmission manufacturer.
its customers represent a wide range of automotive manufacturing giants, including toyota, volkswagen group, psa group, volvo, and the bmw group.
"the auto industry must change drastically to meet the new dynamics of next- generation mobility.
at aisin, we continuously build our technological capabilities, focusing on innovation and future- oriented research and development.
our goal is to advance digital transformation, and to bring new value- added products to global markets," said katsuhiko eguchi, executive general manager, corporate r&d division, aisin seiki co., ltd. "qc ware and aisin's research is focused on solving critical automotive parts design challenges, including quality assurance in automatic transmission software.
we are also exploring how we can remove computational bottlenecks in big data calculations on logistic services.
we want to be well- prepared with top- rate quantum computing skills when quantum computers are ready for commercial use."
according to idc senior research analyst heather west, findings from idc's quantum computing adoption trends: 2020 survey findings indicate that while most manufacturing companies demonstrate a high interest in quantum computing, only a small percentage have been able to jumpstart their quantum computing initiatives partially due to complex technology and skillset limitations.
"qc ware has an interesting top- down approach to making quantum computing practical for businesses, said peter rutten, research director, idc.
"they build algorithms for distinct industrial use cases and then run those algorithms on whatever hardware is most suitable - as a quantum simulation on classical computers, on a d- wave system, on ibm's quantum service, or on rigetti."
rutten added, "idc found that aisin was able to begin understanding the potential of quantum computing for improving their manufacturing operations beyond traditional methods.
they have subsequently initiated two pocs, thanks to the collaborative research with qc ware's algorithms experts."
"aisin is at the forefront of emerging tech and research initiatives in japan and globally," said matt johnson, ceo, qc ware.
"while the main objective of our research collaboration is knowledge transfer and quantum computing skills building for competitive advantage, it also helps us at qc ware to understand how quantum algorithms can address the current and future needs of the automotive industry."
about aisin group
aisin group is the sixth largest, global tier one supplier of automotive components and systems such as brakes, transmissions, navigation systems, drivetrain, chassis, body, engine- related parts, electronics and intelligent transportation systems, and the largest transmission manufacturer in the world.
a $35 billion company, aisin group has over 200 consolidated companies and employs approximately 120,000 people.
in the americas, aisin group companies include 14,000 employees, 36 manufacturing, sales, and r&d centers, including aisin technical center of america located at the north american headquarters in northville, mich., and ft- techno of america, the company's 950- acre test track and proving ground in fowlerville, mich.
about qc ware
qc ware is a quantum computing- as- a- service company building enterprise solutions that run on quantum computing hardware.
qc ware's mission is to be the first company to offer a practical application providing quantum advantage over classical computers.
qc ware is working towards that goal with one of the world's strongest teams of quantum algorithms scientists.
the company is already generating revenue through successful research collaborations with industry leaders in the automotive, aerospace, finance, material design, and oil and gas sectors.
qc ware is headquartered in palo alto, calif., and has a wholly owned subsidiary in paris, qc ware france.
for more information, please visit qcware.com.
qc ware also hosts the annual "q2b - practical quantum computing" conference every december in silicon valley.
the three- day conference brings industry, government and research institutions together to stimulate application discovery and development.
for more information, please visit q2bconference.com
about qc ware's forge cloud service
forge, qc ware's cloud services platform, enables large enterprises and public- sector organizations to start building quantum skills and prepare for the potential disruption that quantum computing will bring to the market in the near future.
forge allows enterprise users with no presumed quantum computing expertise to run problems on a wide range of quantum computing hardware platforms and simulators.
forge users can access end- to- end implementations of proprietary and open- source algorithms for binary optimization, chemistry simulation, and machine learning.
media contact
russian scientists have developed a theoretical model of quantum memory for light, adapting the concept of a hologram to a quantum system.
these findings from anton vetlugin and ivan sokolov from st. petersburg state university in russia are published in a study in the european physical journal d.
the authors demonstrate for the first time that it is theoretically possible to retrieve, on demand, a given portion of the stored quantised light signal of a holographic image - set in a given direction in a given position in time sequence.
this is done by shaping the control field both in space and time.
the ultimate goal is to introduce into quantum holograms the ability not only to store quantum signals but also to perform transformations of their quantum states - an approach useful for quantum communication and computation.
quantum memory differs from conventional memory currently used in computers in its ability to write in and retrieve signals preserving their quantum state.
holograms are well- known classical memory devices that allow optical images to be written and retrieved.
the authors of this study have previously suggested solving the problem of quantum memory for light by extending the idea of a classical hologram to a quantum domain.
the hologram is written on a medium able to store quantum superposition - and not just the intensity of light beam as traditional holograms are.
the readout of both classical and quantum holograms is performed by the illumination of the medium with an external light pulse.
it is referred to as the control field and is scattered on the internal structure of the hologram.
to do so, the authors apply common theoretical methods of quantum optics, including quantum description of cold atoms that compose the storage medium, as well as quantum theory of light propagation and interaction with the medium.
conventional computers store information in a bit, a fundamental unit of logic that can take a value of 0 or 1.
quantum computers rely on quantum bits, also known as a "qubits," as their fundamental building blocks.
bits in traditional computers encode a single value, either a 0 or a 1.
the state of a qubit, by contrast, can simultaneously have a value of both 0 and 1.
this peculiar property, a consequence of the fundamental laws of quantum physics, results in the dramatic complexity in quantum systems.
quantum computing is a nascent and rapidly developing field that promises to use this complexity to solve problems that are difficult to tackle with conventional computers.
a key challenge for quantum computing, however, is that it requires making large numbers of qubits work together- which is difficult to accomplish while avoiding interactions with the outside environment that would rob the qubits of their quantum properties.
new research from the lab of oskar painter, john g braun professor of applied physics and physics in the division of engineering and applied science, explores the use of superconducting metamaterials to overcome this challenge.
metamaterials are specially engineered by combining multiple component materials at a scale smaller than the wavelength of light, giving them the ability to manipulate how particles of light, or photons, behave.
metamaterials can be used to reflect, turn, or focus beams of light in nearly any desired manner.
a metamaterial can also create a frequency band where the propagation of photons becomes entirely forbidden, a so- called "photonic bandgap."
the caltech team used a photonic bandgap to trap microwave photons in a superconducting quantum circuit, creating a promising technology for building future quantum computers.
"in principle, this is a scalable and flexible substrate on which to build complex circuits for interconnecting certain types of qubits," says painter, leader of the group that conducted the research, which was published in nature communications on september 12.
"not only can one play with the spatial arrangement of the connectivity between qubits, but one can also design the connectivity to occur only at certain desired frequencies."
painter and his team created a quantum circuit consisting of thin films of a superconductor- a material that transmits electric current with little to no loss of energy- traced onto a silicon microchip.
these superconducting patterns transport microwaves from one part of the microchip to another.
what makes the system operate in a quantum regime, however, is the use of a so- called josephson junction, which consists of an atomically thin non- conductive layer sandwiched between two superconducting electrodes.
the josephson junction creates a source of microwave photons with two distinct and isolated states, like an atom's ground and excited electronic states, that are involved in the emission of light, or, in the language of quantum computing, a qubit.
"superconducting quantum circuits allow one to perform fundamental quantum electrodynamics experiments using a microwave electrical circuit that looks like it could have been yanked directly from your cell phone," painter says.
"we believe that augmenting these circuits with superconducting metamaterials may enable future quantum computing technologies and further the study of more complex quantum systems that lie beyond our capability to model using even the most powerful classical computer simulations."
the paper is titled "superconducting metamaterials for waveguide quantum electrodynamics."
quantum computingdigital transformation
efforts by a number of companies, including honeywell, cqc and rigetti, are bringing the commercial advent of quantum computing closer than ever.
the term 'quantum computing', however, risks becoming a buzzword without clear definition.
simply put, quantum computing is analogous to regular computing, except for the use of quantum rather than conventional bits.
while a regular bit is, by definition, in one of two states, a quantum bit can be a superposition of both states at once, opening up novel approaches to computation.
the advent of the technology poses questions owing to quantum computers' anticipated capabilities in fields such as cryptography, where, due to their highly parallel nature, they can crack encryption methods that would take conventional computer aeons.
increasingly, however, the narrative is becoming more about the possibilities of the technology, particularly in a commercial space, owing to the interest of some of the world's largest tech companies.
see also:
rigetti, one of the providers behind amazon braket, amazon's service bringing access to quantum computing via the cloud, has just raised $71mn of a planned $80+mn round.
that funding represents the single largest round to date for the company, which has raised almost $200mn total.
while amazon braket targets scientists and researchers as well as developers, more nakedly commercial efforts are afoot.
american industrial conglomerate honeywell has said it is building "the world's most powerful quantum computer" based on quantum volume.
expected to be ready for commercial testing within three months, jpmorgan chase has signed up as the first customer, bringing it to bear on financial calculations.
the broader field of quantum computing is going from strength to strength, with google claiming the achievement of quantum supremacy (the point at which a quantum computer performs a task beyond the means of conventional computers) back in october 2019.
on monday, the uk's cambridge quantum computing (cqc), a company focused on the commercialisation of the technology, announced a collaboration with cern.
that partnership will naturally focus on the application of quantum computing to particle physics, using the increased compute power to crunch through data.
the concept of using quantum mechanics as a basis for building computer systems has far- reaching implications for our collective futures.
the sheer theoretical computational power of a quantum computer and the way such power would change our lives is difficult to comprehend.
of course, that is the very reason why science and tech companies like microsoft are pursuing quantum research so enthusiastically.
quantum physics
the binary machine language of all our current computing devices is based on the bit.
using combinations of zeros and ones, our digital computers can do everything from take a photo to search the universe for extraterrestrial life.
at the quantum level, particles of matter can exist in one of two states.
that means the familiar binary form of machine language could be applied, with a few modifications, at the quantum level with the use of quantum bits (qubits).
it also means the computational mechanisms would be densely packed in very small places.
the computational density that could be achieved by a quantum computer is where the theoretical power would come from.
it is an exciting and potentially evolutionary concept requiring serious research, and microsoft is looking to lead the way.
liqui|> simulator
microsoft research's quantum architectures and computation group (quarc) has developed and released to the public a set of tools that allows computer scientists to simulate a quantum computer's capabilities.
the language- integrated quantum operations (liqui|) simulator is available on github.
the i|> at the end refers to how a quantum operation is written in mathematics.
keep in mind, the whole concept of quantum computing is in its infancy and everything is still theoretical.
therefore, you won't be seeing anything practical, especially at the consumer level, for a very long time.
however, it is important research, and microsoft wants to still be relevant a hundred years from now, so it looks toward the future of computers.
virtual reality
one of the reasons i bring up the potential for quantum computing and microsoft's research is that i think it points to a fundamental flaw in the recent hype about upcoming virtual reality devices.
current computational technology limits the "reality" these devices will be able to produce.
the result is that virtual reality contains little that is real, and most of the applications being touted come off as simplistic and cheesy.
for example, i look at something like the recently announced partnership between microsoft and volvo to use hololens to sell cars and wonder why.
why would either company think this gimmick is a good idea?
it's impractical and will do more to distract potential car buyers than entice them.
virtual reality is not going to make a volvo station wagon sexy.
if we compare the immersive capabilities of 2016's crop of virtual reality devices to the immersive capabilities of video games throughout their history, i would say we are leaving the space invaders era and entering the first- generation microsoft flight simulator era.
in other words, we still have 25 years of development to go before virtual reality becomes a mainstream device everyone will have lying around in their living rooms.
in fact, it may take the creation of a quantum computer to pull it off effectively.
what the future holds
technological changes happen at breakneck speed these days, so it would be foolhardy to predict the future with any certainty beyond a year or two.
however, with that being said, i will take a stab at it and predict that at some point in the next 10 years we will see the development of a working quantum computer.
and when that day comes, everything will change and the pace of that change will exceed our current expectations.
who knows, perhaps with quantum computers, virtual reality will become our preferred reality.
the fact that companies like microsoft, with their deep pockets, are committed to quantum computing research makes it possible.
whether that is a good or bad thing has yet to be determined.
also read...
secrets of the superfast computers of tomorrow (gallery)
your thoughts
the potential of quantum computers is difficult to fathom.
however, we can assume that quantum computers will be highly disruptive to the status quo.
are you looking forward to such a future?
"for certain tasks, quantum computers are more powerful than their classical counterparts.
the task to be performed is the same for quantum or classical systems.
however, the former ones can do it in a more efficient way," david gross tells physorg.com.
"but we can't pinpoint the exact reason why a quantum computer is more powerful.
until now, it has been accepted that the reason is entanglement.
but entanglement is the easy answer, and we have discovered that it is not so simple."
gross, at the institute for mathematical physics in braunschweig, germany, has been working with s.t.
flammia at the perimeter institute for theoretical physics in waterloo, ontario, canada and with jens eisert at the university of potsdam in germany, studying entanglement and trying to understand the role it plays in quantum computing.
one of the more interesting findings from the group is that there is such a thing as too much entanglement.
the exploration of the concept of too much entanglement is presented in physical review letters: "most quantum states are too entangled to be useful as computational resources."
"the conventional wisdom on entanglement is that the more you have, the more powerful your quantum computing will be," says eisert.
"we've found that when it comes to quantum computing, there can be too much entanglement, rendering the quantum information processing attempt useless.
it doesn't matter how smart you are, or how you run your computer model; once you reach a certain threshold of entanglement, you are done."
entanglement, explains both eisert and gross, represents correlations in behavior.
one system is related to another on a global scale, each affecting the other.
in quantum computing, the way systems are entangled - correlated - can help scientists perform powerful computational tasks.
however, entanglement is about more than just correlations.
"entanglement introduces a certain randomness into the system," gross says.
"this randomness appears in the measurement outcomes.
however, as the entanglement goes up, so does the randomness.
when entanglement increases to a certain point, there is so much randomness that the system ends up being about as useful as coins tossed into the air.
you don't get any useful information."
gross and eisert agree that the discovery that entanglement can be too strong could represent a change in currently accepted attitudes about quantum information processing.
"everyone knows that there needs to be a minimum amount of entanglement for quantum computing to work," gross points out, "but almost no one seems to be asking the converse question: can too much entanglement hurt your efforts?"
"this puts the use of entanglement into proportion," eisert insists.
"we know that we have to have some entanglement or quantum computing won't work.
but now we know that if we have too much, it won't work either."
implications for quantum computing, then, change.
while entanglement is obviously important to processing information in this way, it is not the only thing that makes quantum computing work.
there are other forces at play.
"clearly, there is more to what makes quantum computing powerful than just entanglement," gross says.
"the next step is to figure out what else contributes to the why of quantum computing.
we plan to study more aspects of entanglement and quantum computing to try and find an answer to what else is involved."
"in the end," says eisert, "we hope that our work inspires a second look at the role entanglement plays in quantum communication.
hopefully, by looking for and finding the edges, scientists can direct their research in the right regions - the regions where quantum computing actually works."
more information: gross, flammia, eisert, "most quantum states are too entangled to be useful as computational resources."
physical review letters (2009).
available online: http://link.aps.org/doi/10.110 ... ysrevlett.102.190501 .
a team of scientists at usc has verified that quantum effects are indeed at play in the first commercial quantum optimization processor.
the team demonstrated that the d- wave processor housed at the usc- lockheed martin quantum computing center behaves in a manner that indicates that quantum mechanics plays a functional role in the way it works.
the demonstration involved a small subset of the chip's 128 qubits.
this means that the device appears to be operating as a quantum processor - something that scientists had hoped for but have needed extensive testing to verify.
the quantum processor was purchased from canadian manufacturer d- wave nearly two years ago by lockheed martin and housed at the usc viterbi information sciences institute (isi).
as the first of its kind, the task for scientists putting it through its paces was to determine whether the quantum computer was operating as hoped.
"using a specific test problem involving eight qubits we have verified that the d- wave processor performs optimization calculations (that is, finds lowest energy solutions) using a procedure that is consistent with quantum annealing and is inconsistent with the predictions of classical annealing," said daniel lidar, scientific director of the quantum computing center and one of the researchers on the team, who holds joint appointments with the usc viterbi school of engineering and the usc dornsife college of letters, arts and sciences.
quantum annealing is a method of solving optimization problems using quantum mechanics - at a large enough scale, potentially much faster than a traditional processor can.
research institutions throughout the world build and use quantum processors, but most only have a few quantum bits, or "qubits."
qubits have the capability of encoding the two digits of one and zero at the same time - as opposed to traditional bits, which can encode distinctly either a one or a zero.
this property, called "superposition," along with the ability of quantum states to "tunnel" through energy barriers, are hoped to play a role in helping future generations of the d- wave processor to ultimately perform optimization calculations much faster than traditional processors.
with 108 functional qubits, the d- wave processor at usc inspired hopes for a significant advance in the field of quantum computing when it was installed in october 2011 - provided it worked as a quantum information processor.
quantum processors can fall victim to a phenomenon called "decoherence," which stifles their ability to behave in a quantum fashion.
the usc team's research shows that the chip, in fact, performed largely as hoped, demonstrating the potential for quantum optimization on a larger- than- ever scale.
"our work seems to show that, from a purely physical point of view, quantum effects play a functional role in information processing in the d- wave processor," said sergio boixo, first author of the research paper, who conducted the research while he was a computer scientist at isi and research assistant professor at the usc viterbi school of engineering.
boixo and lidar collaborated with tameem albash, postdoctoral research associate in physics at usc dornsife; federico m. spedalieri, computer scientist at isi; and nicholas chancellor, a recent physics graduate at usc dornsife.
their findings will be published in nature communications on june 28.
the news comes just two months after the quantum computing center's original d- wave processor- known commercially as the "rainier" chip- was upgraded to a new 512- qubit "vesuvius" chip.
the quantum computing center, which includes a magnetically shielded box that is kept frigid (near absolute zero) to protect the computer against decoherence, was designed to be upgradable to keep up with the latest developments in the field.
the new vesuvius chip at usc is currently the only one in operation outside of d- wave.
a second such chip, owned by google and housed at nasa's ames research center in moffett field, california, is expected to become operational later this year.
next, the usc team will take the vesuvius chip for a test drive, putting it through the same paces as the rainier chip.
scientists claimed wednesday to have achieved a near- mythical state of computing in which a new generation of machine vastly outperforms the world's fastest super- computer, known as "quantum supremacy".
a team of experts working on google's sycamore machine said their quantum system had executed a calculation in 200 seconds that would have taken a classic computer 10,000 years to complete.
a rival team at ibm has already expressed scepticism about their claim.
but if verified and harnessed, the google device could make even the world's most powerful supercomputers- capable of performing thousands of trillions of calculations per second- look like an early 2000s flip- phone.
regular computers, even the fastest, function in binary fashion: they carry out tasks using tiny fragments of data known as bits that are only ever either 1 or 0.
but fragments of data on a quantum computer, known as qubits, can be both 1 and 0 at the same time.
this property, known as superposition, means a quantum computer, made up of several qubits, can crunch an enormous number of potential outcomes simultaneously.
the computer harnesses some of the most mind- boggling aspects of quantum mechanics, including a phenomenon known as "entanglement"- in which two members of a pair of bits can exist in a single state, even if far apart.
adding extra qubits therefore leads to an exponential boost in processing power.
in a study published in nature, the international team designed the sycamore quantum processer, made up of 54 qubits interconnected in a lattice pattern.
they used the machine to perform a task related to random- number generation, identifying patterns amid seemingly random spools of figures.
the sycamore, just a few millimetres across, solved the task within 200 seconds, a process that on a regular machine would take 10,000 years- several hundreds of millions of times faster, in other words.
google's ceo sundar pichai hailed the result as a sea change in computing.
"for those of us working in science and technology, it's the 'hello world' moment we've been waiting for- the most meaningful milestone to date in the quest to make quantum computing a reality," he wrote in a blog post.
john martinis, from google ai and a study author, told journalists his colleagues were "excited we can start talking" about their discovery.
"the physics was right...
physicists thought this would work, they had faith in quantum physics... and tech companies now will see that this technology is much closer than they thought," he said.
not so fast?
colleague sergio boixo described the discovery as "mind- blowing".
the quest for quantum supremacy is still far from over, however.
the authors themselves acknowledge the need for better hardware and more sophisticated monitoring techniques in order to truly harness the power of quantum.
some immediate applications of quantum computing could be in encryption software and ai, but its calculations could eventually lead to more efficient solar panels, drug design and even smarter and better financial transactions.
wednesday's announcement was not without controversy.
after a leaked draft of the google lab's paper appeared online last month, chip- maker ibm, which runs its own quantum computing programme, said the boasts of the sycamore computer's feats were exaggerated.
instead of 10,000 years for an ordinary supercomputer to match sycamore's performance, ibm scientists claimed it would be more like two- and- a- half days using the most sophisticated traditional processors.
"because the original meaning of the term 'quantum supremacy'... was to describe the point where quantum computers can do things that classical computers can't, this threshold has not been met," they wrote on their blog.
researchers at the vienna university of technology quantum mechanically couple atoms to glass fiber cables.
now, they have shown that their technique enables storage of quantum information over a sufficiently long period of time to realize global quantum networks based on optical fibers.
will emails be quantum encrypted in the future?
will we be able to teleport quantum states over large distances via ordinary glass fiber cables?
laser- cooled atoms which are coupled to ultra- thin glass fibers are ideally suited for applications in quantum communication.
researchers at the vienna university of technology have now demonstrated experimentally that such glass fibers are capable of storing quantum information long enough so that they could be used for entangling atoms hundreds of kilometers apart.
this constitutes a fundamental building block for a global fiber- based quantum communication network.
atoms and light
"in our experiment, we connect two different quantum physical systems," explains arno rauschenbeutel (vienna center for quantum science and technology and institute of atomic and subatomic physics of the vienna university of technology).
"on the one hand, we use fiber- guided light, which is perfect for sending quantum information from a to b, and, on the other hand, we rely on atoms, which are ideal for storing this information."
by trapping atoms at a distance of about 200 nanometers from a glass fiber, which itself only has a diameter of 500 nanometers, a very strong interaction between light and atoms can be implemented.
this allows one to exchange quantum information between the two systems.
this information exchange is the basis for technologies like quantum cryptography and quantum teleportation.
currently, there are different approaches towards performing quantum mechanical operations and exchanging quantum information between light and matter- based memories.
however, for many of these systems it is challenging to store and to retrieve the information efficiently.
the method that has been developed at the vienna university of technology straightforwardly overcomes this problem: "our setup is directly connected to a standard optical glass fiber that is nowadays routinely used for the transmission of data," says rauschenbeutel.
"it will therefore be easy to integrate our quantum glass fiber cable into existing fiber communication networks."
robust quantum memory
in the past, the researchers already demonstrated that atoms can be controlled and efficiently coupled to glass fibers.
however, so far, the suitability of the fiber- coupled atoms for storing quantum information and for long- distance quantum communication remained an open question.
- after some time, the quantum information stored in the atoms is lost as it leaks into the environment - an effect called "decoherence".
"using some tricks, we were able to extend the coherence time of the atoms to several milliseconds, in spite of their small distance to the fiber surface," explains rauschenbeutel.
light in glass fibers travels about 200 kilometers in one millisecond.
as the light carries the quantum information, this defines the separation that could be bridged with such a system via the entanglement of atoms.
a realistic concept for a global quantum network
even in regular glass fiber- based telecommunication, the range of light propagation is limited: the longer the fiber, the weaker the signal.
in order to overcome this problem, repeater stations are inserted into the network.
they amplify the optical signals after a certain distance.
in this way, global communication becomes possible.
this simple concept of signal amplification cannot be implemented in quantum mechanics.
it is nevertheless still possible, albeit more involved, to build so- called "quantum repeaters".
they can be used to link several shorter sections to one long quantum connection.
arno rauschenbeutel is confident that his technique holds great promise: "by using our combined nanofiber- atom- system for setting up an optical quantum network including quantum repeaters, one might transmit quantum information and teleport quantum states around the world."
a scenario of artificial intelligence could see the emergence of circumstances in which models of simple organisms could be capable of experiencing the various phases of life in a controlled virtual environment.
this is what has been designed by the qutis research group at the upv/ehu's department of physical chemistry, but the scenario is that of quantum computers: an artificial life protocol that encodes quantum behaviours belonging to living systems, such as self- replication, mutation, interaction between individuals, birth and death, and has been executed on an ibm ibmqx4 cloud quantum computer.
this is the first experimental realization on a quantum computer of a quantum algorithm of artificial life following darwin's laws of evolution.
the algorithm follows a protocol that the researchers refer to as biomimetic and which encodes quantum behaviours adapted to the same behaviours of living systems.
quantum biomimetics involves reproducing in quantum systems certain properties exclusive to living beings, and this research group had previously managed to imitate life, natural selection, learning and memory by means of quantum systems.
this research aimed, as the authors themselves describe, "to design a set of quantum algorithms based on the imitation of biological processes, which take place in complex organisms, and transfer them to a quantum scale, so we were only trying to imitate the key aspects in these processes."
quantum artificial life with a promising future
in the scenario of artificial life that they designed, a set of models of simple organisms are capable of accomplishing the most common phases of life in a controlled virtual environment, and have proven that microscopic quantum systems are able to encode quantum characteristics and biological behaviours that are normally associated with living systems and natural selection.
the models of organism designed were coined as units of quantum life, each one of which is made up of two qubits that act as genotype and phenotype, respectively, and where the genotype contains the information that describes the type of living unit, and this information is transmitted from generation to generation.
by contrast, the phenotype, the characteristics displayed by individuals, are determined by genetic information as well as by the interaction of the individuals themselves with the environment.
to be able to regard the systems as organisms of artificial life, the basic characteristics of darwinian evolution that were simulated by these systems were birth and its evolution, self- replication, interaction between individuals and the environment, which gradually degrades the phenotype of the individual as it ages and ends in a state representing death.
the protocol also considers interaction between individuals as well as mutations, which are implemented in random rotations of individual qubits.
this experimental test represents the consolidation of the theoretical framework of quantum artificial life in an evolutionary sense, but as the model is scaled up to more complex systems, it will be possible "to implement more accurate quantum emulations with growing complexity towards quantum supremacy," as the authors pointed out.
in the same way, they expect these units of artificial life and their possible applications to have profound implications for the community of quantum simulation and quantum computing in a range of quantum platforms, be they trapped ions, photonic systems, neutral atoms or superconductor circuits.
according to enrique solano, director of the qutis group and leader of this project, "the bases have been established for addressing different levels of classical and quantum complexity.
for example, one could consider the growth of populations of quantum individuals with gender criteria, their life aims both as individuals and as groups, automated behaviours without external controls, quantum robotics processes, intelligent quantum systems, until the threshold of quantum supremacy that could only be reached by a quantum computer can be overcome.
what would emerge after that would be terribly risky questions, such as guessing the microscopic origin of life itself, the intelligent development of individuals and societies, or addressing the origin of awareness and animal and human creativity.
this is only the start; we are at the beginning of the 21st century and we will have many fantasy dreams and questions that we will be able to respond to."
a localization phenomenon boosts the accuracy of solving quantum many- body problems with quantum computers which are otherwise challenging for conventional computers.
this brings such digital quantum simulation within reach on quantum devices available today.
quantum computers promise to solve certain computational problems exponentially faster than any classical machine.
"a particularly promising application is the solution of quantum many- body problems utilizing the concept of digital quantum simulation," says markus heyl from max planck institute for the physics of complex in dresden, germany.
"such simulations could have a major impact on quantum chemistry, materials science and fundamental physics."
within digital quantum simulation the time evolution of the targeted quantum many- body system is realized by a sequence of elementary quantum gates by discretizing time evolution, called trotterization.
"a fundamental challenge, however, is the control of an intrinsic error source, which appears due to this discretization," says markus heyl.
together with peter zoller from the department of experimental physics at the university of innsbruck and the institute of quantum optics and quantum communication at the austrian academy of sciences and philipp hauke from the kirchhoff institute for physics and the institute for theoretical physics at the university of heidelberg they show in a recent paper in science advances that quantum localization- by constraining the time evolution through quantum interference- strongly bounds these errors for local observables.
more robust than expected
"digital quantum simulation is thus intrinsically much more robust than what one might expect from known error bounds on the global many- body wave function," heyl summarizes.
this robustness is characterized by a sharp threshold as a function of the utilized time granularity measured by the so- called trotter step size.
the threshold separates a regular region with controllable trotter errors, where the system exhibits localization in the space of eigenstates of the time- evolution operator, from a quantum chaotic regime where errors accumulate quickly rendering the outcome of the quantum simulation unusable.
"our findings show that digital quantum simulation with comparatively large trotter steps can retain controlled trotter errors for local observables," says markus heyl.
"it is thus possible to reduce the number of quantum gate operations required to represent the desired time evolution faithfully, thereby mitigating the effects of imperfect individual gate operations."
this brings digital quantum simulation for classically challenging quantum many- body problems within reach for current day quantum devices.
story source:
materials provided by university of innsbruck.
note: content may be edited for style and length.
as silicon- based computing approaches physical limits, the government is considering whether to fund basic research into new technologies such as molecular and quantum computing.
earlier this year, the house approved the networking and information technology research and development act.
the legislation is designed to fund basic research in technologies needed to advance computing.
the senate has so far failed to consider it.
the basic research subcommittee of the house science committee met sept. 12 to discuss the issue and to promote molecular and quantum computing as promising alternatives to silicon- based systems.
lawmakers were told that government- funded research into a range of molecular, chemical, quantum and optical devices would begin to emerge over the next decade as silicon- based computing winds down.
"molecular and chemical devices, quantum computers and optical computing and communications are the technologies that we are exploring now in anticipation that one or more will be the leadership technologies in ten or twenty years," said ruzena bajcsy, assistant director for computer and information science and engineering at the national science foundation (nsf).
industry researchers seeking to capitalize on government interest in new computing technologies said they were leveraging their basic research with programs being pursued by government laboratories.
back to basics
an nsf report on quantum information science released last year called for a basic research program to boost quantum computing technology.
charles bennett, a research fellow at ibm's thomas j. watson research center (yorktown heights, n.y.) and a co- author of the nsf report, told the science panel that the government had a "unique role" in supporting basic research in quantum computing as the primary sponsor of university- based fundamental research.
"this research is the base from which new technologies are derived," bennett said.
"this is not research that will get done in the private sector."
as for applications, researchers said both quantum and molecular computers hold promise.
since "molecular and quantum computers, as currently conceived, have built into them a great deal of information about biochemistry and quantum physics, respectively, i would anticipate that the first applications of these emerging technologies will be to building special- purpose devices for solving biological and quantum mechanical problems, respectively," said timothy havel, a researcher affiliated with mit and harvard university.
quantum communication, which ensures absolute data security, is one of the most advanced branches of the "second quantum revolution".
in quantum communication, the participating parties can detect any attempt at eavesdropping by resorting to the fundamental principle of quantum mechanics - a measurement affects the measured quantity.
thus, the mere existence of an eavesdropper can be detected by identifying the traces that his measurements of the communication channel leave behind.
the major drawback of quantum communication today is the slow speed of data transfer, which is limited by the speed at which the parties can perform quantum measurements.
researchers at bar- ilan university have devised a method that overcomes this "speed limit", and enables an increase in the rate of data transfer by more than 5 orders of magnitude!
their findings were published today in the journal nature communications.
homodyne detection is a cornerstone of quantum optics, acting as a fundamental tool for processing quantum information.
however, the standard homodyne method suffers from a strong bandwidth limitation.
while quantum optical phenomena, exploited for quantum communication, can easily span a bandwidth of many thz, the standard processing methods of this information are inherently limited to the electronically accessible mhz- to- ghz range, leaving a dramatic gap between the relevant optical phenomena that is used for carrying the quantum information, and the capability to measure it.
thus, the rate at which quantum information can be processed is strongly limited.
in their work, the researchers replace the electrical nonlinearity that serves as the heart of homodyne detection, which transforms the optical quantum information into a classical electrical signal, with a direct optical nonlinearity, transforming the quantum information into a classical optical signal.
thus, the output signal of the measurement remains in the optical regime, and preserves the enormous bandwidth optical phenomena offers.
"we offer a direct optical measurement that conserves the information bandwidth, instead of an electrical measurement that compromises the bandwidth of the quantum optical information," says dr. yaakov shaked, who conducted the research during his ph.d. studies in the lab of prof. avi pe'er.
to demonstrate this idea, the researchers perform a simultaneous measurement of an ultra- broadband quantum optical state, spanning 55thz, presenting non- classical behavior across the entire spectrum.
such a measurement, using standard method, would be practically impossible.
the research was accomplished through a collaboration between the quantum optics labs of prof. avi pe'er and prof. michael rosenbluh, together with yoad michael, dr. rafi z. vered and leon bello at the department of physics and institute for nanotechnology and advanced materials at bar- ilan university.
this new form of quantum measurement is relevant also to other branches of the "second quantum revolution", such as quantum computing with super powers, quantum sensing with super sensitivity, and quantum imaging with super resolution.
some math problems are so complicated that they can bog down even the world's most powerful supercomputers.
but a wild new frontier in computing that applies the rules of the quantum realm offers a different approach.
a new study led by a physicist at lawrence berkeley national laboratory (berkeley lab), published in the journal scientific reports, details how a quantum computing technique called "quantum annealing" can be used to solve problems relevant to fundamental questions in nuclear physics about the subatomic building blocks of all matter.
it could also help answer other vexing questions in science and industry, too.
seeking a quantum solution to really big problems
"no quantum annealing algorithm exists for the problems that we are trying to solve," said chia cheng "jason" chang, a riken ithems fellow in berkeley lab's nuclear science division and a research scientist at riken, a scientific institute in japan.
"the problems we are looking at are really, really big," said chang, who led the international team behind the study.
"the idea here is that the quantum annealer can evaluate a large number of variables at the same time and return the right solution in the end."
the same problem- solving algorithm that chang devised for the latest study, and that is available to the public via open- source code, could potentially be adapted and scaled for use in systems engineering and operations research, for example, or in other industry applications.
classical algebra with a quantum computer
"we are cooking up small 'toy' examples just to develop how an algorithm works.
the simplicity of current quantum annealers is that the solution is classical- akin to doing algebra with a quantum computer.
you can check and understand what you are doing with a quantum annealer in a straightforward manner, without the massive overhead of verifying the solution classically."
chang's team used a commercial quantum annealer located in burnaby, canada, called the d- wave 2000q that features superconducting electronic elements chilled to extreme temperatures to carry out its calculations.
access to the d- wave annealer was provided via the oak ridge leadership computing facility at oak ridge national laboratory (ornl).
"these methods will help us test the promise of quantum computers to solve problems in applied mathematics that are important to the u.s. department of energy's scientific computing mission," said travis humble, director of ornl's quantum computing institute.
quantum data: a one, a zero, or both at the same time
there are currently two of these machines in operation that are available to the public.
they work by applying a common rule in physics: systems in physics tend to seek out their lowest- energy state.
for example, in a series of steep hills and deep valleys, a person traversing this terrain would tend to end up in the deepest valley, as it takes a lot of energy to climb out of it and the least amount of energy to settle in this valley.
the annealer applies this rule to calculations.
in a typical computer, memory is stored in a series of bits that are occupied by either one or a zero.
but quantum computing introduces a new paradigm in calculations: quantum bits, or qubits.
with qubits, information can exist as either a one, a zero, or both at the same time.
this trait makes quantum computers better suited to solving some problems with a very large number of possible variables that must be considered for a solution.
each of the qubits used in the latest study ultimately produces a result of either a one or a zero by applying the lowest- energy- state rule, and researchers tested the algorithm using up to 30 logical qubits.
the algorithm that chang developed to run on the quantum annealer can solve polynomial equations, which are equations that can have both numbers and variables and are set to add up to zero.
a variable can represent any number in a large range of numbers.
when there are 'fewer but very dense calculations'
berkeley lab and neighboring uc berkeley have become a hotbed for r&d in the emerging field of quantum information science, and last year announced the formation of a partnership called berkeley quantum to advance this field.
chang said that the quantum annealing approach used in the study, also known as adiabatic quantum computing, "works well for fewer but very dense calculations," and that the technique appealed to him because the rules of quantum mechanics are familiar to him as a physicist.
the data output from the annealer was a series of solutions for the equations sorted into columns and rows.
this data was then mapped into a representation of the annealer's qubits, chang explained, and the bulk of the algorithm was designed to properly account for the strength of the interaction between the annealer's qubits.
"we repeated the process thousands of times" to help validate the results, he said.
"solving the system classically using this approach would take an exponentially long time to complete, but verifying the solution was very quick" with the annealer, he said, because it was solving a classical problem with a single solution.
if the problem was quantum in nature, the solution would be expected to be different every time you measure it.
real- world applications for a quantum algorithm
as quantum computers are equipped with more qubits that allow them to solve more complex problems more quickly, they can also potentially lead to energy savings by reducing the use of far larger supercomputers that could take far longer to solve the same problems.
the quantum approach brings within reach direct and verifiable solutions to problems involving "nonlinear" systems- in which the outcome of an equation does not match up proportionately to the input values.
nonlinear equations are problematic because they may appear more unpredictable or chaotic than other "linear" problems that are far more straightforward and solvable.
chang sought the help of quantum- computing experts in quantum computing both in the u.s. and in japan to develop the successfully tested algorithm.
he said he is hopeful the algorithm will ultimately prove useful to calculations that can test how subatomic quarks behave and interact with other subatomic particles in the nuclei of atoms.
while it will be an exciting next step to work to apply the algorithm to solve nuclear physics problems, "this algorithm is much more general than just for nuclear science," chang noted.
"it would be exciting to find new ways to use these new computers."
new research finds that prototype quantum optimization chip operates as hoped.
a team of scientists at usc has verified that quantum effects are indeed at play in the first commercial quantum optimization processor.
the team demonstrated that the d- wave processor housed at the usc- lockheed martin quantum computing center behaves in a manner that indicates that quantum mechanics plays a functional role in the way it works.
the demonstration involved a small subset of the chip's 128 qubits.
this means that the device appears to be operating as a quantum processor - something that scientists had hoped for but have needed extensive testing to verify.
the quantum processor was purchased from canadian manufacturer d- wave nearly two years ago by lockheed martin and housed at the usc viterbi information sciences institute (isi).
as the first of its kind, the task for scientists putting it through its paces was to determine whether the quantum computer was operating as hoped.
"using a specific test problem involving eight qubits we have verified that the d- wave processor performs optimization calculations (that is, finds lowest energy solutions) using a procedure that is consistent with quantum annealing and is inconsistent with the predictions of classical annealing," said daniel lidar, scientific director of the quantum computing center and one of the researchers on the team, who holds joint appointments with the usc viterbi school of engineering and the usc dornsife college of letters, arts and sciences.
quantum annealing is a method of solving optimization problems using quantum mechanics - at a large enough scale, potentially much faster than a traditional processor can.
research institutions throughout the world build and use quantum processors, but most only have a few quantum bits, or "qubits."
qubits have the capability of encoding the two digits of one and zero at the same time - as opposed to traditional bits, which can encode distinctly either a one or a zero.
this property, called "superposition," along with the ability of quantum states to "tunnel" through energy barriers, are hoped to play a role in helping future generations of the d- wave processor to ultimately perform optimization calculations much faster than traditional processors.
with 108 functional qubits, the d- wave processor at usc inspired hopes for a significant advance in the field of quantum computing when it was installed in october 2011 - provided it worked as a quantum information processor.
quantum processors can fall victim to a phenomenon called "decoherence," which stifles their ability to behave in a quantum fashion.
the usc team's research shows that the chip, in fact, performed largely as hoped, demonstrating the potential for quantum optimization on a larger- than- ever scale.
"our work seems to show that, from a purely physical point of view, quantum effects play a functional role in information processing in the d- wave processor," said sergio boixo, first author of the research paper, who conducted the research while he was a computer scientist at isi and research assistant professor at the usc viterbi school of engineering.
boixo and lidar collaborated with tameem albash, postdoctoral research associate in physics at usc dornsife; federico m. spedalieri, computer scientist at isi; and nicholas chancellor, a recent physics graduate at usc dornsife.
their findings will be published in nature communications on june 28.
the news comes just two months after the quantum computing center's original d- wave processor- known commercially as the "rainier" chip- was upgraded to a new 512- qubit "vesuvius" chip.
the quantum computing center, which includes a magnetically shielded box that is kept frigid (near absolute zero) to protect the computer against decoherence, was designed to be upgradable to keep up with the latest developments in the field.
the new vesuvius chip at usc is currently the only one in operation outside of d- wave.
a second such chip, owned by google and housed at nasa's ames research center in moffett field, california, is expected to become operational later this year.
next, the usc team will take the vesuvius chip for a test drive, putting it through the same paces as the rainier chip.
for many years, quantum computers were not much more than an idea.
today, companies, governments and intelligence agencies are investing in the development of quantum technology.
robert konig, professor for the theory of complex quantum systems at the tum, in collaboration with david gosset from the institute for quantum computing at the university of waterloo and sergey bravyi from ibm, has now placed a cornerstone in this promising field.
conventional computers obey the laws of classical physics.
they rely on the binary numbers zero and one.
these numbers are stored and used for mathematical operations.
in conventional memory units, each bit- the smallest unit of information- is represented by a charge that determines whether the bit is set to one or zero.
in a quantum computer, however, a bit can be both zero and one at the same time.
this is because the laws of quantum physics allow electrons to occupy multiple states at one time.
quantum bits, or qubits, thus exist in multiple overlapping states.
this so- called superposition allows quantum computers to perform operations on many values in one fell swoop, whereas a single conventional computer must execute these operations sequentially.
the promise of quantum computing lies in the ability to solve certain problems significantly faster.
from conjecture to proof
konig and his colleagues have now conclusively demonstrated the advantage of quantum computers.
to this end, they developed a quantum circuit that can solve a specific difficult algebraic problem.
the new circuit has a simple structure- it only performs a fixed number of operations on each qubit.
such a circuit is referred to as having a constant depth.
in their work, the researchers prove that the problem at hand cannot be solved using classical constant- depth circuits.
they furthermore answer the question of why the quantum algorithm beats any comparable classical circuit: the quantum algorithm exploits the non- locality of quantum physics.
prior to this work, the advantage of quantum computers had been neither proven nor experimentally demonstrated- notwithstanding that evidence pointed in this direction.
one example is shor's quantum algorithm, which efficiently solves the problem of prime factorization.
however, it is merely a complexity- theoretic conjecture that this problem cannot be efficiently solved without quantum computers.
it is also conceivable that the right approach has simply not yet been found for classical computers.
robert konig considers the new results primarily as a contribution to complexity theory.
"our result shows that quantum information processing really does provide benefits- without having to rely on unproven complexity- theoretic conjectures," he says.
beyond this, the work provides new milestones on the road to quantum computers.
because of its simple structure, the new quantum circuit is a candidate for a near- term experimental realization of quantum algorithms.
physicists have shown that superconducting circuits can function as piston- like mechanical quantum engines.
the new perspective may help researchers design quantum computers and other devices with improved efficiencies.
the physicists, kewin sachtleben, kahio t. mazon, and luis g. c. rego at the federal university of santa catarina in florianopolis, brazil, have published a paper on their work on superconducting qubits in a recent issue of physical review letters.
in their study, the physicists explain that superconducting circuits are functionally equivalent to quantum systems in which quantum particles tunnel in a double- quantum well.
these wells have the ability to oscillate, meaning the width of the well changes repeatedly.
when this happens, the system behaves somewhat like a piston that moves up and down in a cylinder, which changes the volume of the cylinder.
this oscillatory behaviour allows work to be performed on the system.
the researchers show that, in the double- quantum well, part of this work comes from quantum coherent dynamics, which creates friction that decreases the work output.
these results provide a better understanding of the connection between quantum and classical thermodynamic work.
"the distinction between 'classical' thermodynamic work, responsible for population transfer, and a quantum component, responsible for creating coherences, is an important result," mazon told in an interview.
"the creation of coherences, in turn, generates a similar effect to friction, causing a not- completely- reversible operation of the engine.
in our work we have been able to calculate the reaction force caused on the quantum piston wall due to the creation of coherences.
in principle this force can be measured, thus constituting the experimental possibility of observing the emergence of coherences during the operation of the quantum engine."
one of the potential benefits of viewing superconducting qubits as quantum engines is that it may allow researchers to incorporate quantum coherent dynamics into future technologies, in particular quantum computers.
the physicists explain that a similar behaviour can be seen in nature, where quantum coherences improve the efficiency of processes such as photosynthesis, light sensing, and other natural processes.
"quantum machines may have applications in the field of quantum information, where the energy of quantum coherences is used to perform information manipulation in the quantum regime," mazon said.
"it is worth remembering that even photosynthesis can be described according to the working principles of a quantum machine, so unraveling the mysteries of quantum thermodynamics can help us to better understand and interpret various natural processes."
3dr holdings llc announced today that its inside quantum technology scheduled originally for april 2020 is now rescheduled for june 23- 26, 2020 as a virtual event due to covid- 19.
the complete program and list of virtual exhibitors can be found at www.iqtevent.com.
3dr holdings, based in new york city offers events, research, consulting services, and daily news for the fast- growing field of quantum computing.
"we are pleased to be able to offer this comprehensive and long- planned event virtually as well as completely archived.
paid attendees will be able to watch and participate in sessions over four- days, from june 23 through 26," stated alan m. meckler, ceo of 3dr holdings.
"each day we will offer four hours of programming that will include live presentations from exhibitors along with the many sessions.
we see this as an international quantum computing week, as archived sessions will be available for all attendees so they can assure being able to view any session live or at their leisure, which ensures a huge international attendance."
the complete conference schedule will be announced shortly, and will feature a keynote presentation by robert sutor, vp quantum ecosystem development, ibm research.
the new online event format allows for generous discounts for groups for both commercial, government and academic organizations.
for exhibit and sponsor opportunities contact info@3drholdings.com.
for additional information see www.iqtevent.com call +917- 403- 6300 or email info@3drholdings.com
about 3dr holdings
3dr holdings is a technology media organization with website, research and international trade show interests in the fields of 3d printing and quantum computing.
for more information, please visit https://3drholdings.com.
about inside quantum technology
founded by lawrence gasman and alan meckler, inside quantum technology is the first company entirely dedicated to meeting the strategic information and analysis needs of the emerging quantum technology sector.
in addition to arranging conferences and publishing articles of critical importance to the quantum technology sector, the company's consulting group, provides published reports on important revenue opportunities in quantum technology including quantum computer markets and software, quantum key distribution, post- quantum cryptography, quantum sensors, and on important verticals such as the military, the financial sector, big pharma, and more.
for additional information, please visit https://www.insidequantumtechnology.com.
metasurfaces are artificial materials designed at the nanoscale, which can control the scattering of light with exceptionally high precision.
over the past decade or so, these materials have been used to create a variety of technological tools ranging from sensors to lenses and imaging techniques.
a research team led by mikhail lukin at harvard university has recently proposed a new type of metasurface that can control both the spatiotemporal and quantum properties of transmitted and reflected light.
in a paper published in nature physics, the team showed that realizing a quantum metasurface is possible and could be achieved by entangling the macroscopic response of thin atom arrays to light.
"quantum metasurfaces are an entirely new type of materials designed atom by atom, which enable applications such as quantum computation with photons," rivka bekenstein, the lead author of the recent paper, told phys.org.
"we combined a state- of- the- art technique for manipulating the state of many atoms by long- range interactions (i.e., rydberg interactions) with a recent discovery of how a single sheet of atoms can reflect light.
we identified an architecture that can be realized in the laboratory, in which a single layer of atoms can act as a switchable quantum mirror."
as part of their study, bekenstein and her colleagues reviewed different quantum metasurfaces that can be controlled to have different light scattering properties.
one of the most prominent sources for the development of quantum technologies are entangled states, which are unique states that only exist for quantum entities.
the quantum metamaterial proposed by the researchers enables the production of specific entangled states of many light particles (i.e., photons), which are particularly valuable for quantum information processing applications.
in certain environmental conditions, atoms can be manipulated to become transparent using external electrical fields.
recent studies have also demonstrated that a single sheet of atoms can reflect light, resembling a regular mirror.
by employing rydberg interactions that naturally occur in atomic systems, bekenstein and her colleagues were able to identify a scheme in which a single layer of atoms simultaneously reflects and transmits light in a quantum superposition.
in other words, the resulting quantum metasurface could both become transparent and reflect light, like a mirror.
"in quantum mechanics, entities can co- exist in different states- this is called a superposition state," bekenstein said.
"our quantum metasurface is a new type of material that can make light co- exist in two different directions.
this is done by manipulating the atoms' state and then shining a weak laser to scatter from them."
the design strategy employed by bekenstein and her colleagues induces quantum entanglement between different metasurfaces and light, as well as between individual light particles.
notably, the architecture they proposed could also be manipulated to have varying amounts of photons in entangled states, which is a crucial capability for most quantum applications, including quantum computing.
through a series of quantitative calculations, the researchers analyzed how their metasurface enables quantum operations between atoms and photons, allowing for the generation of highly entangled photonic states that are ideal for quantum information processing applications.
"a key advantage of our architecture is that only one atom has to be prepared in a quantum superposition state in the laboratory," bekenstein said.
"hundreds of atoms construct the quantum metasurface, but only one has to be manipulated on the quantum mechanical level, which make this proposal practical.
this is enabled due to the long- range interaction we utilize in the scheme, which naturally exists for atoms in specific energy levels."
remarkably, the recent study by bekenstein and her colleagues introduces a technique to gain quantum control over the response of macroscopic materials to light.
this technique could pave the way for the development of an entirely new type of quantum materials, while also potentially revolutionizing the current understanding of quantum optical materials and their response to light.
"we are currently exploring additional experimental systems that can realize the quantum metasurfaces we proposed," bekenstein said.
"we are also interested in revealing the nonlinear response of these quantum metasurfaces to light, which occur for higher intensity light beams.
finally, we are investigating specific practical applications of the proposed quantum metasurfaces for quantum information processing."
here's the scenario: you have sensitive data and a problem that only a quantum computer can solve.
you have no quantum devices yourself.
you could buy time on a quantum computer, but you don't want to give away your secrets.
what can you do?
writing in physical review x on 11 july, researchers in singapore and australia propose a way you could use a quantum computer securely, even over the internet.
the technique could hide both your data and program from the computer itself.
their work counters earlier hints that such a feat is impossible.
the scenario is not far- fetched.
quantum computers promise new routes to solving problems in cryptography, modelling and machine learning, exciting government and industry.
such problems may involve confidential data or be commercially sensitive.
technology giants are already investing in building such computers- and making them available to users.
for example, ibm announced on 17 may this year that it is making a quantum computer with 16 quantum bits accessible to the public for free on the cloud, as well as a 17- qubit prototype commercial processor.
seventeen qubits are not enough to outperform the world's current supercomputers, but as quantum computers gain qubits, they are expected to exceed the capabilities of any machine we have today.
that should drive demand for access.
"we're looking at what's possible if you're someone just interacting with a quantum computer across the internet from your laptop.
we find that it's possible to hide some interesting computations," says joseph fitzsimons, a principal investigator at the centre for quantum technologies (cqt) at the national university of singapore and associate professor at singapore university of technology and design (sutd), who led the work.
quantum computers work by processing bits of information stored in quantum states.
unlike the binary bits found in our regular (i.e., classical) computers, each a 0 or 1, qubits can be in superpositions of 0 and 1.
the qubits can also be entangled, which is believed to be crucial to a quantum computer's power.
the scheme designed by fitzsimons and his colleagues brings secrecy to a form of quantum computing driven by measurements.
in this scheme, the quantum computer is prepared by putting all its qubits into a special type of entangled state.
then the computation is carried out by measuring the qubits one by one.
the user provides step- wise instructions for each measurement: the steps encode both the input data and the program.
researchers have shown previously that users who can make or measure qubits to convey instructions to the quantum computer could disguise their computation.
the new paper extends that power to users who can only send classical bits - i.e.
most of us, for now.
this is surprising because some computer science theorems imply that encrypted quantum computation is impossible when only classical communication is available.
the hope for security comes from the quantum computer not knowing which steps of the measurement sequence do what.
the quantum computer can't tell which qubits were used for inputs, which for operations and which for outputs.
"it's extremely exciting.
you can use this unique feature of the measurement- based model of quantum computing- the way information flows through the state- as a crypto tool to hide information from the server," says team member tommaso demarie of cqt and sutd.
although the owner of the quantum computer could try to reverse engineer the sequence of measurements performed, ambiguity about the role of each step leads to many possible interpretations of what calculation was done.
the true calculation is hidden among the many, like a needle in a haystack.
the set of interpretations grows rapidly with the number of qubits.
"the set of all possible computations is exponentially large - that's one of the things we prove in the paper- and therefore the chance of guessing the real computation is exponentially small," says fitzsimons.
one question remains: could meaningful computations be so rare among all the possible ones that the guessing gets easier?
that's what the researchers need to check next.
nicolas menicucci at the centre for quantum computation and communication technology at rmit university in melbourne, australia, and atul mantri at sutd, are coauthors on the work.
"quantum computers became famous in the '90s with the discovery that they could break some classical cryptography schemes- but maybe quantum computing will instead be known for making the future of cloud computing secure," says mantri.
inspired by natural selection and the concept of "survival of the fittest," genetic algorithms are flexible optimization techniques that can find the best solution to a problem by repeatedly selecting for and breeding ever "fitter" generations of solutions.
now for the first time, researchers urtzi las heras et al.
at the university of the basque country in bilbao, spain, have applied genetic algorithms to digital quantum simulations and shown that genetic algorithms can reduce quantum errors, and may even outperform existing optimization techniques.
the research, which is published in a recent issue of physical review letters, was led by ikerbasque prof. enrique solano and dr. mikel sanz in the qutis group.
in general, quantum simulations can provide a clearer picture of the dynamics of systems that are impossible to understand using conventional computers due to their high degree of complexity.
whereas computers calculate the behavior of these systems, quantum simulations approximate or "simulate" the behavior.
as a quantum technology, digital quantum simulations face many of the same challenges that confront the quantum computing field in general.
one such challenge is information loss due to decoherence, which occurs when a quantum system interacts with its environment.
in order to protect quantum simulations against this loss, scientists use quantum error correction protocols, which provide a kind of back- up by storing information in entangled states of multiple qubits using quantum gates.
storing information in an entangled state is a highly complex undertaking in the context of quantum error correction.
for a system with just 4 qubits and 7 gates, the number of possible gate arrangements climbs into the trillions.
optimization techniques are used to sift through all of these designs and find the architecture that minimizes the error.
in the new study, the researchers demonstrated that genetic algorithms can identify gate designs for digital quantum simulations that outperform designs identified by standard optimization techniques, resulting in the lowest levels of digital quantum errors achieved so far.
besides reducing errors due to decoherence, genetic algorithms can also reduce two other types of errors in digital quantum simulations.
one type is the digital error created by the reduced number of steps used for approximating the algorithms.
another type of error arises from the imperfections in the construction of each of the gates.
as the researchers explain, one reason why genetic algorithms perform so well is their adaptability.
just like natural selection adapts to changes in environmental conditions, genetic algorithms continually adapt to different constraints imposed by different quantum technologies.
"genetic algorithms are characterized by different features: adaptability and robustness," solano told phys.org.
"their adaptability allows for a flexible and clever technique to solve different problems in different quantum technologies and platforms.
the robustness of the algorithm yields solutions that are resilient against errors, which allows us to cancel different error sources.
[due to these characteristics,] our work provides a new flexible tool in quantum simulations that allows us to reduce the required physical resources while keeping the operation precision.
it also reduces the total decoherence and digital error by seizing on the different unavoidable error sources to mutually cancel each other."
genetic algorithms already have been used in a wide variety of applications, such as finding the most efficient electrical circuit design, finding the mirror orientation that focuses the maximum amount of sunlight onto a solar collector, and designing antennas that are optimally tuned for detecting specific types of signals.
with help from genetic algorithms, future quantum simulations are expected to be useful for gaining a better understanding of complex physics, designing novel materials and chemicals, and solving problems in machine learning and artificial intelligence.
"these techniques could be used to solve problems that require resources unaffordable for present and future digital quantum simulations and gate- based quantum computing, by reducing and optimizing them," solano said.
"also these techniques could easily decompose a problem into quantum gates adapted to different quantum platforms and quantum technologies.
finally, these techniques could also be applied to different problems in quantum computation and quantum information, such as the design of improved qubits, for instance.
needless to say, quantum simulations and quantum computing aim at the big picture: artificial intelligence, pattern recognition, design of new materials and chemicals, solving complex problems in aerodynamics, and quantum field theories, among many others."
quantum computers are inherently different from their classical counterparts because they involve quantum phenomena, such as superposition and entanglement, which do not exist in classical digital computers.
but in a new paper, physicists have shown that a classical analog computer can be used to emulate a quantum computer, along with quantum superposition and entanglement, with the result that the fully classical system behaves like a true quantum computer.
physicist brian la cour and electrical engineer granville ott at applied research laboratories, the university of texas at austin (arl:ut), have published a paper on the classical emulation of a quantum computer in a recent issue of the new journal of physics.
besides having fundamental interest, using classical systems to emulate quantum computers could have practical advantages, since such quantum emulation devices would be easier to build and more robust to decoherence compared with true quantum computers.
"we hope that this work removes some of the mystery and 'weirdness' associated with quantum computing by providing a concrete, classical analog," la cour told phys.org.
"the insights gained should help develop exciting new technology in both classical analog computing and true quantum computing."
as la cour and ott explain, quantum computers have been simulated in the past using software on a classical computer, but these simulations are merely numerical representations of the quantum computer's operations.
in contrast, emulating a quantum computer involves physically representing the qubit structure and displaying actual quantum behavior.
one key quantum behavior that can be emulated, but not simulated, is parallelism.
parallelism allows for multiple operations on the data to be performed simultaneously- a trait that arises from quantum superposition and entanglement, and enables quantum computers to operate at very fast speeds.
to emulate a quantum computer, the physicists' approach uses electronic signals to represent qubits, in which a qubit's state is encoded in the amplitudes and frequencies of the signals in a complex mathematical way.
although the scientists use electronic signals, they explain that any kind of signal, such as acoustic and electromagnetic waves, would also work.
even though this classical system emulates quantum phenomena and behaves like a quantum computer, the scientists emphasize that it is still considered to be classical and not quantum.
"this is an important point," la cour explained.
"superposition is a property of waves adding coherently, a phenomenon that is exhibited by many classical systems, including ours.
"entanglement is a more subtle issue," he continued, describing entanglement as a "purely mathematical property of waves."
"since our classical signals are described by the same mathematics as a true quantum system, they can exhibit these same properties."
he added that this kind of entanglement does not violate bell's inequality, which is a widely used way to test for entanglement.
"entanglement as a statistical phenomenon, as exhibited by such things as violations of bell's inequality, is rather a different beast," la cour explained.
"we believe that, by adding an emulation of quantum noise to the signal, our device would be capable of exhibiting this type of entanglement as well, as described in another recent publication."
in the current paper, la cour and ott describe how their system can be constructed using basic analog electronic components, and that the biggest challenge is to fit a large number of these components on a single integrated circuit in order to represent as many qubits as possible.
considering that today's best semiconductor technology can fit more than a billion transistors on an integrated circuit, the scientists estimate that this transistor density corresponds to about 30 qubits.
an increase in transistor density of a factor of 1000, which according to moore's law may be achieved in the next 20 to 30 years, would correspond to 40 qubits.
this 40- qubit limit is also enforced by a second, more fundamental restriction, which arises from the bandwidth of the signal.
the scientists estimate that a signal duration of a reasonable 10 seconds can accommodate 40 qubits; increasing the duration to 10 hours would only increase this to 50 qubits, and a one- year duration would only accommodate 60 qubits.
due to this scaling behavior, the physicists even calculated that a signal duration of the approximate age of the universe (13.77 billion years) could accommodate about 95 qubits, while that of the planck time scale (10- 43 seconds) would correspond to 176 qubits.
considering that thousands of qubits are needed for some complex quantum computing tasks, such as certain encryption techniques, this scheme clearly faces some insurmountable limits.
nevertheless, the scientists note that 40 qubits is still sufficient for some low- qubit applications, such as quantum simulations.
because the quantum emulation device offers practical advantages over quantum computers and performance advantages over most classical computers, it could one day prove very useful.
for now, the next step will be building the device.
"efforts are currently underway to build a two- qubit prototype device capable of demonstrating entanglement," la cour said.
"the enclosed photo [see above] shows the current quantum emulation device as a lovely assortment of breadboarded electronics put together by one of my students, mr. michael starkey.
we are hoping to get future funding to support the development of an actual chip.
leveraging quantum parallelism, we believe that a coprocessor with as few as 10 qubits could rival the performance of a modern intel core at certain computational tasks.
fault tolerance is another important issue that we studying.
due to the similarities in mathematical structure, we believe the same quantum error correction algorithms used to make quantum computers fault tolerant could be used for our quantum emulation device as well."
intel announced horse ridge on monday, a new cryogenic control chip for development of commercially viable quantum computers.
horse ridge will enable control of multiple quantum bits (qubits) for larger systems, the company said.
horse ridge is intended to simplify the control electronics needed to run a quantum system.
it replaces bulky instruments with an integrated system- on- chip for faster set- up time and improved qubit performance to be able to scale to more qubit counts.
some of today's fastest quantum computers have about 53 qubits but intel said hundreds of thousands of qubits will be needed to demonstrate quantum practicality in the lab.
the connective wires that must be strung into and out of a cryogenic refrigerator where quantum computing occurs will be largely replaced by horse ridge.
industry survey
covid- 19 impact on sensors & electronics industry
as your resource for sensors and electronics news, fierceelectronics is committed to our community and want to bring you the latest information on how covid- 19 is impacting our industry and shine a light on the technologies being used to fight the pandemic.
we'd appreciate you taking this brief, 5- minute survey on how covid- 19 is impacting you and the information that you find most helpful now.
the survey findings, which we will share with the community, will help to shape our coverage moving forward.
"millions of qubits will be needed for a commercially viable quantum solution in the real world," said intel hardware director jim clarke in an online editorial.
"while developing control systems isn't, evidently, as hype- worthy as the increase in qubit count has been, it is a necessity.
and horse ridge could take quantum practicality to the finish line much faster than is currently possible."
the soc is fabricated with intel's 22nm finfet technology.
intel took the opportunity to defend its in- house fabrication of chips, which it said in a statement will "dramatically accelerate the company's ability to design, test and optimize a commercially viable quantum computer."
the ability to control many qubits at a time has been an industry challenge.
"intel recognized that quantum controls were an essential piece of the puzzle needed to solve in order to develop a large- scale commercial quantum system," clarke added in a statement.
"that's why we are investing in quantum error correction and controls."
the control chip is designed to operate at about 4 kelvin, which is only a small amount warmer than absolute zero where atoms stop moving.
today's quantum computers operate at the millikelvin range, a fraction above absolute zero.
silicon spin qubits in use today have properties that would overcome the challenges of cryogenic refrigeration.
as research moves ahead, intel hopes to have cryogenic controls operate at the same temperature as silicon spin qubits.
several major corporations are developing quantum computing systems.
amazon recently announced a fully managed service called braket to allow developers to experiment with quantum computers from d- wave, ionq and rigetti.
ibm has offered access to early quantum computers over its cloud since 2016.
microsoft also is active in the quantum computing arena.
related: aws announces managed quantum computing service called braket
google this year announced a quantum experiment with its sycamore processor to reduce the time needed to generate random strings of numbers to three minutes, down from 10,000 years needed by conventional computers.
ibm challenged the google conclusions and said the calculation could have been done in three days on today's supercomputers.
a lot of attention has been given to the differences between the quantum and classical worlds.
for example, quantum entanglement, superposition, and teleportation are purely quantum phenomena with no classical counterparts.
however, when it comes to certain areas of thermodynamics- specifically, thermal engines and refrigerators- quantum and classical systems so far appear to be nearly identical.
it seems that the same thermodynamic laws that govern the engines in our vehicles may also accurately describe the tiniest quantum engines consisting of just a single particle.
in a new study, physicists raam uzdin, amikam levy, and ronnie kosloff at the hebrew university of jerusalem have investigated whether there is anything distinctly quantum about thermodynamics at the quantum level, or if "quantum" thermodynamics is really the same as classical thermodynamics.
for the first time, they have shown a difference in the thermodynamics of heat machines on the quantum scale: in part of the quantum regime, the three main engine types (two- stroke, four- stroke, and continuous) are thermodynamically equivalent.
this means that, despite operating in different ways, all three types of engines exhibit all of the same thermodynamic properties, including generating the same amounts of power and heat, and doing so at the same efficiency.
this new "thermodynamical equivalence principle" is purely quantum, as it depends on quantum effects, and does not occur at the classical level.
the scientists also showed that, in this quantum regime where all engines are thermodynamically equivalent, it's possible to extract a quantum- thermodynamic signature that further confirms the presence of quantum effects.
they did this by calculating an upper limit on the work output of a classical engine, so that any engine that surpasses this bound must be using a quantum effect- namely, quantum coherence- to generate the additional work.
in this study, quantum coherence, which accounts for the wave- like properties of quantum particles, is shown to be critical for power generation at very fast engine cycles.
"to the best of my knowledge, this is the first time [that a difference between quantum and classical thermodynamics has been shown] in heat machines," uzdin told phys.org.
"what has been surprising [in the past] is that the classical description has still held at the quantum level, as many authors have shown.
the reasons are now understood, and in the face of this classicality, people have started to stray to other types of research, as it was believed that nothing quantum can pop up.
thus, it was very difficult to isolate a generic effect, not just a numerical simulation of a specific case, with a complementing theory that manages to avoid the classicality and demonstrate quantum effects in thermodynamic quantities, such as work and heat."
one important implication of the new results is that quantum effects may significantly increase the performance of engines at the quantum level.
while the current work deals with single- particle engines, the researchers expect that quantum effects may also emerge in multi- particle engines, where quantum entanglement between particles may play a role similar to that of coherence.
quantum computing harnesses enigmatic properties of small particles to process complex information.
but quantum systems are fragile and error- prone, and useful quantum computers have yet to come to fruition.
researchers in the quantum dynamics unit at the okinawa institute of science and technology graduate university (oist) devised a new method -  called image charge detection -  to detect electrons' transitions to quantum states.
electrons can serve as quantum bits, the smallest unit of quantum information; these bits are foundational to larger computational systems.
quantum computers may be used to understand the mechanism of superconductivity, cryptography, artificial intelligence, among other applications.
"there is a huge gap between controlling few quantum bits and building a quantum computer," said dr. erika kawakami, the lead author of a new study, published in physical review letters with editor's suggestion.
"with the current state- of- art quantum bits, a quantum computer would need to be the size of a football field.
our new approach could potentially create a ten- centimeter chip."
a new potential for electrons on helium
electrons need to be immobilized to serve as quantum bits; otherwise they move freely.
to create an electron- capturing system, the researchers used liquid helium, which liquefies at cold temperatures, as a substrate.
since helium is free of impurities, these electrons are expected to retain quantum states longer than in any other materials, which is important for realizing a quantum computer.
prof. denis konstantinov and his collaborators, kawakami and dr. asem elarabi, placed a parallel- plate capacitor inside of a copper cell cooled to 0.2 degrees kelvin (- 272.8 degrees celsius) and filled with condensed liquid helium.
electrons generated by a tungsten filament sat atop the liquid helium's surface, between the two capacitor plates.
then, microwave radiation introduced into the copper cell excited electrons' quantum states, causing the electrons to move away from the bottom capacitor plate and come closer to the top capacitor plate.
the researchers confirmed the excitation of quantum states by observing an electrostatic phenomenon called image charge.
like a reflection in a mirror, image charge precisely reflects the movement of electrons.
if an electron moves further from the capacitor plate, then the image charge moves alongside it.
moving forward, the researchers hope to use this image charge detection to measure an individual electron's spin state, or quantum orbital state, without disrupting the integrity of the quantum systems.
"currently, we can detect the quantum states of an ensemble of many electrons," konstantinov said.
"the strong point of this new method is that we can scale down this technique to a single electron and to use it as a quantum bit."
in classical computing, debugging programs is one of the most time- consuming tasks in software development.
successful debugging relies on software development tools and also on the experience of the programmer.
in quantum computing, researchers predict debugging will be an even greater challenge.
in a paper soon to appear at the acm/ieee 46th annual international symposium for computer architecture (as part of acm's 2019 federated computing research conferences), researchers at princeton university present debugging tools based on statistical tests, with a goal of aiding programmers in building correct quantum programs for near- term quantum computers.
quantum computing promises to change the computing world by offering capabilities beyond any classical computer.
those capabilities come from quantum algorithms- sequences of instructions that tell a quantum computer what to do in order to calculate some result, much like software for classical computers today.
these algorithms cover a wide range of applications.
for example, quantum chemistry algorithms would allow scientists to calculate properties of chemical compounds directly from the governing equations of quantum mechanics, a formidable task beyond the reach of modern computers for all but the simplest molecules.
other algorithms promise to speed up searching inside databases and to enable secure communications resistant to eavesdropping.
for about two decades, these quantum algorithms existed only as abstract equations and specifications, and have never actually been run on real quantum computers.
that research landscape has changed rapidly.
in the past couple of years, researchers have built the first prototype quantum computers capable of running quantum programs.
notably, ibm has made small- scale quantum computers available for the public to run code and see results.
with this burgeoning interest in quantum computing experimentation, a new and urgent challenge lies in helping programmers translate those abstract algorithms into correctly functioning quantum program code.
"we were finding that even researchers who specialize in quantum computing are making subtle mistakes in quantum program code, preventing the programs from giving correct results," yipeng huang, postdoc at princeton university and an author of the paper, said.
"if it is so tricky for experienced quantum programmers, how can students new to quantum computing write correct programs, without the aid of tools such as debuggers?"
in the paper titled "statistical assertions for val ... in quantum programs," huang and margaret martonosi, a professor of computer science at princeton, identify three key difficulties in debugging quantum programs, and evaluate their solutions in addressing those difficulties.
the first difficulty is that programmers cannot easily examine the values of variables of a quantum program, while the program is running.
this limitation makes debugging difficult, considering that one of the go- to moves in debugging programs is to inspect the values of variables step- by- step in the course of a program.
quantum programmers cannot do this kind of debugging because reading quantum variables would involve measuring and "collapsing" the delicate quantum states inside quantum computers.
once a quantum state is collapsed, any observations would not be a complete description of the state of the program.
in their paper, huang and martonosi address this challenge by finding ways to debug quantum programs using only the information about the collapsed quantum states.
they consider debugging programs in two different kinds of settings; in one setting the quantum programs run in simulation on a classical computer, and in the other setting the programs run on real prototype quantum computers.
in both settings, they use multiple runs of the quantum program in order find the distribution of the states inside the quantum program.
the second difficulty is that even when observations or simulations are available, quantum states are in general high- dimensional and difficult to interpret, limiting their usefulness for programmers to debug misbehaving quantum programs.
huang and martonosi's solution to this challenge is to use statistical tests on measurement results, in order to help programmers decide if the results are consistent with three types of states.
they use the chi- square statistical test to decide if the observed states belong to one of classical, superposition, or entangled states.
"we focus our attention on these three types of states because they occur throughout a quantum program, and are easier for programmers to identify," huang said.
"if the states don't match what the programmer expects, the statistical tests help the programmer zoom in and find mistakes in the program code."
the third difficulty is that programmers do not yet have any guidelines for where and what to check when debugging quantum programs.
until recently, quantum algorithms existed mainly as equations; occasionally, the algorithms would be more fleshed- out in the form of quantum circuit diagrams.
the task of coding quantum programs entails translating these quantum circuit diagrams into program code.
"the state- of- the- art in quantum programming is akin to programming classical computers 50 years ago," huang said.
"for the time being, researchers are writing quantum programs operation- by- operation, on very low- level bits of information.
one contribution of our paper is to discuss how the patterns and structures inside quantum algorithms guide programmers to know what to check."
in their paper, huang and martonosi use their debugging tools to test several benchmark quantum programs, including one for factoring integers, one for searching for data, and one in the area of quantum chemistry.
program patterns common inside these algorithms, such as looping operations, nesting operations, and mirroring operations, serve as guides for quantum programmers to know where to use the debugging tools.
supported by the national science foundation through the epiqc expedition project, huang and martonosi's work in debugging tools is a pragmatic approach to the problem of writing correct quantum programs.
it joins a growing field of related approaches, many which are based on formal proofs.
"we are finding that writing correct quantum programs relies on a mix of techniques," huang said.
"just like the case in classical programming, quantum programmers will rely on a mix of pragmatic and formal techniques."
more information: statistical assertions for validating patterns and finding bugs in quantum programs, arxiv:1905.09721 [quant- ph] https://arxiv.org/abs/1905.09721
even on the scale of everyday life, nature is governed by the laws of quantum physics.
these laws explain common phenomena like light, sound, heat, or even the trajectories of balls on a pool table.
but when applied to a large number of interacting particles, the laws of quantum physics actually predict a variety of phenomena that defy intuition.
in order to study quantum systems made of many particles, physicists must first be able to simulate them.
this can be done by solving the equations describing their inner workings on supercomputers.
but while moore's law predicts that the processing power of computers doubles every couple of years, this is a far cry from the power needed to tackle the challenges of quantum physics.
the reason is that predicting the properties of a quantum system is enormously complex, demanding a computational power that grows exponentially with the size of the quantum system- an "intrinsically complex" task, according to professor vincenzo savona, who directs the laboratory of theoretical physics of nanosystems at epfl.
"things become even more complicated when the quantum system is open, meaning that it is subject to the disturbances of its surrounding environment," savona adds.
and yet, tools to efficiently simulate open quantum systems are much needed, as most modern experimental platforms for quantum science and technology are open systems, and physicists are constantly in search of new ways to simulate and benchmark them.
but significant progress has been made thanks to a new computational method that simulates quantum systems with neural networks.
the method was developed by savona and his ph.d. student alexandra nagy at epfl- and independently by scientists at universite paris diderot, the heriot- watt university in edinburgh, and the flatiron institute in new york.
the total body of work is being published across three papers in physical review letters.
"we basically combined advances in neural networks and machine- learning with quantum monte carlo tools," says savona, referring to a large toolkit of computational methods that physicists use to study complex quantum systems.
the scientists trained a neural network to represent simultaneously the many quantum states in which a quantum system can be cast by the influence of its environment.
the neural- network approach allowed the physicists to predict the properties of quantum systems of considerable size and arbitrary geometry.
"this is a novel computational approach that addresses the problem of open quantum systems with versatility and a lot of potential for scaling up," says savona.
the method is set to become a tool of choice for the study of complex quantum systems, and, looking a bit more into the future, for assessing the effects of noise on quantum hardware.
the university of bradford's professor apostol vourdas has been awarded an ps62,000 grant to coordinate a network in quantum computing.
the grant, provided by the engineering and physical sciences research council (epsrc), will help 84 researchers in 17 uk universities and two industries (hitachi and hewlett- packard) to work together.
professor vourdas says the aim of the network is to promote collaboration and advance this "exciting, timely and interdisciplinary" area of academic research.
quantum computing is a novel method of computation based on the laws of quantum mechanics in other words, the laws of nature at ultra- small scales.
professor vourdas, said,: "from a computer science point of view the challenge is to create novel computational algorithms based on quantum logic.
this is fundamentally different from the classical logic used in today's computers.
related to quantum computing are the areas of quantum cryptography and quantum communications.
"from a hardware point of view the challenge is to create quantum gates.
they are much smaller than today's electronic devices and they operate with very few electrons and very few photons.
they are a part of the more general area of nanotechnology which is expected to have important implications in the future."
researchers at the university of waterloo have developed a method that could pave the way to establishing universal standards for measuring the performance of quantum computers.
the new method, called cycle benchmarking, allows researchers to assess the potential of scalability and to compare one quantum platform against another.
"this finding could go a long way toward establishing standards for performance and strengthen the effort to build a large- scale, practical quantum computer," said joel wallman, an assistant professor at waterloo's faculty of mathematics and institute for quantum computing.
"a consistent method for characterizing and correcting the errors in quantum systems provides standardization for the way a quantum processor is assessed, allowing progress in different architectures to be fairly compared."
cycle benchmarking provides a solution that helps quantum computing users to both determine the comparative value of competing hardware platforms and increase the capability of each platform to deliver robust solutions for their applications of interest.
the breakthrough comes as the quantum computing race is rapidly heating up, and the number of cloud quantum computing platforms and offerings is quickly expanding.
in the past month alone, there have been significant announcements from microsoft, ibm and google.
this method determines the total probability of error under any given quantum computing applications when the application is implemented through randomized compiling.
this means that cycle benchmarking provides the first cross- platform means of measuring and comparing the capabilities of quantum processors that is customized to users' applications of interest.
"thanks to google's recent achievement of quantum supremacy, we are now at the dawn of what i call the `quantum discovery era', said joseph emerson, a faculty member at iqc.
"this means that error- prone quantum computers will deliver solutions to interesting computational problems, but the quality of their solutions can no longer be verified by high- performance computers.
"we are excited because cycle benchmarking provides a much- needed solution for improving and validating quantum computing solutions in this new era of quantum discovery."
emerson and wallman founded the iqc spin- off quantum benchmark inc., which has already licensed this technology to several world- leading quantum computing providers, including google's quantum ai effort.
quantum computers offer a fundamentally more powerful way of computing, thanks to quantum mechanics.
compared to a traditional or digital computer, quantum computers can solve certain types of problems more efficiently.
however, qubits- the basic processing unit in a quantum computer- are fragile; any imperfection or source of noise in the system can cause errors that lead to incorrect solutions under a quantum computation.
gaining control over a small- scale quantum computer with just one or two qubits is the first step in a larger, more ambitious endeavour.
a larger quantum computer may be able to perform increasingly complex tasks, like machine learning or simulating complex systems to discover new pharmaceutical drugs.
engineering a larger quantum computer is challenging; the spectrum of error pathways becomes more complicated as qubits are added and the quantum system scales.
characterizing a quantum system produces a profile of the noise and errors, indicating if the processor is performing the tasks or calculations, it is being asked to do.
to understand the performance of any existing quantum computer for a complex problem or to scale up a quantum computer by reducing errors, it's first necessary to characterize all significant errors affecting the system.
wallman, emerson and a group of researchers at the university of innsbruck identified a method to assess all error rates affecting a quantum computer.
they implemented this new technique for the ion trap quantum computer at the university innsbruck and found that error rates don't increase as the size of that quantum computer scales up a very promising result.
"cycle benchmarking is the first method for reliably checking if you are on the right track for scaling up the overall design of your quantum computer," said wallman.
"these results are significant because they provide a comprehensive way of characterizing errors across all quantum computing platforms."
the paper "characterizing large- scale quantum computers via cycle benchmarking" appears in nature communications.
a quantum internet could be used to send un- hackable messages, improve the accuracy of gps, and enable cloud- based quantum computing.
for more than twenty years, dreams of creating such a quantum network have remained out of reach in large part because of the difficulty of sending quantum signals across large distances without loss.
now, harvard and mit researchers have found a way to correct for signal loss with a prototype quantum node that can catch, store and entangle bits of quantum information.
the research is the missing link toward a practical quantum internet and a major step forward in the development of long- distance quantum networks.
researchers have built the missing link for an ultra- secure quantum internet.
image credit: kris snibbe/harvard
"this demonstration is a conceptual breakthrough that could extend the longest possible range of quantum networks and potentially enable many new applications in a manner that is impossible with any existing technologies," said mikhail lukin, co- director of the harvard quantum initiative.
"this is the realization of a goal that has been pursued by our quantum science and engineering community for more than two decades."
the national science foundation- funded research is published in nature.
every form of communication technology - from the first telegraph to today's fiberoptic internet - has had to address the fact that signals degrade and are lost when transmitted over distances.
repeaters, which receive and amplify signals to correct for this loss, were first developed to amplify fading wire telegraph signals in the mid- 1800s.
two hundred years later, repeaters are still an integral part of our long- distance communications infrastructure.
the new device combines the three most important elements of a quantum repeater - a long memory, the ability to efficiently catch information from photons, and a way to process it locally.
"if we compare the quest for a secure quantum internet to the 1960s mission to deliver americans to the surface of the moon and return them safely to earth," says filbert bartoli, director of nsf's division of electrical, communications and cyber systems, "the demonstration of quantum repeaters may be of comparable significance to the demonstration of detachable rocket boosters, which allowed astronauts to escape earth's atmosphere, or to the heat resistant shields that allowed them to return safely to earth and not burn up on reentry to earth's atmosphere."
an international collaboration led by physicists of the university of vienna shines new light on the question of the resources required for achieving quantum information processing.
the scientists demonstrate that less demanding resources, which are easier to prepare and to control, can be used for quantum- enhanced technologies.
in the experiment, which is published in nature physics, the researchers achieve remote quantum state preparation without requiring entanglement as a resource.
a fundamental characteristic of quantum physics is the fact that two or more particles can exhibit correlations stronger than classically allowed.
this unique characteristic applies particularly to quantum entanglement: as soon as the quantum state of a particle is measured the state of its entangled partner changes accordingly, regardless of how far apart the two entangled particles might be.
this feature allows for the remote quantum state preparation, which is an essential ingredient for applications in quantum communication, quantum cryptography, and quantum computation.
the degree of entanglement is often used as a figure of merit for determining its usefulness for quantum technologies.
strongly entangled systems, however, are very sensitive to extrinsic influence and difficult to prepare and to control.
a team of researchers headed by the physicists caslav brukner (theory) and philip walther (experiment) at the university of vienna have been able to show that in order to achieve successful remote state preparation entanglement is not the only way forward.
under certain circumstances, non- entangled states can outperform their entangled counterparts for such tasks - as long as they have a significant amount of so- called "quantum discord".
this novel and not yet fully understood measure of quantum correlations quantifies the disturbance of correlated particles when being measured.
in their experiments, the researchers used a variety of two- photon states with different polarization correlations.
"by measuring the polarization state of a certain photon we prepare the state of the respective partner photon remotely", explains philip walther.
"in the experiment we observe how the quality of our remotely prepared quantum state is affected by changes in the quantum discord."
this work provides an important and significant step towards future quantum information processing schemes that would rely on less demanding resources.
a team of researchers from google, the university of the basque country, the university of california and ikerbasque, basque foundation for science has devised a means for combining the two leading ideas for creating a quantum computer in one machine, offering a possible means for learning more about how to create a true quantum computer sometime in the future.
they have published the details in the journal nature.
computer scientists would really like to figure out how to build a true quantum computer- doing so would allow for solving problems that are simply unsolvable on conventional machines.
but, unfortunately, the idea behind such a computer is still mostly theoretical.
to move some of the ideas from theory to reality, the researchers with this new effort have built an actual machine that is based on two of the strongest approaches to building a quantum computer.
the first approach is based on the gate model, where qubits are linked together to form primitive circuits that together form quantum logic gates.
in such an arrangement, each logic gate is capable of performing one specific type of operation.
thus, to make use of such a computer, each of the logic gates must be programmed ahead of time to carry out certain tasks.
with the second approach the qubits do not interact, instead they are kept at a ground state where they are then caused to evolve into a system capable of solving a particular problem.
the result is known as an adiabatic machine- some have actually been built because they are more versatile than the gate model computers.
unfortunately, they are also not expected to be able to ever fully make use of the full power of quantum computing.
in this new effort, the researchers have attempted to gain the positive attributes of both approaches by creating a machine where they started with a standard quantum computer and then used it to simulate an adiabatic machine.
it uses 9 qubits and has over 1,000 logic gates and allows for communication between qubits to be turned on and off at will.
the end result, the team reports, is one that unlike an adiabatic machine, is able to tackle traditionally difficult computing problems.
they expect it to be useful as a research tool, helping lead the way to the development of a truly quantum computer.
more information: r. barends et al.
digitized adiabatic quantum computing with a superconducting circuit, nature (2016).
doi: 10.1038/nature17658
abstract
quantum mechanics can help to solve complex problems in physics and chemistry, provided they can be programmed in a physical device.
in adiabatic quantum computing, a system is slowly evolved from the ground state of a simple initial hamiltonian to a final hamiltonian that encodes a computational problem.
the appeal of this approach lies in the combination of simplicity and generality; in principle, any problem can be encoded.
in practice, applications are restricted by limited connectivity, available interactions and noise.
a complementary approach is digital quantum computing, which enables the construction of arbitrary interactions and is compatible with error correction, but uses quantum circuit algorithms that are problem- specific.
here we combine the advantages of both approaches by implementing digitized adiabatic quantum computing in a superconducting system.
we tomographically probe the system during the digitized evolution and explore the scaling of errors with system size.
we then let the full system find the solution to random instances of the one- dimensional ising problem as well as problem hamiltonians that involve more complex interactions.
this digital quantum simulation of the adiabatic algorithm consists of up to nine qubits and up to 1,000 quantum logic gates.
the demonstration of digitized adiabatic quantum computing in the solid state opens a path to synthesizing long- range correlations and solving complex computational problems.
when combined with fault- tolerance, our approach becomes a general- purpose algorithm that is scalable.
honeywell has developed and integrated trapped- ion technology into quantum computing.
this is a different category altogether than the quantum computers built by ibm, which use solid- state quantum computing.
honeywell recently claimed a breakthrough achievement in quantum computing, one that will allow the company to increase the capacity of quantum computers so significantly that it predicts that the torch of quantum supremacy will be it's to run with.
put succinctly, the company is putting a stake in the ground: in three months, ibm and google's efforts to achieve quantum supremacy will be surpassed by honeywell's.
the terms by which honeywell made this pronouncement are based on a relatively new unit of measurement known as quantum volume, which measures the computational capability of a quantum computer.
honeywell also announced that it has invested in two prominent quantum software providers.
together with these providers, honeywell will work with jpmorgan chase to develop quantum computing algorithms.
how can honeywell be so sure?
as an alternative to solid- state quantum computing, trapped- ion quantum computing research by honeywell affords the company a new way to approach scalability by exploring and testing differing computational architectures.
the quantum volume of a quantum computer in solid- state quantum computing is achieved by linear arrangements of qubits.
these linear arrangements of qubits have limitations.
specifically, theylimit the number of qubits that can be strung together.
honeywell's trapped- ion technology research is based on superseding these limitations by designing computer architecture in one- and two- dimensional arrays.
honeywell is pinning its hopes on these relatively novel computer architectures coupled with trapped- ion technology to abolish limits on the number of qubits that can be strung together in a quantum computer.
this would pave the way for scaling up the number of qubits available in a quantum computer.
to claim quantum supremacy, honeywell says that it will bring to market a quantum computer with a quantum volume of 64, which is twice as powerful as any existing alternative from any company.
ibm created quantum volume as a unit of measurement for quantum computing in 2017, and it has already hit 32 this year.
ibm predicts that it will double its quantum volume annually.
honeywell is saying that it will beat ibm to a quantum computer with a quantum volume of 64in three months.
quantum volume as a unit of measurement may not consider the many intricate differences in varying approaches to quantum computing and may not prioritize practical market- based applications as an integral part of assessing the overall value of quantum computers.
bottom line
quantum computers are supremely cost- prohibitive right now, and applications are experimental.
there are some cloud- based quantum computing services on the market, and more are expected to follow.
quantum supremacy will likely change hands between two or three companies with powerful state- of- the- art computers.
whether or not honeywell will deliver on its promise to bring a quantum computer to market and achieve quantum supremacy remains to be seen.
researchers at delft university of technology have succeeded in carrying out calculations with two quantum bits, the building blocks of a possible future quantum computer.
the delft researchers are publishing an article about this important step towards a workable quantum computer in this week's issue of nature.
quantum computers have superior qualities in comparison to the type of computers currently in use.
if they are realised, then quantum computers will be able to carry out tasks that are beyond the abilities of all normal computers.
a quantum computer is based on the amazing properties of quantum systems.
in these a quantum bit, also known as a qubit, exists in two states at the same time and the information from two qubits is entangled in a way that has no equivalent whatsoever in the normal world.
it is highly likely that workable quantum computers will need to be produced using existing manufacturing techniques from the chip industry.
working on this basis, scientists at delft university of technology are currently studying two types of qubits: one type makes use of tiny superconducting rings, and the other makes use of 'quantum dots'.
the efficient generation of entanglement between remote quantum nodes is a crucial step in securing quantum communications.
in past research, entanglement has often been achieved using a number of different probabilistic schemes.
recently, some studies have also offered demonstrations of deterministic remote entanglement using approaches based on superconducting qubits.
nonetheless, the deterministic violation of bell's inequality (a strong measure of quantum correlation) in a superconducting quantum communication architecture has so far never been demonstrated.
a team of researchers based at the university of chicago has recently demonstrated a violation of bell's inequality using remotely connected superconducting qubits.
their paper, published in nature physics, introduces a simple and yet robust architecture for achieving this benchmark result in a superconducting system.
"there is a lot of interest and activity in developing experimental systems where quantum mechanics can be used for information processing (e.g.
communication, computation, etc.)
and sensing," andrew cleland, one of the researchers who carried out the study, told phys.org.
"the heart of a quantum information system is a qubit, and uniqueness comes from the quantum states you can store in it, as well as the more complex quantum states you can store using multiple qubits.
we were interested in exploring the transmission of quantum information and quantum states- the fundamentals for quantum communication."
quantum states, as well as the information that is stored within them, are incredibly delicate, far more than classical states and classically stored information.
although theoretically, there are ways to correct errors in a quantum state, one can typically fix only small errors; hence, the communication of a quantum state needs to be done with very high precision.
the high fidelity transmission of a quantum state has so far been achieved using a limited number of methods.
"we wanted to see if we could use some of the best qubits that are available, superconducting qubits, and the best tools for coupling superconducting qubits to communication (transmission) lines, to show we could transmit quantum states with very high precision (i.e.
fidelity)," cleland said.
in quantum physics, the 'gold standard' for testing a certain class of quantum states is bell's inequality.
essentially, a specific set of measurements of a property of a quantum state (usually written as "s") can exceed a classically limited value of two only if the quantum state is prepared, communicated and measured with high levels of precision.
"errors made in preparing, transmitting or measuring the quantum state will tend to make the state more classical, and make it harder to exceed the classical limit of two," cleland explained.
"exceeding this limit is called a violation of a bell inequality, and is a proof of 'quantum- ness'.
this was the measure we set out to achieve, by measuring s for a quantum state using a very precise generation, transmission, and capture of quantum information between two qubits.
happily, we were able to do this."
in their experiment, cleland and his colleagues used two superconducting qubits connected to one another via an approximately 1- meter- long transmission line.
the quantum information was transmitted along this line using microwaves (similar to radio signals), with a frequency similar to that cell phones use to communicate.
"very importantly, we also had electrically controlled 'couplers' between each qubit and the line," cleland said.
"these couplers are very important, because they allow us to control the coupling of the qubits to the line very rapidly, using classical electrical signals."
these electrically controlled couplers are a key component of the researchers' experiment, as they allowed them to 'shape' the coupling in time very precisely.
these couplers ensured that the microwaves carrying the quantum information were transmitted between the two qubits in precisely the right way.
this ultimately made sure that the quantum information was sent and received with minimal errors.
"our experiment shows that very precise quantum information can be sent along a communication path that is quite long, in our case nearly one meter in length," cleland explained.
"the method we used would work with any length line.
this demonstrates that the theoretical methods that had been worked out for this nearly error- free transmission are correct, and holds great promise for future quantum communication systems."
the study carried out by cleland and his colleagues introduced a simple but effective method to achieve a violation in bell's inequality using remote superconducting qubits.
however, as the qubits used in their experiment communicate with microwaves, their method only works at very low temperatures.
to communicate quantum information through air, the researchers would need to develop new techniques that can attain similar results using infrared or visible light.
"we are now planning on doing more complex versions of this experiment, using more qubits and more transmission lines, to test out more advanced theories for quantum communication and quantum error correction," cleland said.
"we are also developing methods to try to do the same thing with infrared light, so the signals can be sent through an optical fiber, or through space."
researchers at the u. s. department of energy's ames laboratory, the university of california, santa barbara, and microsoft station q have made significant advancements in understanding a fundamental problem of quantum mechanics - one that is blocking efforts to develop practical quantum computers with processing speeds far superior to conventional computers.
their respective theoretical and experimental studies investigate how microscopic objects lose their quantum- mechanical properties through interactions with the environment.
the results of the researchers' investigations were announced at the american physical society meeting held march 10- 14 in new orleans and also reported in science express, the advance online publication of the journal science.
"quantum- mechanical particles can interact with their environments: visible light, or photons; molecules of the air; crystal vibrations; and many other things," said viatcheslav dobrovitski, an ames laboratory theoretical physicist.
"all these uncontrollable interactions randomly 'kick' the system, destroying quantum phases, or the ability of particles to preserve coherence between different quantum states."
quantum coherence is essential to developing quantum computers in which information would be stored and processed on quantum mechanical states of quantum bits, called qubits.
so the self- destructive nature of quantum- mechanical states interacting with the environment is a huge problem.
to find out more about how quantum coherence breaks down and to study the dynamics of this decoherence process, the ames lab, ucsb and microsoft station q team studied certain spin systems, called nitrogen- vacancy, n- v, impurity centers, in diamond.
(spin is the intrinsic angular momentum of an elementary particle, such as an electron.)
n- v impurity centers in diamond are interesting because of the ability to control and manipulate the quantum state of a single center, allowing scientists to study the loss of coherence at a single- particle scale.
the ames lab, ucsb and microsoft station q researchers were able to manipulate the n- v centers interacting with an environment of nitrogen spins in a piece of diamond.
amazingly, the physicists were able to tune and adjust the environmental interference extremely well, accessing surprisingly different regimes of decoherence in a single system.
the scientists showed that the degree of interaction between the qubit and the interfering environment could be regulated by applying a moderate magnetic field.
by using analytical theory and advanced computer simulations, they gained a clear qualitative picture of the decoherence process in different regimes, and also provided an excellent quantitative description of the quantum spin dynamics.
the experiments were performed at room temperature rather than the extremely low temperatures often required for most atomic scale investigations.
dobrovitski noted that quantum coherence of n- v centers in diamond is being studied by leading scientific groups worldwide.
"the combined efforts of these groups could help in opening the way to developing a series of interacting qubits - steps to a quantum computer - where each n- v center would act as a qubit," he said.
"in addition to quantum computers, quantum coherence plays an important role for future less exotic, but not less spectacular, applications," said dobrovitski.
"for instance, quantum spins can be employed to develop coherent spintronic devices, which would work much faster than traditional microelectronic elements and dissipate much less energy.
quantum coherence between many spins can be employed to perform measurements with ultrahigh precision for metrology applications or to drastically increase the sensitivity of modern nuclear magnetic resonance, nmr, or electron spin resonance, esr, experiments.
"however, in order to implement these appealing proposals, a very good understanding of quantum coherence and its destruction by the environment is needed," dobrovitski emphasized.
in particular, from the application point of view, it is important to understand the loss of coherence of quantum systems in solid- state environments, which form the basis of modern technology."
article: http://www.sciencemag.org/cgi/ ... ent/abstract/1155400
quantum computing and quantum information processing technology have attracted attention in recently emerging fields.
among many important and fundamental issues in nowadays science, solving schroedinger equation (se) of atoms and molecules is one of the ultimate goals in chemistry, physics and their related fields.
se is "first principle" of non- relativistic quantum mechanics, whose solutions termed wave- functions can afford any information of electrons within atoms and molecules, predicting their physicochemical properties and chemical reactions.
researchers from osaka city university (ocu) in japan, dr. k. sugisaki, profs.
k. sato and t. takui and coworkers have found a quantum algorithm enabling us to perform full configuration interaction (full- ci) calculations for any open shell molecules without exponential/combinatorial explosion.
full- ci gives the exact numerical solutions of se, which are one of the intractable problems with any supercomputers.
the implementation of such a quantum algorithm contributes to the acceleration of implementing practical quantum computers.
the paper has been published on december 13th, 2018 in the first issue of open access journal chemical physics letters x.
they said, "as dirac claimed in 1929 when quantum mechanics was established, the exact application of mathematical theories to solve se leads to equations too complicated to be soluble.
in fact, the number of variables to be determined in the full- ci method grows exponentially against the system size, and it easily runs into astronomical figures such as exponential explosion.
for example, the dimension of the full- ci calculation for benzene molecule c6h6, in which only 42 electrons are involved, amounts to 1044, which are impossible to be dealt with any supercomputers."
according to the ocu research group, quantum computers can date back to a feynman's suggestion in 1982 that the quantum mechanics can be simulated by a computer itself built of quantum mechanical elements which obey quantum mechanical laws.
after more than 20 years later, prof. aspuru- guzik, harvard univ.
(toronto univ.
since 2018) and coworkers proposed a quantum algorithm capable of calculating the energies of atoms and molecules not exponentially but polynomially against the number of the variables of the systems, making a breakthrough in the field of quantum chemistry on quantum computers.
when aspuru's quantum algorithm is applied to the full- ci calculations on quantum computers, good approximate wave- functions close to the exact wave- functions of se under study are required, otherwise bad wave- functions need an extreme number of steps of repeated calculations to reach the exact ones, hampering the advantages of quantum computing.
this problem becomes extremely serious for any open shell systems, which have many unpaired electrons not participating in chemical bonding.
the ocu researchers have tackled this problem, one of the most intractable issues in quantum science, and made a breakthrough in implementing a quantum algorithm generating particular wave- functions termed configuration state functions in polynomial computing time in 2016.
the previously proposed algorithm requires a considerable number of quantum circuit gate operations proportional to the squares of the number of n, which denotes the number of down- spins of the unpaired electrons in the system.
thus, if n increases, the total computing time increases not exponentially but drastically.
additionally, the complexity of the quantum circuits should be reduced for practical usage of the algorithm and quantum programing architecture.
a new quantum algorithm exploits germinal spin functions, termed serber construction, and reduces the number of the gate operations to only 2n, executing parallelism of the quantum gates.
the ocu group said, "this is the first example of practical quantum algorithms, which make quantum chemical calculations realizable on quantum computers equipped with a sizable number of qubits.
these implementations empower practical applications of quantum chemical calculations on quantum computers in many important fields."
story source:
materials provided by osaka city university.
note: content may be edited for style and length.
journal reference:
kenji sugisaki, satoru yamamoto, shigeaki nakazawa, kazuo toyota, kazunobu sato, daisuke shiomi, takeji takui.
open shell electronic state calculations on quantum computers: a quantum circuit for the preparation of configuration state functions based on serber construction.
chemical physics letters: x, 2018; 100002 doi: 10.1016/j.cpletx.2018.100002
ibm has announced a milestone in its race against google and other big tech firms to build a powerful quantum computer.
dario gil, who leads ibm's quantum computing and artificial intelligence research division, said friday that the company's scientists have successfully built and measured a processor prototype with 50 quantum bits, known as qubits.
gil says it's the first time any company has built a quantum computer at this scale.
quantum computing, a technology that's still in its early phases, uses the quirks of quantum physics to perform calculations at far higher speeds than current computers.
seth lloyd, an mit mechanical engineering professor not involved in ibm's research, says it's likely that ibm still has glitches to work out but the 50- qubit announcement is a sign of significant progress.
physicists have designed a new method for transmitting big quantum data across long distances that requires far fewer resources than previous methods, bringing the implementation of long- distance big quantum data transmission closer to reality.
the results may lead to the development of future quantum networks, such as a global- scale quantum internet.
the researchers, michael zwerger and coauthors at the university of innsbruck, austria, have published a paper on the new long- range quantum communication method in a recent issue of physical review letters.
"the greatest significance of our work is that we provide an efficient and scalable scheme for long- distance quantum communication," zwerger told phys.org.
"we believe that this will be an essential ingredient for a future quantum internet, where large amounts of quantum data will be transmitted.
most importantly, in contrast to previous proposals, the required resources (per transmitted qubit) at each repeater station do not scale with the distance, which makes the quantum data transmission more efficient."
the new method relies on an alternative type of quantum repeater- a device that generates quantum entanglement at distant locations on a quantum network in order to combat signal loss, somewhat how an amplifier boosts the signal in classical communication networks.
the biggest advantage of the new quantum repeater is that it can allow quantum data transmission to be scaled up to longer distances much more easily than with previous quantum repeaters.
typically, as the transmission distance increases, more resources (qubits) are needed at each repeater station.
in previous schemes, the number of resources grows polylogarithmically or even polynomially at each repeater station with the distance.
using the new quantum repeater, the number of resources per transmitted qubit remains constant at each repeater station; that is, it is entirely independent of the distance.
this allows for quantum data to be transmitted over arbitrarily long distances using a relatively small amount of resources.
in its current implementation, the method uses a few hundred qubits at each repeater station, and can reach intercontinental distances.
as the physicists explain, the key behind the new quantum repeater is an entanglement distillation protocol called hashing, which generates perfect pairs of entangled qubits.
the researchers also used an optimized measurement- based implementation, which greatly reduces unwanted noise.
these tools provide a high error tolerance and high transmission rates, allowing for quantum data transmission in realistically noisy scenarios, such as a quantum internet.
"just think of the internet as it has grown over the years, where data transmission has increased dramatically," zwerger said.
"one can envision a quantum internet, where rather than classical data quantum information is transmitted.
indeed, a number of very interesting applications of such quantum data transmission have been discussed, among them quantum cryptography, distributed quantum computing and distributed sensing.
truly secure transmission requires large keys, and hence also large quantum transmission rates.
a similar thing can be said about the possibility of distributed quantum computation.
in early proof- of- principle experiments, rates and overheads might not be a big deal, but this for sure will become highly relevant once one scales things up.
this is where our proposal becomes relevant."
in the future, the researchers plan to extend the new quantum repeater devices to work with larger networks.
"the present proposal is for point- to- point communication between a sender and a receiver," zwerger said.
"we plan to use similar ideas for multipartite quantum networks with many users.
in addition, we are currently investigating novel schemes where we try to apply similar techniques on smaller scales- taking some of the ideas of the hashing protocol and design entanglement purification protocols and communication schemes that use only a few qubits.
this might have an impact on a shorter timescale, when first prototype quantum communication systems will be built."
scientists have developed a topological photonic chip to process quantum information, promising a more robust option for scalable quantum computers.
the research team, led by rmit university's dr. alberto peruzzo, has for the first time demonstrated that quantum information can be encoded, processed and transferred at a distance with topological circuits on the chip.
the research is published in science advances.
the breakthrough could lead to the development of new materials, new generation computers and deeper understandings of fundamental science.
in collaboration with scientists from the politecnico di milano and eth zurich, the researchers used topological photonics- a rapidly growing field that aims to study the physics of topological phases of matter in a novel optical context- to fabricate a chip with a 'beamsplitter' creating a high precision photonic quantum gate.
"we anticipate that the new chip design will open the way to studying quantum effects in topological materials and to a new area of topologically robust quantum processing in integrated photonics technology," says peruzzo, chief investigator at the arc centre of excellence for quantum computation and communication technology (cqc2t) and director, quantum photonics laboratory, rmit.
"topological photonics have the advantage of not requiring strong magnetic fields, and feature intrinsically high- coherence, room- temperature operation and easy manipulation" says peruzzo.
"these are essential requirements for the scaling- up of quantum computers."
replicating the well known hong- ou- mandel (hom) experiment- which takes two photons, the ultimate constituents of light, and interfere them according to the laws of quantum mechanics- the team was able to use the photonic chip to demonstrate, for the first time, that topological states can undergo high- fidelity quantum interference.
hom interference lies at the heart of optical quantum computation which is very sensitive to errors.
topologically protected states could add robustness to quantum communication, decreasing noise and defects prevalent in quantum technology.
this is particularly attractive for optical quantum information processing.
"previous research had focussed on topological photonics using 'classical' - laser- light, which behaves as a classical wave.
here we use single photons, which behave according to quantum mechanics" says lead- author jean- luc tambasco, ph.d. student at rmit.
demonstrating high- fidelity quantum interference is a precursor to transmitting accurate data using single photons for quantum communications- a vital component of a global quantum network.
"this work intersects the two thriving fields of quantum technology and topological insulators and can lead to the development of new materials, new generation computers and fundamental science" says peruzzo.
the research is part of the photonic quantum processor program at cqc2t.
the centre of excellence is developing parallel approaches using optical and silicon processors in the race to develop the first quantum computation system.
cqc2t's australian researchers have established global leadership in quantum information.
having developed unique technologies for manipulating matter and light at the level of individual atoms and photons, the team have demonstrated the highest fidelity, longest coherence time qubits in the solid state; the longest- lived quantum memory in the solid state; and the ability to run small- scale algorithms on photonic qubits.
physicists have developed a "quantum stopwatch"- a method that stores time (in the form of states of quantum clocks) in a quantum memory.
in doing so, the method avoids the accumulation of errors that usually occurs when measuring the duration of a sequence of events.
in this way, the quantum stopwatch increases the accuracy of measuring time at the quantum level, which is essential for applications such as gps, astronomy research, and distributed computing.
the physicists, yuxiang yang, giulio chiribella, and masahito hayashi, from the universities of hong kong, oxford, and nagoya, have published a paper on the quantum stopwatch technique in a recent issue of the proceedings of the royal society a.
as the physicists explain in their paper, when it comes to making highly accurate time measurements, some clocks are better than others for technological reasons.
but all clocks- no matter how well- constructed- are subject to a fundamental quantum limit that has its roots in heisenberg's uncertainty principle.
due to this quantum limit, larger clocks have smaller measurement errors, but no clock can be so large that it is completely error- free.
as a result of this limit, when one or more clocks make multiple time measurements- for example, when measuring the total duration of a sequence of events- then the errors accumulate.
this leads to an inaccuracy that grows linearly with the number of measurements.
the quantum stopwatch method addresses this problem by transferring the states of clocks (typically consisting of many identical atoms or ions) to the memory of a quantum computer.
the computer then processes all of the data and determines the length of the time interval using only a single measurement.
as a result, the only error is the error due to the measurement of one clock.
"the quantum stopwatch introduces a new, more accurate way of processing time information," chiribella told phys.org.
"before, most people thought that the only application of quantum clocks was to provide precise, classical information about time.
the clock was quantum, but the output was purely classical information, which could be stored into the memory of a classical computer.
with the stopwatch, we understood that maintaining time information in a quantum form can reduce errors by a very large amount.
the moral is: when we want to combine different pieces of time information, that information had better be quantum."
one of the challenges with this idea is that storing large amounts of information in a quantum memory is very difficult, which leads to the question of how much memory is needed to store time.
in their paper, the physicists derive a "quantum memory bound" that determines the minimum number of qubits required by the memory to store clock states with a certain accuracy.
overall, the physicists hope that, by showing that quantum computers could be used to increase the accuracy of time measurements, the quantum stopwatch will provide additional motivation for the development of quantum computers.
they expect that one of the biggest challenges for experimentally realizing the quantum stopwatch method will be encoding and decoding the states with a high accuracy.
after further improvements, the quantum stopwatch method could have a variety of new applications.
"one exciting area of application is the development of networks of quantum clocks," chiribella said.
"imagine that a number of quantum clocks are sitting at different positions in space, and can communicate to one another through quantum communication links.
by transferring information from one clock to another, we can greatly enhance the accuracy of time measurements in the network.
for example, we can measure the average ticking frequency of the clocks with a precision that would not be possible if the clocks were not connected with one another.
in the long term, these applications could lead to a quantum- enhanced gps technology, which could locate objects with a precision beyond the precision of our current gps devices."
research teams all over the world are exploring different ways to design a working computing chip that can integrate quantum interactions.
now, a team of engineers may have cracked the problem, reimagining the silicon microprocessors we know to create a complete design for a quantum computer chip that can be manufactured using mostly standard industry processes and components.
the new chip design, published in the journal nature communications, details a novel architecture that allows quantum calculations to be performed using existing semiconductor components, known as cmos (complementary metal- oxide- semiconductor)- the basis for all modern chips.
it was devised by andrew dzurak, director of the australian national fabrication facility at the university of new south wales (unsw), and menno veldhorst, lead author of the paper who was a research fellow at unsw when the conceptual work was done.
"we often think of landing on the moon as humanity's greatest technological marvel," said dzurak, who is also a program leader at the centre of excellence for quantum computation and communication technology (cqc2t).
"but creating a microprocessor chip with a billion operating devices integrated together to work like a symphony (that you can carry in your pocket!)
is an astounding technical achievement, and one that's revolutionized modern life.
"with quantum computing, we are on the verge of another technological leap that could be as deep and transformative.
but a complete engineering design to realize this on a single chip has been elusive.
i think what we have developed at unsw now makes that possible.
and most importantly, it can be made in a modern semiconductor manufacturing plant," he added.
veldhorst said the power of the new design is that, for the first time, it charts a conceivable engineering pathway toward creating millions of quantum bits, or qubits.
"remarkable as they are, today's computer chips cannot harness the quantum effects needed to solve the really important problems that quantum computers will.
to solve problems that address major global challenges - like climate change or complex diseases like cancer - it's generally accepted we will need millions of qubits working in tandem.
to do that, we will need to pack qubits together and integrate them, like we do with modern microprocessor chips.
that's what this new design aims to achieve.
"our design incorporates conventional silicon transistor switches to 'turn on' operations between qubits in a vast two- dimensional array, using a grid- based 'word' and 'bit' select protocol similar to that used to select bits in a conventional computer memory chip," he added.
"by selecting electrodes above a qubit, we can control a qubit's spin, which stores the quantum binary code of a 0 or 1.
and by selecting electrodes between the qubits, two- qubit logic interactions, or calculations, can be performed between qubits."
building a quantum computer
a quantum computer exponentially expands the vocabulary of binary code used in modern computers by using two spooky principles of quantum physics - namely, 'entanglement' and 'superposition'.
qubits can store a 0, a 1, or an arbitrary combination of 0 and 1 at the same time.
and just as a quantum computer can store multiple values at once, so it can process them simultaneously, doing multiple operations at once.
this would allow a universal quantum computer to be millions of times faster than any conventional computer when solving a range of important problems.
but to solve complex problems, a useful universal quantum computer will need a large number of qubits, possibly millions, because all types of qubits we know are fragile, and even tiny errors can be quickly amplified into wrong answers.
"so we need to use error- correcting codes which employ multiple qubits to store a single piece of data," said dzurak.
"our chip blueprint incorporates a new type of error- correcting code designed specifically for spin qubits, and involves a sophisticated protocol of operations across the millions of qubits.
it's the first attempt to integrate into a single chip all of the conventional silicon circuitry needed to control and read the millions of qubits needed for quantum computing."
"we expect that there will still be modifications required to this design as we move towards manufacture, but all of the key components that are needed for quantum computing are here in one chip.
and that's what will be needed if we are to make quantum computers a workhorse for calculations that are well beyond today's computers," dzurak added.
"it shows how to integrate the millions of qubits needed to realise the true promise of quantum computing."
building such a universal quantum computer has been called the 'space race of the 21st century'.
for a range of calculations, they will be much faster than existing computers, and for some challenging problems they could find solutions in days, maybe even hours, when today's best supercomputers would take millions of years.
quantum computing
there are at least five major quantum computing approaches being explored worldwide: silicon spin qubits, ion traps, superconducting loops, diamond vacancies and topological qubits; unsw's design is based on silicon spin qubits.
the main problem with all of these approaches is that there is no clear pathway to scaling the number of quantum bits up to the millions needed without the computer becoming huge a system requiring bulky supporting equipment and costly infrastructure.
that's why unsw's new design is so exciting: relying on its silicon spin qubit approach- which already mimics much of the solid- state devices in silicon that are the heart of the $380- billion usd global semiconductor industry- it shows how to dovetail spin qubit error correcting code into existing chip designs, enabling true universal quantum computation.
unlike almost every other major group elsewhere, cqc2t's quantum computing effort is focused on creating solid- state devices in silicon, from which all of the world's computer chips are made.
and they're not just creating ornate designs to show off how many qubits can be packed together, but aiming to build qubits that could one day be easily fabricated and scaled up.
"it's kind of swept under the carpet a bit, but for large- scale quantum computing, we are going to need millions of qubits," said dzurak.
"here, we show a way that spin qubits can be scaled up massively.
and that's the key."
it was only two years ago, in a paper in nature, that dzurak and veldhorst showed, for the first time, how quantum logic calculations could be done in a real silicon device, with the creation of a two- qubit logic gate: the central building block of a quantum computer.
"those were the first baby steps, the first demonstrations of how to turn this radical quantum computing concept into a practical device using components that underpin all modern computing," said mark hoffman, unsw's dean of engineering.
"our team now has a blueprint for scaling that up dramatically.
"we've been testing elements of this design in the lab, with very positive results.
we just need to keep building on that - which is still a hell of a challenge, but the groundwork is there, and it's very encouraging.
it will still take great engineering to bring quantum computing to commercial reality, but clearly the work we see from this extraordinary team at cqc2t puts australia in the driver's seat," he added.
the unsw team has struck a $83- million aus deal between unsw, telstra, commonwealth bank and the australian and new south wales governments to develop a 10- qubit prototype silicon quantum integrated circuit by 2022, marking the first step in building the world's first quantum computer in silicon.
for more quantum computing news, read about this major leap toward a global quantum internet.
us manufacturing and technology group honeywell said tuesday it will bring to market "the world's most powerful quantum computer" aimed at tackling complex scientific and business challenges.
the company said it had achieved a breakthrough in quantum computing, which uses subatomic particles to speed up processing, and would launch the new computers within three months.
it released a scientific paper describing the accelerated quantum capability.
honeywell said it had entered into partnerships with two quantum software and algorithm providers, cambridge quantum computing and zapata computing to accelerate its efforts and find new ways to deploy quantum computing.
"quantum computing will enable us to tackle complex scientific and business challenges, driving step- change improvements in computational power, operating costs and speed," honeywell chief executive darius adamczyk said.
"materials companies will explore new molecular structures.
transportation companies will optimize logistics.
financial institutions will need faster and more precise software applications.
pharmaceutical companies will accelerate the discovery of new drugs.
honeywell is striving to influence how quantum computing evolves and to create opportunities for our customers to benefit from this powerful new technology."
quantum computing is based on the use of quantum bits or qubits, which can perform trillions of calculations per second and in some cases outperform the fastest traditional supercomputers.
honeywell said it was collaborating with jpmorgan chase on financial applications for the technology.
it is also working with microsoft to allow enterprise users to access honeywell's quantum computer through the microsoft azure cloud platform.
the company said it hopes to address computing challenges that have been impractical to tackle with traditional computers.
"there are a number of industries that will be profoundly impacted by the advancement and ultimate application of at- scale quantum computing," said tony uttley, president of honeywell quantum solutions.
honeywell worked in stealth mode with partners before revealing its quantum plans, according to chief executive ilyas khan of cambridge quantum computing.
khan said honey became an investor in the british- based firm "after a period of very close working cooperation," and added: "it may well be one of the technology world's best kept secrets for over a generation."
the announcement comes after google claimed last year to have achieved "quantum supremacy" by developing a machine outperforming the world fastest supercomputers.
google said its sycamore solved a computing problem within 200 seconds which would have taken 10,000 years on a traditional computer.
ibm, which runs its own quantum computing program, said the boasts of the sycamore computer's feats were exaggerated.
what does a 1980s experimental aircraft have to do with state- of- the art quantum technology?
lots, as shown by new research from the quantum control laboratory at the university of sydney, and published in nature physics today.
over several years a team of scientists has taken inspiration from aerospace research and development programs to make unusually shaped experimental aircraft fly.
"it always amazed me that the x- 29, an american airplane that was designed like a dart being thrown backwards, was able to fly.
achieving this, in 1984, came through major advances in a discipline called control engineering that were able to stabilise the airplane," said associate professor michael biercuk, from the school of physics and director of the quantum control laboratory.
"we became interested in how similar concepts could play a role in bringing quantum technologies to reality.
if control engineering can turn an unstable dart into a high- performance fighter jet, it's pretty amazing to think what it can do for next- generation quantum technologies."
the result is that the researchers have been able to turn fragile quantum systems into useful pieces of advanced tech useful for everything from computation and communications to building specialised sensors for industry.
the trick was figuring out how to protect them from their environments using control theory.
the big challenge facing quantum technologies is they are very sensitive to random 'noise' in surrounding environments, said professor biercuk.
"noise, in this case, is a bit like local electromagnetic weather experienced by a piece of hardware.
imagine your television only worked when the weather was perfectly sunny.
something needs to be done to make that technology more functional, even on the grey days."
the new field of quantum control engineering provided a path forward.
the first step was trying to pinpoint how noise would affect a quantum system while it performed some task, which is fiendishly difficult.
"we were able to calculate how much damage is done to a quantum state using so- called transfer functions tailored to specific operations - for instance, manipulating a quantum system as a part of a computation," according to co- lead author, phd student harrison ball.
the next issue was to show that the theoretical techniques actually worked.
"one of our main achievements has been to show - using experiments on real quantum systems in the form of atoms in a special trap - that the transfer functions were excellent at predicting how quantum systems changed in response to environmental noise."
with new capabilities to predict the effect of the environment on quantum systems, it became possible to protect them by applying the right control techniques.
"similar to the control system that kept an aerodynamically unstable plane aloft, experiments revealed that our new techniques were able to keep the atoms performing useful computations," said biercuk.
"turn off the new control techniques and they would crash and burn."
"achieving this is a grand challenge for the entire community," according to ball, and it is especially important as researchers move from proof- of- principle demonstrations to trying to develop real quantum technologies.
working to make those technologies a reality is the aim of prof. biercuk and his colleagues in the arc centre for engineered quantum systems.
"this may sound like futuristic fantasy, but the navigation system in your car works because of an early quantum technology - atomic clocks," according to biercuk.
"we know that exotic phenomena like quantum systems being in two places at once, and even the ability to teleport quantum states, are real and accessible in the laboratory.
now we are trying to actually put them to work, and that means figuring out how to coax quantum systems into doing new and useful things."
original release: http://www.eurekalert.org/pub_releases/2014- 10/uos- 1aa102014.php
scientists at the department of energy's oak ridge national laboratory are the first to successfully simulate an atomic nucleus using a quantum computer.
the results, published in physical review letters, demonstrate the ability of quantum systems to compute nuclear physics problems and serve as a benchmark for future calculations.
quantum computing, in which computations are carried out based on the quantum principles of matter, was proposed by american theoretical physicist richard feynman in the early 1980s.
unlike normal computer bits, the qubit units used by quantum computers store information in two- state systems, such as electrons or photons, that are considered to be in all possible quantum states at once (a phenomenon known as superposition).
"in classical computing, you write in bits of zero and one," said thomas papenbrock, a theoretical nuclear physicist at the university of tennessee and ornl who co- led the project with ornl quantum information specialist pavel lougovski.
"but with a qubit, you can have zero, one, and any possible combination of zero and one, so you gain a vast set of possibilities to store data."
in october 2017 the multidivisional ornl team started developing codes to perform simulations on the ibm qx5 and the rigetti 19q quantum computers through doe's quantum testbed pathfinder project, an effort to verify and validate scientific applications on different quantum hardware types.
using freely available pyquil software, a library designed for producing programs in the quantum instruction language, the researchers wrote a code that was sent first to a simulator and then to the cloud- based ibm qx5 and rigetti 19q systems.
the team performed more than 700,000 quantum computing measurements of the energy of a deuteron, the nuclear bound state of a proton and a neutron.
from these measurements, the team extracted the deuteron's binding energy -  the minimum amount of energy needed to disassemble it into these subatomic particles.
the deuteron is the simplest composite atomic nucleus, making it an ideal candidate for the project.
"qubits are generic versions of quantum two- state systems.
they have no properties of a neutron or a proton to start with," lougovski said.
"we can map these properties to qubits and then use them to simulate specific phenomena -  in this case, binding energy."
a challenge of working with these quantum systems is that scientists must run simulations remotely and then wait for results.
ornl computer science researcher alex mccaskey and ornl quantum information research scientist eugene dumitrescu ran single measurements 8,000 times each to ensure the statistical accuracy of their results.
"it's really difficult to do this over the internet," mccaskey said.
"this algorithm has been done primarily by the hardware vendors themselves, and they can actually touch the machine.
they are turning the knobs."
the team also found that quantum devices become tricky to work with due to inherent noise on the chip, which can alter results drastically.
mccaskey and dumitrescu successfully employed strategies to mitigate high error rates, such as artificially adding more noise to the simulation to see its impact and deduce what the results would be with zero noise.
"these systems are really susceptible to noise," said gustav jansen, a computational scientist in the scientific computing group at the oak ridge leadership computing facility (olcf), a doe office of science user facility located at ornl.
"if particles are coming in and hitting the quantum computer, it can really skew your measurements.
these systems aren't perfect, but in working with them, we can gain a better understanding of the intrinsic errors."
at the completion of the project, the team's results on two and three qubits were within 2 and 3 percent, respectively, of the correct answer on a classical computer, and the quantum computation became the first of its kind in the nuclear physics community.
the proof- of- principle simulation paves the way for computing much heavier nuclei with many more protons and neutrons on quantum systems in the future.
quantum computers have potential applications in cryptography, artificial intelligence, and weather forecasting because each additional qubit becomes entangled -  or tied inextricably -  to the others, exponentially increasing the number of possible outcomes for the measured state at the end.
this very benefit, however, also has adverse effects on the system because errors may also scale exponentially with problem size.
papenbrock said the team's hope is that improved hardware will eventually enable scientists to solve problems that cannot be solved on traditional high- performance computing resources -  not even on the ones at the olcf.
in the future, quantum computations of complex nuclei could unravel important details about the properties of matter, the formation of heavy elements, and the origins of the universe.
a new computer program that spots when information in a quantum computer is escaping to unwanted states will give users of this promising technology the ability to check its reliability without any technical knowledge for the first time.
researchers from the university of warwick's department of physics have developed a quantum computer program to detect the presence of 'leakage', where information being processed by a quantum computer escapes from the states of 0 and 1.
their method is presented in a paper published today (19 march) in the journal physical review a, and includes experimental data from its application on a publicly accessible machine, that shows that undesirable states are affecting certain computations.
quantum computing harnesses the unusual properties of quantum physics to process information in a wholly different way to conventional computers.
taking advantage of the behaviour of quantum systems, such as existing in multiple different states at the same time, this radical form of computing is designed to process data in all of those states simultaneously, lending it a huge advantage over conventional computing.
in conventional computing, quantum computers use combinations of 0s and 1s to encode information, but quantum computers can exploit quantum states that are both 0 and 1 at the same time.
however, the hardware that encodes that information may sometimes encode it incorrectly in another state, a problem known as 'leakage'.
even a miniscule leakage accumulating over many millions of hardware components can cause miscalculations and potentially serious errors, nullifying any quantum advantage over conventional computers.
as a part of a much wider set of errors, leakage is playing its part in preventing quantum computers from being scaled up towards commercial and industrial application.
armed with the knowledge of how much quantum leakage is occurring, computer engineers will be better able to build systems that mitigate against it and programmers can develop new error- correction techniques to take account of it.
dr. animesh datta, associate professor of physics, said: "commercial interest in quantum computing is growing so we wanted to ask how we can say for certain that these machines are doing what they are supposed to do.
"quantum computers are ideally made of qubits, but as it turns out in real devices some of the time they are not qubits at all- but in fact are qutrits (three state) or ququarts (four state systems).
such a problem can corrupt every subsequent step of your computing operation.
"most quantum computing hardware platforms suffer from this issue- even conventional computer drives experience magnetic leakage, for example.
we need quantum computer engineers to reduce leakage as much as possible through design, but we also need to allow quantum computer users to perform simple diagnostic tests for it.
"if quantum computers are to enter common usage, it's important that a user with no idea of how a quantum computer works can check that it is functioning correctly without requiring technical knowledge, or if they are accessing that computer remotely."
the researchers applied their method using the ibm q experience quantum devices, through ibm's publicly accessible cloud service.
they used a technique called dimension witnessing: by repeatedly applying the same operation on the ibm q platform, they obtained a dataset of results that could not be explained by a single quantum bit, and only by a more complicated, higher dimensional quantum system.
they have calculated that the probability of this conclusion arising from mere chance is less than 0.05%.
while conventional computers use binary digits, or 0s and 1s, to encode information in transistors, quantum computers use subatomic particles or superconducting circuits known as transmons to encode that information as a qubit.
this means that it is in a superposition of both 0 and 1 at the same time, allowing users to compute on different sequences of the same qubits simultaneously.
as the number of qubits increases, the number of processes also increases exponentially.
certain kinds of problems, like those found in codebreaking (which relies on factoring large integers) and in chemistry (such as simulating complicated molecules), are particularly suited to exploiting this property.
transmons (and other quantum computer hardware) can exist in a huge number of states: 0, 1, 2, 3, 4 and so on.
an ideal quantum computer only uses states 0 and 1, as well as superpositions of these, otherwise errors will emerge in the quantum computation.
dr. george knee, whose work was funded by a research fellowship from the royal commission for the exhibition of 1851, said: "it is quite something to be able to make this conclusion at a distance of several thousand miles, with very limited access to the ibm chip itself.
although our program only made use of the permitted 'single qubit' instructions, the dimension witnessing approach was able to show that unwanted states were being accessed in the transmon circuit components.
i see this as a win for any user who wants to investigate the advertised properties of a quantum machine without the need to refer to hardware- specific details."
computing prime factors may sound like an elementary math problem, but try it with a large number, say one that contains more than 600 digits, and the task becomes enormously challenging and impossibly time- consuming.
now, a group of researchers at uc santa barbara has designed and fabricated a quantum processor capable of factoring a composite number - in this case the number 15 - into its constituent prime factors, 3 and 5.
although modest compared to a 600- digit number, the achievement represents a milestone on the road map to building a quantum computer capable of factoring much larger numbers, with significant implications for cryptography and cybersecurity.
the results are published in the advance online issue of the journal nature physics.
"fifteen is a small number, but what's important is we've shown that we can run a version of peter shor's prime factoring algorithm on a solid state quantum processor.
this is really exciting and has never been done before," said erik lucero, the paper's lead author.
now a postdoctoral researcher in experimental quantum computing at ibm, lucero was a doctoral student in physics at ucsb when the research was conducted and the paper was written.
"what is important is that the concepts used in factoring this small number remain the same when factoring much larger numbers," said andrew cleland, a professor of physics at ucsb and a collaborator on the experiment.
"we just need to scale up the size of this processor to something much larger.
this won't be easy, but the path forward is clear."
practical applications motivated the research, according to lucero, who explained that factoring very large numbers is at the heart of cybersecurity protocols, such as the most common form of encoding, known as rsa encryption.
"anytime you send a secure transmission - like your credit card information - you are relying on security that is based on the fact that it's really hard to find the prime factors of large numbers," he said.
using a classical computer and the best- known classical algorithm, factoring something like rsa laboratory's largest published number - which contains over 600 decimal digits - would take longer than the age of the universe, he continued.
a quantum computer could reduce this wait time to a few tens of minutes.
"a quantum computer can solve this problem faster than a classical computer by about 15 orders of magnitude," said lucero.
"this has widespread effect.
a quantum computer will be a game changer in a lot of ways, and certainly with respect to computer security."
so, if quantum computing makes rsa encryption no longer secure, what will replace it?
the answer, lucero said, is quantum cryptography.
"it's not only harder to break, but it allows you to know if someone has been eavesdropping, or listening in on your transmission.
imagine someone wiretapping your phone, but now, every time that person tries to listen in on your conversation, the audio gets jumbled.
with quantum cryptography, if someone tries to extract information, it changes the system, and both the transmitter and the receiver are aware of it."
to conduct the research, lucero and his colleagues designed and fabricated a quantum processor to map the problem of factoring the number 15 onto a purpose- built superconducting quantum circuit.
"we chose the number 15 because it is the smallest composite number that satisfies the conditions appropriate to test shor's algorithm - it is a product of two prime numbers, and it's not even," he explained.
the quantum processor was implemented using a quantum circuit composed of four superconducting phase qubits - the quantum equivalents of transistors - and five microwave resonators.
the complexity of operating these nine quantum elements required building a control system that allows for precise operation and a significant degree of automation - a prototype that will facilitate scaling up to larger and more complex circuits.
the research represents a significant step toward a scalable quantum architecture while meeting a benchmark for quantum computation, as well as having historical relevance for quantum information and cryptography.
"after repeating the experiment 150,000 times, we showed that our quantum processor got the right answer just under half the time" lucero said.
"the best we can expect from shor's algorithm is to get the right answer exactly 50 percent of the time, so our results were essentially what we'd expect theoretically."
the next step, according to lucero, is to increase the quantum coherence times and go from nine quantum elements to hundreds, then thousands, and on to millions.
"now that we know 15=3x5, we can start thinking about how to factor larger - dare i say - more practical numbers," he said.
through randomly selected measurements, austrian physicists can now determine the quantum entanglement of many- particle systems.
with the newly developed method, quantum simulations can be extended to a larger number of quantum particles.
in science, physicists from innsbruck, austria, report on the first successful demonstration of this method.
quantum phenomena are experimentally difficult to deal with.
the effort increases dramatically with the size of the system.
scientists can control small quantum systems and investigate quantum properties.
such quantum simulations are considered promising early applications of quantum technologies that could solve problems where simulations on conventional computers fail.
however, the quantum systems used as quantum simulators need further development.
the entanglement of many particles is still a phenomenon that is difficult to understand.
"in order to operate a quantum simulator consisting of 10 or more particles in the laboratory, we must characterize the states of the system as accurately as possible," explains christian roos from the institute of quantum optics and quantum information at the austrian academy of sciences.
so far, quantum state tomography has been used for the characterization of quantum states with which the system can be completely described.
this method, however, involves a very high measuring and computing effort and is not suitable for systems with more than a half- dozen particles.
two years ago, the researchers led by christian roos, together with colleagues from germany and great britain, presented an efficient method for the characterization of complex quantum states.
however, only weakly entangled states could be described with this method.
this issue has now been circumvented by a new method presented last year by the theorists led by peter zoller, which can be used to characterize any entangled state.
together with experimental physicists rainer blatt and christian roos and their team, they have now demonstrated this method in the laboratory.
quantum simulations on larger systems
"the new method is based on the repeated measurement of randomly selected transformations of individual particles.
the statistical evaluation of the measurement results then provides information about the degree of entanglement of the system," explains andreas elben from peter zoller's team.
the austrian physicists demonstrated the process in a quantum simulator consisting of several ions arranged in a row in a vacuum chamber.
starting from a simple state, the researchers let the individual particles interact with the help of laser pulses and thus generate entanglement in the system.
"we perform 500 local transformations on each ion and repeat the measurements a total of 150 times in order to then be able to use statistical methods to determine information about the entanglement state from the measurement results," explains ph.d. student tiff brydges from the institute of quantum optics and quantum information.
in the work now published in science, the innsbruck physicists characterized the dynamic development of a system consisting of 10 ions as well as a subsystem consisting of ten ions of a 20- ion chain.
"in the laboratory, this new method helps us a lot, because it enables us to understand our quantum simulator even better, and, for example, to assess the purity of the entanglement more precisely," says christian roos, who assumes that the new method can be successfully applied to quantum systems with up to several dozen particles.
a new manufacturing technique could lead to noise- resistant qubits that can be scaled up in quantum computers, according to a paper published in nature on wednesday, led by the university of purdue and microsoft, with participation from researchers at the university of chicago and the weizmann institute of science in israel.
the manufacturing technique combines a semiconductor- indium arsenide- with a superconductor- aluminum- into a planar device.
this combination creates a state of "topological superconductivity," which can shield against changes in the physical environment of a qubit that interfere with the ability to reliably sample results from calculations performed on quantum computers comprised of several connected qubits.
these disruptive changes affect the length of time a quantum system can execute a given task.
see: quantum computing: an insider's guide (free pdf) (techrepublic)
experiments using this technique have been shown to create a josephson junction, and can support majorana zero modes, which scientists have predicted possess topological protection against decoherence, the phenomenon that disrupts the operation of a quantum computer.
importantly, the planar nature of this manufacturing technique can allow it to scale, as planar surfaces are already used for building classical microprocessors, used in smartphones and laptops.
(while it is possible to "stack" nand flash to increase density, a technology marketed as "3d nand," heat dissipation issues have limited three dimentional microprocessors to laboratory proofs- of- concept.)
according to a press release, "it's also been known that aluminum and indium arsenide work well together because a supercurrent flows well between them.
this is because unlike most semiconductors, indium arsenide doesn't have a barrier that prevents the electrons of one material from entering another material.
this way, the superconductivity of aluminum can make the top layers of indium arsenide, a semiconductor, superconducting, as well."
this research represents a first step toward building quantum processors.
though there are presently devices marketed as quantum computers by d- wave and ibm, these are relatively noisy machines that represent a fraction of the ability which proponents of quantum computing research contend that true quantum computers will be capable of in the future.
quantum computers could revolutionize distribution logistics, as early small- scale path optimization problems calculated on existing quantum systems can be used to increase efficiency in warehousing and trucking.
these current quantum systems are primarily held back by limitations in manufacturing techniques, which this research aims to solve.
for more on quantum computing, learn about how helium shortages will impact quantum computer research, and how ibm is reducing noise in quantum computing, increasing accuracy of calculations.
also see
a team at the university of sydney and microsoft, in collaboration with stanford university in the us, has miniaturised a component that is essential for the scale- up of quantum computing.
the work constitutes the first practical application of a new phase of matter, first discovered in 2006, the so- called topological insulators.
beyond the familiar phases of matter -  solid, liquid, or gas -  topological insulators are materials that operate as insulators in the bulk of their structures but have surfaces that act as conductors.
manipulation of these materials provide a pathway to construct the circuitry needed for the interaction between quantum and classical systems, vital for building a practical quantum computer.
theoretical work underpinning the discovery of this new phase of matter was awarded the 2016 nobel prize in physics.
the sydney team's component, coined a microwave circulator, acts like a traffic roundabout, ensuring that electrical signals only propagate in one direction, clockwise or anti- clockwise, as required.
similar devices are found in mobile phone base- stations and radar systems, and will be required in large quantities in the construction of quantum computers.
a major limitation, until now, is that typical circulators are bulky objects the size of your hand.
this invention, reported by the sydney team today in the journal nature communications, represents the miniaturisation of the common circulator device by a factor of 1000.
this has been done by exploiting the properties of topological insulators to slow the speed of light in the material.
this minaturisation paves the way for many circulators to be integrated on a chip and manufactured in the large quantities that will be needed to build quantum computers.
the leader of the sydney team, professor david reilly, explained that the work to scale- up quantum computing is driving breakthroughs in related areas of electronics and nanoscience.
"it is not just about qubits, the fundamental building blocks for quantum machines.
building a large- scale quantum computer will also need a revolution in classical computing and device engineering," professor reilly said.
"even if we had millions of qubits today, it is not clear that we have the classical technology to control them.
realising a scaled- up quantum computer will require the invention of new devices and techniques at the quantum- classical interface."
lead author of the paper and phd candidate alice mahoney said: "such compact circulators could be implemented in a variety of quantum hardware platforms, irrespective of the particular quantum system used."
a practical quantum computer is still some years away.
scientists expect to be able to carry out currently unsolveable computations with quantum computers that will have applications in fields such as chemistry and drug design, climate and economic modelling, and cryptography.
professor david reilly is director of the university of sydney's microsoft quantum laboratory, a multimillion dollar partnership, which is part of a global effort by microsoft to build the world's first practical quantum computer.
the partnership is housed in the sydney nanoscience hub, the flagship building of the university of sydney nano institute.
researchers have succeeded in combining the power of quantum computing with the security of quantum cryptography and have shown that perfectly secure cloud computing can be achieved using the principles of quantum mechanics.
they have performed an experimental demonstration of quantum computation in which the input, the data processing, and the output remain unknown to the quantum computer.
the international team of scientists will publish the results of the experiment, carried out at the vienna center for quantum science and technology (vcq) at the university of vienna and the institute for quantum optics and quantum information (iqoqi), in the forthcoming issue of science.
quantum computers are expected to play an important role in future information processing since they can outperform classical computers at many tasks.
considering the challenges inherent in building quantum devices, it is conceivable that future quantum computing capabilities will exist only in a few specialized facilities around the world - much like today's supercomputers.
users would then interact with those specialized facilities in order to outsource their quantum computations.
the scenario follows the current trend of cloud computing: central remote servers are used to store and process data - everything is done in the "cloud."
the obvious challenge is to make globalized computing safe and ensure that users' data stays private.
the latest research, to appear in science, reveals that quantum computers can provide an answer to that challenge.
"quantum physics solves one of the key challenges in distributed computing.
it can preserve data privacy when users interact with remote computing centers," says stefanie barz, lead author of the study.
this newly established fundamental advantage of quantum computers enables the delegation of a quantum computation from a user who does not hold any quantum computational power to a quantum server, while guaranteeing that the user's data remain perfectly private.
the quantum server performs calculations, but has no means to find out what it is doing - a functionality not known to be achievable in the classical world.
the scientists in the vienna research group have demonstrated the concept of "blind quantum computing" in an experiment: they performed the first known quantum computation during which the user's data stayed perfectly encrypted.
the experimental demonstration uses photons, or "light particles" to encode the data.
photonic systems are well- suited to the task because quantum computation operations can be performed on them, and they can be transmitted over long distances.
the process works in the following manner.
the user prepares qubits - the fundamental units of quantum computers - in a state known only to himself and sends these qubits to the quantum computer.
the quantum computer entangles the qubits according to a standard scheme.
the actual computation is measurement- based: the processing of quantum information is implemented by simple measurements on qubits.
the user tailors measurement instructions to the particular state of each qubit and sends them to the quantum server.
finally, the results of the computation are sent back to the user who can interpret and utilize the results of the computation.
even if the quantum computer or an eavesdropper tries to read the qubits, they gain no useful information, without knowing the initial state; they are "blind."
more information: "demonstration of blind quantum computing" stefanie barz, elham kashefi, anne broadbent, joseph fitzsimons, anton zeilinger, philip walther.
doi: 10.1126/science.1214707
a quantum internet could be used to send unhackable messages, improve the accuracy of gps, and enable cloud- based quantum computing.
for more than twenty years, dreams of creating such a quantum network have remained out of reach in large part because of the difficulty to send quantum signals across large distances without loss.
now, harvard and mit researchers have found a way to correct for signal loss with a prototype quantum node that can catch, store and entangle bits of quantum information.
the research is the missing link towards a practical quantum internet and a major step forward in the development of long- distance quantum networks.
"this demonstration is a conceptual breakthrough that could extend the longest possible range of quantum networks and potentially enable many new applications in a manner that is impossible with any existing technologies," said mikhail lukin, the george vasmer leverett professor of physics and a co- director of harvard quantum initiative.
"this is the realization of a goal that has been pursued by our quantum science and engineering community for more than two decades."
the research is published in nature.
every form of communication technology- from the first telegraph to today's fiber optic internet- has had to address the fact that signals degrade and are lost when transmitted over distances.
the first repeaters, which receive and amplify signals to correct for this loss, were developed to amplify fading wire telegraph signals in the mid- 1800s.
two hundred years later, repeaters are an integral part of our long- distance communications infrastructure.
in a classical network, if alice in new york wants to send bob in california a message, the message travels from coast to coast in more or less a straight line.
along the way, the signal passes through repeaters, where it is read, amplified and corrected for errors.
the whole process is at any point vulnerable to attacks.
if alice wants to send a quantum message, however, the process is different.
quantum networks use quantum particles of light- individual photons- to communicate quantum states of light over long distances.
these networks have a trick that classical systems don't: entanglement.
entanglement- what einstein called "spooky action at a distance"- allows bits of information to be perfectly correlated across any distance.
because quantum systems can't be observed without changing, alice could use entanglement to message bob without any fear of eavesdroppers.
this notion is the foundation for applications such quantum cryptography- security that is guaranteed by the laws of quantum physics.
quantum communication over long distances, however, is also affected by conventional photon losses, which is one of the major obstacles for realizing large- scale quantum internet.
but, the same physical principle that makes quantum communication ultra- secure also makes it impossible to use existing, classical repeaters to fix information loss.
how can you amplify and correct a signal if you can't read it?
the solution to this seemingly impossible task involves a so- called quantum repeater.
unlike classical repeaters, which amplify a signal through an existing network, quantum repeaters create a network of entangled particles through which a message can be transmitted.
in essence, a quantum repeater is a small, special- purpose quantum computer.
at each stage of such a network, quantum repeaters must be able to catch and process quantum bits of quantum information to correct errors and store them long enough for the rest of the network to be ready.
until now, that has been impossible for two reasons: first, single photons are very difficult to catch.
second, quantum information is notoriously fragile, making it very challenging to process and store for long periods of time.
lukin's lab, in collaboration with marko loncar, the tiantsai lin professor of electrical engineering at the harvard john a. paulson school of engineering and applied sciences (seas),
hongkun park, mark hyman jr.
professor of chemistry at the harvard faculty of arts and sciences (fas), and dirk englund, associate professor of electrical engineering and computer science at massachusetts institute of technology (mit), has been working to harness a system that can perform both of these tasks well- silicon- vacancy color centers in diamonds.
these centers are tiny defects in a diamond's atomic structure that can absorb and radiate light, giving rise to a diamond's brilliant colors.
"over the past several years, our labs have been working to understand and control individual silicon- vacancy color centers, particularly around how to use them as quantum memory devices for single photons," said mihir bhaskar, a graduate student in the lukin group.
the researchers integrated an individual color- center into a nanofabricated diamond cavity, which confines the information- bearing photons and forces them to interact with the single color- center.
they then placed the device in a dilution refrigerator, which reaches temperatures close to absolute zero, and sent individual photons through fiber optic cables into the refrigerator, where they were efficiently caught and trapped by the color- center.
the device can store the quantum information for milliseconds- long enough for information to be transported over thousands of kilometers.
electrodes embedded around the cavity were used to deliver control signals to process and preserve the information stored in the memory.
"this device combines the three most important elements of a quantum repeater- a long memory, the ability to efficiently catch information off photons, and a way to process it locally," said bart machielse, a graduate student in the laboratory for nanoscale optics.
"each of those challenges have been addressed separately but no one device has combined all three."
"currently, we are working to extend this research by deploying our quantum memories in real, urban fiber- optic links," said ralf riedinger, a postdoctoral candidate in the lukin group.
"we plan to create large networks of entangled quantum memories and explore the first applications of the quantum internet."
"this is the first system- level demonstration, combining major advances in nanofabrication, photonics and quantum control, that shows clear quantum advantage to communicating information using quantum repeater nodes.
we look forward to starting to explore new, unique applications using these techniques," said lukin.
according to recent research, tiny clusters of atoms known as quantum dots may be excellent media for quantum teleportation, a physics phenomenon in which information - in the form of a quantum state, a very specific mathematical "signature" of an atom - can be transmitted almost instantaneously to a distant location without having to physically travel through space.
teleportation is one facet of quantum information science, a developing field that could have a major impact on computing and communications.
here, researchers focus on semiconductor quantum dots containing as few as a thousand atoms of a semiconducting element, such as silicon, and having diameters as small as one nanometer.
they are often referred to as "artificial atoms" because their behavior can be quite similar to a single atom.
for example, a semiconductor quantum dot's electrons can be confined in a way similar to how a single atom's electrons are bound.
therefore, a quantum dot can be described by a single quantum state, despite consisting of hundreds or thousands of atoms.
researchers leong chuan kwek and k.w.
choo of nanyang technological university in singapore modeled a teleportation system consisting of two quantum dots.
they first investigated how the fidelity of the teleportation process would be affected by quantum "entanglement" - when two or more quantum states have to be described with reference to each other, even when spatially separated.
the model revealed that the entanglement of the dots is proportional to the fidelity: as one increases, the other increases.
kwek and choo then developed general equations that would yield a suitable magnetic field, dot- to- dot distance, and temperature (which would all depend on the size of the dots and other variables) such that the fidelity of the system would be better than the average fidelity of non- quantum communications.
"our work provides some preliminary estimates on the way self- assembled quantum dots might be fabricated in terms of inter- dot distances for experiments at room temperature," kwek told physorg.com.
he and choo also studied how the system would handle "decoherence," the often unavoidable interaction of a quantum system with its environment, which compromises the transfer process.
decoherence is one major issue facing quantum computing.
two decoherence models show, however, that the quantum- dot system could still function under a moderate amount of decoherence.
finally, the researchers used the model to study a quantum- state transfer or swapping of states between two quantum dots, an ability necessary for quantum information processing.
they determined that a "perfect" transfer is possible using a quantum- dot teleportation system, meaning that the transferred quantum state is exactly the same as the initial quantum state of the first dot.
citation: k.w.
choo and l. c. kwek, "quantum dot as a resource for teleportation and state swapping."
phys.
rev.
b 75 205321 (2007)
cambridge quantum computing (cqc) and french quantum computing start- up pasqal announced today that they have entered into a partnership in which cqc's quantum software development platform t|ket>  will be utilised on pasqal's cutting edge quantum information processors.
t|ket>  - translates machine independent algorithms into executable circuits, optimising for physical qubit layout whilst reducing the number of required operations.
"we are excited to work with pasqal, a trailblazer in the quantum computing industry," said ilyas khan, ceo of cqc.
"with pasqal using t|ket>  on their full stack computer, our quantum computing software will be running on more platforms than ever, helping to advance quantum computation for real life applications," he added.
"through this partnership, we offer access to cqc's t|ket>  therefore allowing our customers and partners to exploit the full capabilities of our neutral atom device," said christophe jurczak, chairman of pasqal.
in april 2020, cqc released a new version of t|ket>  v0.5 with important enhancements to existing capabilities as well as a variety of new features.
these enhancements include the addition of backend support for new quantum processors such as those built by aqt and honeywell quantum solutions, and also for the microsoft q# simulator and resource estimator.
with simpler interfaces to backends and more flexible pass constructors, as well as new methods for error mitigation, t|ket>  v0.5 represents a major new tool that will allow the widest variety of users to more efficiently access the widest variety of quantum computers and quantum simulators.
pasqal is building quantum processors made of arrays of 2d and 3d arrays of neutral atoms manipulated by optical tweezers.
this technology has unique properties in terms of scalability and connectivity between qubits, which makes it a prime candidate to achieve quantum advantage for practical applications in fields such as computer- aided drug design and finance.
about cqc
cambridge quantum computing (cqc) is a world- leading quantum computing software company with over 60 scientists across offices in cambridge (uk), london, san francisco area, washington, dc and tokyo.
cqc builds tools for the commercialisation of quantum technologies that will have a profound global impact.
cqc combines expertise in quantum software, specifically a quantum development platform (t|ket> (tm)), enterprise applications in the area of quantum chemistry (eumen), quantum machine learning (qml), quantum natural language processing (qnlp) and quantum augmented cybersecurity (ironbridge(tm)).
about pasqal
pasqal is a quantum computing startup, located in france and born out of works performed at institut d'optique / cnrs.
pasqal's teams build on a decade of expertise and numerous achievements in the engineering of lasers, atom manipulation and detection systems, to manufacture quantum information processors and hybrid quantum - classical full stack computers.
pasqal is backed by quantonation, a leading early stage venture capital fund focusing on deep physics and quantum technologies.
researchers at the vienna university of technology quantum mechanically couple atoms to glass fiber cables.
now, they have shown that their technique enables storage of quantum information over a sufficiently long period of time to realize global quantum networks based on optical fibers.
will emails be quantum encrypted in the future?
will we be able to teleport quantum states over large distances via ordinary glass fiber cables?
laser- cooled atoms which are coupled to ultra- thin glass fibers are ideally suited for applications in quantum communication.
researchers at the vienna university of technology have now demonstrated experimentally that such glass fibers are capable of storing quantum information long enough so that they could be used for entangling atoms hundreds of kilometers apart.
this constitutes a fundamental building block for a global fiber- based quantum communication network.
atoms and light
"in our experiment, we connect two different quantum physical systems," explains arno rauschenbeutel (vienna center for quantum science and technology and institute of atomic and subatomic physics of the vienna university of technology).
"on the one hand, we use fiber- guided light, which is perfect for sending quantum information from a to b, and, on the other hand, we rely on atoms, which are ideal for storing this information."
by trapping atoms at a distance of about 200 nanometers from a glass fiber, which itself only has a diameter of 500 nanometers, a very strong interaction between light and atoms can be implemented.
this allows one to exchange quantum information between the two systems.
this information exchange is the basis for technologies like quantum cryptography and quantum teleportation.
currently, there are different approaches towards performing quantum mechanical operations and exchanging quantum information between light and matter- based memories.
however, for many of these systems it is challenging to store and to retrieve the information efficiently.
the method that has been developed at the vienna university of technology straightforwardly overcomes this problem: "our setup is directly connected to a standard optical glass fiber that is nowadays routinely used for the transmission of data," says rauschenbeutel.
"it will therefore be easy to integrate our quantum glass fiber cable into existing fiber communication networks."
robust quantum memory
in the past, the researchers already demonstrated that atoms can be controlled and efficiently coupled to glass fibers.
however, so far, the suitability of the fiber- coupled atoms for storing quantum information and for long- distance quantum communication remained an open question.
- after some time, the quantum information stored in the atoms is lost as it leaks into the environment - an effect called "decoherence".
"using some tricks, we were able to extend the coherence time of the atoms to several milliseconds, in spite of their small distance to the fiber surface," explains rauschenbeutel.
light in glass fibers travels about 200 kilometers in one millisecond.
as the light carries the quantum information, this defines the separation that could be bridged with such a system via the entanglement of atoms.
a realistic concept for a global quantum network
even in regular glass fiber- based telecommunication, the range of light propagation is limited: the longer the fiber, the weaker the signal.
in order to overcome this problem, repeater stations are inserted into the network.
they amplify the optical signals after a certain distance.
in this way, global communication becomes possible.
this simple concept of signal amplification cannot be implemented in quantum mechanics.
it is nevertheless still possible, albeit more involved, to build so- called "quantum repeaters".
they can be used to link several shorter sections to one long quantum connection.
arno rauschenbeutel is confident that his technique holds great promise: "by using our combined nanofiber- atom- system for setting up an optical quantum network including quantum repeaters, one might transmit quantum information and teleport quantum states around the world."
the scientific article appears in physical review letters this week.
information about the experiment can also be found on arxiv.org
the quantum computer is the holy grail of quantum technology.
its computing power would eclipse even the fastest classical computers we have today.
a team of researchers from tu wien (vienna) the national institute for informatics (tokyo) and ntt basic research labs in japan has now proposed a new architecture for quantum computing, based on microscopic defects in diamond.
a reliable quantum computer capable of solving complex problems would have to consist of billions of quantum systems, and such a device is still out of reach.
but the researchers are convinced that the basic elements of their newly proposed architecture are better suited to be miniaturized, mass- produced and integrated on a chip than previously suggested quantum computing concepts.
experiments towards the new quantum computing architecture are already being undertaken at tu wien.
fragile quantum superpositions
for decades, scientists have been trying to use quantum systems for logical calculations.
"in a classical computer, one bit can only store a number: zero or one.
quantum physics, however, allows superpositions of states.
a quantum bit can be in the state zero and the state one at the same time - and this opens up unbelievable possibilities for computing", says jorg schmiedmayer (tu wien).
such superposition states can be implemented in different kinds of quantum systems, such as ions, captured in electromagnetic traps, or in superconducting quantum bits.
the architecture which has now been published in the journal "physical review x" is different: nitrogen atoms which can occupy two different spin states are injected into a small diamond.
every nitrogen defect is trapped in an optical resonator made of two mirrors.
via glass fibres, photons are coupled to the quantum system consisting of the resonator, the diamond and the nitrogen atom.
this way, it is possible to read and manipulate the state of the quantum system without destroying the quantum properties of the spins in the diamond.
realistic quantum computers need error correction
each system - made up of mirrors, diamond and a nitrogen defect - can store one quantum bit of information: zero, one, or an arbitrary superposition of both.
but usually such a quantum bit is very unstable.
error correction procedures are needed to build a quantum computer that works reliably.
"if error correction is used, a quantum bit cannot be stored in one single quantum particle any more.
instead, a complex architecture of interconnected quantum systems is required", says michael trupke (tu wien).
the researchers calculated how the resonators, diamonds and nitrogen atoms can be assembled to create an error resistant two dimensional quantum system, a so- called "topologically protected quantum computer".
according to the calculations, about 4.5 billion such quantum systems would be sufficient to implement the algorithm "shor- 2048", which is able to calculate prime factors of a 2048- bit- number.
this huge number of quantum elements is required in any quantum computer architecture, no matter whether ion traps, superconducting quantum bits or nitrogen spins in diamonds are used.
"our approach has the big advantage that we know how to make the elements smaller.
this architecture has great potential for miniaturization and mass production", says michael trupke.
"whole industries are working with diamonds, materials science is progressing rapidly.
there are still many obstacles to overcome, but connecting nitrogen spins in solid materials opens up a path that could finally lead to a functioning quantum computer."
only the beginning - just like the transistor
trupke compares the current state of quantum computing with the early days of electronic computing: "when the first transistors were built, nobody could imagine placing them on a small chip by the billions.
today, we carry around such chips in our pockets.
these nitrogen spins in diamond could develop just like transistors did in classical computer science."
at tu wien, researchers have begun to create a small- scale realisation of this new architecture.
"we have the great advantage of being able to collaborate with a number of internationally renowned research teams in materials research and quantum technology right here at tu wien", says jorg schmiedmayer.
friedrich aumayr works on methods to inject the nitrogen atoms into the diamonds, peter mohn obtains numerical data in large- scale computer simulations.
the microcavity arrays are the result of an ongoing collaboration with ulrich schmid at the centre for micro- and nanostructures (zmns) within tu wien.
diamond chips are routinely analysed in the university's own x- ray centre.
there may still be a long way to go before algorithms like shor- 2048 run on a quantum computer.
but scientists believe that it should become possible to entangle quantum building blocks, creating larger cluster cells, within the next few years.
"once this happens, the scale- up will be fast", says kae nemoto of the national institute of informatics.
"in the end," schmiedmayer says, "it all depends on whether we manage to enter an era of mass production and miniaturization in quantum technology.
i do not see any physical laws that should keep us from doing that."
the university of california, riverside, has won a university of california multicampus- national lab collaborative research and training award of $3.75 million that will allow the campus to focus on enabling scalable quantum computing.
quantum computers are expected to greatly outperform the most powerful conventional computers on certain tasks, such as modeling complex chemical processes, finding large prime numbers, and designing new molecules that have applications in medicine.
these computers store quantum information in the form of quantum bits, or qubits - quantum systems that can exist in two different states.
for quantum computers to be truly powerful, however, they need to be "scalable," meaning they must be able to scale up to include many more qubits, making it possible to solve some challenging problems.
"the goal of this collaborative project is to establish a novel platform for quantum computing that is truly scalable up to many qubits," said boerge hemmerling, an assistant professor of physics and astronomy at uc riverside and the lead principal investigator of the three- year project.
"current quantum computing technology is far away from experimentally controlling a large number of qubits required for fault- tolerant computing.
this stands in large contrast to what has been achieved in conventional computer chips in classical computing."
hemmerling's research team will use completely new technology for the project, such as 3d- printing technology from lawrence livermore national laboratory, or llnl, to make microstructure ion traps.
ions, which are charged atomic particles, store qubits.
quantum information is transferred when the ions move in a specially designed trap.
trapped ions are deemed to have the best potential for realizing quantum computing.
uc berkeley, ucla, and uc santa barbara will also take part with ucr serving as project coordinator.
uc berkeley will demonstrate high- fidelity quantum gates with the ion traps; ucla will develop and test fiber optics integration with the traps; uc santa barbara will test the traps in cryogenic environments and demonstrate shuttling of ion strings; and facilities at lawrence berkeley national laboratory will be used to characterize and develop materials.
ucr will demonstrate simplified cooling schemes and explore the possibility of trapping electrons with these traps.
"we have a unique opportunity here to join various groups within the uc system and combine their expertise to make something bigger than a single group could achieve," hemmerling said.
the award to ucr is an outcome of the 2020 university of california laboratory fees research program competition.
six proposals, totaling more than $21 million over three years, were awarded in three targeted areas of research that leverage uc- national lab synergy: accelerator research, quantum information science, and wildfire- related research.
"we anticipate that the microstructure 3d- printed ion traps will outperform ion traps that have been used to date in terms of the storage time of the ions and ability to maintain and manipulate quantum information," hemmerling said.
"most importantly, our envisioned structures will be scalable in that we plan to build arrays of interconnected traps, similar to the very successful conventional computer chip design.
we hope to establish these novel 3d- printed traps as a standard laboratory tool for quantum computing with major improvements over currently used technology."
hemmerling said the research project should bring scientists closer to realizing a scalable quantum computer.
"many quantum computing approaches, while very promising, still have fallen short of providing a scalable platform that carries out useful calculations," he said.
"if we want to build a machine that does something useful, we need to consider new routes.
this is one possible new route."
featured news from related categories:
the core circuits of quantum teleportation, which generate and detect quantum entanglement, have been successfully integrated into a photonic chip by an international team of scientists from the universities of bristol, tokyo, southampton and ntt device technology laboratories.
these results pave the way to developing ultra- high- speed quantum computers and strengthening the security of communication.
qubits (quantum bits) are sensitive quantum versions of today's computer 0's and 1's (bits) and are the foundation of quantum computers.
photons are particles of light and they are a promising way to implement excellent qubits.
one of the most important tasks is to successfully enable quantum teleportation, which transfers qubits from one photon to another.
however, the conventional experimental implementation of quantum teleportation fills a laboratory and requires hundreds of optical instruments painstakingly aligned, a far cry from the scale and robustness of device required in a modern day computer or handheld device.
in 2013, professor furusawa and his colleagues succeeded in realising perfect quantum teleportation, however, this required a set- up covering several square metres; took many months to build, and reached the limit in terms of scalability.
new research at the university of bristol led by professor jeremy o'brien has taken those optical circuits and implemented them on to a silicon microchip measuring just a few millimetres (0.0001 square metres) using state- of- the- art nano- fabrication methods.
this is the first time quantum teleportation has been demonstrated on a silicon chip and the result has radically solved the problem of scalability.
the team of researchers have taken a significant step closer towards their ultimate goal of integrating a quantum computer into a photonic chip.
while there has been significant progress in current computing technology, its performance is now reaching the fundamental limit of classical physics.
on the other hand, it has been predicted that principles of quantum mechanics will enable the development of ultra- secure quantum communication and ultra- powerful quantum computers, overcoming the limit of current technologies.
one of the most important steps in achieving this is to establish technologies for quantum teleportation (transferring signals of quantum bits in photons from a sender to a receiver at a distance).
the implementation of teleportation on to a micro- chip is an important building block unlocking the potential for practical quantum technologies.
professor akira furusawa from the university of tokyo said: "this latest achievement enables us to perform the perfect quantum teleportation with a photonic chip.
the next step is to integrate whole the system of quantum teleportation."
professor jeremy o'brien, director of the centre for quantum photonics at the university of bristol, who led the bristol elements of the research, said: "being able to replicate an optical circuit which would normally require a room sized optical table on a photonic chip is a hugely significant achievement.
in effect, we have reduced a very complex quantum optical system by ten thousand in size."
the research is published this week in nature photonics.
when scientists develop a full quantum computer, the world of computing will undergo a revolution of sophistication, speed and energy efficiency that will make even our beefiest conventional machines seem like stone age clunkers by comparison.
but, before that happens, quantum physicists like the ones in uc santa barbara's physics professor john martinis' lab will have to create circuitry that takes advantage of the marvelous computing prowess promised by the quantum bit ("qubit"), while compensating for its high vulnerability to environmentally- induced error.
in what they are calling a major milestone, the researchers in the martinis lab have developed quantum circuitry that self- checks for errors and suppresses them, preserving the qubits' state(s) and imbuing the system with the highly sought- after reliability that will prove foundational for the building of large- scale superconducting quantum computers.
it turns out keeping qubits error- free, or stable enough to reproduce the same result time and time again, is one of the major hurdles scientists on the forefront of quantum computing face.
"one of the biggest challenges in quantum computing is that qubits are inherently faulty," said julian kelly, graduate student researcher and co- lead author of a research paper that was published in the journal nature.
"so if you store some information in them, they'll forget it."
unlike classical computing, in which the computer bits exist on one of two binary ("yes/no", or "true/false") positions, qubits can exist at any and all positions simultaneously, in various dimensions.
it is this property, called "superpositioning," that gives quantum computers their phenomenal computational power, but it is also this characteristic which makes qubits prone to "flipping," especially when in unstable environments, and thus difficult to work with.
"it's hard to process information if it disappears," said kelly.
however, that obstacle may just have been cleared by kelly, postdoctoral researcher rami barends, staff scientist austin fowler and others in the martinis group.
the error process involves creating a scheme in which several qubits work together to preserve the information, said kelly.
to do this, information is stored across several qubits.
"and the idea is that we build this system of nine qubits, which can then look for errors," he said.
qubits in the grid are responsible for safeguarding the information contained in their neighbors, he explained, in a repetitive error detection and correction system that can protect the appropriate information and store it longer than any individual qubit can.
"this is the first time a quantum device has been built that is capable of correcting its own errors," said fowler.
for the kind of complex calculations the researchers envision for an actual quantum computer, something up to a hundred million qubits would be needed, but before that a robust self- check and error prevention system is necessary.
key to this quantum error detection and correction system is a scheme developed by fowler, called the surface code.
it uses parity information- the measurement of change from the original data (if any)- as opposed to the duplication of the original information that is part of the process of error detection in classical computing.
that way, the actual original information that is being preserved in the qubits remains unobserved.
why?
because quantum physics.
"you can't measure a quantum state, and expect it to still be quantum," explained barends.
the very act of measurement locks the qubit into a single state and it then loses its superpositioning power, he said.
therefore, in something akin to a sudoku puzzle, the parity values of data qubits in a qubit array are taken by adjacent measurement qubits, which essentially assess the information in the data qubits by measuring around them.
"so you pull out just enough information to detect errors, but not enough to peek under the hood and destroy the quantum- ness," said kelly.
this development represents a meeting of the best in the science behind the physical and the theoretical in quantum computing- the latest in qubit stabilization and advances in the algorithms behind the logic of quantum computing.
"it's a major milestone," said barends.
"because it means that the ideas people have had for decades are actually doable in a real system."
the martinis group continues to refine its research to develop this important new tool.
this particular quantum error correction has been proved to protect against the "bit- flip" error, however the researchers have their eye on correcting the complimentary error called a "phase- flip," as well as running the error correction cycles for longer periods to see what behaviors might emerge.
martinis and the senior members of his research group have, since this research was performed, entered into a partnership with google.
a new progress in the scaling of semiconductor quantum dot based qubit has been achieved at key laboratory of quantum information and synergetic innovation center of quantum information & quantum physics of ustc.
professor guo guoping with his co- workers, xiao ming, li haiou and cao gang, designed and fabricated a quantum processor with six quantum dots, and experimentally demonstrated quantum control of the toffoli gate.
this is the first time for the realization of the toffoli gate in the semiconductor quantum dot system, which motivates further research on larger scale semiconductor quantum processor.
the result was published as 'controlled quantum operations of a semiconductor three- qubit system ' (physical review applied 9, 024015 (2018)).
developing the scalable semiconductor quantum chip that is compatible with modern semiconductor- techniques is an important research area.
in this area, the fabrication, manipulation and scaling of semiconductor quantum dot based qubits are the most important core technologies.
professor guo guoping's group aims to master these technologies and has been devoted to this area for a long time.
before the demonstration of the three- qubit gate, they have realized ultrafast universal control of the charge qubit based on semiconductor quantum dots in 2013(nature communications.
4:1401 (2013)), and achieved the controlled rotation of two charge qubits in 2015(nature communications.
6:7681 (2015)).
the toffoli gate is a three- qubit operation that changed the state of a target qubit conditioned on the state of two control qubits.
it can be used for universal reversible classical computation and also forms a universal set of qubit gates in quantum computation together with a hadamard gate.
furthermore, it is a key element in quantum error correction schemes.
implementation of the toffoli gate with only single- and two- qubit operations requires six controlled- not gates and ten single- qubit operations.
as a result, a single- step toffoli gate can reduce the number of quantum operations dramatically, which can break the limit of coherence time and improve the efficiency of quantum computing.
researchers from guo's group found the t- shaped six quantum dot architecture with openings between control qubits and the target qubit can strengthen the coupling between qubits with different function and minimize it between qubits with the same function, which satisfies the requirements of the toffoli gate well.
using this architecture with optimized high frequency pulses, researchers demonstrated the toffoli gate in semiconductor quantum dot system in the world for the first time, which paves the way and lays a solid foundation for the scalable semiconductor quantum processor.
the reviewer spoke highly of this work, and thought this is an important progress in the field of semiconductor quantum dot based quantum computing.
"the work is detailed and clearly demonstrates a high level of experimental technique and would be of high interest to people working in the field of electrostatically defined quantum dots for quantum computation."
in the burgeoning field of quantum metrology, quantum effects are exploited to improve the precision when measuring a variety of parameters, such as phase, frequency, and magnetic fields.
a main goal of this research is to develop high- accuracy measurement devices that could benefit many areas of science.
in a way, explains physicist gerardo adesso at the university of nottingham in the uk, this goal is similar to preparing to answer the most difficult question on an academic exam.
"alice is reviewing for an exam, helped by her friend bob," adesso told phys.org.
"how can she make sure she will be prepared to answer even the hardest question?
we solve this problem here: she needs to share quantum correlations with bob in the first place, of a general type which can manifest even in the absence of entanglement."
adesso and his coauthors have investigated the quantum version of alice's exam problem in a paper accepted for publication in physical review letters.
in the quantum scenario, the goal is to measure as precisely as possible the angle at which a quantum state has been rotated after a transformation.
a high degree of precision is equivalent to a high exam score.
as all good students know, the key to achieving a high exam score lies in preparation.
the same is true for achieving a high degree of precision when measuring a quantum state's angle of rotation.
the quantum state must be prepared in a way so that it is sensitive to rotations in all directions, even the most insensitive direction (this is the worst- case scenario, which is equivalent to the most difficult exam question).
one way to prepare a quantum state that meets these requirements is, as adesso explained above, correlating the probe quantum system with another quantum system using quantum discord.
in the exam analogy, you might think of a cheating party outside the room which the student under examination is in touch with.
"precisely, we find quantum discord to exactly quantify the minimum guaranteed precision in estimating phase shifts (angles) when the direction is not known a priori," adesso said.
"that is, it exactly measures the minimum score you would get on the test, which is the score you would get if you happen to get the question you are least prepared for.
if the state in the beginning has only classical correlations (zero discord) then you can be unlucky and get the question you are completely unprepared for, and you score zero.
if you have any amount of quantum discord, your score can never fall below that."
by demonstrating that quantum discord- type correlations, but not classical correlations, guarantee a non- zero degree of precision in measuring the angle of rotation, the results provide the first example of the usefulness of quantum discord, which is a relatively new form of quantum correlations.
the researchers established these results both theoretically and experimentally with nuclear magnetic resonance.
the results shed new light on the potential of quantum correlations for quantum technologies, particularly under adverse conditions.
"this has a lot of applications," adesso said.
"for the moment it is a proof- of- principle which shows that discord is a fundamental resource.
basically this shows that discord has a precise meaning linked to quantum coherence of a subsystem in all possible bases.
in short, if you require coherence (superposition in a certain preferred basis) for applications such as metrology, sensing, quantum cryptography, etc., then using correlated states with discord guarantees you a success in your protocol even if you do not know the preferred basis."
currently, adesso added, the uk is investing 270 million gb ... quantum technologies, including sensing and metrology implementations for commercial applications.
the new results here show that quantum discord can provide a cheaper resource than entanglement, making it a key ingredient for reliable quantum technology.
in the future, the researchers plan to further explore the properties of quantum discord, as well as potential applications.
"in particular we want to extend the setting to optical metrology with squeezed light, which has important applications for gravitational wave detectors," adesso said.
"i would like to further look into applications for medical imaging and magnetometry as well.
"more fundamentally, i am interested in understanding the physical properties of general quantum correlations and characterizing their resilience to noise.
the picture we had in this paper was for the estimation of unitary transformations.
if the transformations are noisy, discord can be degraded but in some cases it gets enhanced instead.
one can then check whether this would result in a noise- empowered precision in estimation.
perhaps this is already exploited in natural phenomena, where coherence is found to flourish in noisy environments and has a functional role for the system's optimization (e.g., in light harvesting).
after all, we clearly showed how coherence is nothing but the daughter of quantum discord."
cambridge quantum computing (cqc) announced today that they have entered into a multi- year partnership with total s.a. to develop quantum algorithms and quantum computing solutions for advanced carbon capture, utilization and storage (ccus) technologies.
the collaboration will use cqc's expertise in quantum computing and quantum chemistry, including the utilization of cqc's industry- leading quantum chemistry platform "eumen," to support and help develop total's ccus r&d efforts.
ilyas khan, ceo of cqc, said: "we are very excited to be working with total.
this is a topic of critical importance for the future of the planet.
total has a proven long- term commitment to ccus solutions, and we are confident that our work with them will lead to a meaningful contribution towards a cleaner and greener future."
the announcement caps a period of significant developments for cqc, including investments from ibm and honeywell; the launch of a quantum technologies cybersecurity platform "ironbridge" at the recent rsa conference, partnerships with organizations like cern and most recently a groundbreaking experiment that showed the world the first natural language processing implementation on a quantum computer.
about cqc
cambridge quantum computing (cqc) is a world- leading quantum computing software company with over 60 scientists across offices in cambridge (uk), london, san francisco area, washington, dc and tokyo.
cqc builds tools for the commercialization of quantum technologies that will have a profound global impact.
cqc combines expertise in quantum software, specifically a quantum development platform (t|ket> (tm)), enterprise applications in the area of quantum chemistry (eumen), quantum machine learning (qml), quantum natural language processing (qnlp) and quantum augmented cybersecurity (ironbridge(tm)).
for more information about cqc, visit www.cambridgequantum.com.
for the past 100 years, physicists have been studying the weird features of quantum physics, and now they're trying to put these features to good use.
one prominent example is that quantum superposition (also known as quantum coherence)- which is the property that allows an object to be in two states at the same time- has been identified as a useful resource for quantum communication technologies.
recently, physicists have been developing ways to measure the amount of quantum coherence in a system.
now in two new papers, a team of physicists and mathematicians (carmine napoli, et al., and marco piani, et al.)
has introduced a way to quantify the usefulness of quantum coherence by looking at this property from a purely operational perspective.
the new measurement method can answer questions such as "how useful will a system's quantum coherence be for a task like encoding and decoding secret messages?"
in other words, the new method quantifies the advantage of using quantum mechanics.
"we introduce a new way to quantify quantum coherence, the quintessential signature of quantum mechanics, capturing the extent to which a system can live in a superposition of distinct states (like a coin being simultaneously heads and tails, or a famous cat dead and alive)," the researchers wrote.
as the scientists explain, the usefulness of quantum coherence can be described by a measure that they introduce as the "robustness of quantum coherence."
basically, this measures how easy it is to destroy a state's quantum coherence.
the concept is a specific version of a more general measure the scientists introduce: the "robustness of asymmetry."
when a quantum system is asymmetrical, it's possible to distinguish between different 'rotations' of the system.
physicists can then use the system as a physical reference frame, or for quantum metrology applications, it could be used to make extremely precise measurements that would not be possible in the absence of asymmetry.
overall, the physicists view the results as a step forward in the quest to turn the weird fundamental features of quantum mechanics into something useful.
besides benefitting physics applications such as quantum metrology and secure communication, the new measure could also be used to quantify quantum coherence in biological systems, such as photosynthesis and bird navigation.
"the realization that quantum properties can be harnessed for practical applications is presently fueling a heated international race to develop and deploy quantum technologies," the physicists wrote.
"this is no coincidence: the improved study and test of fundamental quantum properties and our increased ability to exploit them go hand in hand."
more information: carmine napoli, et al.
"robustness of coherence: an operational and observable measure of quantum coherence."
physical review letters.
doi: 10.1103/physrevlett.116.150502
also at arxiv:1601.03781 [quant- ph]
marco piani, et al.
"robustness of asymmetry and coherence of quantum states."
physical review a. doi: 10.1103/physreva.93.042107
also at arxiv:1601.03782 [quant- ph]
a new test to spot where the ability to exploit the power of quantum mechanics has evolved in nature has been developed by physicists at the university of warwick.
the test identifies a tell- tale hallmark of quantum coherence, classifying the properties of particles in a quantum state that are interacting with a real- world environment.
the test should allow scientists to quantify and track quantum coherence in the natural world using laboratory experiments.
published this week in the journal physical review a, the theoretical work could lead to experiments that help solve the debate on whether biological processes exploit quantum mechanics to their advantage, and whether evolution could provide us with a template for quantum technologies such as computers, sensors and energy sources.
microscopic particles in a quantum state are very difficult to spot as the act of observing them changes their state.
these stealthy particles can exist in many locations or configurations simultaneously, a feature known as quantum coherence.
the effect underpins technologies such as quantum computers, quantum sensors and quantum communication systems, which use ordered systems isolated from the rest of the world.
however, whether quantum coherence exists in the noisier and messier real world is more difficult to identify.
the test involves a procedure to destroy quantum coherence, and then to observe the change in later measurements.
where a measurably large impact is observed, scientists can demonstrate that there must have been quantum coherence in the system.
the new work clarifies the possible exceptions to this conclusion, which depend on how quickly the special procedure can destroy the coherence.
dr. george knee, 1851 royal commission research fellow from the university's department of physics, said: "to demonstrate the presence of quantum coherence in a biological system would constitute a paradigm shift, away from the idea that only humans have the ability to engineer systems capable of exhibiting and exploiting quantum coherence.
it would also be a step toward the schroedinger's cat thought experiment, where a living organism is placed in a state where it is, quantum coherently, both dead and alive."
co- author dr. animesh datta said: "the results from this test will be valuable in improving our understanding of how chemistry and biology works, and may allow us to answer the question of whether quantum physics has played a part in evolutionary processes."
according to quantum physics, a particle, such as one carrying energy in a photosynthetic organism, can travel along multiple different pathways between an input and an output.
the energy carried by the particle could be lost at any moment after it is created.
should the particle move towards its destination faster, there is a lower chance of loss and greater efficiency could be achieved.
coherence allows for interference between the two pathways, allowing the particle to travel further on average than it could otherwise during the same time period.
this suggests that quantum effects might have lent an evolutionary advantage to those organisms adapted to exploiting them.
dr. knee added: "the possibilities are tantalising: if our proposed test were carried out in a biological system, and returned a positive result, we might be able to learn quantum engineering design principles from nature.
we could then try to create biomimetic technologies that are more robust and perhaps even more powerful than the current generation of quantum technologies, which are almost exclusively based on highly isolated systems.
if we were able to turbocharge artificial light harvesting, such as in a solar cell for example, there would be a huge potential for providing affordable, renewable energy."
research teams all over the world are exploring different ways to design a working computing chip that can integrate quantum interactions.
now, unsw engineers believe they have cracked the problem, reimagining the silicon microprocessors we know to create a complete design for a quantum computer chip that can be manufactured using mostly standard industry processes and components.
the new chip design, published in the journal nature communications, details a novel architecture that allows quantum calculations to be performed using existing semiconductor components, known as cmos (complementary metal- oxide- semiconductor) -  the basis for all modern chips.
it was devised by andrew dzurak, director of the australian national fabrication facility at the university of new south wales (unsw), and dr menno veldhorst, lead author of the paper who was a research fellow at unsw when the conceptual work was done.
"we often think of landing on the moon as humanity's greatest technological marvel," said dzurak, who is also a program leader at australia's famed centre of excellence for quantum computation and communication technology (cqc2t).
"but creating a microprocessor chip with a billion operating devices integrated together to work like a symphony -  that you can carry in your pocket!
-  is an astounding technical achievement, and one that's revolutionised modern life.
"with quantum computing, we are on the verge of another technological leap that could be as deep and transformative.
but a complete engineering design to realise this on a single chip has been elusive.
i think what we have developed at unsw now makes that possible.
and most importantly, it can be made in a modern semiconductor manufacturing plant," he added.
veldhorst, now a team leader in quantum technology at qutech -  a collaboration between delft university of technology and tno, the netherlands organisation for applied scientific research -  said the power of the new design is that, for the first time, it charts a conceivable engineering pathway toward creating millions of quantum bits, or qubits.
"remarkable as they are, today's computer chips cannot harness the quantum effects needed to solve the really important problems that quantum computers will.
to solve problems that address major global challenges -  like climate change or complex diseases like cancer -  it's generally accepted we will need millions of qubits working in tandem.
to do that, we will need to pack qubits together and integrate them, like we do with modern microprocessor chips.
that's what this new design aims to achieve.
"our design incorporates conventional silicon transistor switches to 'turn on' operations between qubits in a vast two- dimensional array, using a grid- based 'word' and 'bit' select protocol similar to that used to select bits in a conventional computer memory chip," he added.
"by selecting electrodes above a qubit, we can control a qubit's spin, which stores the quantum binary code of a 0 or 1.
and by selecting electrodes between the qubits, two- qubit logic interactions, or calculations, can be performed between qubits."
a quantum computer exponentially expands the vocabulary of binary code used in modern computers by using two spooky principles of quantum physics -  namely, 'entanglement' and 'superposition'.
qubits can store a 0, a 1, or an arbitrary combination of 0 and 1 at the same time.
and just as a quantum computer can store multiple values at once, so it can process them simultaneously, doing multiple operations at once.
this would allow a universal quantum computer to be millions of times faster than any conventional computer when solving a range of important problems.
but to solve complex problems, a useful universal quantum computer will need a large number of qubits, possibly millions, because all types of qubits we know are fragile, and even tiny errors can be quickly amplified into wrong answers.
"so we need to use error- correcting codes which employ multiple qubits to store a single piece of data," said dzurak.
"our chip blueprint incorporates a new type of error- correcting code designed specifically for spin qubits, and involves a sophisticated protocol of operations across the millions of qubits.
it's the first attempt to integrate into a single chip all of the conventional silicon circuitry needed to control and read the millions of qubits needed for quantum computing."
"we expect that there will still be modifications required to this design as we move towards manufacture, but all of the key components that are needed for quantum computing are here in one chip.
and that's what will be needed if we are to make quantum computers a workhorse for calculations that are well beyond today's computers," dzurak added.
"it shows how to integrate the millions of qubits needed to realise the true promise of quantum computing."
building such a universal quantum computer has been called the 'space race of the 21st century'.
for a range of calculations, they will be much faster than existing computers, and for some challenging problems they could find solutions in days, maybe even hours, when today's best supercomputers would take millions of years.
there are at least five major quantum computing approaches being explored worldwide: silicon spin qubits, ion traps, superconducting loops, diamond vacancies and topological qubits; unsw's design is based on silicon spin qubits.
the main problem with all of these approaches is that there is no clear pathway to scaling the number of quantum bits up to the millions needed without the computer becoming huge a system requiring bulky supporting equipment and costly infrastructure.
that's why unsw's new design is so exciting: relying on its silicon spin qubit approach -  which already mimics much of the solid- state devices in silicon that are the heart of the us$380 billion global semiconductor industry -  it shows how to dovetail spin qubit error correcting code into existing chip designs, enabling true universal quantum computation.
unlike almost every other major group elsewhere, cqc2t's quantum computing effort is obsessively focused on creating solid- state devices in silicon, from which all of the world's computer chips are made.
and they're not just creating ornate designs to show off how many qubits can be packed together, but aiming to build qubits that could one day be easily fabricated -  and scaled up.
"it's kind of swept under the carpet a bit, but for large- scale quantum computing, we are going to need millions of qubits," said dzurak.
"here, we show a way that spin qubits can be scaled up massively.
and that's the key."
the design is a leap forward in silicon spin qubits; it was only two years ago, in a paper in nature, that dzurak and veldhorst showed, for the first time, how quantum logic calculations could be done in a real silicon device, with the creation of a two- qubit logic gate -  the central building block of a quantum computer.
"those were the first baby steps, the first demonstrations of how to turn this radical quantum computing concept into a practical device using components that underpin all modern computing," said mark hoffman, unsw's dean of engineering.
"our team now has a blueprint for scaling that up dramatically.
"we've been testing elements of this design in the lab, with very positive results.
we just need to keep building on that -  which is still a hell of a challenge, but the groundwork is there, and it's very encouraging.
it will still take great engineering to bring quantum computing to commercial reality, but clearly the work we see from this extraordinary team at cqc2t puts australia in the driver's seat," he added.
other cqc2t researchers involved in the design published in the nature communications paper were henry yang and gertjan eenink, the latter of whom has since joined veldhorst at qutech.
the unsw team has struck a a$83 million deal between unsw, telstra, commonwealth bank and the australian and new south wales governments to develop, by 2022, a 10- qubit prototype silicon quantum integrated circuit -  the first step in building the world's first quantum computer in silicon.
in august, the partners launched silicon quantum computing pty ltd, australia's first quantum computing company, to advance the development and commercialisation of the team's unique technologies.
the nsw government pledged a$8.7 million, unsw a$25 million, the commonwealth bank a$14 million, telstra a$10 million and the australian government a$25 million.
cite this page:
mla
apa
chicago
the race for quantum technology is increasingly intriguing.
google and ibm are beginning to develop quantum computers that would revolutionize computing at speeds now inconceivable.
a much- awaited strength of quantum technology lies in cryptography.
with the proliferation of iot and the next 5g, it is extremely important to create secure channels.
by contrast, quantum channels that carry information have security protocols built into the encrypted data.
each channel is uniquely different from one another, allowing the risk of interception during transmission to be reduced.
the safest and most widely used methods to protect the confidentiality and integrity of data transmission are today based on symmetric cryptography, while an even more reliable security is provided with a form of mathematically unbreakable cryptography called "one- time pad," in which the data is encrypted using a random key of the same length as the encrypted data.
with quantum cryptography, all the cryptography techniques of today would be void.
today, in order to decrypt confidential, encrypted information, it is necessary to have the relative private key.
and today, running a so- called "brute force" attack - trying all the possible combinations of letters, special characters, and numbers - would make it impossible to decipher an rsa key because the computers we have available would take too long.
with quantum technology, however, the same operation would take a few seconds.
researchers at nanyang technological university in singapore (ntu singapore) have developed a quantum chip 1,000 times smaller than current quantum configurations.
led by professor liu ai qun of ntu's school of electrical and electronic engineering and associate professor kwek leong chuan, the team's results were published in nature photonics.
the results offer a new opportunity for implementing quantum cryptography methods in many financial systems.
this new chip will improve the security context in various communication methods, from the withdrawal of cash from the atm to the purchase of goods online (see figure).
these are technologies that today are not very safe and whose communications can be intercepted.
only 3 to 4 mm wide, the chip uses quantum communication algorithms and provides a higher level of security than existing standards thanks to quantum key distribution (qkd).
qkd uses quantum properties for the exchange of secret information, such as an encrypted key, which can then be used to encrypt messages transmitted over an insecure channel.
the security of qkd is based on fundamental laws of nature and physics, which are invulnerable to increasing computing power, new attack algorithms, or quantum computers.
in the case of quantum cryptography, the key (the medium that allows a message to be encrypted and thus to become readable again) is encoded in a series of photons that are passed between the two parties who must share a message (for example, through the optical fiber).
according to the heisenberg uncertainty principle (in which it is not possible to know two characteristics of a quantum object at the same time), anyone interested in obtaining the key cannot do it without disturbing these (polarized) photons.
in fact, anyone who interferes with the communication alters its characteristics, leaving a footprint that allows them to notice the intrusion to the interested parties.
professor liu said, "in today's world, cybersecurity is very important, as so much of our data is stored and communicated digitally.
almost all digital platforms and repositories require users to input their passwords and biometric data, and as long as this is the case, it could be eavesdropped on or deciphered.
quantum technology eliminates this, as both the password and information are integrated within the message being sent, forming a quantum key."
the discoveries that quantum calculus could bring in areas such as artificial intelligence, medical diagnostics, and pharmacological therapies, just to name a few, are difficult to imagine.
a quantum computer is instead a tool in which the number of operations required to achieve a particular result is exponentially lower.
innovation is not in the greater speed of every single operation but in the smaller number of operations required to reach the result.
the security of our information, however, risks being thrown into crisis by the advent of future quantum computers, equipped with large computing resources, potentially able to overcome current cryptographic techniques.
for this reason, quantum cryptography and post- quantum cryptography will become increasingly important.
this story originally appeared on ee times asia.
maurizio holds a ph.d. in physics and is a telecommunication engineer and journalist.
he has worked on various international projects in the field of gravitational wave research.
he collaborates with research institutions to design data acquisition and control systems for space applications.
he is the author of several books published by springer, as well as numerous scientific and technical publications on electronics design.
this all seems rather alarmist.
a quantum computer (which i'm still not convinced will ever exist) does not necessitate qkd.
aes- 256 is still cryptographically secure even with quantum computers.
(a quantum computer can weaken it to the level of aes- 128, but that is still secure.)
the problem with the current aes systems is that the key exchange protocol usually used is elliptic- curve diffie- hellman, which would be weak in the era of quantum computers.
but there are other algorithms, such as mceliece, which are quantum- resistant.
the keys are longer than ec diffie- hellman, but it's not a big deal.
you don't have to change keys that often.
so we can keep using all of our aes- 256 systems, but just update the key exchange algorithm (which is a low- bit- rate process usually done in software).
qkd just seems to be a lot of trouble for no apparent gain.
you still have to authenticate the channel using the current methods.
if you look at the paper referenced in this article, the new chip only provides 140bps of data.
that means that you can't use it for a one- time- pad: you still have to use it to create keys for something like aes, so i don't see how this is better than mceliece with aes.
also, how do you use repeaters when you want to communicate over long distances?
you would have to trust all the repeaters.
also, this doesn't seem like it would help with iot or 5g.
currently qkd systems are only optical, so how does this help with cellphones?
you could use qkd between cell stations, but if the base- station/cellphone link is unencrypted, it doesn't do much good.
however, you could use classical post- quantum mceliece/aes over any type of link.
this site uses akismet to reduce spam.
learn how your comment data is processed.
a research group led by scientists from the university of bristol has demonstrated the quantum operation of new components that will enable compact circuits for future photonic quantum computers.
quantum computers, holding the great promise of tremendous computational power for particular tasks, have been the goal of worldwide efforts by scientists for several years.
tremendous advances have been made but there is still a long way to go.
building a quantum computer will require a large number of interconnected components - gates - which work in a similar way to the microprocessors in current personal computers.
currently, most quantum gates are large structures and the bulky nature of these devices prevents scalability to the large and complex circuits required for practical applications.
recently, the researchers from the university of bristol's centre for quantum photonics showed, in several important breakthroughs, that quantum information can be manipulated with integrated photonic circuits.
such circuits are compact (enabling scalability) and stable (with low noise) and could lead in the near future to mass production of chips for quantum computers.
now the team, in collaboration with dr terry rudolph at imperial college, london, shows a new class of integrated divides that promise further reduction in the number of components that will be used for building future quantum circuits.
these devices, based on optical multimode interference (and therefore often called mmis) have been widely employed in classical optics as they are compact and very robust to fabrication tolerances.
"while building a complex quantum network requires a large number of basic components, mmis can often enable the implementation with much fewer resources," said alberto peruzzo, phd student working on the experiment.
until now it was not clear how these devices would work in the quantum regime.
bristol researchers have demonstrated that mmis can perform quantum interference at the high fidelity required.
scientists will now be able to implement more compact photonics circuits for quantum computing.
mmis can generate large entangled states, at the heart of the exponential speedup promised by quantum computing.
"applications will range from new circuits for quantum computation to ultra precise measurement and secure quantum communication," said professor jeremy o'brien, director of the centre for quantum photonics.
the team now plans to build new sophisticated circuits for quantum computation and quantum metrology using mmi devices.
more information: the research will be published online in the next issue of nature communications (tuesday 1 march).
the open- access paper can be downloaded from: http://dx.doi.org/10.1038/ncomms1228
xanadu, a full- stack quantum computing and advanced ai company developing quantum hardware and software solutions, has been awarded a defense advanced research projects agency (darpa) grant.
the grant will enable xanadu to undertake a comprehensive investigation of the performance of quantum machine learning (qml) algorithms on currently available quantum computing hardware.
"there is a strong crossover happening right now between quantum computing and ai," said nathan killoran, who heads up xanadu's quantum machine learning team.
"many signature ideas and concepts from ai can be ported to be quantum- aware, and run on quantum computing hardware.
but it's a bit of a wild west at the moment.
we don't really have a good sense yet which of these ported machine learning methods are best suited to quantum computing, especially with today's noisy and imperfect quantum hardware devices."
xanadu has been developing an open- source software platform for qml, known as pennylane over the past two years.
pennylane allows users to connect quantum computing hardware and software from multiple vendors- xanadu, ibm, google, rigetti, and microsoft- with popular machine learning libraries like tensorflow and pytorch.
this seamless integration will enable xanadu researchers to systematically explore the performance of a variety of near- term qml algorithms on multiple hardware devices.
xanadu will leverage the expertise of its in- house team of dedicated scientists, whose work in qml is globally recognized, to carry out the darpa- funded research project over a twelve- month period.
"xanadu's team has been at the forefront of cutting- edge qml research, implementing several world firsts in quantum software and delivering several signature papers in the field over the past two years," said christian weedbrook, the company's founder and ceo.
"this award recognizes that strength and enables us to continue pushing the area of qml forward, towards eventual applications in a wide range of industries."
about xanadu
xanadu is a full- stack photonic quantum hardware company with the mission to build quantum computers that are useful and available to people everywhere.
customers will soon be able to access our hardware through the cloud to begin building applications and algorithms using xanadu's open- source software.
a new finding by researchers at the university of chicago promises to improve the speed and reliability of current and next generation quantum computers by as much as ten times.
by combining principles from physics and computer science, the researchers developed a new scalable compiler that makes software aware of the underlying quantum hardware, offering significant performance benefits as scientists race to build the first practical quantum computers.
the uchicago research group comprises computer scientists and physicists from the epiqc (enabling practical- scal ... ation) collaboration, an nsf expedition in computing that kicked off in 2018.
epiqc aims to bridge the gap from existing theoretical algorithms to practical quantum computing architectures on near- term devices.
merging approaches from computer science and physics
the core technique behind the epiqc team's paper adapts quantum optimal control, an approach developed by physicists long before quantum computing was possible.
quantum optimal control fine- tunes the control knobs of quantum systems in order to continuously drive particles to desired quantum states- or in a computing context, implement a desired program.
if successfully adapted, quantum optimal control would allow quantum computers to execute programs at the highest possible efficiency...but that comes with a performance tradeoff.
"physicists have actually been using quantum optimal control to manipulate small systems for many years, but the issue is that their approach doesn't scale," said researcher yunong shi.
even with cutting- edge hardware, it takes several hours to run quantum optimal control targeted to a machine with just 10 quantum bits (qubits).
moreover, this running time scales exponentially, which makes quantum optimal control untenable for the 20- 100 qubit machines expected in the coming year.
meanwhile, computer scientists have developed their own methods for compiling quantum programs down to the control knobs of quantum hardware.
the computer science approach has the advantage of scalability- compilers can easily compile programs for machines with thousands of qubits.
however, these compilers are largely unaware of the underlying quantum hardware.
often, there is a severe mismatch between the quantum operations that the software deals with versus the ones that the hardware executes.
as a result, the compiled programs are inefficient.
the epiqc team's work merges the computer science and physics approaches by intelligently splitting large quantum programs into subprograms.
each subprogram is small enough that it can be handled by the physics approach of quantum optimal control, without running into performance issues.
this approach realizes both the program- level scalability of traditional compilers from the computer science world and the subprogram- level efficiency gains of quantum optimal control.
the intelligent generation of subprograms is driven by an algorithm for exploiting commutativity- a phenomenon in which quantum operations can be rearranged in any order.
across a wide range of quantum algorithms, relevant both in the near- term and long- term, the epiqc team's compiler achieves two to ten times execution speedups over the baseline.
but due to the fragility of qubits, the speedups in quantum program execution translate to exponentially higher success rates for the ultimate computation.
as shi emphasizes, "on quantum computers, speeding up your execution time is do- or- die."
breaking abstraction barriers
this new compiler technique is a significant departure from previous work.
"past compilers for quantum programs have been modeled after compilers for modern conventional computers," said fred chong, seymour goodman professor of computer science at uchicago and lead pi for epiqc.
but unlike conventional computers, quantum computers are notoriously fragile and noisy, so techniques optimized for conventional computers don't port well to quantum computers.
"our new compiler is unlike the previous set of classically- inspired compilers because it breaks the abstraction barrier between quantum algorithms and quantum hardware, which leads to greater efficiency at the cost of having a more complex compiler."
while the team's research revolves around making the compiler software aware of the underlying hardware, it is agnostic to the specific type of underlying hardware.
this is important since there are several different types of quantum computers currently under development, such as ones with superconducting qubits and trapped ion qubits.
the team expects to see experimental realizations of their approach within the coming months, particularly now that an open industry standard, openpulse, has been defined.
this standard will enable operation of quantum computers at the lowest possible level, as needed for quantum optimal control techniques.
ibm's quantum roadmap highlights openpulse support as a key objective for 2019, and other companies are expected to announce similar plans as well.
the team's full paper, "optimized compilation of aggregated instructions for realistic quantum computers" is now published on arxiv and will be presented at the asplos computer architecture conference in rhode island on april 17.
in addition to shi and chong, co- authors include nelson leung, pranav gokhale, zane rossi, david i. schuster, and henry hoffman, all at the university of chicago.
more information: yunong shi et al.
optimized compilation of aggregated instructions for realistic quantum computers, proceedings of the twenty- fourth international conference on architectural support for programming languages and operating systems - asplos '19 (2019).
doi: 10.1145/3297858.3304018
tubingen university physicists are the first to link atoms and superconductors in key step towards new hardware for quantum computers and networks.
today's quantum technologies are set to revolutionize information processing, communications, and sensor technology in the coming decades.
the basic building blocks of future quantum processors are, for example, atoms, superconducting quantum electronic circuits, spin crystals in diamonds, and photons.
in recent years it has become clear that none of these quantum building blocks is able to meet all the requirements such as receiving and storing quantum signals, processing and transmitting them.
a research group headed by professors jozsef fortagh, reinhold kleiner and dieter kolle of the university of tubingen institute of physics has succeeded in linking magnetically- stored atoms on a chip with a superconducting microwave resonator.
the linking of these two building blocks is a significant step towards the construction of a hybrid quantum system of atoms and superconductors which will enable the further development of quantum processors and quantum networks.
the study has been published in the latest nature communications.
quantum states allow especially efficient algorithms which far outstrip the conventional options to date.
quantum communications protocols enable, in principle, unhackable data exchange.
quantum sensors yield the most precise physical measurement data.
"to apply these new technologies in everyday life, we have to develop fundamentally new hardware components," fortagh says.
instead of the conventional signals used in today's technology -  bits -  which can only be a one or a zero, the new hardware will have to process far more complex quantum entangled states.
"we can only achieve full functionality via the combination of different quantum building blocks," fortagh explains.
in this way, fast calculations can be made using superconducting circuits; however storage is only possible on very short time scales.
neutral atoms hovering over a chip's surface, due to their low strength for interactions with their environment, are ideal for quantum storage, and as emitters of photons for signal transmission.
for this reason, the researchers connected two components to make a hybrid in their latest study.
the hybrid quantum system combines nature's smallest quantum electronic building blocks -  atoms -  with artificial circuits -  the superconducting microwave resonators.
"we use the functionality and advantages of both components," says the study's lead author, dr. helge hattermann, "the combination of the two unequal quantum systems could enable us to create a real quantum processor with superconducting quantum lattices, atomic quantum storage, and photonic qubits."
qubits are -  analogous to bits in conventional computing -  the smallest unit of quantum signals.
the new hybrid system for future quantum processors and their networks forms a parallel with today's technology, which is also a hybrid, as a look at your computer hardware shows: calculations are made by microelectronic circuits; information is stored on magnetic media, and data is carried through fiber- optic cables via the internet.
"future quantum computers and their networks will operate on this analogy -  requiring a hybrid approach and interdisciplinary research and development for full functionality," fortagh says.
cambridge quantum computing ("cqc") today announced an important breakthrough in quantum chemistry that will enhance and accelerate the commercialisation of quantum computing in an essential area of human endeavour - the search for new materials in sectors such as energy and pharmaceuticals.
accurately simulating how atoms and molecules behave when absorbing energy is essential in developing advanced materials, such as efficient solar panels.
quantum computers provide a route to highly accurate simulations of such processes that are beyond the reach of today's classical computers.
whilst quantum algorithms, such as the well- known variational quantum eigensolver ("vqe"), are particularly adept at running on current quantum devices, vqe has, until now, been limited to simulating electrons in their lowest energy state, which is not useful for example, for modelling sunlight hitting a solar panel to excite an electron and generate electricity.
to simulate such so- called "excited" states, one had to run a vqe calculation for the lowest energy state followed by other algorithms designed for excited states, which consumes valuable computational resources.
cqc's cambridge based team led by scientists david munoz ramo and gabriel greene- diniz have released a scientific preprint paper which details a ground- breaking achievement that breaks a logjam in exactly the problems noted above.
in a recent article "calculation of excited states via symmetry constraints in the variational quantum eigensolver," cqc has, for the first time, exhibited how it is possible to adapt the vqe algorithm to directly calculate excited states in particular molecules, bypassing the need to first calculate the lowest energy state.
this improves the efficiency of excited state calculations for many molecules of industrial interest and is an important and critical first step in developing next generation materials.
the breakthrough will be applied by cqc with immediate effect through their unique enterprise software platform for quantum chemistry calculations "eumen."
read the complete scientific paper here: https://arxiv.org/abs/1910.05168.
about cambridge quantum computing
cambridge quantum computing (cqc) is a world- leading quantum computing software company with over 60 scientists including 35 phd's across offices in cambridge (uk), san francisco, london and tokyo.
cqc builds tools for the commercialisation of quantum technologies that will have a profound global impact.
cqc combines expertise in quantum software, specifically a quantum development platform (t|ket> tm), enterprise applications in the area of quantum chemistry (eumen), quantum machine learning (qml), quantum natural language processing (qnlp) and quantum augmented cybersecurity (ironbridge).
scientists at the department of energy's oak ridge national laboratory are conducting fundamental physics research that will lead to more control over mercurial quantum systems and materials.
their studies will enable advancements in quantum computing, sensing, simulation, and materials development.
the researchers' experimental results were recently published in physical review b rapid communication and optics letters.
quantum information is considered fragile because it can be lost when the system in which it is encoded interacts with its environment, a process called dissipation.
scientists with ornl's computing and computational sciences and physical sciences directorates and vanderbilt university have collaborated to develop methods that will help them control -  or drive -  the "leaky," dissipative behavior inherent in quantum systems.
"our goal is to develop experimental platforms that allow us to probe and control quantum coherent dynamics in materials," said benjamin lawrie, a research scientist in the quantum sensing team in ornl's quantum information science group.
"to do that, you often have to be able to understand what's going on at the nanoscale."
bringing perspectives from quantum information science, nanoscience and electron microscopy, the scientists exploit existing knowledge of matter and the physics of light and sound to examine the quantum nature of nanostructures -  structures that measure about one- billionth of a meter.
one project focused on driving nitrogen vacancy center defects in nanodiamonds with plasmons.
the naturally occurring defects are created when a nitrogen atom forms in place of the typical carbon atom, adjacent to an atomless vacancy.
the defects are being investigated for use in tests of entanglement, a state that will allow substantially more information to be encoded in a quantum system than can be accomplished with classical computing.
electrons generate an electric field.
when an electron beam is applied to a material, the material's electrons are spurred to motion -  a state called excitation -  creating a magnetic field that can then be detected as light.
working with plasmons, electron excitations that couple easily with light, allows scientists to examine electromagnetic fields at the nanoscale.
matthew feldman, a vanderbilt university graduate student conducting doctoral research at ornl through the national defense science and engineering graduate fellowship program and a member of the quantum sensing team, used a high- energy electron beam to excite nitrogen vacancy centers in diamond nanoparticles, causing them to emit light.
he then used a cathodoluminescence microscope owned by ornl's materials science and technology division, which measures the visible- spectrum luminescence in irradiated materials, to collect the emitted photons and characterize high- speed interactions among nitrogen vacancy centers, plasmons and vibrations within the nanodiamond.
in other research, jordan hachtel, a postdoctoral fellow with ornl's center for nanophase materials sciences, used the cathodoluminescence microscope to excite plasmons in gold nanospirals.
he explored how the geometry of the spirals could be harnessed to focus energy in nanoscale systems.
andy lupini served the project as a microscopy consultant, providing expertise regarding equipment optimization and troubleshooting.
precise control over nanoscale energy transfer is required to enable long- lived entanglement in a model explored by eugene dumitrescu, a research scientist in ornl's quantum information science group.
dumitrescu's research, published in physical review a in late 2017, showed that the photon statistics feldman collected could be used in calculations to show entanglement.
"this work advances our knowledge of how to control light- matter interactions, providing experimental proof of a phenomenon that had previously been described by simulations," lawrie said.
closed systems, in which quantum information can be kept away from its surroundings, theoretically can prevent dissipation, but real- world quantum systems are open to numerous influences that result in information leakage.
"the elephant in the room in discussions of quantum systems is decoherence," feldman said.
"if we can model an environment to influence how a quantum system works, we can enable entanglement."
dumitrescu agreed.
"we know quantum systems will be leaky.
one remedy is to drive them," he said.
"the driving mechanisms we're exploring cancel out the effects of dissipation."
dumitrescu used the analogy of a musical instrument to explain the researchers' attempts to control quantum systems.
"if you pluck a violin string, you get the sound, but it begins to dissipate through the environment, the air," he said.
"but if you slowly draw the bow across the string, you get a more stable, longer- lasting sound.
you've brought control to the system."
feldman thinks these are fascinating times for quantum physicists because the field of quantum computing is at the same phase classical computing was in the mid- 20th century.
"what excites me most is how current research could change our understanding of quantum systems and materials," he said.
journal reference:
matthew a. feldman, eugene f. dumitrescu, denzel bridges, matthew f. chisholm, roderick b. davidson, philip g. evans, jordan a. hachtel, anming hu, raphael c. pooser, richard f. haglund, benjamin j. lawrie.
colossal photon bunching in quasiparticle- mediated nanodiamond cathodoluminescence.
physical review b, 2018; 97 (8) doi: 10.1103/physrevb.97.081404
pnnl quantum algorithm theorist and developer nathan wiebe is applying ideas from data science and gaming hacks to quantum computing.
everyone working on quantum computers knows the devices are error prone.
the basic unit of quantum programming - the quantum gate - fails about once every hundred operations.
and that error rate is too high.
the quantum gate hack.
image credit: timothy holland, pacific northwest national laboratory
while hardware developers and programming analysts are fretting over failure rates, pnnl's nathan wiebe is forging ahead writing code that he is confident will run on quantum computers when they are ready.
in his joint appointment role as a professor of physics at the university of washington, wiebe is training the next generation of quantum computing theorists and programmers.
on one hand, wiebe laments that "there's such a huge gulf between where we are right now versus where we need to be."
but just as quickly, he brushes aside doubt and explains that "we are already at the point where we are doing things that are really interesting."
it's this forge- ahead mentality that has placed him as a global leader in quantum algorithm development with a dozen different international partnerships and 91 publications on quantum algorithms published in the last five years alone.
gaming rules apply to quantum gates
coding for quantum computers requires leaps of imagination that can be daunting on one level, but wiebe points out that any 15- year- old minecraft enthusiast would have no trouble understanding the basics of how it works.
the wildly popular building block video game has spawned a community of enthusiastic coders who create virtual computers inside the game environment.
minecraft coders have simulated real- world physics and created virtual calculators, among other feats.
the minecraft universe has its own internal rules and some of them don't quite make sense - much like some of the rules of the quantum universe don't seem clear, even to physicists.
despite not understanding why the rules in minecraft work the way they do, players instead learn how the physics of minecraft work and further how to exploit that knowledge to perform tasks the games creators may not have intended.
quantum computer programmers have a similar challenge.
they are faced with the strange rules of quantum mechanics and try to find creative ways to "hack" them to build computers that, in some cases, can solve problems trillions of times faster than ordinary computers by using quantum effects like interference and entanglement that ordinary computers lack.
"on a quantum computer, when you try to measure the quantum bits, they revert to ordinary bits.
in the process, they lose the very features that give quantum computing its power," wiebe said.
"with a quantum computer you have to be more subtle than you do with ordinary computers.
you have to coax out information about the system without damaging the information that was encoded in there."
"we found these weird rules of quantum mechanics," he said.
"but only now are we asking how we can exploit these rules in order to allow us to compute."
it's like steam engines
wiebe likes to use the analogy of james watt, inventor of the first modern steam engine.
in the late 1700s, the limits to power that could be extracted from a steam engine weren't understood.
only later did the french physicist sadi carnot discover that there were immutable physical laws that limited heat engine efficiency.
this observation became known as the second law of thermodynamics and is now seen as a cornerstone of science.
just as the study of the efficiency of heat engines revealed the second law of thermodynamics, the study of quantum computing has the potential to reveal a deeper understanding of the limits that physics places on our ability to compute, as well as the new opportunities it provides to collaborate among fields.
quantum computing is not simply physics, wiebe said.
it exists in the intersection between many fields, including physics, computer science, mathematics, materials science, and increasingly, data science.
indeed, he sees a huge untapped role for data science and machine learning in quantum computing.
"like watt and carnot, we don't necessarily need to capture all of the minutia that is happening inside the system," wiebe said.
"all we have to be able to do is predict input and output.
so data science and machine learning tools could have a lot of influence in making quantum computers work in practical terms."
diamonds in the rough
one of the first useful quantum technologies is likely to be quantum sensors - devices that use quantum signals to measure things like temperature and magnetic fields.
wiebe worked with an international team of colleagues to apply machine learning techniques to a tricky problem in quantum sensing.
biologists want to use these sensors to measure what's going on inside individual cells.
the sensors are made of diamonds with certain defects that can be used to send quantum signals.
the problem is that, at room temperature, the quantum sensor signals contain too many errors to be practical.
the research team could not get the experiments to work unless the whole thing was cooled to liquid helium temperatures (-452.2degf), which obviously isn't good for living cells.
wiebe and his colleagues solved the problem by running the experiments at room temperature and then applying an algorithm that used techniques from data analytics and machine learning to correct for the error- prone, noisy signal.
"we got same sensitivity as the very cold cryogenic experiment at no additional cost," he said.
wiebe said that applying the same principles may be just the thing needed to correct for noisy, error- prone quantum gates.
the question he asks is: "how much quantum error correction do i need to guarantee that my algorithms are going to run?"
wiebe is adamant that making quantum computing practical will require the combined interdisciplinary efforts of researchers in many fields learning to speak each other's languages.
"if we can build a quantum computer, then we have the ability to solve currently intractable problems in chemistry and materials science and physics," he said.
"the challenge both imposes limitations and provides new opportunities.
quantum computing forces us to get a deeper understanding of what it means to compute."
cambridge quantum computing ("cqc") announced today that it has joined cern openlab in a collaboration - named the quaternion project - to explore the application of quantum technologies to particle physics.
cqc is a global leader in the quantum industry with a deep commitment to the cultivation of world class scientific research.
"we are excited to collaborate with cern, the european laboratory for particle physics, on this innovative quantum computing based research project," said ilyas khan, founder and ceo of cqc.
"cqc is focussed on using the world's best science to develop technologies for the coming quantum age.
joining cern openlab is a special development for any organization and we look forward to developing advances together," he added.
"our unique public- private partnership works to accelerate the development of cutting- edge computing technologies for our research community," said alberto di meglio, the head of cern openlab.
"quantum computing research is one of the most exciting areas of study today; we are pleased to welcome cqc and their world- class scientists into collaboration with us."
researchers at cern are exploring the potential offered by quantum computers.
their enhanced computational capabilities could help to improve the analysis and classification of their vast data sets, thus helping to push back the boundaries of particle physics.
working in collaboration with major hardware vendors and users of quantum computing, cern openlab has launched a number of projects in this domain.
to this end, the cern openlab team will leverage the power of t|ket> (tm), cqc's proprietary quantum development platform.
cqc's t|ket> (tm) converts machine- independent quantum circuits into executable circuits, crucially reducing the number of required operations whilst optimising physical qubit arrangements.
the architecture- agnostic nature of t|ket> (tm) will help the members of the cern openlab project team to work across multiple platforms to achieve optimal results even on today's noisy quantum hardware.
the quaternion project will also investigate the application of cqc's four qubit quantum technology device named ironbridge(tm)* to cern's monte carlo methods for data analysis.
such methods are not only a vital component of particle physics research, but are also applicable to many other areas, such as financial and climate modelling.
monte carlo methods use high- quality entropy sources to simulate and analyse complex data.
using cqc's ironbridge(tm) platform, the world's first commercially available device- independent and quantum- certifiable cryptographic device, the teams will investigate for the first time the effects of certified entropy on monte carlo simulations.
about cambridge quantum computing
cambridge quantum computing (cqc) is a world- leading quantum computing software company with over 62 scientists including 37 phd's across offices in cambridge (uk), san francisco, london and tokyo.
cqc builds tools for the commercialisation of quantum technologies that will have a profound global impact.
cqc combines expertise in quantum software, specifically a quantum development platform (t|ket> (tm)), enterprise applications in the areas of quantum chemistry (eumen), quantum machine learning (qml), and quantum augmented cybersecurity (ironbridge(tm)).
quantum computers are being developed by teams working not only at universities but also at google, ibm, microsoft and d- wave, a start- up company.
and things are evolving quickly, says nicolas sangouard, snsf professor at the university of basel.
"in a few years at most, i expect the computing power of quantum computers to significantly outstrip the computing power of ordinary computers.
we call that 'quantum supremacy'".
sangouard and his co- workers recently showed how to check that these computers are fit for purpose.
for they are not just powerful but also very delicate: some operate at temperature extremes as low as 270 degrees below zero.
the researchers' approach enables them to certify all the components of a quantum computer - from short- and long- term memory, to information processors, to the converters required to connect the computer to a secure quantum communications network.
the protocol offers an additional advantage: it only uses the components already in the computer, thus obviating the need for additional devices.
in principle, the protocol will work with any type of quantum computer, whatever the technology behind it.
a machine that tests itself
"the power of quantum computers is what makes them difficult to certify," says sangouard.
"even the fastest ordinary computers are too slow to check the calculations made by such devices."
moreover, quantum computers will eventually be able to communicate with each other securely through a dedicated quantum communications network.
so it's important to make sure that they aren't a weak link, says sangouard.
that's why the research team has developed a completely quantum certification method that uses the computer's own building blocks.
"we were inspired by bell tests, which were devised by a physicist working at cern in the 1960s," says sangouard.
"normally, these tests are used to check whether particles are behaving according to quantum rules.
we modified the tests to enable them to check the operation of the various components of a quantum computer.
because such a device is basically capable of doing the tests, our procedure is very simple to set up and doesn't require any special skills."
"what prompted the project was a seminar talk by a scientist invited to the university of basel," says sangouard.
"the talk dealt with a complicated aspect of quantum physics, but we were motivated to translate it into a useful method for quantum computers.
for me, that's a perfect example of how a conference is not just a means of learning in a passive way but also offers significant opportunities to innovate."
at the 2019 consumer electronics show (ces), ibm today unveiled ibm q system one(tm), the world's first integrated universal approximate quantum computing system designed for scientific and commercial use.
ibm also announced plans to open its first ibm q quantum computation center for commercial clients in poughkeepsie, new york in 2019.
ibm q system one enables universal approximate superconducting quantum computers to operate outside the research lab for the first time.
it's a major step forward in the commercialization of quantum computing, which could one day enable breakthroughs in such areas as materials and drug discovery, financial services, and artificial intelligence.
ibm q system one enables universal approximate superconducting quantum computers to operate outside the research lab for the first time.
it's a major step forward in the commercialization of quantum computing, which could one day enable breakthroughs in such areas as materials and drug discovery, financial services, and artificial intelligence.
ibm q systems are designed to one day tackle problems that are currently seen as too complex and exponential in nature for classical systems to handle.
future applications of quantum computing may include finding new ways to model financial data and isolating key global risk factors to make better investments, or finding the optimal path across global systems for ultra- efficient logistics and optimizing fleet operations for deliveries.
designed by ibm scientists, systems engineers and industrial designers, ibm q system one has a sophisticated, modular and compact design optimized for stability, reliability and continuous commercial use.
for the first time ever, ibm q system one enables universal approximate superconducting quantum computers to operate beyond the confines of the research lab.
much as classical computers combine multiple components into an integrated architecture optimized to work together, ibm is applying the same approach to quantum computing with the first integrated universal quantum computing system.
ibm q system one is comprised of a number of custom components that work together to serve as the most advanced cloud- based quantum computing program available, including:
quantum hardware designed to be stable and auto- calibrated to give repeatable and predictable high- quality qubits;
cryogenic engineering that delivers a continuous cold and isolated quantum environment;
high precision electronics in compact form factors to tightly control large numbers of qubits;
quantum firmware to manage the system health and enable system upgrades without downtime for users; and
classical computation to provide secure cloud access and hybrid execution of quantum algorithms.
designing a first: ibm q system one
ibm assembled a world- class team of industrial designers, architects, and manufacturers to work alongside ibm research scientists and systems engineers to design ibm q system one, including uk industrial and interior design studios map project office and universal design studio, and goppion, a milan- based manufacturer of high- end museum display cases that protect some of the world's most precious art including the mona lisa at the louvre, and the crown jewels at the tower of london.
together these collaborators designed the first quantum system to consolidate thousands of components into a glass- enclosed, air- tight environment built specifically for business use, a milestone in the evolution of commercial quantum computers.
this integrated system aims to address one of the most challenging aspects of quantum computing: continuously maintaining the quality of qubits used to perform quantum computations.
powerful yet delicate, qubits quickly lose their special quantum properties, typically within 100 microseconds (for state- of- the- art superconducting qubits), due in part to the interconnected machinery's ambient noise of vibrations, temperature fluctuations, and electromagnetic waves.
protection from this interference is one of many reasons why quantum computers and their components require careful engineering and isolation.
the design of ibm q system one includes a nine- foot- tall, nine- foot- wide case of half- inch thick borosilicate glass forming a sealed, airtight enclosure that opens effortlessly using "roto- translation," a motor- driven rotation around two displaced axes engineered to simplify the system's maintenance and upgrade process while minimizing downtime - another innovative trait that makes the ibm q system one suited to reliable commercial use.
a series of independent aluminum and steel frames unify, but also decouple the system's cryostat, control electronics, and exterior casing, helping to avoid potential vibration interference that leads to "phase jitter" and qubit decoherence.
a replica of ibm q system one will be on display at ces.
the ibm q quantum computation center
the ibm q quantum computation center opening later this year in poughkeepsie, new york, will expand the ibm q network commercial quantum computing program, which already includes systems at the thomas j. watson research center in yorktown, new york.
this new center will house some of the world's most advanced cloud- based quantum computing systems, which will be accessible to members of the ibm q network, a worldwide community of leading fortune 500 companies, startups, academic institutions, and national research labs working with ibm to advance quantum computing and explore practical applications for business and science.
ibm poughkeepsie's unique history in computing stretches back to the development of ibm's first line of production business computers in the 1950s, the ibm 700 series, and the ibm system/360 in the 1960s, which revolutionized the world by changing the way businesses thought about computer hardware.
now home to one of the world's most- powerful classical system, the ibm mainframe, ibm poughkeepsie is positioned to be one of the few places in the world with the technical capabilities, infrastructure and expertise to run a quantum computation center, including access to high performance computing systems and a high availability data center needed to work alongside quantum computers.
"the ibm q system one is a major step forward in the commercialization of quantum computing," said arvind krishna, senior vice president of hybrid cloud and director of ibm research.
"this new system is critical in expanding quantum computing beyond the walls of the research lab as we work to develop practical quantum applications for business and science."
this new system marks the next evolution of ibm q, the industry's first effort to introduce the public to programmable universal quantum computing through the cloud- based ibm q experience, and the commercial ibm q network platform for business and science applications.
the free and publicly available ibm q experience has been continuously operating since may of 2016 and now boasts more than 100,000 users, who have run more than 6.7 million experiments and published more than 130 third- party research papers.
developers have also downloaded qiskit, a full- stack, open- source quantum software development kit, more than 140,000 times to create and run quantum computing programs.
the ibm q network includes the recent additions of argonne national laboratory, cern, exxonmobil, fermilab, and lawrence berkeley national laboratory.
physicists who work on quantum technologies are always looking for ways to manage decoherence, which occurs when a quantum system unavoidably interacts with the surrounding environment.
in the past few years, scientists have discovered that some quantum correlations can be "frozen" in a constant state and remain that way in the presence of noise, potentially offering a protective mechanism against decoherence.
so far, however, quantum freezing has been shown to exist only on a case- by- case basis and under certain conditions, and its potential protective effect has not been fully exploited.
now in a new paper to be published in nature scientific reports, physicists marco cianciaruso, thomas bromley, wojciech roga, rosario lo franco, and gerardo adesso have shown that the freezing of quantum correlations is universal- that is, it is independent of the method used for measuring the correlations.
while working on this proof, the scientists also found that it may be possible to not just prevent but also reverse the effects of decoherence under certain circumstances, which they show by demonstrating the existence of an intriguing new phenomenon modelled by a global rephasing channel.
"the greatest significance of the work is to show that a general form of quantum correlations must remain frozen and protected in the presence of some, usually detrimental, decoherence effects," lo franco, who is with the university of nottingham, the university of palermo, and the university of sao paulo, told phys.org.
"this fact implies that any further well- defined measure of quantum correlations must freeze during the evolution under the same conditions.
our work thus appears to settle the debate about the universality of the freezing phenomenon of discord- based quantum correlations."
overall, much of the research on quantum correlations has focused on entanglement, which is one specific type of quantum correlation.
as the physicists explain in their paper, there is a general consensus that all quantum correlations between two systems in a pure state can be considered entanglement.
although it sounds like entanglement is the dominant type of quantum correlation, in reality entanglement may represent a negligibly small portion of all quantum correlations because not all states are pure.
when two systems are exposed to a noisy environment, they will generally be in a mixed state, and a certain portion of mixed states are non- entangled.
consequently, quantum correlations between these systems do not necessarily reduce to entanglement, but can manifest in a more general form called discord- type quantum correlations.
since realistic quantum systems often involve mixed states, discord- type correlations are much more prevalent than entanglement, although they are not as well understood.
in this context, the freezing phenomenon is particularly interesting because it is exhibited specifically by discord- type correlations, while a similar feature for entanglement has remained elusive.
this difference aligns with other emerging research that suggests that discord- type quantum correlations are more robust than entanglement against noise.
for instance, entanglement can suffer so- called "sudden death" when exposed to noise, but such rapid vanishing cannot been observed for discord- type correlations.
in this sense, freezing can be considered a very extreme form of robustness to noise, as it could theoretically allow any quantum protocol to operate with a performance that is completely unaffected by noise.
such protection would be extraordinarily appealing for future developments of quantum technologies, including communication, computation, sensing, and metrology.
the scientists' proof of the universality of quantum freezing consists of a lengthy mathematical description, but they explain that the main requirement is that the method used to measure the discord- type correlations must be a genuine distance- based geometric method, "exploiting distances which are convex and non- increasing under quantum channels."
as long as the method meets these basic requirements, freezing occurs independent of the specific distance used.
by proving universality, the results show that previous demonstrations of freezing were not merely mathematical accidents, but that the freezing of discord- type quantum correlations is a true physical phenomenon.
the physicists' additional finding- that a global quantum control channel can reverse the effects of decoherence for certain mixed states- could also have implications for quantum technologies, in particular quantum error correction.
they explain that the channel can be understood as "refocusing" the qubits that have been affected by noise.
the proof still leaves room for further research aimed at proving the universality of freezing from only the basic properties of quantum correlations, without the need to use a geometric method.
the results of such a study could lead to a better understanding of the physical origins of frozen quantum correlations, and may even shed light on the scientists' hunch that the phenomenon is related to classical correlations.
"we are currently researching conditions for other quantum features to remain frozen or almost frozen under various noisy dynamics," said adesso, at the university of nottingham.
"in another paper, some of us have already proven that quantum coherence can remain frozen in the same conditions as discord- type correlations, which is quite a strong result since long- lived coherence is a useful resource for physical and biological sciences.
we are also working on experimental investigations with quantum optics and nuclear magnetic resonance, with the aim to exploit mixed quantum states with frozen discord in order to preserve their operational value in applications such as quantum metrology.
the physical cause of freezing is not known yet, but our results suggest that a deep physical origin of this phenomenon is expected so that we are investigating it."
quantum xchange, a leading provider of secure communications for a quantum- safe world, has been named to fast company's prestigious annual list of the world's most innovative companies for 2020, ranking #4 in the security category.
the list honors the companies making a profound impact on both industry and culture.
this year's mic list features 434 businesses from 39 countries.
"the world's largest tech companies as well as governments around the world are investing heavily in the race to develop a commercially viable quantum computer - but these advancements bring new cybersecurity threats," said john prisco, president & ceo at quantum xchange.
"quantum computers are powerful enough to crack all of our current encryption methods, which puts our most sensitive data, intellectual property, and even our national security at risk.
that's why organizations must prepare now in order to protect their communications networks and data in the quantum era.
we are proud to have fast company recognize our efforts to bring crypto- agility and an affordable onramp to ultra- secure quantum key distribution (qkd) to the enterprise."
quantum xchange believes that quantum readiness, or crypto agility, is critical to protecting and securing data now and fending off future threats.
the company pioneered qkd technology in the u.s. with the launch of phio in june 2018, the first quantum fiber network in the u.s. and the only solution in the world to overcome previous distance and delivery limitations of qkd.
quantum xchange's recent innovation, phio trusted xchange (tx), launched in september 2019, is a quantum security gamechanger.
the complete, quantum key management system extends the life of existing encryption investments by making traditional keys quantum- safe now; has made post- quantum cryptographic (pqc) algorithms a standard feature in the appliance; and allows organizations to easily layer in qkd- level security as, and when, needed.
the flexibility of the phio family of products enables organizations at all stages of quantum readiness to protect highly sensitive data without needing to rip and replace their current security infrastructure.
this makes quantum xchange a sound investment, and its phio tx, a must- have technology for any organization seeking to easily upgrade defenses as the threat landscape evolves.
the company currently serves customers in the finance, energy and government industries.
fast company's editors and writers sought out the most groundbreaking businesses on the planet and across myriad industries.
they also judged nominations received through their application process.
the world's most innovative companies is fast company's signature franchise and one of its most highly anticipated editorial efforts of the year.
it provides both a snapshot and a road map for the future of innovation across the most dynamic sectors of the economy.
"at a time of increasing global volatility, this year's list showcases the resilience and optimism of businesses across the world.
these companies are applying creativity to solve challenges within their industries and far beyond," said fast company senior editor amy farley, who oversaw the issue with deputy editor david lidsky.
fast company's most innovative companies issue (march/april 2020) is now available online at fastcompany.com/most- innovative- companies/2020, as well as in app form via itunes and on newsstands beginning march 17, 2020.
the hashtag is #fcmostinnovative.
about quantum xchange
quantum xchange gives commercial enterprises and government agencies the ultimate solution for secure communications.
its complete key management system, phio trusted xchange (tx), is uniquely capable of making existing encryption keys quantum safe and supports both post- quantum crypto (pqc) and quantum key distribution (qkd) for true crypto agility and quantum readiness.
as the operator of the first quantum fiber network in the u.s., quantum xchange also holds the unique distinction of being the only company in the world to make qkd commercially viable by solving the distance and delivery limitations inherent with all other offerings.
with a dynamic security infrastructure in place, organizations can enhance their existing encryption environment, select the level of protection needed based on their risk tolerance, and seamlessly scale to qkd at any time, across any distance, between multiple transmission points.
to learn more about being quantum safe today and quantum ready for tomorrow's threats, visit quantumxc.com or follow us on twitter @quantum_xchange.
about fast company
fast company is the only media brand fully dedicated to the vital intersection of business, innovation, and design, engaging the most influential leaders, companies, and thinkers on the future of business.
since 2011, fast company has received some of the most prestigious editorial and design accolades, including the american society of magazine editors (asme) national magazine award for "magazine of the year," adweek's hot list for "hottest business publication," and six gold medals and 10 silver medals from the society of publication designers.
the editor- in- chief is stephanie mehta and the publisher is amanda smith.
headquartered in new york city, fast company is published by mansueto ventures llc, along with our sister publication inc., and can be found online at www.fastcompany.com.
researchers from the institute for quantum computing (iqc) at the university of waterloo led the development of a new extensible wiring technique capable of controlling superconducting quantum bits, representing a significant step towards to the realization of a scalable quantum computer.
"the quantum socket is a wiring method that uses three- dimensional wires based on spring- loaded pins to address individual qubits," said jeremy bejanin, a phd candidate with iqc and the department of physics and astronomy at waterloo.
he and thomas mcconkey, phd candidate from iqc and the department of electrical and computer engineering at waterloo, are lead authors on the study that appears in the journal physical review applied as an editors' suggestion and is featured in physics.
"the technique connects classical electronics with quantum circuits, and is extendable far beyond current limits, from one to possibly a few thousand qubits."
one promising implementation of a scalable quantum computing architecture uses a superconducting qubit, which is similar to the electronic circuits currently found in a classical computer, and is characterized by two states, 0 and 1.
quantum mechanics makes it possible to prepare the qubit in superposition states, meaning that the qubit can be in states 0 and 1 at the same time.
to initialize the qubit in the 0 state, superconducting qubits are brought down to temperatures close to - 273 degrees celsius inside a cryostat, or dilution refrigerator.
to control and measure superconducting qubits, the researchers use microwave pulses.
the pulses are typically sent from dedicated sources and pulse generators through a network of cables connecting the qubits in the cryostat's cold environment to the room- temperature electronics.
the network of cables required to access the qubits inside the cryostat is a complex infrastructure and, until recently, has presented a barrier to scaling the quantum computing architecture.
"all wire components in the quantum socket are specifically designed to operate at very low temperatures and perform well in the microwave range required to manipulate the qubits," said matteo mariantoni, a faculty member at iqc and the department of physics and astronomy at waterloo and senior author on the paper.
"we have been able to use it to control superconducting devices, which is one of the many critical steps necessary for the development of extensible quantum computing technologies."
the paper, three- dimensional wiring for extensible quantum computing: the quantum socket, is a collaborative effort of researchers at ingun prufmittelbau gmbh, germany, ingun usa, and google in the united states, plus the following researchers from iqc and waterloo: jeremy bejanin, thomas mcconkey, john rinehart, carolyn earnest, corey rae mcrae, daryoush shiri, james bateman, yousef rohanizadegan and matteo mariantoni.
more information: the quantum socket: three- dimensional wiring for extensible quantum computing arxiv:1606.00063 [quant- ph] https://arxiv.org/abs/1606.00063
when future users of quantum computers need to analyze their data or run quantum algorithms, they will often have to send encrypted information to the computer.
because of this requirement, researchers from dtu physics and the university of toronto have investigated whether a quantum computer can work equally well with encrypted and unencrypted signals.
the results indicate that the efficiency remains almost unchanged.
the development of a universal quantum computer is generally considered the ultimate goal within the area of physics called quantum information theory.
if this goal is achieved it will enable huge progress within a long list of research fields where quantum effects are important.
this could for example by in designing new medicine or new types of materials for construction or electronics.
inspired by the history of the development of the classical computer, the researchers expect that the first generation of quantum computers will be large, expensive and difficult to operate and maintain.
for these reasons it is also expected that these devices will, at least initially, only be available to large organizations and governments.
can a blind quantum computer be useful?
this leads to the idea of delegated quantum computing, where a user obtains access to a centralized quantum computer through a network, often thought of as a quantum version of the internet.
if the user wants the request forwarded to the quantum computer to be secret, even to the quantum computer itself, she is able to encrypt them.
the question is then if a quantum computer that is working in the dark, because the input is encrypted, is as efficient as when it is working on the plain input.
a universal quantum computer consists of a number of so- called gates.
more generally, a gate is a logical operation.
both quantum and ordinary computers make use of gates, though they behave quite differently.
a classical logical operation could for example be an and gate.
this gate takes two inputs and returns an output based on the inputs.
for example to inputs, each with the value 1, would return the output 1.
it is possible to show mathematically which types of gates are necessary to give a quantum computer with the required properties, and the researchers have now investigated some of these gates to see how they react to the encryption procedure.
by comparing the gate output for an encrypted and unencrypted input, the researchers have been able to measure how large an effect the encryption has on the gate output, and thusly the efficiency of the quantum computer.
it turns out that there is no significant reduction in this efficiency.
in other words, a quantum computer works equally well with encrypted and unencrypted signals.
for the first time, physicists have experimentally demonstrated a quantum secure direct communication (qsdc) protocol combined with quantum memory, which is essential for storing and controlling the transfer of information.
until now, qsdc protocols have used fiber delay lines as a substitute for quantum memory, but the use of quantum memory is necessary for future applications, such as long- distance communication over secure quantum networks.
the researchers, wei zhang et al., from the university of science and technology of china and nanjing university of posts and telecommunications, have published a paper on their experimental demonstration in a recent issue of physical review letters.
qsdc is one of several different types of quantum communication methods, and has the ability to directly transmit secret messages over a quantum channel.
unlike most other quantum communication methods, qsdc does not require that the two parties communicating share a private key in advance.
similar to other kinds of quantum communication, the security of the method relies on some of the basic principles of quantum mechanics, such as the uncertainty principle and the no- cloning theorem.
as the physicists explain, a quantum memory is necessary for qsdc protocols in order to effectively control the transfer of information in future quantum networks.
however, experimentally realizing quantum memory with qsdc is challenging because it requires storing entangled single photons and establishing the entanglement between separated memories.
in their experiments, the researchers demonstrated most of the essential steps of the protocol, including entanglement generation; channel security; and the distribution, storage, and encoding of entangled photons.
due to the difficulty of decoding entangled photons in the optimal way (which requires distinguishing between four quantum states), the researchers used an alternative decoding method that is easier to implement.
in the future, the researchers expect that it will be possible to demonstrate qsdc across distances of 100 km or more in free space, similar to the recent demonstrations of quantum key distribution, quantum teleportation and entanglement distribution over these distances.
achieving this goal will mark an important step in realizing satellite- based long- distance and global- scale qsdc in the future.
yale university researchers have demonstrated one of the key steps in building the architecture for modular quantum computers: the "teleportation" of a quantum gate between two qubits, on demand.
the findings appear online sept. 5 in the journal nature.
the key principle behind this new work is quantum teleportation, a unique feature of quantum mechanics that has previously been used to transmit unknown quantum states between two parties without physically sending the state itself.
using a theoretical protocol developed in the 1990s, yale researchers experimentally demonstrated a quantum operation, or "gate," without relying on any direct interaction.
such gates are necessary for quantum computation that relies on networks of separate quantum systems- an architecture that many researchers say can offset the errors that are inherent in quantum computing processors.
through the yale quantum institute, a yale research team led by principal investigator robert schoelkopf and former graduate student kevin chou is investigating a modular approach to quantum computing.
modularity, which is found in everything from the organization of a biological cell to the network of engines in the latest spacex rocket, has proved to be a powerful strategy for building large, complex systems, the researchers say.
a quantum modular architecture consists of a collection of modules that function as small quantum processors connected into a larger network.
modules in this architecture have a natural isolation from each other, which reduces unwanted interactions through the larger system.
yet this isolation also makes performing operations between modules a distinct challenge, according to the researchers.
teleported gates are a way to implement inter- module operations.
"our work is the first time that this protocol has been demonstrated where the classical communication occurs in real- time, allowing us to implement a 'deterministic' operation that performs the desired operation every time," chou said.
fully useful quantum computers have the potential to reach computation speeds that are orders of magnitude faster than today's supercomputers.
yale researchers are at the forefront of efforts to develop the first fully useful quantum computers and have done pioneering work in quantum computing with superconducting circuits.
quantum calculations are done via delicate bits of data called qubits, which are prone to errors.
in experimental quantum systems, "logical" qubits are monitored by "ancillary" qubits in order to detect and correct errors immediately.
"our experiment is also the first demonstration of a two- qubit operation between logical qubits," schoelkopf said.
"it is a milestone toward quantum information processing using error- correctable qubits."
new research is examining the use of artificial intelligence to handle the calculations necessary for quantum computers to function.
maybe quantum computing is a job for artificial intelligence.
to call quantum computing complicated is a gross understatement.
rather than any single complex challenge, quantum computing is a series of obstacles all superimposed (pun intended) onto each other.
even though quantum processors based on superconducting circuits already exist in labs today, they don't compare in speed or processing power to today's typical desktop, laptop, and tablet computers.
even if you can settle on materials, a physical architecture, and a form factor for your quantum device, you're still faced with the very real difficulties of actually measuring quantum signals so you can take advantage of the processing and storage enhancements offered by quantum computing.
speaking during a keynote at designcon 2019, dr. irfan siddiqi, a professor of physics at the quantum nanoscience laboratory and the department of physics at the university of california berkeley, proposed artificial intelligence as a possible solution to this.
"can we teach a machine quantum mechanics?
can a machine learn the rules of quantum- mechanics?
the answer is, absolutely," siddiqi said.
unlike a traditional bit, which can only be either 1 or 0, a quantum bit (qubit) must exist in a superimposed state, where it is occupying more than one state simultaneously.
this is the key to the leap in computing power promised by qubits.
but since qubits can occupy exponentially more states than a simple one or zero of an analog or digital bit, actually measuring the state of these bits is also much more difficult.
in order to measure the state of qubit you have to observe it, and in doing so you create interference that runs contrary to its quantum nature.
measuring a qubit then is not a simple act of observation.
instead, researchers have to employ complex mathematical models designed to predict what state a qubit will be in at any given time.
now, one qubit alone can't provide the horsepower we really want out of quantum computers, so naturally you want to create a system that uses many, many more.
the problem is once you have more than one qubit you're vastly increasing computing power, but you're also now dealing with entanglement, wherein the qubits affect each other.
entanglement makes the measurement problem even more difficult since now each qubit is affecting the superimposition of the qubits around it.
ultimately, as siddiqi explained, the question at hand is, "how can you measure a quantum signal's phases when as soon as you start to measure it you disturb it?"
siddiqi has co- authored a recent study, "using a recurrent neural network to reconstruct quantum dynamics of a superconducting qubit from physical observations" in which he and fellow researchers from uc berkeley, the israel institute of technology, and universite paris- saclay trained a neural network to perform the predictions necessary to measure quantum states.
according to the paper, "this method has potential to greatly simplify and enhance tasks in quantum systems such as noise characterization, parameter estimation, feedback, and optimization of quantum control."
neural networks are a particular subset of artificial intelligence algorithms designed to mimic the biology of the human brain.
a neural network employs an interconnected group of nodes - all working toward solving the same problem, similar to the way the neurons of the human brain cooperate with one another.
neural networks excel at making sense of large datasets.
in siddiqi's research the team used a specific type of neural network called a recurrent neural network (rnn), which is typically applied to speech- related tasks such as language translation and voice recognition.
the strength of rnns is that they learn by example.
you don't need to feed it an english dictionary for it to learn the context of words, it can infer the context on its own given enough examples.
"interestingly, quantum filtering can be seen as a similar task in which noisy experimental signals must be translated into meaningful quantum information," the paper reads.
siddiqi explained to the designcon audience that the external environment has also has a tremendous impact on quantum circuits.
"making a system where you are the only observer is really hard," he said.
external factors like the size of a quantum machine, the temperature, and other objects around it, can all affect a quantum machine.
thus, in siddiqi's words, a quantum circuit needs to be "small, cold, and isolated" to function properly.
what ai is offering is a way to do quantum calculations around all these limiting factors.
after all, an algorithm doesn't need to physically observe quantum signals to make calculations.
that is not to say ai can understand or comprehend quantum mechanics the same way humans can.
the advantage using rnns affords is that they also do not need to be trained in quantum mechanics or taught to employ any particular prediction models.
by being "model- agnostic" neural networks can be generalized to larger quantum systems as well.
according to the paper this means that they can become a valuable tool in calibrating quantum processors and can enable researchers to characterize errors, noise, and other imperfections that have been outside the usual scope.
it may possible even enable researchers to identify and quantity effects that are currently unknown.
research like siddiqi's is part of a growing body of work that represents an emerging feedback loop between ai and quantum computers.
the thinking goes that ai will help bring quantum computers to fruition, and quantum computers, in turn, will expand the potential of ai beyond anything known today.
a 2016 paper co- authored by researchers from the swiss federal institute of technology in zurich and microsoft research, examined the use of neural networks to address the many- body problems of quantum physics (essentially helping to understand how quantum particles interact with one another, or entangle).
in 2018 researchers at the max planck institute for the science of light in germany published a paper in which they explored the use of neural networks in quantum error correction.
the researchers suggest ai can perform repairs needed to keep quantum information in tact as the qubits that make up that information entangle with one another.
should quantum computers achieve the level of performance it is theorized they can achieve, many predict artificial intelligence will undergo a similar transformation and see an exponential increase in performance and "intelligence."
researchers at google, for example, are already experimenting with "quantum neural networks" to model how neural networks may function and perform on quantum processors.
should the loop complete itself many applications enhanced by ai and machine learning from online search, to cybersecurity, chip design, and even automotive applications could find themselves further enhanced by quantum technology.
chris wiltz is a senior editor at design news covering emerging technologies including ai, vr/ar, and robotics.
yale university researchers have demonstrated one of the key steps in building the architecture for modular quantum computers: the "teleportation" of a quantum gate between two qubits, on demand.
the findings appear online sept. 5 in the journal nature.
the key principle behind this new work is quantum teleportation, a unique feature of quantum mechanics that has previously been used to transmit unknown quantum states between two parties without physically sending the state itself.
using a theoretical protocol developed in the 1990s, yale researchers experimentally demonstrated a quantum operation, or "gate," without relying on any direct interaction.
such gates are necessary for quantum computation that relies on networks of separate quantum systems -  an architecture that many researchers say can offset the errors that are inherent in quantum computing processors.
through the yale quantum institute, a yale research team led by principal investigator robert schoelkopf and former graduate student kevin chou is investigating a modular approach to quantum computing.
modularity, which is found in everything from the organization of a biological cell to the network of engines in the latest spacex rocket, has proved to be a powerful strategy for building large, complex systems, the researchers say.
a quantum modular architecture consists of a collection of modules that function as small quantum processors connected into a larger network.
modules in this architecture have a natural isolation from each other, which reduces unwanted interactions through the larger system.
yet this isolation also makes performing operations between modules a distinct challenge, according to the researchers.
teleported gates are a way to implement inter- module operations.
"our work is the first time that this protocol has been demonstrated where the classical communication occurs in real- time, allowing us to implement a 'deterministic' operation that performs the desired operation every time," chou said.
fully useful quantum computers have the potential to reach computation speeds that are orders of magnitude faster than today's supercomputers.
yale researchers are at the forefront of efforts to develop the first fully useful quantum computers and have done pioneering work in quantum computing with superconducting circuits.
quantum calculations are done via delicate bits of data called qubits, which are prone to errors.
in experimental quantum systems, "logical" qubits are monitored by "ancillary" qubits in order to detect and correct errors immediately.
"our experiment is also the first demonstration of a two- qubit operation between logical qubits," schoelkopf said.
"it is a milestone toward quantum information processing using error- correctable qubits."
quantum simulation plays an irreplaceable role in diverse fields, beyond the scope of classical computers.
in a recent study, keren li and an interdisciplinary research team at the center for quantum computing, quantum science and engineering and the department of physics and astronomy in china, u.s. germany and canada.
experimentally simulated spin- network states by simulating quantum spacetime tetrahedra on a four- qubit nuclear magnetic resonance (nmr) quantum simulator.
the experimental fidelity was above 95 percent.
the research team used the quantum tetrahedra prepared by nuclear magnetic resonance to simulate a two- dimensional (2- d) spinfoam vertex (model) amplitude, and display local dynamics of quantum spacetime.
li et al.
measured the geometric properties of the corresponding quantum tetrahedra to simulate their interactions.
the experimental work is an initial attempt and a basic module to represent the feynman diagram vertex in the spinfoam formulation, to study loop quantum gravity (lqg) using quantum information processing.
the results are now available on communication physics.
classical computers cannot study large quantum systems despite successful simulations of a variety of physical systems.
the systematic constraints of classical computers occurred when the linear growth of quantum system sizes corresponded to the exponential growth of the hilbert space, a mathematical foundation of quantum mechanics.
quantum physicists aim to overcome the issue using quantum computers that process information intrinsically or quantum- mechanically to outperform their classical counterparts exponentially.
in 1982, physicist richard feynman defined quantum computers as quantum systems that can be controlled to mimic or simulate the behaviour or properties of relatively less accessible quantum systems.
in the present work, li et al.
used nuclear magnetic resonance (nmr) with a high controllable performance on the quantum system to develop simulation methods.
the strategy facilitated the presentation of quantum geometries of space and spacetime based on the analogies between nuclear spin states in nmr samples and spin- network states in quantum gravity.
quantum gravity aims to unite the einstein gravity with quantum mechanics to expand our understanding of gr ... to the planck scale (1.22 x 1019 gev).
at the planck scale (magnitudes of space, time and energy) einstein gravity and the continuum of spacetime breakdown can be replaced via quantum spacetime.
research approaches toward understanding quantum spacetimes are presently rooted in spin networks (a graph of lines and nodes to represent the quantum state of space at a certain point in time), which are an important, non- perturbative framework of quantum gravity.
in 1971, physicist roger penrose proposed spin networks motivated by the twistor theory with subsequent applications to loop quantum gravity (lqg).
the spin networks were quantum states representing fundamentally discrete quantum geometries of space at the planck scale.
in the present study, the research team represented the spin network using a graph with links and nodes colored by spin halves.
for example, any node with edges corresponded to a geometry and therefore a graph containing four- valent nodes corresponded to quantum tetrahedron geometry.
the research team developed a "network" containing a number of three- dimensional (3- d) world sheets (2- d surfaces) and their intersections.
they showed that each vertex where the surfaces met, led to a quantum transition that changed the spin network to represent local dynamics of quantum geometry.
much like feynman diagrams (schematic representations of mathematical expressions describing the behavior of subatomic particles), quantum spacetimes encoded the transition amplitudes and spinfoam amplitudes between the initial and final spin networks.
the quantum spacetimes and spinfoam amplitudes developed in the study provided a consistent and promising approach to quantum gravity.
li et al.
featured the nmr simulation by the capability to control individual qubits with high precision.
the quantum tetrahedra and vertex amplitudes served as building blocks of lqg (loop quantum gravity) to open a new window to include lqg in quantum experiments.
the scientists first derived equations to describe a quantum tetrahedron within a spin network.
in a schematic 3+1- dimensional dynamic quantum spacetime model, they demonstrated an atom as a 3- sphere enclosing a portion of the quantum spacetime surrounding a vertex.
the team modeled the boundary of the enclosed quantum spacetime precisely as a spin network and showed the possibility of simulating large quantum spacetimes with many vertices by quantum gluing the atoms.
the resulting structure resembled vertex amplitude of quantum spacetime similar to previously developed ooguri's topological lattice m ... s in four dimensions.
the researchers showed lqg to identify quantum tetrahedron geometries with the quantum angular momenta.
the identification allowed them to simulate quantum geometries with quantum registers (quantum mechanical analogue of a classical processor register).
in general, a quantum register can be mathematically achieved using tensor products.
during the experiments, li et al.
simulated 10 quantum tetrahedra by preparing the corresponding invariant- tensor states.
they labeled these states using 10 colored points on the bloch sphere (geometrical representation) and conducted the experiments on a 700- mhz drx bruker spectrometer at room temperature.
for all experiments, the research team used the crotonic acid molecule with four c nuclei suited for the four- qubit system.
the scientists developed the experimental system to prepare quantum tetrahedra and simulate its local dynamics in three parts.
for state preparation, first they initialized the entire system to a pseudo- pure state.
they obtained a fidelity above 99 percent using the spatial average method.
then they drove the system into 10 invariant- tensor states or transformations, which they implemented using 10 shaped pulses of 20 ms.
next, for geometry measurements, the team presented the measured geometry properties using a 3- d histogram.
the experimental uncertainty at this point resulted from the nmr spectrum- fitting process.
the coincidence between experimental and theoretical simulations implied that the invariant tensor states prepared in the experiments matched the building blocks- quantum tetrahedra.
during amplitude simulation, the spin- network states served as the boundary data of 3+1- dimensional quantum spacetime.
the vertex amplitude defined in the study determined the spinfoam amplitude and described the local dynamics of quantum gravity in 4- d quantum spacetime, to display the properties of these boundary data.
in order to obtain the vertex amplitudes, the researchers calculated the inner products between five different quantum tetrahedron states.
ideally, the researchers could have used a 20- qubit quantum computer, establishing two- qubit maximally entangled states between two arbitrary tetrahedra.
however, since a quantum computer of such dimensions is presently beyond commercialized cutting- edge technology, the researchers alternately conducted full tomography of the state preparation to obtain information of quantum tetrahedron states.
when the scientists calculated the fidelities between the experimental quantum tetrahedron states and theory, the results were well above 95 percent.
using the quantum tetrahedra, the research team simulated the vertex amplitude.
they compared the results between the experiment and the numeric simulation among all five tetrahedra.
accordingly, saddle points of the amplitude in the experiments occurred where the five interacting tetrahedra demonstrated a simple geometric meaning as they glued to form a geometric four- simplex.
in this way, keren li and co- workers used a quantum register in the nmr system to create 10 invariant- tensor states to represent 10 quantum tetrahedra.
they achieved a fidelity above 95 percent and subsequently measured the dihedral angles (two plane faces) of the model.
they considered the spectrum- fitting errors and geometrical identification to understand the success in simulating quantum tetrahedra in the study.
the new research work presented a first- step to explore spin- network states and spinfoam amplitudes using a quantum simulator.
the accompanying work also demonstrated valid experiments to study lgq.
scientists at the department of energy's oak ridge national laboratory are the first to successfully simulate an atomic nucleus using a quantum computer.
the results, published in physical review letters, demonstrate the ability of quantum systems to compute nuclear physics problems and serve as a benchmark for future calculations.
quantum computing, in which computations are carried out based on the quantum principles of matter, was proposed by american theoretical physicist richard feynman in the early 1980s.
unlike normal computer bits, the qubit units used by quantum computers store information in two- state systems, such as electrons or photons, that are considered to be in all possible quantum states at once (a phenomenon known as superposition).
"in classical computing, you write in bits of zero and one," said thomas papenbrock, a theoretical nuclear physicist at the university of tennessee and ornl who co- led the project with ornl quantum information specialist pavel lougovski.
"but with a qubit, you can have zero, one, and any possible combination of zero and one, so you gain a vast set of possibilities to store data."
in october 2017 the multidivisional ornl team started developing codes to perform simulations on the ibm qx5 and the rigetti 19q quantum computers through doe's quantum testbed pathfinder project, an effort to verify and validate scientific applications on different quantum hardware types.
using freely available pyquil software, a library designed for producing programs in the quantum instruction language, the researchers wrote a code that was sent first to a simulator and then to the cloud- based ibm qx5 and rigetti 19q systems.
the team performed more than 700,000 quantum computing measurements of the energy of a deuteron, the nuclear bound state of a proton and a neutron.
from these measurements, the team extracted the deuteron's binding energy- the minimum amount of energy needed to disassemble it into these subatomic particles.
the deuteron is the simplest composite atomic nucleus, making it an ideal candidate for the project.
"qubits are generic versions of quantum two- state systems.
they have no properties of a neutron or a proton to start with," lougovski said.
"we can map these properties to qubits and then use them to simulate specific phenomena- in this case, binding energy."
a challenge of working with these quantum systems is that scientists must run simulations remotely and then wait for results.
ornl computer science researcher alex mccaskey and ornl quantum information research scientist eugene dumitrescu ran single measurements 8,000 times each to ensure the statistical accuracy of their results.
"it's really difficult to do this over the internet," mccaskey said.
"this algorithm has been done primarily by the hardware vendors themselves, and they can actually touch the machine.
they are turning the knobs."
the team also found that quantum devices become tricky to work with due to inherent noise on the chip, which can alter results drastically.
mccaskey and dumitrescu successfully employed strategies to mitigate high error rates, such as artificially adding more noise to the simulation to see its impact and deduce what the results would be with zero noise.
"these systems are really susceptible to noise," said gustav jansen, a computational scientist in the scientific computing group at the oak ridge leadership computing facility (olcf), a doe office of science user facility located at ornl.
"if particles are coming in and hitting the quantum computer, it can really skew your measurements.
these systems aren't perfect, but in working with them, we can gain a better understanding of the intrinsic errors."
at the completion of the project, the team's results on two and three qubits were within 2 and 3 percent, respectively, of the correct answer on a classical computer, and the quantum computation became the first of its kind in the nuclear physics community.
the proof- of- principle simulation paves the way for computing much heavier nuclei with many more protons and neutrons on quantum systems in the future.
quantum computers have potential applications in cryptography, artificial intelligence, and weather forecasting because each additional qubit becomes entangled- or tied inextricably- to the others, exponentially increasing the number of possible outcomes for the measured state at the end.
this very benefit, however, also has adverse effects on the system because errors may also scale exponentially with problem size.
papenbrock said the team's hope is that improved hardware will eventually enable scientists to solve problems that cannot be solved on traditional high- performance computing resources- not even on the ones at the olcf.
in the future, quantum computations of complex nuclei could unravel important details about the properties of matter, the formation of heavy elements, and the origins of the universe.
results from the study, titled "cloud quantum computing of an atomic nucleus," were published in physical review letters.
rice university physicists have created a tiny "electron superhighway" that could one day be useful for building a quantum computer, a new type of computer that will use quantum particles in place of the digital transistors found in today's microchips.
in a recent paper in physical review letters, rice physicists rui- rui du and ivan knez describe a new method for making a tiny device called a "quantum spin hall topological insulator."
the device, which acts as an electron superhighway, is one of the building blocks needed to create quantum particles that store and manipulate data.
today's computers use binary bits of data that are either ones or zeros.
quantum computers would use quantum bits, or "qubits," which can be both ones and zeros at the same time, thanks to the quirks of quantum mechanics.
this quirk gives quantum computers a huge edge in performing particular types of calculations, said du, professor of physics and astronomy at rice.
for example, intense computing tasks like code- breaking, climate modeling and biomedical simulation could be completed thousands of times faster with quantum computers.
"in principle, we don't need many qubits to create a powerful computer," he said.
"in terms of information density, a silicon microprocessor with 1 billion transistors would be roughly equal to a quantum processor with 30 qubits."
in the race to build quantum computers, researchers are taking a number of approaches to creating qubits.
regardless of the approach, a common problem is making certain that information encoded into qubits isn't lost over time due to quantum fluctuations.
this is known as "fault tolerance."
the approach du and knez are following is called "topological quantum computing."
topological designs are expected to be more fault- tolerant than other types of quantum computers because each qubit in a topological quantum computer will be made from a pair of quantum particles that have a virtually immutable shared identity.
the catch to the topological approach is that physicists have yet to create or observe one of these stable pairs of particles, which are called "majorana fermions" (pronounced mah- yor- ah- na fur- mee- ons).
the elusive majorana fermions were first proposed in 1937, although the race to create them in a chip has just begun.
in particular, physicists believe the particles can be made by marrying a two- dimensional topological insulator -  like the one created by du and knez -  to a superconductor.
topological insulators are oddities; although electricity cannot flow through them, it can flow around their narrow outer edges.
if a small square of a topological insulator is attached to a superconductor, knez said, the elusive majorana fermions are expected to appear precisely where the materials meet.
if this proves true, the devices could potentially be used to generate qubits for quantum computing, he said.
knez spent more than a year refining the techniques to create rice's topological insulator.
the device is made from a commercial- grade semiconductor that's commonly used in making night- vision goggles.
du said it is the first 2- d topological insulator made from a material that physicists already know how to attach to a superconductor.
"we are well- positioned for the next step," du said.
"meanwhile, only experiments can tell whether we can find majorana fermions and whether they are good candidates for creating stable qubits."
the arrival of superfast quantum computing is closer following recent breakthroughs by an international team led by unsw researchers.
superfast quantum computing is closer than ever following recent breakthroughs by an international team led by researchers from the university of new south wales.
quantum computing relies on controlling and observing the behaviour of quantum particles - for instance individual electrons - to deliver enormous processing power.
in the two breakthroughs, written up in the international journals nano letters and applied physics letters, researchers have for the first time demonstrated two ways to deliberately place an electron in a nano- sized device on a silicon chip.
the achievements set the stage for the next crucial steps of being able to observe and then control the electron's quantum state or "spin", to create
a quantum bit.
multiple quantum bits coupled together make up the processor of a quantum computer.
professor andrew dzurak, the nsw node director of the australian national fabrication facility at unsw and dr andrea morello, manager of the quantum measurement and control chip program at the arc centre of excellence for quantum computer technology, were leaders in the breakthrough work.
in research just published in applied physics letters, the team, including phd student wee han lim, were able to accurately localise a single electron in silicon without it being attached to an atom.
this "artificial atom" is known as a "quantum dot".
dr morello said the quantum dot avoided the difficulty of having to introduce single atoms in precise positions in a silicon chip.
in a separate project, published in the journal nano letters, the researchers, including phd student kuan yen tan, used "nature's own way" to localise electrons, by binding them to single atoms.
quantum computing's power comes from the fact that electrons can have a "spin" pointing in one of two directions.
the spin position can be used in the same way that zeroes and ones represent data in today's computers.
however electrons can also hold intermediate spin positions, or quantum states, which is what gives quantum computing its power.
while today's computers increase their power linearly with the number of bits added, quantum bits, when coupled together, can deliver an exponential increase in their ability to represent data.
the other leaders of the research team are professor david jamieson at the university of melbourne, and dr mikko mottonen at the helsinki university of technology.
students wee han lim and kuan yen tan have just completed their phd degrees in the unsw school of electrical engineering and telecommunications.
physicists at the national institute of standards and technology (nist) have used charged atoms (ions) to demonstrate a quantum physics version of computer memory lasting longer than 10 seconds- more than 100,000 times longer than in previous experiments on the same ions.
the advance improves prospects for making practical, reliable quantum computers (which make use of the properties of quantum systems rather than transistors for performing calculations or storing information).
quantum computers, if they can be built, could break today's best encryption systems, accelerate database searching, develop novel products such as fraud- proof digital signatures or simulate complex biological systems to help design new drugs.
as described in the aug. 5, 2005, issue of physical review letters, nist scientists stored information in single beryllium ions for longer periods of time by using a different pair of the ions' internal energy levels to represent 1 and 0 than was used in the group's previous quantum computing experiments.
this new set of quantum states is unaffected by slight variations in magnetic fields, which previously caused memory losses in ions stored in electromagnetic traps.
quantum memory must be able to store "superpositions," an unusual property of quantum physics in which a quantum bit (qubit) such as an ion represents both 0 and 1 at the same time.
the new approach enables qubits to maintain superpositions over 1 million times longer than might be needed to carry out the information processing steps in a future quantum computer.
the advance is, therefore, an important step toward the goal of designing a "fault tolerant" quantum computer because it significantly reduces the computing resources needed to correct memory errors.
in related experiments also described in the paper, nist scientists demonstrated that pairs of "entangled" ions can retain their quantum states for up to about 7 seconds.
entanglement is another unusual property of quantum physics that correlates the behavior of physically separated ions.
superposition and entanglement are the two key properties expected to give quantum computers great power.
the research was supported by the advanced research and development activity/national security agency.
more information about nist's quantum computing research is available at http://qubit.nist.gov.
a team of physicists and engineers has demonstrated exquisite control of single particles of light - photons - on a silicon chip to make a major advance towards the long sought after goal of a super- powerful quantum computer.
dr jeremy o'brien, his phd student alberto politi, and their colleagues at bristol university have demonstrated the world's smallest optical controlled- not gate - the building block of a quantum computer.
the team were able to fabricate their controlled- not gate from silica wave- guides on a silicon chip, resulting in a miniaturised device and high- performance operation.
"this is a crucial step towards a future optical quantum computer, as well as other quantum technologies based on photons," said dr o'brien.
the team reports its results in the march 27 2008 science express - the advanced online publication of the journal science.
quantum technologies with photons
quantum technologies aim to exploit the unique properties of quantum mechanics, the physics theory that explains how the world works at very small scales.
for example a quantum computer relies on the fact that quantum particles, such as photons, can exist in a "superposition" of two states at the same time - in stark contrast to the transistors in a pc which can only be in the state "0" or "1".
photons are an excellent choice for quantum technologies because they are relatively noise free; information can be moved around quickly - at the speed of light; and manipulating single photons is easy.
making two photons "talk" to each other to realise the all- important controlled- not gate is much harder, but dr o'brien and his colleagues at the university of queensland demonstrated this back in 2003 [nature 426, 264].
photons must also "talk" to each other to realise the ultra- precise measurements that harness the laws of quantum mechanics - quantum metrology.
last year dr o'brien and his collaborator professor takeuchi and co- workers at hokkaido university reported such a quantum metrology measurement with four photons [science 316, 726].
silica- on- silicon wave- guide quantum circuits
"despite these and other impressive demonstrations, quantum optical circuits have typically relied on large optical elements with photons propagating in air, and consuming a square metre of optical table.
this has made them hard to build and difficult to scale up," said alberto politi.
"for the last several years the centre for quantum photonics has been working towards building controlled- not gates and other important quantum circuits on a chip to solve these problems," added dr o'brien.
the team's chips, fabricated at cip technologies, have dimensions measured in millimetres.
this impressive miniaturisation was permitted thanks to the silica- on- silicon technology used in commercial devices for modern optical telecommunications, which guides light on a chip in the same way as in optical fibres.
the team generated pairs of photons which each encoded a quantum bit or qubit of information.
they coupled these photons into and out of the controlled- not chip using optical fibres.
by measuring the output of the device they confirmed high- fidelity operation.
in the experimental characterisation of the quantum chips the researchers also proved that one of the strangest phenomena of the quantum world, namely "quantum entanglement", was achieved on- chip.
quantum entanglement of two particles means that the state of either of the particles is not defined, but only their collective state.
this on- chip entanglement has important applications in quantum metrology.
"as well as quantum computing and quantum metrology, on- chip photonic quantum circuits could have important applications in quantum communication, since they can be easily integrated with optical fibres to send photons between remote locations," said alberto politi.
a team of scientists, led by professor winfried hensinger at the university of sussex, has made a major breakthrough concerning one of the biggest problems facing quantum computing: how to reduce the disruptive effects of environmental "noise" on the highly sensitive function of a large- scale quantum computer.
in the real- world, technological developments need to operate in imperfect conditions; what can be successfully tested in a highly controlled laboratory may fail when presented with realistic environmental factors, such as the fluctuations in voltage from an electronic component or stray electromagnetic fields emitted by everyday electronic equipment.
the university of sussex's ion quantum technology group have managed to dramatically reduce the effects of such environmental "noise" affecting trapped ion quantum computers, reporting their findings in an article that has today, thursday 1 november 2018, been published in the journal physical review letters.
it means the team is one step closer to building a large- scale quantum computer with the capability to solve challenging real- world problems.
small- scale quantum computers currently in existence only contain a handful of quantum bits -  components of quantum computers that store information and can exist in multiple states, also referred to as qubits.
as such, current quantum computers are small enough to be operated in a highly controlled environment inside a specialized laboratory.
however, such machines do not have the processing power required to solve complex problems because of the limited number of qubits.
when built, large- scale quantum computers will be able to solve certain problems that would take even the fastest super computers billions of years to calculate.
in order to create a quantum computer that can solve such problems, scientists will need to increase the number of qubits, which in turn will increase the size of the quantum computer.
the problem is that the more qubits that are added, the more difficult it becomes to isolate the computer from any realistic "noise" that would disrupt the computing processes.
hensinger's team of university of sussex physicists have made a quantum computing breakthrough that is capable of mitigating some of these problems.
they collaborated with theoretical scientist dr florian mintert and colleagues from imperial college london, who proposed a theory of how one might be able to solve this problem by manipulating the strange quantum effects in use inside a quantum computer.
the theory allows -  making use of the strange properties of quantum physics -  the execution of quantum computations in such a way that changes in the initial operational parameters of the machine do not lead to a substantial change in the end result of the computation.
this in turn helps to insulate the quantum computer from the effects of environmental 'noise'.
dr sebastian weidt, senior scientist in the sussex ion quantum technology group, explains the significance: "realising this technique may have a profound impact on the ability to develop commercial ion trap quantum computers beyond use in an academic laboratory."
the sussex team went to work to see whether they could actually implement this theory.
they used complicated radio- frequency and microwave signals capable of manipulating the quantum effects inherent in individual charged atoms (ions), to demonstrate this in practical experiments.
their implementation is based on microwave technology, such as that present in mobile phones.
following months of intensive work in the laboratory, the sussex scientists have managed to make this new method a reality, experimentally demonstrating its capabilities to substantially reduce the effect of "noise" on a trapped ion quantum computer.
prof hensinger, head of the ion quantum technology group at the university of sussex -  which last year unveiled the first blueprint for a large- scale quantum computer -  says: "with this advance we have made another practical step towards constructing quantum computers that can host millions of qubits.
such machines are capable of solving certain problems that even the fastest supercomputer may take billions of years to calculate and be of great benefit to humanity; they may be able to help us create new pharmaceuticals; find new cures for diseases, such as dementia; create powerful tools for the financial sector; be of benefit to agriculture, through more efficient fertilizer production, among many other applications.
we are only starting to understand the tremendous potential of these machines."
hensinger's group is now utilising this new technique as they put the final touches to a powerful quantum computer prototype that is currently in their laboratory at the university of sussex.
hensinger says: "it's now time to translate academic achievements into the construction of practical machines.
we're in a fantastic position to do this at sussex and my team is working round the clock to make large- scale quantum computing a future reality."
here is a video of winfried hensinger explaining the research: https://www.youtube.com/watch?v=c5fnbv7dtxe
a team of physicists from the university of vienna and the austrian academy of sciences have demonstrated a new quantum computation scheme in which operations occur without a well- defined order.
the researchers led by philip walther and caslav brukner used this effect to accomplish a task more efficiently than a standard quantum computer.
moreover, these ideas could set the basis for a new form of quantum computing, potentially providing quantum computers with an even larger computational speed- up.
their results will be published in an upcoming issue of nature communications.
since its conception, quantum mechanics has defied our natural way of thinking, and it has forced physicists to come to grips with peculiar ideas.
although they may be difficult to digest, quantum phenomena are real.
what's more, in the last decades, scientists have shown that these bizarre quantum effects can be used for many astonishingly powerful applications: from ultra- secure communication to hacking existing secure communications, and from simulating complex quantum systems to efficiently solving large systems of equations.
one of the most exciting and most difficult proposed quantum technologies is the quantum computer.
quantum logic gates are the basic building blocks of a quantum computer, but constructing enough of them to perform a useful computation is difficult.
in the usual approach to quantum computing, quantum gates are applied in a specific order, one gate before another.
but it was recently realized that quantum mechanics permits one to "superimpose quantum gates".
if engineered correctly, this means that a set of quantum gates can act in all possible orders at the same time.
surprisingly, this effect can be used to reduce the total number of gates required for certain quantum computations.
all orders at once
a team led by philip walther recently realized that superimposing the order of quantum gates, an idea which was theoretically designed by the group of caslav brukner, could be implemented in the laboratory.
in a superposition of quantum gate orders, it is impossible - even in principle - to know if one operation occurred before another operation, or the other way around.
this means that two quantum logic gates a and b can be applied in both orders at the same time.
in other words, gate a acts before b and b acts before a.
the physicists from philip walther's group designed an experiment in which the two quantum logic gates were applied to single photons in both orders.
the results of their experiment confirm that it is impossible to determine which gate acted first - but the experiment was not simply a curiosity.
"in fact, we were able to run a quantum algorithm to characterize the gates more efficiently than any previously known algorithm," says lorenzo procopio, lead author of the study.
from a single measurement on the photon, they probed a specific property of the two quantum gates thereby confirming that the gates were applied in both orders at once.
as more gates are added to the task, the new method becomes even more efficient compared to previous techniques.
the way forward
this is the first time that a superposition of quantum gates has been implemented in the lab.
at the same time, it was used to successfully demonstrate a new kind of quantum computing.
the scientists were able to accomplish a computation with an efficiency that cannot be achieved within the old scheme of quantum computing.
this work opens a door for future studies on novel types of quantum computation.
although its full implications are still unknown, this work represents a new, exciting way to connect theoretical research on the foundations of physics to experimental quantum computing.
press the start button, switch on the monitor, grab a cup of coffee and off you go.
that is pretty much how most us experience booting up a computer.
but with a quantum computer the situation is very different.
so far, researchers have had to spend hours making dozens of adjustments and fine calibrations in order to set up a chip with just five quantum bits so that it can be used for experimental work.
(one quantum bit or 'qubit' is the quantum physical equivalent of a single bit in a conventional computer).
any small errors in the adjustment and calibration procedure and the chip would not work.
the problem is that, not unlike musical instruments, quantum computers react to small changes in the local environment.
if, for example, it is a little warmer or a little colder or if the ambient air pressure is a little higher or a little lower than the day before then the complex network of qubits will no longer function - the computer is detuned and has to be readjusted before it can be used.
'up until now, experimental quantum physicists have had to sit down each day and see how conditions have changed compared to the day before.
they then had to remeasure each parameter and carefully recalibrate the chip,' explains professor wilhelm- mauch, professor for theoretical quantum and solid- state physics at saarland university.
only a very small error rate of less than 0.1 percent is permissible when measuring ambient conditions.
frank wilhelm- mauch explains this sensitivity thus: 'that means that an error can occur in only one in a thousand measurements.
if just two in a thousand measurements are in error, the software will be unable to correct for the errors and the quantum computer will not operate correctly.'
with around 50 different parameters involved in the calibration process, one begins to get an idea of the sheer effort involved in calibrating a quantum computer.
working together with his doctoral student, wilhelm- mauch began to consider a fundamentally new approach to the problem.
'we asked ourselves the question: why is it necessary each and every day to understand how conditions differ from those of the day before?'
the answer we eventually came up with was that it isn't necessary.
what's important is that the setup procedure produces the right results.
why it produces the right results is not so relevant.'
it was this pragmatic approach that underlay the work carried out by wilhelm- mauch and egger.
'for the calibration procedure we used an algorithm from engineering mathematics, strictly speaking from the field of civil and structural engineering, as that's another area in which experiments are costly,' explains professor wilhelm- mauch.
using this technique, the two theoreticians were able to reduce the calibration error rate to below the required 0.1 percent threshold, while at the same time speeding up the calibration process from six hours to five minutes.
the saarbrucken methodology, which goes under the name ad- hoc (adaptive hybrid optimal control), has now been subjected to rigorous testing by a group of experimental physicists from the university of california in santa barbara.
their experimental work is published in the issue of physical review letters that also contains the saarbrucken paper.
this development is of major importance for future experimental research into quantum computing.
physicists in quantum computing laboratories no longer have to spend hours every day preparing their system for just a short period of experimental work.
'as many of the parameters, such as temperature, light and air pressure do not remain stable during the long calibration phase, this can further shorten the time window in which the chip is running error- free and in which it can therefore be used for experiments,' says wilhelm- mauch, adding that the new method is scalable.
up until now, technical constraints have meant that experiments have been carried out using a single chip housing five qubits that perform the actual computational operations.
the new method, in contrast, is not restricted to chips of this magnitude and can be applied to quantum processors of almost any size.
frank wilhelm- mauch jokingly points out another appealing feature of the new methodology: 'unlike the previous approach of manual calibration, our method is fully automated.
the researcher really does just push a button like on a conventional computer.
they can then go off to get themselves a coffee while the quantum computer boots up.'
a major improvement in the life of experimental research scientists working in the field.
background
the fundamental principle of quantum technology is that a particle (e.g.
an atom, electron or photon) can be in two quantum- mechanical states at the same time.
this is referred to as a superposition of states.
in a conventional computer, information is represented by bits with each bit assuming either the value 0 or 1.
in a quantum computer, in contrast, information is carried by quantum bits (or 'qubits') with each qubit able to assume the values 0 or 1 or any combination ('superposition') of the two.
one way of realizing a quantum computer is with a memory unit composed of atoms whose quantum states can be excited and manipulated in a controlled manner using laser light.
computational operations can then be performed simultaneously (or 'in parallel') on both parts of the superposition state (1 and 0).
in the time it takes for a 32- bit conventional computer to process one of its 232 possible states, a quantum computer can process all of these states in parallel.
the quantum computer can therefore carry out computations orders of magnitude faster than a normal computer.
however, quantum computing power can only be exploited for special problems for which appropriate quantum algorithms have been developed.
in many of the superposition states, the quantum bits are 'entangled', which means that the superposition can only be described as a whole and not in terms of the independent states of the particles involved.
however, both superposed and entangled states are highly sensitive to any interaction with their environment and rapidly lose their quantum character.
for quantum computing, this means that a great deal of effort has to be put into screening the system from environmental influences.
in another area of quantum technology, this sensitivity to environmental factors is being specifically exploited.
in the field of quantum communication, confidential information can be encoded in the form of entangled or superposed states.
anyone endeavouring to access the information would end up destroying the quantum state and the attempted interception would be discovered.
simulating molecules on quantum computers just got much easier with ibm's superconducting quantum hardware.
in a recent research article published in nature, hardware- efficient variational quantum eigensolver for small molecules and quantum magnets, we implement a new quantum algorithm capable of efficiently computing the lowest energy state of small molecules.
by mapping the electronic structure of molecular orbitals onto a subset of our purpose- built seven qubit quantum processor, we studied molecules previously unexplored with quantum computers, including lithium hydride (lih) and beryllium hydride (beh2).
the particular encoding from orbitals to qubits studied in this work can be used to simplify simulations of even larger molecule and we expect the opportunity to explore such larger simulations in the future, when the quantum computational power (or "quantum volume") of ibm q systems has increased.
while beh2 is the largest molecule ever simulated by a quantum computer to date, the considered model of the molecule itself is still simple enough for classical computers to simulate exactly.
this made it a test case to push the limits of what our seven qubit processor could achieve, further our understanding of the requirements to enhance the accuracy of our quantum simulations, and lay the foundational elements necessary for exploring such molecular energy studies.
the best simulations of molecules today are run on classical computers that use complex approximate methods to estimate the lowest energy of a molecular hamiltonian.
a "hamiltonian" is a quantum mechanical energy operator that describes the interactions between all the electron orbitals and nuclei of the constituent atoms.
the "lowest energy" state of the molecular hamiltonian dictates the structure of the molecule and how it will interact with other molecules.
such information is critical for chemists to design new molecules, reactions, and chemical processes for industrial applications.
qubit:orbital
although our seven qubit quantum processor is not fully error- corrected and fault- tolerant, the coherence times of the individual qubits last about 50 us.
it is thus really important to use a very efficient quantum algorithm to make the most out of our precious quantum coherence and try to understand molecular structures.
the algorithm has to be efficient in terms of number of qubits used and number of quantum operations performed.
our scheme contrasts from previously studied quantum simulation algorithms, which focus on adapting classical molecular simulation schemes to quantum hardware - and in so doing not effectively taking into account the limited overheads of current realistic quantum devices.
so instead of forcing classical computing methods onto quantum hardware, we have reversed the approach and asked: how can we extract the maximal quantum computational power out of our seven qubit processor?
our answer to this combines a number of hardware- efficient techniques to attack the problem:
first, a molecule's fermionic hamiltonian is transformed into a qubit hamiltonian, with a new efficient mapping that reduces the number of qubits required in the simulation.
a hardware- efficient quantum circuit that utilizes the naturally available gate operations in the quantum processor is used to prepare trial ground states of the hamiltonian.
the quantum processor is driven to the trial ground state, and measurements are performed that allow us to evaluate the energy of the prepared trial state.
the measured energy values are fed to a classical optimization routine that generates the next quantum circuit to drive the quantum processor to, in order to further reduce the energy.
iterations are performed until the lowest energy is obtained to the desired accuracy.
with future quantum processors, that will have more quantum volume, we will be able to explore the power of this approach to quantum simulation for increasingly complex molecules that are beyond classical computing capabilities.
the ability to simulate chemical reactions accurately, is conductive to the efforts of discovering new drugs, fertilizers, even new sustainable energy sources.
the experiments we detail in our paper were not run on our currently publically available five qubit and 16 qubit processors on the cloud.
but developers and users of the ibm q experience can now access quantum chemistry jupyter notebooks on the qiskit github repo.
on the five qubit system, users can explore ground state energy simulation for the small molecules hydrogen and lih.
notebooks for larger molecules are available for those with beta access to the upgraded 16- qubit processor.
amazon's aws cloud service on monday announced a fully managed service for developers and researchers to experiment with quantum computers from three hardware providers: d- wave, ionq and rigetti.
noting that quantum computers are expensive to build and run and must operate in a very extreme environment, an aws blogger argued that the cloud- based, on- demand model will better suit most organizations.
"it may well be the case that production- scale quantum computers are the first cloud- only technology," wrote jeff barr, chief evangelist for aws.
the service is called amazon braket (pronounced bra- ket), referring to a notation that denotes quantum mechanical states.
it will launch to customers in 2020, but previews were available monday.
boeing plans to collaborate with aws on potential applications for quantum computing, according to a statement.
industry survey
covid- 19 impact on sensors & electronics industry
as your resource for sensors and electronics news, fierceelectronics is committed to our community and want to bring you the latest information on how covid- 19 is impacting our industry and shine a light on the technologies being used to fight the pandemic.
we'd appreciate you taking this brief, 5- minute survey on how covid- 19 is impacting you and the information that you find most helpful now.
the survey findings, which we will share with the community, will help to shape our coverage moving forward.
aws also announced a research center for quantum computing next to caltech to accelerate development of quantum computing hardware and software.
also, a new quantum solutions lab has been created to connect aws customers with experts in quantum from amazon and its partners.
there aren't any commercial grade quantum computers, but ibm has offered access to customers to early quantum computers over its cloud, starting in 2016.
google announced a quantum experiment with its sycamore processor that generated random strings of numbers in three minutes that it argued would have taken the world's fastest conventional computers 10,000 years.
ibm disagreed and said the calculation could have been done in less than three days on supercomputers.
microsoft is also active in the quantum computing arena.
amazon's blog included a brief description of quantum computing.
ordinary computers use bits, either 0 or 1, while quantum computers use quantum bits, or qubits.
each qubit can operate as a 1 or a 0, but also both 1 and 0 simultaneously.
quantum computing is still very young technology but is considered valuable in expanding computing potential beyond the limits of moore's law.
the computing potential of quantum means that public key encryption could easily be broken, which is why government researchers for major countries are studying the technology.
a breakthrough into the full characterisation of quantum states has been published today as a prestigious editors' suggestion in the journal physical review letters.
the full characterisation (tomography) of quantum states is a necessity for future quantum computing.
however, standard techniques are inadequate for the large quantum bit- strings necessary in full scale quantum computers.
a research team from the quantum photonics laboratory at rmit university and equs at the university of sydney has demonstrated a new technique for quantum tomography- self- guided quantum tomography- which opens future pathways for characterisation of large quantum states and provides robustness against inevitable system noise.
dr alberto peruzzo, director of the quantum photonics laboratory, said: "this is a big step forward in quantum tomography.
our technique can be applied to all quantum computing architectures in laboratories around the world."
"characterising quantum states is a serious bottleneck in quantum information science.
self- guided quantum tomography uses a search algorithm to iteratively 'find' the quantum state.
"this technique significantly reduces the necessary resources by removing the need for any data storage or post- processing."
robert chapman, lead author and rmit phd student, said the technique employed was far more robust against inevitable noise and experimental errors than standard techniques.
"we experimentally characterise quantum states encoded in single photons- single particles of light.
"photons are a strong candidate for future quantum computing, however, our method can be applied to other quantum computing architectures, such as ion traps and superconducting qubits.
"any experiment suffers from measurement noise which degrades results.
in our experiment, we engineer the level of noise up to extreme levels to test the performance of our algorithm.
we show that self- guided quantum tomography is significantly more robust against noise than standard tomography.
"we hope research groups can employ our technique as a tool for characterising large quantum states and benefit future quantum technologies."
the research, "experimental demonstration of self- guided quantum tomography", has been published in physical review letters and can be accessed online.
researchers from the department of applied mathematics and the institute for quantum computing at the university of waterloo have developed a versatile new way of controlling quantum systems that can affect the reliability of experiments.
to develop quantum technologies, it is critical to first build capabilities to control extraordinarily fragile quantum systems.
the team from waterloo found a way to control a quantum system without exposing it to vibration or other interference.
the new technique was published in physical review a.
"the idea is to avoid interacting with a quantum system directly," said david layden, a master's student in the faculty of mathematics at waterloo and lead author of the paper.
"instead, you introduce a second, so- called auxiliary quantum system, such as an atom, for example.
you then manipulate it and use it to indirectly affect, and ultimately control, the main system."
researchers have already used indirect approaches to manipulating quantum systems in several different experiments.
but the techniques they used differed based on the particular laboratory setup involved.
each new type of experiment required a different technique.
now, the waterloo researchers' one- size- fits- all method of indirectly controlling quantum systems is applicable to any experiment.
it involves soft, frequent touches to the main system from the auxiliary one, which allow researchers to freely steer a quantum system while keeping its quantum nature intact.
"these touches are strong enough to fully control the target quantum systems, but short enough to avoid destroying their quantum properties," said professor eduardo martin- martinez, of both the department of applied mathematics and institute for quantum computing at waterloo, and a co- author of this work.
"to achieve this level of control, we must use an auxiliary system that also possesses quantum properties," said professor achim kempf, university research chair in the department of applied mathematics at waterloo, and co- author of the study.
the new technique could play an important role in a number of quantum technologies, which in turn, promise to impact a wide range of fields, from high performance computing to pharmaceutical drug discovery.
dartmouth college and griffith university researchers have devised a new way to "sense" and control external noise in quantum computing.
quantum computing may revolutionize information processing by providing a means to solve problems too complex for traditional computers, with applications in code breaking, materials science and physics, but figuring out how to engineer such a machine remains elusive.
the findings appear in the journal physical review letters.
"quantum noise spectroscopy" is an emerging field within quantum physics that seeks to characterize and control the noise affecting quantum systems.
quantum systems, which include tiny objects such as atoms, electrons and photons, display counterintuitive properties, such as the ability to be in a superposition of two different states simultaneously.
these quantum properties are essential for quantum computing, but they are easily lost through decoherence, when quantum systems are subject to "noise" in an external environment.
because a quantum system is always embedded in a larger environment, some noise is unavoidable.
a quantitative understanding of environmental noise is, therefore, crucial to accurately model the behavior of quantum systems and determine whether they can perform in applications such as quantum computing.
quantum noise spectroscopy offers an elegant solution to this challenge by using a quantum system as a "probe" of its own environment.
typically, an experimenter may control the state of a quantum system through the application of external fields, such as optical or magnetic fields.
in a quantum noise spectroscopy protocol, the quantum system is subjected to a "control sequence"- that is, a suitably designed application of these fields.
the quantum system evolves dynamically due to both the control sequence and the unavoidable interactions with the environment.
careful selection of control sequences combined with measurement of the quantum system enables the researchers to extract information about the environmental noise.
"prior to our work, quantum noise spectroscopy had two major shortcomings: it was restricted to environmental noise that was (1) classical and (2) gaussian," says co- author lorenza viola, a professor of physics at dartmouth.
"the assumption of gaussianity implies that the noise has very special properties- it can be fully describes solely in terms of "two- point correlation functions"- while the assumption of classicality precludes the possibility that the environment is itself in a quantum- mechanical regime.
these assumptions break down in many realistic situations of interest, which prohibits accurate and general characterization of environmental noise.
for example, superconducting qubits, one of the most promising systems for scalable quantum computing, are subject to noise with observable deviations from gaussianity."
in their new work, the dartmouth- griffith researchers designed a new family of control sequences and show how they can extract information about the higher- dimensional (beyond two- point) correlation functions of the noise.
knowledge of these correlation functions offers a complete characterization of the noise, enabling accurate modeling of the interaction between a quantum system and its environment.
the researchers demonstrate noise spectroscopy protocols that apply to both classical, non- gaussian and a class of paradigmatic quantum, non- gaussian environments.
to the researchers' knowledge, the study of higher dimensional correlation functions for quantum noise sources is an entirely new research area.
"quantum technologies have the potential to revolutionize computing and communication," viola says.
"one of the primary obstacles towards realizing these technologies in the lab, however, is the decoherence of quantum systems through interactions with the environment.
quantum noise spectroscopy characterizes environmental noise, enabling detailed dynamical modeling and offering physical insight into the process of decoherence.
this information can be used to devise strategies to optimize the protection of quantum systems from environmental noise.
previous work did not apply to quantum or non- gaussian noise sources, excluding a large class of quantum systems.
our work overcomes these limitations."
an organism in a life system has clear biological attributes such as self- replication, mutation, interaction, birth and death.
while these are readily apparent in humans, animals and microbes, just how small of a system can demonstrate these behaviors?
currently, there isn't a lone technology that can make that determination.
researchers from upv/ehu- university of the basque country's quantum technologies for information sciences (qutis) still wanted answers, so they combined two areas of study- artificial intelligence and quantum computing- to create virtual tiny life forms that can experience various phases of life just like real- world life forms.
"our research connects two previously unrelated areas as are artificial life and quantum computing," said researcher lucas lamata.
"the former is an extensive research field where the aim is to reproduce biological behaviors in artificial systems, while the latter is an area that [has been] growing fast in the past few years and could revolutionize computation and communication."
using an ibm qx4 quantum computer, the team set out to determine whether life behaviors occur at the macroscopic level of a dna module or at the few- atom level where quantum physics dominates.
their research has resulted in the first experimental realization of a quantum algorithm of artificial life following darwin's laws of evolution.
their established system adheres to a biomimetic protocol that encodes quantum behaviors with the same behaviors of living systems.
their simulated individuals, coined as a group as quantum life, are represented by two quantum bits, or qubits, that act as a genotype and phenotype.
the genotype holds the information that determines the type of living unit, which is transmitted from generation to generation.
the phenotype are the characteristics displayed by individuals.
in addition to being determined by genetic information, these characteristics are also determined by interaction with other individuals and the environment.
"the bases have been established for addressing different levels of classical and quantum complexity," said enrique solano, qutis director.
"for example, one could consider the growth of populations of quantum individuals with gender criteria, their life aims both as individuals and as groups, automated behaviors without external controls, quantum robotics processes, intelligent quantum systems- until the threshold of quantum supremacy that could only be reached by a quantum computer can be overcome.
what would emerge after that would be terribly risky questions, such as guessing the microscopic origin of life itself, the intelligent development of individuals and societies, or addressing the origin of awareness and animal and human creativity."
while their experiment cements their theoretical framework in the real world, the team knows that this is only the beginning.
this research is perhaps the start to answering the mysteries of life, as well as setting humanity on a new path moving forward.
"we may easily find several applications, still to be developed, around quantum game theory and optimization problems," solano said.
"the latter are commonplace for applications in econom[ic], design, aerodynamics and complex biological systems.
the natural merge of this research with artificial intelligence methods will create a novel paradigm for exploring the growth of complexity, an important asset of present and future studies from molecular systems to astrophysical objects and social behaviors."
interested in more ways the quantum computing is edging the world closer to a fourth industrial revolution?
check out micius satellite enables intercontinental quantum communication and what is quantum cryptography and how exactly can it benefit iot?
ionq, the leader in quantum computing, today announced it has secured funding from lockheed martin, robert bosch venture capital gmbh (rbvc) and cambium, a new multi- stage venture capital firm focused on investments in the future of computational paradigms.
this funding contributes to ionq's series b round, bringing the company's total amount raised to $84 million.
additionally, ionq today announced the addition of four new advisory board members - luminaries in quantum computing, information science and technology.
ionq's trapped- ion quantum computers have demonstrated unmatched ability, demonstrating performance benchmarks that no other quantum computer has been able to match.
the company's hardware also outperforms all other available quantum hardware.
with this new investment, ionq is well positioned to lead the hardware field into the future.
in addition, ionq is thrilled to announce additions to its advisory team, including:
umesh vazirani, roger a. strauch professor of electrical engineering and computer sciences and the co- director of the berkeley quantum computation center (bqic)
david wineland, nobel laureate and philip h. knight distinguished research chair, university of oregon, department of physics
margaret (peg) williams, former senior vice president of research and development, cray inc.
kenneth brown, associate professor at duke university, department of electrical and computer engineering
through their work, the advisors will continue to provide direct support around ionq's algorithmic development and benchmarking, hardware design, and efforts on trapped- ion atomic physics.
"this additional backing marks a watershed moment for ionq," says peter chapman, ionq ceo & president.
"with investors and advisors at the forefront of fueling the growth of quantum computing, we are well- positioned to move the world forward through the power of quantum."
"lockheed martin has been an early supporter of quantum computing research," said christopher moran, vice president and general manager of lockheed martin ventures.
"and we believe trapped- ion technology offers the potential for outstanding coherence and operation fidelity.
as government customers are increasingly interested in quantum computing systems, we believe this investment will allow us to remain at the forefront of quantum computing."
"since our investment, ionq continues to impress us with its stellar scientific and business team," says jan westerhues, investment partner at rbvc.
"we are proud to be part of commercial quantum computers becoming a reality."
"we are excited to work with ionq to bring quantum computers to the market," says cambium managing partner landon downs.
"quantum computers will fundamentally change what is possible to compute, and we believe that ionq is well positioned for both near- term and long- term success."
ionq previously announced $55m in funding from samsung electronics, mubadala capital, gv, amazon, and nea.
the company has made its quantum computers available via the cloud through strategic partnerships with amazon aws braket and microsoft azure quantum.
about ionq
ionq is the leader in quantum computing.
by making our quantum hardware accessible through the cloud, we're empowering millions of organizations and developers to build new applications to solve the world's most complex problems in business, and across society.
ionq's unique approach to quantum computing is to start with nature: using individual atoms as the heart of our quantum processing units.
we levitate them in space with electric potentials applied to semiconductor- defined electrodes on a chip, and then use lasers to do everything from initial preparation to final readout and the quantum gate operations in between.
it requires atomic physics, precision optical and mechanical engineering, and fine- grained firmware control over a variety of components.
leveraging this approach, ionq provides both a viable technological roadmap to scale and the flexibility necessary to explore a wide range of application spaces in the near term.
ionq was founded in 2015 by jungsang kim and christopher monroe and their systems are based on foundational research at the university of maryland and duke university.
james sanders spoke with julie love, senior director, microsoft quantum business development, at microsoft ignite 2019 in orlando about microsoft's role in quantum computing.
the following is a transcript of the interview.
julie love: at microsoft quantum, we're all about quantum impact.
delivering solutions, helping our customers solve their toughest compute challenges by building the world's most scalable compute stack.
it's really this ambition for scale that we took the next step on our quantum journey to deliver impact through announcing azure quantum.
azure quantum is a full end- to- end open cloud ecosystem for quantum development, really bringing the tools of quantum computing to developers at organizations, people around the world.
we're doing this building on the work that we've been doing on applications, solving tough problems like the ones that we solved with case western reserve university that satya nadella talked about this morning, our work on quantum cryptography and post quantum cryptography to secure our data in a quantum world, building on q#, our quantum native programming language in the quantum development kit, which has now been downloaded more than 200,000 times by developers around the world, and breakthroughs that we've had across devices, cryogenic controllers across the whole stack.
we've announced azure quantum to bring this ecosystem to the world together with partners.
we've partnered with ionq, honeywell, qci, and 1qbit to bring the most diverse, most scalable quantum solutions across software solutions and hardware.
cryptography is one of the areas where there's been a lot of excitement and talk because there's a notable quantum algorithm which will break modern cryptography.
microsoft is at the forefront of developing post quantum cryptographic solutions, so these are cryptographic protocols which are resistant to both classical and quantum attacks.
i think it's hard to say when exactly that will be a reality.
there is advancements happening every day across all elements of the stack.
but, we look at it from a position, is if you believe that quantum is coming, which we do, we think it's important to get ahead of this technology.
if you think that might be a possibility within the next decade, it's time for organizations to start implementing these quantum resistant protocols now, so ensuring that they have quantum agility to be able to shift their cryptographic systems to new protocols, and to start testing these systems in their environment today.
we've had quantum software efforts at microsoft for a long time, and as we developed earlier systems that we used for programming scalable quantum computers, we discovered that we really needed a new language.
and so, we developed a quantum- native language and it really understands the language of quantum mechanics.
there's a lot of things that come with having a quantum native language that make it easier and more scalable for developers to develop for that language.
for example, in quantum mechanics and quantum computing, all operations have to be reversible.
and so, q# has built in the mechanisms to take care of that for developers, and so you're not having to take care of that overhead for yourself.
it's allowed us to create really a modern programming environment with the abstraction layers needed for developers to create durable, scalable code.
our aim is also to make this incredibly accessible for developers.
we see this as a need, not just to create a language for physicists working in the lab, but for programmers around the world.
this goes back to the whole aspiration for impact.
for us to achieve the impact that we want to have from quantum computing, we need this to be accessible for a whole range of people.
not just programmers, but domain experts in areas like quantum chemistry, material science, optimization.
we've expanded on what we've offered with q# and the quantum development kit to do integrations with languages like python.
classical computers will continue to play a huge role in the quantum future.
if you think about the environment that we've deployed in azure today, it's a highly heterogeneous compute fabric where you have cpus, gpus, fpgas deployed at scale, and those are accelerators to the main core classical workloads.
for the foreseeable future, we see quantum as being one of those acceleration options that will exist within azure.
also see
as quantum technology continues to come into its own, investment is happening on a global scale.
soon, we could see improvements in machine learning models, financial risk assessment, efficiency of chemical catalysts and the discovery of new medications.
as numerous scientists, companies and governments rush to invest in the new era of quantum technology, a crucial piece of this wave of innovation is the quantum sensor.
improving these devices could mean more powerful computers, better detectors of disease and technological advances scientists can't even predict yet.
a scientific study from the university of chicago's institute for molecular engineering published oct. 17 in nature communications could have exciting implications for the developing world of quantum sensing- and quantum technology as a whole.
"we took a recently proposed idea to make better optical classical sensors and asked whether the same idea would work in a quantum setting," said aashish clerk, one of the study's authors and a professor at the institute for molecular engineering.
"we found that this idea doesn't really work in quantum settings, but that another somewhat related approach could give you a huge advantage."
in a quantum setting, optical sensors are typically limited because light is made up of particles, and this discreteness leads to unavoidable noise.
but this study revealed an unexpected method to combat that limitation.
"we think we've uncovered a new strategy for building extremely powerful quantum sensors," clerk continued.
the path to the directional principle
clerk and co- author hoi- kwan lau, a postdoctoral scholar at uchicago, were inspired by recent high- profile studies that showed how to drastically enhance a common optical sensing technique.
the "trick" involves tuning systems to an exceptional point, or a point at which two or more modes of light come together at one specific frequency.
lau and clerk wanted to see whether this method could succeed in settings where quantum effects were important.
the goal was to account for unavoidable "quantum" noise- fluctuations associated with the fact that light has both a wave- like and a particle- like character, clerk explained.
the study found the exceptional point technique to be unhelpful in a quantum setting, but the research still led to promising results.
"the good news is we found another way to build a powerful new type of sensor that has advantages even in quantum regimes," clerk said.
"the idea is to construct a system that is 'directional,' meaning photons can move in one direction only."
this directional principle- one based on photons being able to move in only one direction- is a brand- new development in quantum sensing.
new developments in quantum sensing
in terms of real- world applications, highly effective quantum sensors could be game- changing.
quantum systems are sensitive to the slightest environmental changes, so these detectors have the potential to be incredibly powerful.
in addition, some of the stranger aspects of quantum behavior, such as quantum entanglement, could make them even stronger.
quantum entanglement, a puzzling phenomenon even for scientists, describes how two particles can be separated by a vast distance yet actions performed on one particle immediately affect the other.
this entanglement can be harnessed to make quantum sensors surprisingly resilient against certain kinds of noise.
in the future, new developments in quantum sensing could translate to significant advances in a variety of areas.
the class of optical sensors described in the study can be used to detect viruses in liquids, for example.
they also can act as readout devices for quantum bits in a superconducting quantum computer.
"we think our idea has the potential to generate major improvements in many of these applications," clerk explained.
the study's implications for quantum computing are especially exciting.
not only do quantum computers have the potential to dramatically increase computing speeds, but they could also tackle problems that are completely unfeasible with traditional computing.
lau and clerk plan to do further research on their enhanced sensing technique.
clerk still has a lot of questions: "what sets how fast our sensor is?
are there fundamental limits on its speed?
can it be used to detect signals that aren't necessarily small?"
their biggest hope, clerk explained, is to inspire other researchers to build improved quantum sensors that harness this newly uncovered principle.
adding an interesting wrinkle to the esoteric field of quantum computing, researchers have found a way to use quantum states to create software applications.
ibm corp. had been experimenting with creating algorithms out of quantum mechanical forces, but a team at the company's almaden research laboratory, working with microsoft corp. scientists, has devised a way to create one- use- only software by exploiting quantum states.
published in the latest edition of the journal nature , the results are purely theoretical and probably will not become products for decades.
but the research does extend the already- bizarre possibilities of quantum computing, uncovering "a regime of physics we have never encountered before," said isaac chuang, ibm almaden researcher.
the idea stems from quantum teleportation, the recently discovered ability to transmit quantum states without sending any quantum information.
if those states could be stored ahead of time, they could act as prefab software: the states would be transmitted to a user and would represent bits that had been run through a particular function or algorithm.
transmission would occur over a type of "quantum internet," a phrase coined by researchers at los alamos national laboratory.
the act of using the software would alter the quantum states and thus destroy the software.
in part, that's why microsoft was involved in this area of chuang's research.
quantum computing exploits quantum mechanics to do otherwise- impossible computations.
in particular, if an atomic nucleus can be put in a state between two stable energy levels, quantum mechanics holds that it will exist at both levels at the same time, essentially being in two places at once.
in computing applications, it means a bit can be a 0 and a 1 simultaneously.
this means that a quantum computer could examine multiple possibilities of a problem it could study both sides of a coin with only one toss, for example.
a more telling example lies in databases.
chuang and his colleagues have devised a three- qubit (quantum bit) machine that can search an eight- item database for one "special" item.
a standard computer will average just under four tries to find the special item.
for the quantum computer, which can test multiple possibilities simultaneously, the average is just under two.
the quantum bits are particular nuclei that exhibit "coherence" the ability to stay in the simultaneous 0- and- 1 state for useful amounts of time.
different atoms will become coherent under different pulses of magnetism, so to perform particular computations, molecules must be found that combine the desired nuclei.
"i've learned a lot of chemistry doing this," chuang said.
multiple methods for quantum computing are being explored.
ibm almaden is using magnetic spins to record information, controlling nuclear states using electromagnetic pulses at particular frequencies.
using these methods, chuang and his colleagues have developed the largest quantum- computing systems to date.
quantum software would act on a principle called "teleportation," demonstrated two years ago.
if two particles share an "entangled pair" of photons, they can communicate quantum states without sending any information.
thus, it's conceivable that an entangled pair could be created that would receive a set of quantum bits as if a particular function had been performed on them.
such "software" could be used only once; per heisenberg's principle, the act of viewing the quantum states alters them.
in addition, the entangled pair of photons is fragile and must be precisely prepared.
chuang originally researched optical networking but was inspired to investigate quantum computing by a book of essays by the late physicist richard feynman, who predicted quantum computing decades ago.
after decades of miniaturization, the electronic components we've relied on for computers and modern technologies are now starting to reach fundamental limits.
faced with this challenge, engineers and scientists around the world are turning toward a radically new paradigm: quantum information technologies.
quantum technology, which harnesses the strange rules that govern particles at the atomic level, is normally thought of as much too delicate to coexist with the electronics we use every day in phones, laptops and cars.
however, scientists with the university of chicago's pritzker school of molecular engineering announced a significant breakthrough: quantum states can be integrated and controlled in commonly used electronic devices made from silicon carbide.
"the ability to create and control high- performance quantum bits in commercial electronics was a surprise," said lead investigator david awschalom, the liew family professor in molecular engineering at uchicago and a pioneer in quantum technology.
"these discoveries have changed the way we think about developing quantum technologies- perhaps we can find a way to use today's electronics to build quantum devices."
in two papers published in science and science advances, awschalom's group demonstrated they could electrically control quantum states embedded in silicon carbide.
the breakthrough could offer a means to more easily design and build quantum electronics- in contrast to using exotic materials scientists usually need to use for quantum experiments, such as superconducting metals, levitated atoms or diamonds.
these quantum states in silicon carbide have the added benefit of emitting single particles of light with a wavelength near the telecommunications band.
"this makes them well suited to long- distance transmission through the same fiber- optic network that already transports 90 percent of all international data worldwide," said awschalom, senior scientist at argonne national laboratory and director of the chicago quantum exchange.
moreover, these light particles can gain exciting new properties when combined with existing electronics.
for example, in the science advances paper, the team was able to create what awschalom called a "quantum fm radio;" in the same way music is transmitted to your car radio, quantum information can be sent over extremely long distances.
"all the theory suggests that in order to achieve good quantum control in a material, it should be pure and free of fluctuating fields," said graduate student kevin miao, first author on the paper.
"our results suggest that with proper design, a device can not only mitigate those impurities, but also create additional forms of control that previously were not possible."
in the science paper, they describe a second breakthrough that addresses a very common problem in quantum technology: noise.
"impurities are common in all semiconductor devices, and at the quantum level, these impurities can scramble the quantum information by creating a noisy electrical environment," said graduate student chris anderson, a co- first author on the paper.
"this is a near- universal problem for quantum technologies."
but, by using one of the basic elements of electronics- the diode, a one- way switch for electrons- the team discovered another unexpected result: the quantum signal suddenly became free of noise and was almost perfectly stable.
"in our experiments we need to use lasers, which unfortunately jostle the electrons around.
it's like a game of musical chairs with electrons; when the light goes out everything stops, but in a different configuration," said graduate student alexandre bourassa, the other co- first author on the paper.
"the problem is that this random configuration of electrons affects our quantum state.
but we found that applying electric fields removes the electrons from the system and makes it much more stable."
by integrating the strange physics of quantum mechanics with well- developed classical semiconductor technology, awschalom and his group are paving the way for the coming quantum technology revolution.
"this work brings us one step closer to the realization of systems capable of storing and distributing quantum information across the world's fiber- optic networks," awschalom said.
"such quantum networks would bring about a novel class of technologies allowing for the creation of unhackable communication channels, the teleportation of single electron states and the realization of a quantum internet."
more information: christopher p. anderson et al.
electrical and optical control of single spins integrated in scalable semiconductor devices, science (2019).
doi: 10.1126/science.aax9406
kevin c. miao et al.
electrically driven optical interferometry with spins in silicon carbide, science advances (2019).
doi: 10.1126/sciadv.aay0527
you hear a lot about quantum computers.
how they'll be super fast and super powerful.
there are even companies claiming to make the first simple versions of quantum computers.
but what makes a computer "quantum?"
here are five things to know about quantum computers.
1.
quantum computers use qubits.
while classical computers encode bits as zeros and ones.
qubits can be one, zero or a superposition of both.
2.
because qubits can be in multiple states at once, a quantum computer has inherent paralellism.
that means a while your computer can work on one thing at a time, albeit very fast on today's processors, quantum computers can work on millions of things a at a time.
3.
quantum computers will be best at factoring large numbers, making them super fast at breaking encryption or searching a large database.
4.
quantum computers can read data without looking at it.
measuring a qubit can change its state and affect the outcome.
so quantum computers entangle atoms, meaning one atom always reflects the state of another.
that way you can know what state the first atom is without measuring it and changing its state.
5.
there's debate about whether we're really there yet.
the uncertainty principle in this case is just how quantum our computers are.
companies like d- wave use quantum principles in their computing but most agree that practical quantum computers are still years away.
i know what you're thinking.
you're in a superposition of both understanding and not understanding quantum computers.
well here's more from techrepublic to help you out:
see: quantum computing: the smart person's guide
see: d- wave quantum computers: the smart person's guide
also see:
physicists at the university of calgary and at the institute for quantum computing in waterloo have published new research in nature physics which builds on the original ideas of einstein and adds a new ingredient: a third entangled particle.
quantum entanglement is one of the central principles of quantum physics, which is the science of sub- atomic particles.
multiple particles, such as photons, are connected with each other even when they are very far apart and what happens to one particle can have an effect on the other one at the same moment, even though these effects can not be used to send information faster than light.
the new form of three- particle entanglement demonstrated in this experiment, which is based on the position and momentum properties of photons, may prove to be a valuable part of future communications networks that operate on the rules of quantum mechanics, and could lead to new fundamental tests of quantum theory that deepen our understanding of the world around us.
"this work opens up a rich area of exploration that combines fundamental questions in quantum mechanics and quantum technologies," says christoph simon, paper co- author and researcher at the university of calgary.
this research extends the theories of einstein, seventy- seven years later.
in 1935, albert einstein, boris podolsky, and nathan rosen, commonly referred to as epr, published a thought experiment designed to show that quantum mechanics, by itself, is not sufficient to describe reality.
using two entangled particles epr tried to demonstrate that there must be some hidden parameters that quantum mechanics does not account for.
later john bell and others showed that the kind of hidden parameters epr had in mind are incompatible with our observations.
the mystery at the heart of quantum mechanics thus remains intact.
but the entanglement first proposed by epr is now a valuable resource in emerging quantum technologies like quantum computing, quantum cryptography, and quantum precision measurements.
"it is exciting, after all this time, to be able to finally create, control, and entangle, quantum particles in this new way.
using these new states of light it may be possible to interact with and entangle distant quantum computer memories based on exotic atomic gases, " says thomas jennewein, whose group at the university of waterloo carried out the experiment.
the next step for the researchers is to try to combine the position and momentum entanglement between their three photons with more traditional types of entanglement based on angular momentum.
this will allow the creation of hybrid quantum systems that combine multiple unique properties of light at the same time.
when it comes to finding a vaccine that can halt and eradicate the deadly covid- 19 virus, today's supercomputers can only do so much.
while supercomputers can do amazing things, they are not complex enough to find answers to nature's deepest and most complicated secrets, such as quickly and carefully mapping out the molecular structures of viruses so they can be defeated with modern medicines and treatments.
but an answer awaits perhaps five to 10 years away in the form of quantum computers, which are exponentially more powerful than traditional classic computers, according to computer scientists and other researchers.
see: coronavirus: critical it policies and tools every business needs (techrepublic premium)
recently a public- private partnership was formed to create a covid- 19 high performance computing consortium, which is working to harness the power of high- performance computing resources to massively increase the speed and capacity of coronavirus research.
and though that work is today welcome in the fight against covid- 19, it won't unlock all the incredibly difficult secrets that are held closely by such viruses.
for most pharmaceutical companies, supercomputers are used regularly to help research, find, and identify new drug treatments, including the identification of virus structures so cures can be found.
yet supercomputers used today in virus and other pharmaceutical research are still based on classical computing architectures that view all data as a series of binary bits with a value of zero or one.
those machines face the limitations of modern bit- based computer architectures and power that is available today but can't theoretically or physically handle all the tremendously detailed research that is still needed.
that's where the future promise of quantum computing is expected to one day provide the vast computational power that could allow researchers to truly map out molecular structures in real time to solve medical mysteries and help quickly identify new drugs and treatments, said chirag dekate, a supercomputing and high- performance computing analyst with gartner.
"if you're trying to do a quantum realistic simulation of the molecules and interactions of a virus, that is where classical computing starts falling short," dekate said.
"in classical computing, what you are able to simulate is only a fraction of what you can do with quantum computing."
the problem, though, is that true quantum computing capabilities are probably at least five to 10 years away from actual use, dekate said.
"when two molecules or compounds interact, in order to do a quantum computing simulation, you have to be able to simulate the electrostatic forces of the interaction at the atomic level between those things," dekate said.
"this is where the computational complexity increases exponentially," requiring the power of quantum computing over traditional classical computing architecture.
see: coronavirus: what business pros need to know (techrepublic)
quantum computers are based on qubits rather than bits, which are far more complex and allow information to be stored in new ways, giving them added dimensions of computing power.
but that intense power requires many more technical requirements to make it possible, and much work is still to be done to enable the technology.
dr. itamar sivan, a physicist and the founder and ceo of quantum machines, a quantum computing technology company, said the promise of quantum computing will someday help during times of crisis, such as today's coronavirus pandemic.
such machines are expected to be able to solve incredibly complex scientific problems in minutes in the future, compared with many years by even the most powerful supercomputers of 2020.
"quantum computing is not a new field- it is already decades old," sivan said.
"in academia it is being investigated, and in the last five years in industry as well.
the interest in quantum computing stems from a promise of immense computational power that we will never be able to achieve with classical computation."
see: quantum computing: when to expect the next major leap (techrepublic)
for researchers, quantum machines will provide power that will transform medical research and a wide range of other fields, he said.
"if you would want to have an exact simulation of a molecule such as penicillin, you would never be able to do it with any classical computer because it is too complex.
but quantum computers with hundreds of logical qubits will be able to do this task."
just how much more powerful is a quantum computer compared with a classical computer?
"in order to explain the information in a quantum computer with 300 qubits you would need a classical processor which is built from more bits than there are atoms are in the universe," sivan said.
"it's one of the toughest moonshots that we face as a society, but if we can do it it's going to change the whole world."
sivan agreed that such machines are easily a decade away before they would be able to perform the quantum simulations that are needed for virus research breakthroughs.
"for some problems, it's not about just running an algorithm faster, it's about making the impossible possible," he said.
"this is why in drug discovery today, the majority of the process is done with the molecules themselves in test tubes and culture dishes, because you can't simulate them and look at their reactions and behavior using classic computers."
the challenges of achieving usable quantum computing are huge, including the extremely delicate state of quantum data when it is used.
in operation, quantum data is rapidly lost in experiments being done over the last few years, preventing stable use of the machines.
"there are immense challenges all over the stack to get to the holy grail of quantum computing," sivan said.
"once we solve the problem of loss of information, we will be fine."
the coronavirus has infected almost 2 million people and killed 121,000 around the world so far.
while many patients with covid- 19 have mild symptoms and don't require hospitalization, with the incredibly wide scale of the pandemic, even at a 5% hospitalization rate large numbers of patients have been requiring emergency care in hospitals and other medical facilities that are struggling to keep up.
also see
researchers with the department of energy's oak ridge national laboratory have demonstrated a new level of control over photons encoded with quantum information.
their research was published in optica.
joseph lukens, brian williams, nicholas peters, and pavel lougovski, research scientists with ornl's quantum information science group, performed distinct, independent operations simultaneously on two qubits encoded on photons of different frequencies, a key capability in linear optical quantum computing.
qubits are the smallest unit of quantum information.
quantum scientists working with frequency- encoded qubits have been able to perform a single operation on two qubits in parallel, but that falls short for quantum computing.
"to realize universal quantum computing, you need to be able to do different operations on different qubits at the same time, and that's what we've done here," lougovski said.
according to lougovski, the team's experimental system -  two entangled photons contained in a single strand of fiber- optic cable -  is the "smallest quantum computer you can imagine.
this paper marks the first demonstration of our frequency- based approach to universal quantum computing."
"a lot of researchers are talking about quantum information processing with photons, and even using frequency," said lukens.
"but no one had thought about sending multiple photons through the same fiber- optic strand, in the same space, and operating on them differently."
the team's quantum frequency processor allowed them to manipulate the frequency of photons to bring about superposition, a state that enables quantum operations and computing.
unlike data bits encoded for classical computing, superposed qubits encoded in a photon's frequency have a value of 0 and 1, rather than 0 or 1.
this capability allows quantum computers to concurrently perform operations on larger datasets than today's supercomputers.
using their processor, the researchers demonstrated 97 percent interference visibility -  a measure of how alike two photons are -  compared with the 70 percent visibility rate returned in similar research.
their result indicated that the photons' quantum states were virtually identical.
the researchers also applied a statistical method associated with machine learning to prove that the operations were done with very high fidelity and in a completely controlled fashion.
"we were able to extract more information about the quantum state of our experimental system using bayesian inference than if we had used more common statistical methods," williams said.
"this work represents the first time our team's process has returned an actual quantum outcome."
williams pointed out that their experimental setup provides stability and control.
"when the photons are taking different paths in the equipment, they experience different phase changes, and that leads to instability," he said.
"when they are traveling through the same device, in this case, the fiber- optic strand, you have better control."
stability and control enable quantum operations that preserve information, reduce information processing time, and improve energy efficiency.
the researchers compared their ongoing projects, begun in 2016, to building blocks that will link together to make large- scale quantum computing possible.
"there are steps you have to take before you take the next, more complicated step," peters said.
"our previous projects focused on developing fundamental capabilities and enable us to now work in the fully quantum domain with fully quantum input states."
lukens said the team's results show that "we can control qubits' quantum states, change their correlations, and modify them using standard telecommunications technology in ways that are applicable to advancing quantum computing."
once the building blocks of quantum computers are all in place, he added, "we can start connecting quantum devices to build the quantum internet, which is the next, exciting step."
much the way that information is processed differently from supercomputer to supercomputer, reflecting different developers and workflow priorities, quantum devices will function using different frequencies.
this will make it challenging to connect them so they can work together the way today's computers interact on the internet.
this work is an extension of the team's previous demonstrations of quantum information processing capabilities on standard telecommunications technology.
furthermore, they said, leveraging existing fiber- optic network infrastructure for quantum computing is practical: billions of dollars have been invested, and quantum information processing represents a novel use.
the researchers said this "full circle" aspect of their work is highly satisfying.
"we started our research together wanting to explore the use of standard telecommunications technology for quantum information processing, and we have found out that we can go back to the classical domain and improve it," lukens said.
lukens, williams, peters, and lougovski collaborated with purdue university graduate student hsuan- hao lu and his advisor andrew weiner.
the research is supported by ornl's laboratory directed research and development program.
a new angry birds- style game is set to help launch a new understanding of quantum science.
some find the concepts of quantum science confusing or unintuitive.
einstein even called quantum effects "spooky."
to help people better understand some of the core concepts of quantum science, the institute for quantum computing (iqc) at the university of waterloo is launching a game - the quantum cats.
the game uses cats with the behaviours of four key quantum terms - classical, superposition, tunnelling and uncertainty - to save the kittens trapped in boxes in the game's world.
players launch the various cats and use their quantum behaviours to break open the boxes and save the kittens.
quantum cats was developed in partnership with the university of waterloo games institute.
students and faculty members from across campus collaborated in the ideation, the science and development of the game.
quantum technologies are emerging from research labs around the world faster and faster.
from highly secure communications to ultra- sensitive devices to powerful quantum computers, these technologies promise to transform how we live, work and play.
"this game is a great way for people to become familiar with concepts of quantum science," said tobi day- hamilton, associate director, communications and strategic initiatives at iqc.
"we wanted to take science that people think is hard and make it fun.
working with the games institute, we were able to create something that exceeded our expectations."
quantum cats is available for download from the google play app store now and will be available soon in itunes and blackberry world.
four cats are introduced through the game:
classy - a classic cat.
she acts like you would expect - you know where she's going and where she's been.
she follows the laws of newton and galileo.
schro - you never know if schro is in the box or if he's not.
he has the ability to be in multiple states at the same time.
he could be here, he could be there or both!
digger - watch this cat!
he will suddenly appear on the other side of a barrier, even when he doesn't have the energy to go over it or break it down.
fuzzy - you are never certain where you'll find this cat.
there's a fundamental limit on how much information you can know about her.
"collaborating with iqc and their students on this game was ideal," says neil randall, director of waterloo's games institute.
"iqc brought the scientific knowledge and we brought the gaming knowledge.
together, we created a multi- disciplinary team that came up with a game that shares a few complex ideas with the world.
the result demonstrates the effectiveness of fun games as a teaching tool, something the games institute is committed to."
the institute for quantum computing is a world- class research centre in quantum information science and technology at the university of waterloo.
top experimentalists and theorists are making powerful new advances at iqc, deepening the understanding of quantum information and accelerating the development of quantum technology with applications from quantum computing to quantum sensors, unbreakable cryptography to new quantum materials.
since its founding in 2002, iqc has become an engine driving the creation of quantum information science and technology.
iqc is sparking commercialization initiatives that will benefit society for generations to come, transforming the way we work, live and play, and establishing waterloo as canada's quantum valley.
quantum information algorithms have the potential to solve some problems exponentially faster than current classical methods.
however, most research on quantum information systems has concentrated on models that use multiple quantum bits.
in a new study, physicists have demonstrated how to solve a difficult classical problem that completely encapsulates a quantum model that requires only one quantum bit.
the scientists, gina passante, et al., from the university of waterloo in ontario, canada, have presented their experimental results for the quantum solution of the approximation of the jones polynomial, which is a knot invariant.
by approximating the jones polynomial, researchers can determine whether two knots are different.
making this distinction is a fundamental problem in knot theory, and has applications in statistical mechanics, quantum field theory, and quantum gravity.
although approximation of the jones polynomial is a classical problem that is very difficult to solve, the results show that the problem can be solved using a "one quantum bit model."
the study is published in a recent issue of physical review letters.
in order to approximate the jones polynomial, the scientists implemented a quantum algorithm called deterministic quantum computation with one quantum bit (dqc1).
for the purposes of the algorithm, the knots were written as braids, or a series of strands crossing over and under each other with the top and bottom ends connected.
given the braid representations of different knots, the quantum algorithm could distinguish between distinct knots.
as the researchers explain, the dqc1 algorithm extracts the power of one bit of quantum information alongside a register of several qubits.
"the 'one quantum bit model' means that there is one initialized qubit in the experiment, meaning that we can only control the initial state of one qubit," passante told physorg.com.
"for four qubits, the other three qubits are initially in a completely random state."
the scientists experimentally implemented the algorithm using a liquid state nuclear magnetic resonance (nmr) quantum information processor.
they implemented the model with the molecule transcrotonic acid, with its four carbon nuclei representing the algorithm's four qubits.
then the researchers generated radio frequency pulses, starting randomly and improving through iterations.
"successful experimental implementation of this algorithm relies on our ability to manipulate the qubits to perform the unitary transformations," passante said.
"these manipulations must be done very accurately and quickly in order to get a reliable result, since the quantum states are very fragile."
in simulations, the researchers found that, in the case of knots whose braid representations have four strands and three crossings, the algorithm could identify distinct knots 91% of the time.
in the future, the scientists plan to apply the quantum algorithm to larger knots, and determine what size knot can be experimentally implemented before noise and control errors destroy the quantum advantage.
"this work demonstrates the use of an nmr quantum computer to solve an important and practical problem that is not feasible on classical computers," passante said.
"it is the first experimental implementation of a complete problem for the class of dqc1.
in the near future, quantum information processing devices hope to solve exciting problems with countless applications, and this experiment is an important stepping stone to realizing larger quantum computers."
more information: g. passante, o. moussa, c.a.
ryan, and r. laflamme.
"experimental approximation of the jones polynomial with one quantum bit."
physical review letters 103, 250501 (2009)
a team of cambridge researchers have found a way to control the sea of nuclei in semiconductor quantum dots so they can operate as a quantum memory device.
quantum dots are crystals made up of thousands of atoms, and each of these atoms interacts magnetically with the trapped electron.
if left alone to its own devices, this interaction of the electron with the nuclear spins, limits the usefulness of the electron as a quantum bit- a qubit.
led by professor mete atature, a fellow at st john's college, university of cambridge, the research group, located at the cavendish laboratory, exploit the laws of quantum physics and optics to investigate computing, sensing or communication applications.
atature said: "quantum dots offer an ideal interface, as mediated by light, to a system where the dynamics of individual interacting spins could be controlled and exploited.
because the nuclei randomly 'steal' information from the electron they have traditionally been an annoyance, but we have shown we can harness them as a resource."
the cambridge team found a way to exploit the interaction between the electron and the thousands of nuclei using lasers to 'cool' the nuclei to less than 1 millikelvin, or a thousandth of a degree above the absolute zero temperature.
they then showed they can control and manipulate the thousands of nuclei as if they form a single body in unison, like a second qubit.
this proves the nuclei in the quantum dot can exchange information with the electron qubit and can be used to store quantum information as a memory device.
the findings have been published in science today.
quantum computing aims to harness fundamental concepts of quantum physics, such as entanglement and superposition principle, to outperform current approaches to computing and could revolutionise technology, business and research.
just like classical computers, quantum computers need a processor, memory, and a bus to transport the information backwards and forwards.
the processor is a qubit which can be an electron trapped in a quantum dot, the bus is a single photon that these quantum dots generate and are ideal for exchanging information.
but the missing link for quantum dots is quantum memory.
atature said: "instead of talking to individual nuclear spins, we worked on accessing collective spin waves by lasers.
this is like a stadium where you don't need to worry about who raises their hands in the mexican wave going round, as long as there is one collective wave because they all dance in unison.
"we then went on to show that these spin waves have quantum coherence.
this was the missing piece of the jigsaw and we now have everything needed to build a dedicated quantum memory for every qubit."
in quantum technologies, the photon, the qubit and the memory need to interact with each other in a controlled way.
this is mostly realised by interfacing different physical systems to form a single hybrid unit which can be inefficient.
the researchers have been able to show that in quantum dots, the memory element is automatically there with every single qubit.
dr. dorian gangloff, one of the first authors of the paper and a fellow at st john's, said the discovery will renew interest in these types of semiconductor quantum dots.
dr. gangloff explained: "this is a holy grail breakthrough for quantum dot research- both for quantum memory and fundamental research; we now have the tools to study dynamics of complex systems in the spirit of quantum simulation."
the long term opportunities of this work could be seen in the field of quantum computing.
last month, ibm launched the world's first commercial quantum computer, and the chief executive of microsoft has said quantum computing has the potential to 'radically reshape the world'.
gangloff said: "the impact of the qubit could be half a century away but the power of disruptive technology is that it is hard to conceive of the problems we might open up- you can try to think of it as known unknowns but at some point you get into new territory.
we don't yet know the kind of problems it will help to solve which is very exciting."
quantum machines (qm), creators of the quantum orchestration platform, a complete hardware and software solution, has announced that they have agreed to join ibm's q network.
as part of the ibm and qm collaboration, a compiler between ibm's quantum computing programming languages, and those of qm will be developed and offered to customers.
the compiler will include the algorithmics required to translate different programming languages and will be provided alongside a quantum algorithms library.
the compiler is expected to be available in q2 of 2020.
the ibm q network brings together a diverse group of startups, universities, research labs, and fortune 500 companies including, the university of oxford, oak ridge national laboratory, exxonmobil, accenture and others, together with ibm scientists and engineers to explore the viability of quantum solutions to real- world problems.
as part of the ibm q network, organizations have access to ibm's quantum expertise and resources, open source qiskit software and developer tools, as well as cloud- based access to the ibm quantum computation center, which now includes 15 of the most- advanced quantum computers commercially available to explore practical applications for business and science, including a 53- qubit system - the most in the industry.
qm's full- stack quantum orchestration platform enables an entirely new approach to controlling and operating quantum processors.
capable of running even the most complex algorithms - from near- term applications of quantum computers to challenges of quantum- error- correction - the quantum orchestration platform optimizes the use of all quantum processors, right out- of- the- box from an intuitive and convenient interface.
according to qm co- founder & ceo, dr. itamar sivan "qm's inclusion in the ibm q network positions us to advance the prospects for useful quantum solutions.
with our recent breakthrough in our quantum orchestration platform combined with the q network's world- recognized collection of top quantum physics and engineers, qm is eager to use our collective resources to realize the quantum age coming to fruition before our eyes.
the q network manifests the spirit of the global effort to make quantum computing a reality.
this collaboration brings together corporate giants and startups to rapidly accelerate a new path towards the golden age of quantum computing and qm is excited to add our expertise and quantum advancements to the q network."
media contact
lazer cohen
things are getting real for researchers in the uc santa barbara john martinis/google group.
they are making good on their intentions to declare supremacy in a tight global race to build the first quantum machine to outperform the world's best classical supercomputers.
but what is quantum supremacy in a field where horizons are being widened on a regular basis, in which teams of the brightest quantum computing minds in the world routinely up the ante on the number and type of quantum bits ("qubits") they can build, each with their own range of qualities?
"let's define that, because it's kind of vague," said google researcher charles neill.
simply put, he continued, "we would like to perform an algorithm or computation that couldn't be done otherwise.
that's what we actually mean."
neill is lead author of the group's new paper, "a blueprint for demonstrating quantum supremacy with superconducting qubits," now published in the journal science.
fortunately, nature offers up many such complex situations, in which the variables are so numerous and interdependent that classical computers can't hold all the values and perform the operations.
think chemical reactions, fluid interactions, even quantum phase changes in solids and a host of other problems that have daunted researchers in the past.
something on the order of at least 49 qubits -  roughly equivalent to a petabyte (one million gigabytes) of classical random access memory -  could put a quantum computer on equal footing with the world's supercomputers.
just recently, neill's google/martinis colleagues announced an effort toward quantum supremacy with a 72- qubit chip possessing a "bristlecone" architecture that has yet to be put through its paces.
but according to neill, it's more than the number of qubits on hand.
"you have to generate some sort of evolution in the system which leads you to use every state that has a name associated with it," he said.
the power of quantum computing lies in, among other things, the superpositioning of states.
in classical computers, each bit can exist in one of two states -  zero or one, off or on, true or false -  but qubits can exist in a third state that is a superposition of both zero and one, raising exponentially the number of possible states a quantum system can explore.
additionally, say the researchers, fidelity is important, because massive processing power is not worth much if it's not accurate.
decoherence is a major challenge for anyone building a quantum computer -  perturb the system, the information changes.
wait a few hundredths of a second too long, the information changes again.
"people might build 50 qubit systems, but you have to ask how well it computed what you wanted it to compute," neill said.
"that's a critical question.
it's the hardest part of the field."
experiments with their superconducting qubits have demonstrated an error rate of one percent per qubit with three- and nine- qubit systems, which, they say, can be reduced as they scale up, via improvements in hardware, calibration, materials, architecture and machine learning.
building a qubit system complete with error correction components -  the researchers estimate a range of 100,000 to a million qubits -  is doable and part of the plan.
and still years away.
but that doesn't mean their system isn't already capable of doing some heavy lifting.
just recently it was deployed, with spectroscopy, on the issue of many- body localization in a quantum phase change -  a quantum computer solving a quantum statistical mechanics problem.
in that experiment, the nine- qubit system became a quantum simulator, using photons bouncing around in their array to map the evolution of electrons in a system of increasing, yet highly controlled, disorder.
"a good reason why our fidelity was so high is because we're able to reach complex states in very little time," neill explained.
the more quickly a system can explore all possible states, the better the prediction of how a system will evolve, he said.
if all goes smoothly, the world should be seeing a practicable ucsb/google quantum computer soon.
the researchers are eager to put it through its paces, gaining answers to questions that were once accessible only through theory, extrapolation and highly educated guessing -  and opening up a whole new level of experiments and research.
"it's definitely very exciting," said google researcher pedram roushan, who led the many- body quantum simulation work published in science in 2017.
they expect their early work to stay close to home, such as research in condensed matter physics and quantum statistical mechanics, but they plan to branch out to other areas, including chemistry and materials, as the technology becomes more refined and accessible.
"for instance, knowing whether or not a molecule would form a bond or react in some other way with another molecule for some new technology... there are some important problems that you can't roughly estimate; they really depend on details and very strong computational power," roushan said, hinting that a few years down the line they may be able to provide wider access to this computing power.
"so you can get an account, log in and explore the quantum world."
journal reference:
c. neill, p. roushan, k. kechedzhi, s. boixo, s. v. isakov, v. smelyanskiy, a. megrant, b. chiaro, a. dunsworth, k. arya, r. barends, b. burkett, y. chen, z. chen, a. fowler, b. foxen, m. giustina, r. graff, e. jeffrey, t. huang, j. kelly, p. klimov, e. lucero, j. mutus, m. neeley, c. quintana, d. sank, a. vainsencher, j. wenner, t. c. white, h. neven, j. m. martinis.
a blueprint for demonstrating quantum supremacy with superconducting qubits.
science, 2018; 360 (6385): 195 doi: 10.1126/science.aao4309
an international group of researchers has achieved the world's first multi- qubit demonstration of a quantum chemistry calculation performed on a system of trapped ions, one of the leading hardware platforms in the race to develop a universal quantum computer.
the research, led by university of sydney physicist dr cornelius hempel, explores a promising pathway for developing effective ways to model chemical bonds and reactions using quantum computers.
it is published today in physicial review x of the american physical society.
"even the largest supercomputers are struggling to model accurately anything but the most basic chemistry.
quantum computers simulating nature, however, unlock a whole new way of understanding matter.
they will provide us with a new tool to solve problems in materials science, medicine and industrial chemistry using simulations."
with quantum computing still in its infancy, it remains unclear exactly what problems these devices will be most effective at solving, but most experts agree that quantum chemistry is going to be one of the first 'killer apps' of this emergent technology.
quantum chemistry is the science of understanding the complicated bonds and reactions of molecules using quantum mechanics.
the 'moving parts' of anything but the most- simple chemical processes are beyond the capacity of the biggest and fastest supercomputers.
by modelling and understanding these processes using quantum computers, scientists expect to unlock lower- energy pathways for chemical reactions, allowing the design of new catalysts.
this will have huge implications for industries, such as the production of fertilisers.
other possible applications include the development of organic solar cells and better batteries through improved materials and using new insights to design personalised medicines.
working with colleagues at the institute for quantum optics and quantum information in innsbruck, austria, dr hempel used just four qubits on a 20- qubit device to run algorithms to simulate the energy bonds of molecular hydrogen and lithium hydride.
these relatively simple molecules are chosen as they are well understood and can be simulated using classical computers.
this allows scientists to check the results provided by the quantum computers under development.
dr hempel said: "this is an important stage of the development of this technology as it is allowing us to set benchmarks, look for errors and plan necessary improvements."
instead of aiming for the most accurate or largest simulation to date, dr hempel's work focused on what can go wrong in a promising quantum- classical hybrid algorithm known as variational quantum eigensolver or vqe.
by looking at different ways to encode the chemistry problem, the researchers are after ways to suppress errors that arise in today's imperfect quantum computers and stand in the way of near- term usefulness of those machines.
error suppression is at the core of research pursued in the university of sydney's quantum control laboratory, led by professor michael biercuk, who recently launched australia's first private quantum start- up, q- ctrl.
dr hempel, who did the experiments while at the university of innsbruck, now hopes to leverage sydney's expertise to improve what can be accomplished with these kinds of simulations.
the paper, published today in journal physical review x, was jointly written with innsbruck professor rainer blatt, a pioneer in quantum computing, and former harvard professor alan aspuru- guzik, who has since moved to the university of toronto.
professor blatt, from iqoqi in innsbruck, said: "quantum chemistry is an example where the advantages of a quantum computer will very soon become apparent in practical applications."
head of the university of sydney nano institute's quantum science domain, dr ivan kassal, said: "this work is a remarkable implementation of one of the most promising approaches to quantum chemistry, proving its mettle on a real quantum- information processor."
he said that dr hempel's decision to move to the university of sydney in 2016 was an excellent addition to the strong quantum team on campus.
"theoretical chemistry and materials science are strengths at this university and they will be augmented by these latest techniques in quantum computation," he said.
story source:
materials provided by university of sydney.
note: content may be edited for style and length.
quantum computing market research report: by offering (hardware, software, service), deployment type (on- premises, cloud- based), application (optimization, simulation and data problems, sampling, machine learning), technology (quantum dots, trapped ions, quantum annealing), industry (bfsi, aerospace & defense, manufacturing, healthcare, it & telecom, energy & utilities) - industry share, growth, drivers, trends and demand forecast to 2030
read the full report: https://www.reportlinker.com/p05879070/?utm_source=prn
the quantum computing market valued $507.1 million in 2019, from where it is projected to grow at a cagr of 56.0% during 2020- 2030 (forecast period), to ultimately reach $64,988.3 million by 2030.
machine learning (ml) is expected to progress at the highest cagr, during the forecast period, among all application categories, owing to the fact that quantum computing is being integrated in ml for improving the latter's use case.
government support for the development and deployment of the technology is a prominent trend in the quantum computing market, with companies as well as public bodies realizing the importance of a coordinated funding strategy.
for instance, the national quantum initiative act, which became a law in december 2018, included a funding of $1.2 billion from the u.s. house of representatives for the national quantum initiative program.
the aim behind the funding was to facilitate the development of technology applications and quantum information science, over a 10- year period, by setting its priorities and goals.
moreover, efforts are being made to come with standards for the quantum computing technology.
among the numerous standards being developed by the ieee standards association quantum computing working group are the benchmarks and performance matrix, which would help in analyzing the performance of quantum computers against that of conventional computers.
other noteworthy standards are those related to the nomenclature and definitions, in order to create a common language for quantum computers.
in 2019, the quantum computing market was dominated by the quantum annealing category, on the basis of technology.
this is because the physical challenges in its development have been overcome, and it is now being deployed in larger systems.
that year, the banking, financial services, and insurance (bfsi) division held the largest share in the market, on account of the rapid expansion of this industry.
additionally, banks and other financial institutions are quickly deploying this technology to make their business process streamlined as well as secure their data.
by 2030, europe and north america are expected to account for more than 78.0% in the quantum computing market, as canada, the u.s., the u.k., germany, and russia are witnessing heavy investments in the field.
for instance, the national security agency (nsa), national aeronautics and space administration (nasa), and los alamos national laboratory are engaged in quantum computing technology development.
additionally, an increasing number of collaborations and partnerships are being witnessed in these regions, along with the entry of several startups.
the major players operating in the highly competitive quantum computing market are telstra corporation limited, international business machines (ibm) corporation, silicon quantum computing, ionq inc., alphabet inc., huawei investment & holding co. ltd., microsoft corporation, rigetti & co. inc., zapata computing inc., d- wave systems inc., and intel corporation.
google llc, the main operating subsidiary of alphabet inc. is establishing the quantum ai laboratory, in collaboration with the nsa, wherein the quantum computers developed by d- wave systems inc. are being used.
read the full report: https://www.reportlinker.com/p05879070/?utm_source=prn
about reportlinker
reportlinker is an award- winning market research solution.
reportlinker finds and organizes the latest industry data so you get all the market research you need - instantly, in one place.
__________________________
scientists in australia have developed a new approach to reducing the errors that plague experimental quantum computers; a step that could remove a critical roadblock preventing them scaling up to full working machines.
by taking advantage of the infinite geometric space of a particular quantum system made up of bosons, the researchers, led by dr. arne grimsmo from the university of sydney, have developed quantum error correction codes that should reduce the number of physical quantum switches, or qubits, required to scale up these machines to a useful size.
"the beauty of these codes is they are 'platform agnostic' and can be developed to work with a wide range of quantum hardware systems," dr. grimsmo said.
"many different types of bosonic error correction codes have been demonstrated experimentally, such as 'cat codes' and 'binomial codes'," he said.
"what we have done in our paper is unify these and other codes into a common framework."
the research, published this week in physical review x, was jointly written with dr. joshua combes from the university of queensland and dr. ben baragiola from rmit university.
the collaboration is across two leading quantum research centres in australia, the arc centre of excellence for engineered quantum machines and the arc centre of excellence for quantum computation and communication technology.
robust qubits
"our hope is that the robustness offered by 'spacing things out' in an infinite hilbert space gives you a qubit that is very robust, because it can tolerate common errors like photon loss," said dr. grimsmo from the university of sydney nano institute and school of physics.
scientists in universities and at tech companies across the planet are working towards building a universal, fault- tolerant quantum computer.
the great promise of these devices is that they could be used to solve problems beyond the reach of classical supercomputers in fields as varied as materials science, drug discovery and security and cryptography.
with google last year declaring it has a machine that has achieved 'quantum supremacy' - performing an arguably useless task but beyond the scope of a classical computer- interest in the field of quantum computing and engineering continues to rise.
but to build a quantum machine that can do anything useful will require thousands, if not millions of quantum bits operating without being overwhelmed with errors.
and qubits are, by their very nature, error prone.
the 'quantumness' that allows them to perform a completely different type of computing operation means they are highly fragile and susceptible to electromagnetic and other interference.
identifying, removing and reducing errors in quantum computation is one of the central tasks facing physicists working in this field.
fragile superpositions
quantum computers perform their tasks by encoding information utilising quantum superposition- a fundamental facet of nature where a final outcome of a physical system is unresolved until it is measured.
until that point, the information exists in a state of multiple possible outcomes.
dr. grimsmo said: "one of the most fundamental challenges for realising quantum computers is the fragile nature of quantum superpositions.
fortunately, it is possible to overcome this issue using quantum error correction."
this is done by encoding information redundantly, allowing the correction of errors as they happen during a quantum computation.
the standard approach to achieve this is to use a large number of distinguishable particles as information carriers.
common examples are arrays of electrons, trapped ions or quantum electrical circuits.
however, this creates a large network of 'physical qubits' in order to operate a single, logical qubit that does the processing work you require.
this need to create a large network of physical qubits to support the work of a single operating qubit is a non- trivial barrier towards constructing large- scale quantum machines.
indistinguishable bosons
dr. grimsmo said: "in this work, we consider an alternative approach based on encoding quantum information in collections of bosons."
the most common type of boson is the photon, a packet of electromagnetic energy and massless 'light particle'.
by trapping bosons in a particular microwave or optical cavity, they become indistinguishable from one another, unlike, say, an array of trapped ions, which are identifiable by their location.
"the advantage of this approach is that large numbers of bosons can be trapped in a single quantum system such as photons trapped in a high- quality optical or microwave cavity," dr. grimsmo said.
"this could drastically reduce the number of physical systems required to build a quantum computer."
the researchers hope their foundational work will help build a roadmap towards fault tolerance in quantum computing.
engineering researchers have demonstrated proof- of- principle for a device that could serve as the backbone of a future quantum internet.
university of toronto engineering professor hoi- kwong lo and his collaborators have developed a prototype for a key element for all- photonic quantum repeaters, a critical step in long- distance quantum communication.
a quantum internet is the 'holy grail' of quantum information processing, enabling many novel applications including information- theoretic secure communication.
today's internet was not specifically designed for security, and it shows: hacking, break- ins and computer espionage are common challenges.
nefarious hackers are constantly poking holes in sophisticated layers of defence erected by individuals, corporations and governments.
in light of this, researchers have proposed other ways of transmitting data that would leverage key features of quantum physics to provide virtually unbreakable encryption.
one of the most promising technologies involves a technique known as quantum key distribution (qkd).
qkd exploits the fact that the simple act of sensing or measuring the state of a quantum system disturbs that system.
because of this, any third- party eavesdropping would leave behind a clearly detectable trace, and the communication can be aborted before any sensitive information is lost.
until now, this type of quantum security has been demonstrated in small- scale systems.
lo and his team are among a group of researchers around the world who are laying the groundwork for a future quantum internet by working to address some of the challenges in transmitting quantum information over great distances, using optical fibre communication.
because light signals lose potency as they travel long distances through fibre- optic cables, devices called repeaters are inserted at regular intervals along the line.
these repeaters boost and amplify the signals to help transmit the information along the line.
but quantum information is different, and existing repeaters for quantum information are highly problematic.
they require storage of the quantum state at the repeater sites, making the repeaters much more error prone, difficult to build, and very expensive because they often operate at cryogenic temperatures.
lo and his team have proposed a different approach.
they are working on the development of the next generation of repeaters, called all- photonic quantum repeaters, that would eliminate or reduce many of the shortcomings of standard quantum repeaters.
with collaborators at osaka university, toyama university and ntt corporation in japan, lo and his team have demonstrated proof- of- concept of their work in a paper recently published in nature communications.
"we have developed all- photonic repeaters that allow time- reversed adaptive bell measurement," says lo.
"because these repeaters are all- optical, they offer advantages that traditional- quantum- memory- based matter- repeaters do not.
for example, this method could work at room temperature."
a quantum internet could offer applications that are impossible to implement in the conventional internet, such as impenetrable security and quantum teleportation.
"an all- optical network is a promising form of infrastructure for fast and energy- efficient communication that is required for a future quantum internet," says lo.
"our work helps pave the way toward this future."
magnetic quantum effects have been harnessed for the first time at the lithographic scale of semiconductors by researchers at the national institute of standards and technology (nist) and the isis particle accelerator (u.k.).
the international team reports chaining together 100 atoms of yttrium barium nickel oxide into a quantum spin- chain that, in effect, turned the 30- nanometer long magnetic molecule into a single element.
the observed quantum effect holds the promise of using these unusually large magnetic molecules as switch, memory, or computing elements in future semiconductor circuits.
"this is the first significant step toward quantum coherence on a length- scale appropriate for lithography in solid- state circuits," said collin broholm, a physics professor at johns hopkins university.
"it is unusual to see quantum coherence well beyond the atomic length- scale, and that is why we are so excited by this discovery."
quantum mechanics already enables tiny atomic- scale phenomena to be harnessed by electronics circuits, from the quantum dots in the orion quantum computer and the quantum cascade laser, to the quantum wells of metal- insulator electronics.
however, the observation of magnetic quantum spin- chains at a scale measuring 30 nanometers could enable lithographic techniques to more easily harness such molecular- scale quantum effects in future semiconductor circuits.
"many enormous challenges remain to be overcome to use the quantum coherence of a this spin- chain for quantum computing," said broholm.
"however, ees should be interested, because this moves us closer to a technological application of quantum coherence.
specifically, we see quantum coherence at a length scale that is approaching the feature widths in modern ic technology."
the experiment was performed at the nist center for neutron research in the united states, and at the isis particle accelerator at the rutherford appleton laboratory in the united kingdom.
broholm performed the work with colleagues at johns hopkins, the u.s. department of energy's brookhaven national laboratory, the university college london, louisiana state university, rutherford appleton laboratory (u.k.), the national institute of advanced industrial science and technology (japan) and the university of tokyo.
the experiment was funded by the u.s. department of energy, the national science foundation, the wolfson- royal society (u.k.), and by the basic technologies program of the u.k. research councils.
in a recent issue of physical review a, argonne researchers reported a new method for alleviating the effects of "noise" in quantum information systems, a challenge scientists around the globe are working to meet in the race toward a new era of quantum technologies.
the new method has implications for the future of quantum information science, including quantum computing and quantum sensing.
many current quantum information applications, such as carrying out an algorithm on a quantum computer, suffer from "decoherence"- a loss of information due to "noise," which is inherent to quantum hardware.
matthew otten, a maria goeppert mayer fellow at argonne, and stephen gray, group leader of theory and modeling at the center for nanoscale materials, a u.s. department of energy office of science user facility, have developed a new technique that recovers this lost information by repeating the quantum process or experiment many times, with slightly different noise characteristics, and then analyzing the results.
after gathering results by running the process many times in sequence or parallel, the researchers construct a hypersurface where one axis represents the result of a measurement and the other two (or more) axes represent different noise parameters.
this hypersurface yields an estimate of the noise- free observable and gives information about the effect of each noise rate.
"it's like taking a series of flawed photographs," said otten.
"each photo has a flaw, but in a different place in the picture.
when we compile all the clear pieces from the flawed photos together, we get one clear picture."
applying this technique effectively reduces quantum noise without the need for additional quantum hardware.
"this is a versatile technique that can be done with separate quantum systems undergoing the same process at the same time," said otten.
"one could create several small quantum devices and run them in parallel," said gray.
"using our method, one would combine the results on the hypersurface and generate approximate noise- free observables.
the results would help extend the usefulness of the quantum devices before decoherence sets in."
"we successfully performed a simple demonstration of our method on the rigetti 8q- agave quantum computer," said otten.
"this class of methods will likely see much use in near- term quantum devices."
the researchers' work described above appears in physical review a and is entitled "recovering noise- free quantum observables."
otten and gray have also developed a similar and somewhat less computationally complex process to achieve noise- reduction results based on correcting one qubit at a time to approximate the result for all qubits being simultaneously corrected.
a qubit, or quantum bit, is the equivalent in quantum computing to the binary digit or bit used in classical computing.
"in this approach, we assume that the noise can be reduced on each qubit individually, which, while experimentally challenging, leads to a much simpler data processing problem and results in an estimate of the noise- free result," noted otten.
one of the most well- known "rules" of quantum physics is that all quantum properties are lost due to environmental interaction.
this rule, though, may not hold true in all situations.
"we have discovered the first counter to this common rule," sabrina maniscalco tells physorg.com.
maniscalco is a researcher at the turku centre for quantum physics, university of turku in finland.
along with fellow researcher (and husband) jyrki piilo, and ph.d. student laura mazzola, maniscalco has identified evidence that some quantum correlations can remain intact.
the results of their work are published in physical review letters: "sudden transition between classical and quantum decoherence."
"laura really did most of the work," maniscalco says.
"she was studying different interactions between quantum properties and the environment, and looking for different correlations in noisy quantum systems.
no one expected her to find strange behavior.
when she first called jyrki and me with the result, we thought it must be a mistake."
maniscalco and piilo went over the data and realized that it wasn't a mistake.
"we checked the calculations again, and found that the time evolution of quantum correlations in this case remains constant for a long time.
it represented a transition between classical and quantum decoherence, and the quantum property was not lost."
this particular correlation can be found, for example, in quantum systems comprising of two qubits.
"these qubits, each with different properties, such as different polarizations, have to interact with a type of noise that doesn't change the energy of the qubits," maniscalco explains.
"instead of changing the energy, the noise just changes the phase, such as flipping polarizations.
the type of noise that we have considered is one that contains all frequencies in a way that is very similar to white noise."
while this discovery is theoretical, maniscalco says that it has an experimental basis as well.
"a very recent experiment has confirmed a type of quantum correlation that is not affected by the environment.
and this is not a weird type of environment; it's a natural environment that we could work in right now."
(for more on this experiment, see jin- shi xu, et.
al., "experimental investigation of classical and quantum correlations under decoherence," nature communications (april 2010).
doi:10.1038/ncomms1005.)
in the last 20 years, maniscalco points out, technology has advanced to the point where it is possible to use single atoms or photons to build quantum logic gates for future quantum computers, or perform communication, measurement and cryptography tasks.
"we've learned that it is possible to exploit the quantumness of the microscopic state, but in order for us to succeed, the quantum properties have to remain intact for a long time.
that is a challenge, since once the properties are lost through interaction with the environment, a device can't exploit quantumness."
this discovery that certain quantum correlations are not lost in presence of the environment could lead an increased ability to exploit the quantum world for use in technological devices.
maniscalco points out that the idea that all quantum properties need not be lost through interaction with the environment presents more than interesting fundamental implications.
"while this work has a surprising fundamental aspect," she says, "it opens up a whole range of possibilities with applications in quantum technology, including computing, communications, metrology and cryptography."
next, maniscalco says that her group, and piilo's group, at the university of turku will need to study this effect.
"we need to learn the most general conditions for this behavior, and see if it holds for other environments.
we are also working toward deigning a quantum protocol that uses this state, so that we can demonstrate an application of this effect in practice."
for more information, you can visit the open quantum systems and entanglement group and the non- markovian processes and complex systems group web pages.
more information: laura mazzola, jyrki piilo, and sabrina maniscalco, "sudden transition between classical and quantum decoherence, physical review letters (may 2010).
available online: http://link.aps.org/doi/10.110 ... ysrevlett.104.200401
as quantum technology continues to come into its own, investment is happening on a global scale.
soon, we could see improvements in machine learning models, financial risk assessment, efficiency of chemical catalysts and the discovery of new medications.
as numerous scientists, companies and governments rush to invest in the new era of quantum technology, a crucial piece of this wave of innovation is the quantum sensor.
improving these devices could mean more powerful computers, better detectors of disease and technological advances scientists can't even predict yet.
a scientific study from the university of chicago's institute for molecular engineering published oct. 17 in nature communications could have exciting implications for the developing world of quantum sensing- and quantum technology as a whole.
"we took a recently proposed idea to make better optical classical sensors and asked whether the same idea would work in a quantum setting," said aashish clerk, one of the study's authors and a professor at the institute for molecular engineering.
"we found that this idea doesn't really work in quantum settings, but that another somewhat related approach could give you a huge advantage."
in a quantum setting, optical sensors are typically limited because light is made up of particles, and this discreteness leads to unavoidable noise.
but this study revealed an unexpected method to combat that limitation.
"we think we've uncovered a new strategy for building extremely powerful quantum sensors," clerk continued.
the path to the directional principle
clerk and co- author hoi- kwan lau, a postdoctoral scholar at uchicago, were inspired by recent high- profile studies that showed how to drastically enhance a common optical sensing technique.
the "trick" involves tuning systems to an exceptional point, or a point at which two or more modes of light come together at one specific frequency.
lau and clerk wanted to see whether this method could succeed in settings where quantum effects were important.
the goal was to account for unavoidable "quantum" noise- fluctuations associated with the fact that light has both a wave- like and a particle- like character, clerk explained.
the study found the exceptional point technique to be unhelpful in a quantum setting, but the research still led to promising results.
"the good news is we found another way to build a powerful new type of sensor that has advantages even in quantum regimes," clerk said.
"the idea is to construct a system that is 'directional,' meaning photons can move in one direction only."
this directional principle- one based on photons being able to move in only one direction- is a brand- new development in quantum sensing.
new developments in quantum sensing
in terms of real- world applications, highly effective quantum sensors could be game- changing.
quantum systems are sensitive to the slightest environmental changes, so these detectors have the potential to be incredibly powerful.
in addition, some of the stranger aspects of quantum behavior, such as quantum entanglement, could make them even stronger.
quantum entanglement, a puzzling phenomenon even for scientists, describes how two particles can be separated by a vast distance yet actions performed on one particle immediately affect the other.
this entanglement can be harnessed to make quantum sensors surprisingly resilient against certain kinds of noise.
in the future, new developments in quantum sensing could translate to significant advances in a variety of areas.
the class of optical sensors described in the study can be used to detect viruses in liquids, for example.
they also can act as readout devices for quantum bits in a superconducting quantum computer.
"we think our idea has the potential to generate major improvements in many of these applications," clerk explained.
the study's implications for quantum computing are especially exciting.
not only do quantum computers have the potential to dramatically increase computing speeds, but they could also tackle problems that are completely unfeasible with traditional computing.
lau and clerk plan to do further research on their enhanced sensing technique.
clerk still has a lot of questions: "what sets how fast our sensor is?
are there fundamental limits on its speed?
can it be used to detect signals that aren't necessarily small?"
their biggest hope, clerk explained, is to inspire other researchers to build improved quantum sensors that harness this newly uncovered principle.
where do iot, ai and quantum computing intersect?
the short answer is that they meet where data is growing exponentially.
the long answer is... well, it's complicated.
last week, a panel at the design automation conference (dac) drilled deep into this difficult topic.
participating were the winners of the "2019 under 40 innovation awards" - young engineers and researchers working on next- generation technologies.
the panelists, all eager for the dawn of a new era in design automation, called upon the electronic design automation (eda) industry for more sophisticated tools to help them advance the internet of things (iot), artificial intelligence (ai) and even quantum computing.
but given the slowdown of moore's law, what more can design automation contribute to electronic design?
how exactly does eda connect with ai?
vijay raghunathan, professor of electrical and computer engineering at purdue university, explained: "one of the things eda did for designs in general is to convert what was basically dark art [i.e.
electronic design] into a very structured science."
raghunathan continued: "if you talk to ai researchers today, you realize the design of these algorithms and neural networks is really a dark art.
one of the most interesting things going forward is to see if eda can bring the same level of rigorous structure to the design of neural networks and design of ai algorithms."
he added, "to me, that is one of the most interesting aspects that connects the world of eda with the world of ai."
2019 under 40 innovation awards panel at dac
from left to right: huichu liu, a staff research scientist at intel; vijay raghunathan, professor at purdue univ.
; robert wille, professor at johannes kepler univ.
; rasit onur topaloglu, senior hardware developer and program manager at ibm
new- generation researchers also expect eda tool vendors to jump into quantum computing - not in a few years, but today.
robert wille, a professor at johannes kepler university in linz, austria, said, "everybody knows about moore's law and design gaps we've experienced in decades with conventional computing technology."
despite the widespread belief that the era of quantum computing is still far away, wille stressed: "it makes absolutely sense for the design automation industry to start developing efficient and sophisticated eda tools for quantum computing - right now."
what is quantum computing for?
the panel, consisting of winners of this year's under- 40 innovation awards, included huichu liu, a staff research scientist at intel, rasit onur topaloglu, senior hardware developer and program manager at ibm, wille, and raghunathan.
yunji chen, professor at the institute of computing technology, chinese academy of sciences, was another honoree, but couldn't participate because he was not able to get a u.s. visa in time.
given their diverse backgrounds, the panel covered a lot of ground, touched on diverse topics that ranged from iot to ai and quantum computing.
for a layman, quantum computing is a mystery beyond solution.
asked why the world needs quantum computing, wille said, "first, it's important to understand that the quantum computer is not replacing the conventional computer we know."
instead, he said, it will be one of the many computing technologies of the future.
the tech world has high hopes for quantum computers, which show promises, for example, in accelerating the exploitation of huge search bases.
robert wille
"but the first killer app for quantum computing is what's known as shor's algorithm - a quantum algorithm - developed years ago," wille explained.
"the algorithm will help make factorization much more efficient.
this is still considered as the holy grail of quantum computing, as [many believe] this will change the entire world [in theory]."
but in reality, "this is still the furthest away."
in recent years, as big industry players such as ibm, google and microsoft research jump into the fray, the quantum computing community is seeing the emergence of new commercial applications.
for example, quantum computers can be used "for simulating climate change, solving optimization problems ... or... quantum chemistry is a huge topic," said wille.
so, although the quantum computer's big goal of factorization is still far away, "we see today a variety of applications where [the use of] quantum computer may be beneficial," said wille.
as promising as this prospect might sound, purdue's raghunathan cautioned that there are big challenges in quantum computing.
"from an outsider's perspective, i see there is a class of very hard problems - computationally hard problems - where quantum computing holds a lot of promise."
solving optimization problems is one, by computing in the exponential space, for example, raghunathan.
"but the challenge is, how do you really extract true benefits [of quantum computing] for a class of really wide- spread hard computational problems which seem to be all over the place?"
infrastructure for quantum computing
obviously, none of the young innovators expects the eda industry to stand still.
by stressing the need for automation tools for quantum computers, wille said, "we need them to find out what's possible to fabricate and what's possible to design."
he said, "we need to be prepared for the day when quantum computing becomes scalable."
ibm's rasit onur topaloglu, another young innovation award winner, noted, "we also thought about this problem at ibm...we asked, when do we need automation tools to design quantum computing?"
he said, "we've concluded that up until 200 qubits, maybe we can still do it manually."
he added, "i am not going to project when we will reach 200 qubits, but we already have an 80 qubits architecture."
although there is at least a seven year- gap from academic research to a product, he concluded that the research for design automation tools for quantum computing needs to start today.
"when the idea [of quantum computing] takes off, we need those tools in the industry right away."
excited photo- emitters can cooperate and radiate simultaneously, a phenomenon called superfluorescence.
researchers from empa and eth zurich, together with colleagues from ibm research zurich, have recently been able to create this effect with long- range ordered nanocrystal superlattices.
this discovery could enable future developments in led lighting, quantum sensing, quantum communication and future quantum computing.
the study has just been published in the renowned journal nature.
some materials spontaneously emit light if they are excited by an external source, for instance a laser.
this phenomenon is known as fluorescence.
however, in several gases and quantum systems a much stronger emission of light can occur, when the emitters within an ensemble spontaneously synchronize their quantum mechanical phase with each other and act together when excited.
in this way, the resulting light output can be much more intense than the sum of the individual emitters, leading to an ultrafast and bright emission of light - superfluorescence.
it only occurs, however, when those emitters fulfill stringent requirements, such as having the same emission energy, high coupling strength to the light field and a long coherence time.
as such, they are strongly interacting with each other but at the same time are not easily disturbed by their environment.
this has not been possible up to now using technologically relevant materials.
colloidal quantum dots could just be the ticket; they are a proven, commercially appealing solution already employed in the most advanced lcd television displays - and they fulfill all the requirements.
researchers at empa and eth zurich, led by maksym kovalenko, together with colleagues from ibm research zurich, have now shown that the most recent generation of quantum dots made of lead halide perovskites offer an elegant and practically convenient path to superfluorescence on- demand.
for this, the researchers arranged perovskite quantum dots into a three- dimensional superlattice, which enables the coherent collective emission of photons - thus creating superfluorescence.
this provides the basis for sources of entangled multi- photon states, a missing key resource for quantum sensing, quantum imaging and photonic quantum computing.
"birds of a feather flock together"
a coherent coupling among quantum dots requires, however, that they all have the same size, shape and composition because "birds of a feather flock together" in the quantum universe, too.
"such long- range ordered superlattices could only be obtained from a highly monodisperse solution of quantum dots, the synthesis of which had been carefully optimized over the last few years," said maryna bodnarchuk, a senior scientist at empa.
with such "uniform" quantum dots of various sizes, the research team could then form superlattices by properly controlling the solvent evaporation.
the final proof of superfluorescence came from optical experiments performed at temperatures of around minus 267 degrees celsius.
the researchers discovered that photons were emitted simultaneously in a bright burst: "this was our 'eureka! '
moment.
the moment we realized that this was a novel quantum light source," said gabriele raino from eth zurich and empa who was part of the team that carried out the optical experiments.
the researchers consider these experiments as a starting point to further exploit collective quantum phenomena with this unique class of material.
"as the properties of the ensemble can be boosted compared to just the sum of its parts, one can go way beyond engineering the individual quantum dots," added michael becker from eth zurich and ibm research.
the controlled generation of superfluorescence and the corresponding quantum light could open new possibilities in led lighting, quantum sensing, quantum- encrypted communication and future quantum computing.
scientists at the university of sydney have demonstrated the ability to "see" the future of quantum systems, and used that knowledge to preempt their demise, in a major achievement that could help bring the strange and powerful world of quantum technology closer to reality.
the applications of quantum- enabled technologies are compelling and already demonstrating significant impacts - especially in the realm of sensing and metrology.
and the potential to build exceptionally powerful quantum computers using quantum bits, or qubits, is driving investment from the world's largest companies.
however a significant obstacle to building reliable quantum technologies has been the randomisation of quantum systems by their environments, or decoherence, which effectively destroys the useful quantum character.
the physicists have taken a technical quantum leap in addressing this, using techniques from big data to predict how quantum systems will change and then preventing the system's breakdown from occurring.
the research is published today in nature communications.
"much the way the individual components in mobile phones will eventually fail, so too do quantum systems," said the paper's senior author professor michael j. biercuk.
"but in quantum technology the lifetime is generally measured in fractions of a second, rather than years."
professor biercuk, from the university of sydney's school of physics and a chief investigator at the australian research council's centre for engineered quantum systems, said his group had demonstrated it was possible to suppress decoherence in a preventive manner.
the key was to develop a technique to predict how the system would disintegrate.
professor biercuk highlighted the challenges of making predictions in a quantum world: "humans routinely employ predictive techniques in our daily experience; for instance, when we play tennis we predict where the ball will end up based on observations of the airborne ball," he said.
"this works because the rules that govern how the ball will move, like gravity, are regular and known.
but what if the rules changed randomly while the ball was on its way to you?
in that case it's next to impossible to predict the future behavior of that ball.
"and yet this situation is exactly what we had to deal with because the disintegration of quantum systems is random.
moreover, in the quantum realm observation erases quantumness, so our team needed to be able to guess how and when the system would randomly break.
"we effectively needed to swing at the randomly moving tennis ball while blindfolded."
the team turned to machine learning for help in keeping their quantum systems - qubits realised in trapped atoms - from breaking.
what might look like random behavior actually contained enough information for a computer program to guess how the system would change in the future.
it could then predict the future without direct observation, which would otherwise erase the system's useful characteristics.
the predictions were remarkably accurate, allowing the team to use their guesses preemptively to compensate for the anticipated changes.
doing this in real time allowed the team to prevent the disintegration of the quantum character, extending the useful lifetime of the qubits.
"we know that building real quantum technologies will require major advances in our ability to control and stabilise qubits - to make them useful in applications," professor biercuk said.
our techniques apply to any qubit, built in any technology, including the special superconducting circuits being used by major corporations.
"we're excited to be developing new capabilities that turn quantum systems from novelties into useful technologies.
the quantum future is looking better all the time," professor biercuk said.
physicists at eth zurich have developed a method for precisely controlling quantum systems by exploiting a trick that helps cats to land on their feet and motorists to fit their cars into parking spots.
in the longer run, the method could lead to the development of more reliable quantum computers.
to maneuver a car into a parking spot parallel to the road can be quite a challenge.
it would be an easy task, of course, if only the vehicle could move sideways.
as this is not possible, the sideways motion must be pieced together - sometimes elegantly, sometimes less so - in a series of forward and backward movements and turns on the steering wheel.
such a finely tuned sequence of movements also enables cats to almost always land on their feet after a free fall.
researchers at eth zurich have now used a similar principle for steering a quantum system into a desired state.
this new type of control should be useful in situations in which quantum systems must be precisely controlled, not least in the context of quantum computers.
big quantum world
for their research, scientists in the group of andreas wallraff, a professor at the department of physics, use "artificial atoms" made of electronic circuits, which they control with microwave pulses.
these circuits comprise superconducting components - that is, components in which electric currents can flow without resistance- and typically measure fractions of a millimeter.
"for a quantum physicist, these circuits are enormously large objects, but they display behavior that is very similar to that of atoms," explains wallraff.
unlike in natural quantum systems, such as atoms, electrons or photons, the design and properties of the quantum circuits can be changed and adapted to different applications.
moreover, the fragile quantum states can survive for several microseconds in these superconducting circuits - a relatively long time for quantum objects.
during this time the state can be manipulated with microwave pulses, in order to study the quantum state itself or to make use of it in a quantum computation.
finding the right twist
these favorable properties notwithstanding, the quantum circuits are highly sensitive to external disturbances (caused, for example, by imperfect shielding), just as natural quantum systems.
under the direction of stefan filipp, a scientist in the wallraff group, the eth zurich researchers have now found a possible way to render the quantum states more robust against disturbances.
they make use of the geometry of so- called hilbert spaces; these abstract spaces are the 'natural habitat' of any quantum system.
similarly as a car is driven through a two- dimensional space, a quantum system is steered through its hilbert space.
both for parallel parking and for controlling quantum systems, the specific sequence of operations is important.
for example, when a motorist first performs all steering- wheel movements and then all forward and backward movements, then she or he will hardly end up in the parking spot.
the situation is comparable for the physicists' artificial atoms, which they control with microwave pulses.
"we obtain different results depending on the order in which we apply the individual pulses, even if the pulses have an identical shape, the same energy and the same length.
this can only be explained by the different routes the system takes through its hilbert space," says stefan filipp.
path towards a quantum computer
"this is the first time that somebody obtained this specific type of control over an isolated quantum object and was able to study the process in detail," adds abdufarrukh abdumalikov, scientist in the wallraff group.
an important factor for the eth physicists' success was that they could work with relatively short microwave pulses.
"this allowed us to perform operations quickly, before the quantum state was irrevocably destroyed," says abdumalikov.
the researchers expect that their method may provide a viable path towards a practical quantum computer.
the development of such devices, which use the laws of quantum mechanics to tackle computational tasks, is a very active field of current study.
quantum physics opens up a whole range of new possibilities for information processing, and one day quantum computers may help solve problems that are computationally too complex for any conventional computer to solve within reasonable time.
more information: abdumalikov aa, fink jm, juliusson k, pechal m, berger s, wallraff a, filipp s: experimental realization of non- abelian non- adiabatic geometric gates.
nature, 2013, doi: 10.1038/nature12010
researchers have demonstrated the first quantum light- emitting diode (led) that emits single photons and entangled photon pairs with a wavelength of around 1550 nm, which lies within the standard telecommunications window.
a single- photon source that operates at this wavelength is expected to serve as a key component in future quantum networks, long- distance quantum communication systems, quantum cryptography devices, and other applications.
the researchers, tina muller et al., at toshiba research europe limited, the university of sheffield, and the university of cambridge, have published a paper on the new quantum light source in a recent issue of nature communications.
"for the first time, quantum devices can meet the fundamental requirements of state- of- the art quantum key distribution and quantum communication systems," muller told phys.org.
the ability to emit single photons and entangled photon pairs in the telecom window has been a goal in the field of quantum optics for a long time.
although a variety of different light sources exist that can emit single and entangled photons (from individual atoms to color centers in diamond), until now they have been largely limited to shorter wavelengths that are unsuitable for quantum network applications.
in the new study, the researchers fabricated light- emitting quantum dot devices based on indium phosphide, a material that is currently used in quantum dot lasers to generate laser light with a 1550- nm wavelength.
to enable this material to emit single photons and entangled photon pairs at this wavelength, the researchers used a growth method called metalorganic vapor phase epitaxy to grow individual indium phosphide quantum dot "droplets," which form the basis for the quantum leds.
another advantage of the new quantum leds is that they can operate at temperatures of up to 93 k, which is significantly higher than the operating temperatures of other quantum light sources.
a higher operating temperature allows for easier integration with existing devices, and the researchers expect that the operating temperature of the new devices could be improved even further with some modifications.
going forward, the researchers anticipate that the new quantum leds will have a significant impact on the development of quantum network technology, including the quantum internet.
for example, the devices can be integrated with quantum relays and repeaters to extend the range of quantum networks.
the researchers also expect that the quantum light sources can operate in pulsed mode when integrated with radio frequency electronics.
their next steps will be to make improvements in order to realize these applications.
"we will further optimize the performance and size of our devices to facilitate integration in long- distance quantum communications systems," muller said.
one of the most ambitious endeavors in quantum physics right now is to build a large- scale quantum network that could one day span the entire globe.
in a new study, physicists have shown that describing quantum networks in a new way- as mathematical graphs- can help increase the distance that quantum information can be transmitted.
compared to classical networks, quantum networks have potential advantages such as better security and being faster under certain circumstances.
"a worldwide quantum network may appear quite similar to the internet- a huge number of devices connected in a way that allows the exchange of information between any of them," coauthor michael epping, a physicist at the university of waterloo in canada, told phys.org.
"but the crucial difference is that the laws of quantum theory will be dominant for the description of that information.
for example, the state of the fundamental information carrier can be a superposition of the basis states 0 and 1.
by now, several advantages in comparison to classical information are known, such as prime number factorization and secret communication.
however, the biggest benefit of quantum networks might well be discovered by future research in the rapidly developing field of quantum information theory."
quantum networks involve sending entangled particles across long distances, which is challenging because particle loss and decoherence tend to scale exponentially with the distance.
in their study published in the new journal of physics, epping and coauthors hermann kampermann and dagmar bruss at the heinrich heine university of dusseldorf in germany have shown that describing physical quantum networks as abstract mathematical graphs offers a way to optimize the architecture of quantum networks and achieve entanglement across the longest possible distances.
"a network is a physical system," epping explained.
"examples of a network are the internet and labs at different buildings connected by optical fibers.
these networks may be described by mathematical graphs at an abstract level, where the network structure- which consists of nodes that exchange quantum information via links- is represented graphically by vertices connected by edges.
an important task for quantum networks is to distribute entangled states amongst the nodes, which are used as a resource for various information protocols afterwards.
in our approach, the graph description of the network, which might come to your mind quite naturally, is related to the distributed quantum state."
in the language of graphs, this distributed quantum state becomes a quantum graph state.
the main advantage of the graph state description is that it allows researchers to compare different quantum networks that produce the same quantum state, and to see which network is better at distributing entanglement across large distances.
quantum networks differ mainly in how they use quantum repeaters- devices that offer a way to distribute entanglement across large distances by subdividing the long- distance transmission channels into shorter channels.
here, the researchers produced an entangled graph state for a quantum network by initially defining vertices with both nodes and quantum repeaters.
then they described how measurements at the repeater stations modify this graph state.
due to these modifications, the vertices associated with quantum repeaters are removed so that only the network nodes serve as vertices in the final quantum state, while the connecting quantum repeater lines become edges.
in the final graph state, the weights of the edges correspond to the number of quantum repeaters and how far apart they are.
consequently, by changing the weights of the edges, the new approach can optimize a given performance metric, such as security or speed.
in other words, the method can determine the best way to use quantum repeaters to achieve long- distance entanglement for large- scale quantum networks.
in the future, the researchers plan to investigate the demands for practical implementation.
they also want to extend these results to a newer research field called "quantum network coding" by generalizing the quantum repeater concept to quantum routers, which can make quantum networks more secure against macroscopic errors.
the market for quantum networking is projected to reach $5.5 billion by 2025, according to a new report from inside quantum technology (iqt).
while all computing systems rely on the ability to store and manipulate information in individual bits, quantum computers "leverage quantum mechanical phenomena to manipulate information" and to do so requires the use of quantum bits, or qubits, according to ibm.
see: quantum computing: an insider's guide (techrepublic)
quantum computing is seen as the panacea for solving the problems computers are not equipped to handle now.
"for problems above a certain size and complexity, we don't have enough computational power on earth to tackle them,'' ibm said.
this requires a new kind of computing, and this is where quantum comes in.
iqt says that quantum networking revenue comes primarily from quantum key distribution (qk), quantum cloud computing, and quantum sensor networks.
eventually, these strands will merge into a quantum internet, the report said.
cloud access to quantum computers is core to the business models of many leading quantum computer companies- such as ibm, microsoft and rigetti- as well as several leading academic institutions, according to the report.
microsoft, for instance, designed a special programming language for quantum computers, called q#, and released a quantum development kit to help programmers create new applications, according to cbinsights.
one of google's quantum computing projects involves working with nasa to apply the tech's optimization abilities to space travel.
the quantum internet network will have the same "geographical breadth of coverage as today's internet," the iqt report stated.
it will provide a powerful platform for communications among quantum computers and other quantum devices, the report said.
and will enable a quantum version of the internet of things.
"finally, quantum networks can be the most secure networks ever built - completely invulnerable if constructed properly," the report said.
the report, "quantum networks: a ten- year forecast and opportunity analysis," forecasts demand for quantum network equipment, software and services in both volume and value terms.
"the time has come when the rapidly developing quantum technology industry needs to quantify the opportunities coming out of quantum networking," said lawrence gasman, president of inside quantum technology, in a statement.
quantum key distribution (qkd) adds unbreakable coding of key distribution to public key encryption, making it virtually invulnerable, according to the report.
qkd is the first significant revenue source to come from the emerging quantum internet and will create almost $150 million in revenue in 2020, the report said.
qkd's early success is due to potential users- big financial and government organizations- have an immediate need for 100% secure encryption, the iqt report stated.
by 2025, iqt projects that revenue from "quantum clouds" are expected to exceed $2 billion.
although some large research and government organizations are buying quantum computers for on- premise use, the high cost of the machines coupled with the immaturity of the technology means that the majority of quantum users are accessing quantum through clouds, the report explained.
quantum sensor networks promise enhanced navigation and positioning and more sensitive medical imaging modalities, among other use cases, the report said.
"this is a very diverse area in terms of both the range of applications and the maturity of the technology."
however, by 2025 revenue from quantum sensors is expected to reach about $1.2 billion.
also see
the microprocessor inside a computer is a single multipurpose chip that has revolutionised people's life, allowing them to use one machine to surf the web, check emails and keep track of finances.
now, researchers from the university of bristol and nippon telegraph and telephone (ntt), have pulled off the same feat for light in the quantum world by developing an optical chip that can process photons in an infinite number of ways.
it's a major step forward in creating a quantum computer to solve problems such as designing new drugs, superfast database searches, and performing otherwise intractable mathematics that aren't possible for super computers.
the fully reprogrammable chip brings together a multitude of existing quantum experiments and can realise a plethora of future protocols that have not even been conceived yet, marking a new era of research for quantum scientists and engineers at the cutting edge of quantum technologies.
since before newton held a prism to a ray of sunlight and saw a spectrum of colour, scientists have understood nature through the behaviour of light.
in the modern age of research, scientists are striving to understand nature at the quantum level and to engineer and control quantum states of light and matter.
a major barrier in testing new theories for quantum science and quantum computing is the time and resources needed to build new experiments, which are typically extremely demanding due to the notoriously fragile nature of quantum systems.
this result shows a step change for experiments with photons, and what the future looks like for quantum technologies.
dr anthony laing, who led the project, said: "a whole field of research has essentially been put onto a single optical chip that is easily controlled.
the implications of the work go beyond the huge resource savings.
now anybody can run their own experiments with photons, much like they operate any other piece of software on a computer.
they no longer need to convince a physicist to devote many months of their life to painstakingly build and conduct a new experiment."
the team demonstrated the chip's unique capabilities by re- programming it to rapidly perform a number of different experiments, each of which would previously have taken many months to build.
jacques carolan, phd student, bristol, one of the researchers, added: "once we wrote the code for each circuit, it took seconds to re- programme the chip, and milliseconds for the chip to switch to the new experiment.
we carried out a year's worth of experiments in a matter of hours.
what we're really excited about is using these chips to discover new science that we haven't even thought of yet."
professor jeremy o'brien, director of the centre for quantum photonics, bristol university, explained: "over the last decade, we have established an ecosystem for photonic quantum technologies, allowing the best minds in quantum information science to hook up with established research and engineering expertise in the telecommunications industry.
it's a model that we need to encourage if we are to realise our vision for a quantum computer."
the university of bristol's pioneering 'quantum in the cloud' is the first and only service to make a quantum processor publicly accessible and plans to add more chips like this one to the service so others can discover the quantum world for themselves.
ibm research has announced that for the first time ever it is making quantum computing available to members of the public, who can access and run experiments on ibm's quantum processor.
ibm scientists have built a quantum processor that users can access through a first- of- a- kind quantum computing platform delivered via the ibm cloud onto any desktop or mobile device.
ibm believes quantum computing is the future of computing and has the potential to solve certain problems that are impossible to solve on today's supercomputers.
the cloud- enabled quantum computing platform, called ibm quantum experience, will allow users to run algorithms and experiments on ibm's quantum processor, work with the individual quantum bits (qubits), and explore tutorials and simulations around what might be possible with quantum computing.
the quantum processor is composed of five superconducting qubits and is housed at the ibm t.j. watson research center in new york.
the five- qubit processor represents the latest advancement in ibm's quantum architecture that can scale to larger quantum systems.
it is the leading approach towards building a universal quantum computer.
a universal quantum computer can be programmed to perform any computing task and will be exponentially faster than classical computers for a number of important applications for science and business.
a universal quantum computer does not exist today, but ibm envisions medium- sized quantum processors of 50- 100 qubits to be possible in the next decade.
with a quantum computer built of just 50 qubits, none of today's top500 supercomputers could successfully emulate it, reflecting the tremendous potential of this technology.
the community of quantum computer scientists and theorists is working to harness this power, and applications in optimisation and chemistry will likely be the first to demonstrate quantum speed- up.
"quantum computers are very different from today's computers, not only in what they look like and are made of, but more importantly in what they can do.
quantum computing is becoming a reality and it will extend computation far beyond what is imaginable with today's computers," said arvind krishna, senior vice president and director, ibm research.
"this moment represents the birth of quantum cloud computing.
by giving hands- on access to ibm's experimental quantum systems, the ibm quantum experience will make it easier for researchers and the scientific community to accelerate innovations in the quantum field, and help discover new applications for this technology."
with moore's law running out of steam, quantum computing will be among the technologies that could usher in a new era of innovation across industries.
this leap forward in computing could lead to the discovery of new pharmaceutical drugs and completely safeguard cloud computing systems.
it could also unlock new facets of artificial intelligence (which could lead to future, more powerful watson technologies), develop new materials science to transform industries, and search large volumes of big data.
quantum information is very fragile and needs to be protected from any errors that can result from heat and electromagnetic radiation.
signals are sent in and out of a cryogenic dilution refrigerator to measure operations on the quantum processor.
the ibm team has made a number of robust engineering advances both at the device level and in the electronic controls to give ibm quantum experience users unprecedented and reliably high- quality performance in this five- qubit processor.
coupled with software expertise from the ibm research ecosystem, the team has built a dynamic user interface on the ibm cloud platform that allows users to easily connect to the quantum hardware via the cloud.
the team sees the introduction to the public of this complete quantum computing framework as just the start of a new user community, which embraces the quantum world and how it works.
in the future, users will have the opportunity to contribute and review their results in the community hosted on the ibm quantum experience and ibm scientists will be directly engaged to offer more research and insights on new advances.
ibm plans to add more qubits and different processor arrangements to the ibm quantum experience over time, so users can expand their experiments and help uncover new applications for the technology.
we live in a world where classical physics defines our experiences and our intuition, and ultimately how we process information.
however, nature at the atomic level is governed by a different set of rules known as quantum mechanics.
it is beyond the reach of classical computers to solve problems that exist in nature in which quantum mechanics plays a role, for example, understanding how molecules behave.
to overcome this, in 1981, richard feynman proposed to build computers based on the laws of quantum mechanics.
over three decades later, ibm is helping to make this a reality.
quantum computing works fundamentally differently from today's computers.
a classical computer makes use of bits to process information, where each bit represents either a one or a zero.
in contrast, a qubit can represent a one, a zero, or both at once, which is known as superposition.
this property along with other quantum effects enable quantum computers to perform certain calculations vastly faster than is possible with classical computers.
most of today's quantum computing research in academia and industry is focused on building a universal quantum computer.
the major challenges include creating qubits of high quality and packaging them together in a scalable way, so they can perform complex calculations in a controllable way.
ibm employs superconducting qubits that are made with superconducting metals on a silicon chip and can be designed and manufactured using standard silicon fabrication techniques.
last year, ibm scientists demonstrated critical breakthroughs to detect quantum errors by combining superconducting qubits in latticed arrangements, and whose quantum circuit design is the only physical architecture that can scale to larger dimensions.
now, ibm scientists have achieved a further advance by combining five qubits in the lattice architecture, which demonstrates a key operation known as a parity measurement - the basis of many quantum error correction protocols.
the road towards universal quantum computing hinges upon the achievement of quantum error correction, and the ibm team has taken another important step down this challenging path.
there has been tremendous progress and interest in the field of quantum of computing in recent years.
by giving users access to the ibm quantum experience, it will help businesses and organizations begin to understand the technology's potential, for universities to grow their teaching programs in quantum computing and related subjects, and for students to become aware of promising new career paths.
"it is a beautiful challenge to pursue the path to build the first universal quantum computer, but it requires us to change how we think about the world.
access to early quantum computing prototypes will be key in imagining and developing future applications," said dario gil, vice president of science and solutions, ibm research.
"if you want to understand what a true quantum computer will do for you and how it works, this is the place to do it.
you won't experience it anywhere else."
ibm's quantum computing platform is a core initiative within the newly formed ibm research frontiers institute.
the frontiers institute is a consortium that develops and shares ground- breaking computing technologies to spur world- changing innovations.
companies from diverse industries can leverage ibm's research talent and cutting- edge infrastructure to explore what the future of quantum computing may mean for their organization and business.
founding members of the frontiers institute include samsung, jsr, and honda.
digital logic, or bits, is the only paradigm for the it world, and up to now researchers used it almost exclusively to study quantum information processing.
but european scientists, in a series of firsts, have proved that an analogue approach is far easier in the quantum world.
modern computing is digital, a series of 1s and 0s that, once combined, create powerful information processing systems.
the system is so simple - on or off, yes or no - that it almost seems dumb.
it is that very simplicity that gives digital computing its power.
it works very well.
but we have a problem.
silicon circuits are getting so small that they will soon be bumping up against a fundamental physical limit.
"we know very well that, as the miniaturisation of computers continues, at some point the carriers of information will have a size that approaches that of atoms," warns nicolas cerf, coordinator of the covaqial project.
"as classical physics becomes inapplicable, we will have to look at quantum mechanics for our future information processing systems."
and that is exactly what quantum scientists have been doing for the last 20 years.
essentially, they have been attempting to reproduce the classical, digital, computer of 1s and 0s in the microscopic world by using particles to carry information as quantum bits, or qubits.
up to now, it really was the only game in quantum town.
logic, but not as we know it
but this is changing.
covaqial led the charge for a new type of quantum information processing when it began four years ago.
it looked at an analogue logic paradigm for the quantum world, using continuous variables instead of 1s and 0s.
"in classical computing, there have been attempts to create an analogue logic, but no major success," notes cerf.
"but it turns out, for a variety of reasons, that using an analogue approach, like continuous variables, might work very well in quantum computing.
we felt it was a promising approach, so that is why we started up covaqial."
unlike qubits, where one atom or particle carries the information, continuous variables (cv) use an ensemble of atoms or photons to carry the information - the first with matter and the second with light.
both digital and analogue approaches to quantum information science use the peculiar properties of quantum particles as the 'signifier' of the information carried, such as the spin of a single electron or the polarisation of a photon for qubits, or the analogue properties of a group of electrons or photons for cv.
"it is the collective property of this group of electrons, or photons, that becomes the information carrier in cv.
when you have this many particles you can call it continuous even though there are many very small steps in the information- encoding variable," relates cerf.
the upshot, though, and what makes cv interesting, is that it is much easier to manipulate, control and experiment with than individual particles.
quantum teleportation using qubits, for example, was described in the early 1990s and proved experimentally five years later.
in contrast, teleportation with cv was proved experimentally just one year after it was theorised.
all because cvs are much easier to use.
cat out of the bag
the field looked promising, and after a series of spectacular results covaqial proved that cv could provide elegant solutions to some of the fundamental issues affecting quantum information processing.
"we achieved the first major result after less than one year.
it was an experiment demonstrating quantum memory," explains cerf.
"it's like classical memory, so it is really a prerequisite for the field."
the team demonstrated memory for a light pulse stored in an atomic 'ensemble' during one millisecond using cv.
it might not sound like much, but remember light travels several hundred kilometres in that time.
even if looped in an optical fibre, the energy is so delicate that it would disappear in well under a millisecond.
they did this at room temperature, whereas atomic qubits generally need to be super- cooled.
the second result created an optical 'schroedinger's cat'.
schrodinger's cat was a thought experiment that illustrated how objects can have two distinct states at the same time, in this case a dead cat and a live cat.
covaqial created a light pulse - an ensemble of photons - simultaneously in two states.
"it is very important for the development of a quantum repeater, which will allow quantum communications to extend to much further distances," cerf reveals.
finally, for the first time ever, an experiment demonstrated interspecies quantum teleportation.
teleportation occurs where the state of one particle is moved onto another particle.
"it had been done before with photons or atoms, but this is the first time it worked from photons to atoms.
these were our most impressive results, but we had many more," notes cerf.
as a result of their work, cvs are now a hot topic in quantum information processing, and covaqial propelled europe to leadership in the field.
now, the team will continue their work in a new european commission project, compas, starting in a few months.
"strictly speaking, covaqial was about quantum communication, but all the results will be essential for the development of quantum computing," explains cerf.
"compas will attack directly the challenges of quantum information processing using cvs."
further helping to usher in the era of the analogue quantum computer.
quantum computers use the fundamentals of quantum mechanics to potentially speed up the process of solving complex computations.
suppose you need to perform the task of searching for a specific number in a phone book.
a classical computer will search each line of the phone book until it finds a match.
a quantum computer could search the entire phone book at the same time by assessing each line simultaneously and return a result much faster.
the difference in speed is due to the computer's basic unit for processing information.
in a classical computer, that basic unit is called a bit, an electrical or optical pulse that represents either 0 or 1.
a quantum computer's basic unit is a qubit, which can represent numerous combinations of values from 0 and 1 at the same time.
it is this characteristic that may allow quantum computers to speed up calculations.
the downside of qubits is that they exist in a fragile quantum state that is vulnerable to environmental noise, such as changes in temperature.
as a result, generating and managing qubits in a controlled environment poses significant challenges for researchers.
uc santa barbara engineer galan moody, an assistant professor of electrical and computer engineering, has proposed a solution to overcome the poor efficiency and performance of existing quantum computing prototypes that use light to encode and process information.
optical systems are attractive because they naturally link quantum computing and networking in the same physical framework.
however, existing technology still requires off- chip optical operations, which dramatically reduce efficiency, performance and scalability.
in his project, "heterogeneous iii- v/silicon photonics for all- on- chip: linear optical quantum computing," moody aims to create an optical quantum computing platform in which all of the essential components are integrated onto a single semiconductor chip.
"integrated electronic circuits enabled revolutionary advancements in classical computing.
our goal is to create integrated photonic circuits that have the same impact on quantum computing," said moody, who joined ucsb's college of engineering this fall after spending six years at the national institute of standards and technology as a postdoctoral fellow and research scientist.
"this could lead to a dramatic improvement in efficiency and processing speed and enable entirely new methods of processing and transmitting information using light."
moody's research project has now received a significant boost from the united states air force.
he is one of 40 early- career scientists selected for a 2019 young investigator award from the air force office of scientific research.
winners receive $450,000 over three years to support their work.
the program is intended to foster research by young scientists that supports the air force's mission to control and maximize utilization of air, space and cyberspace, as well as related challenges in science and engineering.
"it's an honor to be among this group of talented awardees, and i am grateful for being selected," said moody.
"this award will allow my research group to make a more meaningful impact on the exciting and rapidly evolving quantum- information landscape."
in order to develop an all- electrical, all- on- chip quantum photonic platform, moody proposes to integrate three technologies that have been developed for different platforms and applications.
the components are electrically driven quantum dot single- photon sources, silicon- based photonics for optical operations, and superconducting nanowire single- photon detectors.
"we'll use physical modeling to guide the design and fabrication of the device," he said.
"quantum optical spectroscopy will give us insight into material properties and noise sources, and on- chip optical interferometers will enable measurements allowing us to improve material purity, monitor the light source and perform computations.
ultimately, we want to better understand and leverage any advantages that quantum mechanics can provide for computing and networking."
according to moody, the new technology could also have transformative impacts in areas like turn- key quantum light sources for secure communications, and for reducing the size, weight and power consumption of classical photonic devices such as lasers and leds.
physics students spend many years learning to master the often counterintuitive laws and effects of quantum mechanics.
for instance, the quantum state of a physical system may be undetermined until a measurement is made, and a measurement on one part of the system can influence the state of a distant part without any exchange of information.
it is enough to make the mind boggle.
once the students graduate and start doing research, the problems continue: to exactly determine the state of some quantum system in an experiment, one has to carefully prepare it and make lots of measurements, over and over again.
very often, what one is actually interested in cannot even be measured directly.
an international team of researchers led by giuseppe carleo, a lecturer at the institute for theoretical physics of eth zurich, has now developed machine learning software that enables a computer to "learn" the quantum state of a complex physical system based on experimental observations and to predict the outcomes of hypothetical measurements.
in the future, their software could be used to test the accuracy of quantum computers.
quantum physics and handwriting
the principle of his approach, carleo explains, is rather simple.
he uses an intuitive analogy that avoids the complications of quantum physics: "what we do, in a nutshell, is like teaching the computer to imitate my handwriting.
we will show it a bunch of written samples, and step by step it then learns to replicate all my a's, l's and so forth."
the way the computer does this is by looking at the ways, for instance, in which an "l" is written when it follows an "a."
these may not always be the same, so the computer will calculate a probability distribution that expresses mathematically how often a letter is written in a certain way when it is preceded by some other letter.
"once the computer has figured out that distribution, it could then reproduce something that looks very much like my handwriting," carleo says.
quantum physics is, of course, much more complicated than a person's handwriting.
still, the principle that carleo (who recently moved to the flatiron institute in new york), together with matthias troyer, guglielmo mazzola (both at eth) and giacomo torlai from the university of waterloo as well as colleagues at the perimeter institute and the company d- wave in canada have used for their machine learning algorithm is quite similar.
the quantum state of the physical system is encoded in a so- called neural network, and learning is achieved in small steps by translating the current state of the network into predicted measurement probabilities.
those probabilities are then compared to the actually measured data, and adjustments are made to the network in order to make them match better in the next round.
once this training period is finished, one can then use the quantum state stored in the neural network for "virtual" experiments without actually performing them in the laboratory.
faster tomography for quantum states
"using machine learning to extract a quantum state from measurements has a number of advantages," carleo explains.
he cites one striking example, in which the quantum state of a collection of just eight quantum objects (trapped ions) had to be experimentally determined.
using a standard approached called quantum tomography, around one million measurements were needed to achieve the desired accuracy.
with the new method, a much smaller number of measurements could do the same job, and substantially larger systems, previously inaccessible, could be studied.
this is encouraging, since common wisdom has it that the number of calculations necessary to simulate a complex quantum system on a classical computer grows exponentially with the number of quantum objects in the system.
this is mainly because of a phenomenon called entanglement, which causes distant parts of the quantum system to be intimately connected although they do not exchange information.
the approach used by carleo and his collaborators takes this into account by using a layer of "hidden" neurons, which allow the computer to encode the correct quantum state in a much more compact fashion.
testing quantum computers
being able to study quantum systems with a large number of components- or "qubits," as they are often called- also has important implications for future quantum technologies, as carleo points out: "if we want to test quantum computers with more than a handful of qubits, that won't be possible with conventional means because of the exponential scaling.
our machine learning approach, however, should put us in a position to test quantum computers with as many as 100 qubits."
also, the machine learning software can help experimental physicists by allowing them to perform virtual measurements that would be hard to do in the laboratory, such as measuring the degree of entanglement of a system composed of many interacting qubits.
so far, the method has only been tested on artificially generated data, but the researchers plan to use it for analysing real quantum experiments very soon.
professor michelle simmons' team at unsw sydney has demonstrated a compact sensor for accessing information stored in the electrons of individual atoms -  a breakthrough that brings us one step closer to scalable quantum computing in silicon.
the research, conducted within the simmons group at the centre of excellence for quantum computation and communication technology (cqc2t) with phd student prasanna pakkiam as lead author, was published today in the journal physical review x (prx).
quantum bits (or qubits) made from electrons hosted on single atoms in semiconductors is a promising platform for large- scale quantum computers, thanks to their long- lasting stability.
creating qubits by precisely positioning and encapsulating individual phosphorus atoms within a silicon chip is a unique australian approach that simmons' team has been leading globally.
but adding in all the connections and gates required for scale up of the phosphorus atom architecture was going to be a challenge -  until now.
"to monitor even one qubit, you have to build multiple connections and gates around individual atoms, where there is not a lot of room," says professor simmons.
"what's more, you need high- quality qubits in close proximity so they can talk to each other -  which is only achievable if you've got as little gate infrastructure around them as possible."
compared with other approaches for making a quantum computer, simmons' system already had a relatively low gate density.
yet conventional measurement still required at least 4 gates per qubit: 1 to control it and 3 to read it.
by integrating the read- out sensor into one of the control gates the team at unsw has been able to drop this to just two gates: 1 for control and 1 for reading.
"not only is our system more compact, but by integrating a superconducting circuit attached to the gate we now have the sensitivity to determine the quantum state of the qubit by measuring whether an electron moves between two neighbouring atoms," lead author pakkiam states.
"and we've shown that we can do this real- time with just one measurement -  single shot -  without the need to repeat the experiment and average the outcomes."
"this represents a major advance in how we read information embedded in our qubits," concludes simmons.
"the result confirms that single- gate reading of qubits is now reaching the sensitivity needed to perform the necessary quantum error correction for a scalable quantum computer."
australia's first quantum computing company
since may 2017, australia's first quantum computing company, silicon quantum computing pty limited (sqc), has been working to create and commercialise a quantum computer based on a suite of intellectual property developed at the australian centre of excellence for quantum computation and communication technology (cqc2t).
co- located with cqc2t on the unsw campus in sydney, sqc is investing in a portfolio of parallel technology development projects led by world- leading quantum researchers, including australian of the year and laureate professor michelle simmons.
its goal is to produce a 10- qubit demonstration device in silicon by 2022 as the forerunner to a commercial scale silicon- based quantum computer.
sqc believes that quantum computing will ultimately have a significant impact across the global economy, with possible applications in software design, machine learning, scheduling and logistical planning, financial analysis, stock market modelling, software and hardware verification, climate modelling, rapid drug design and testing, and early disease detection and prevention.
created via a unique coalition of governments, corporations and universities, sqc is competing with some of the largest tech multinationals and foreign research laboratories.
as well as developing its own proprietary technology and intellectual property, sqc will continue to work with cqc2t and other participants in the australian and international quantum computing ecosystems, to build and develop a silicon quantum computing industry in australia and, ultimately, to bring its products and services to global markets.
cambridge quantum computing ("cqc") announces that they have used the "natively quantum" structure of natural language to open up an entirely new realm of possible applications by translating grammatical sentences into quantum circuits, and then implementing the resulting programs on a quantum computer and actually performing question- answering.
this is the first time that natural language processing has been executed on a quantum computer.
furthermore, by achieving the results without relying on quantum ram, cqc scientists have created a path to truly applicable quantum advantage within the noisy intermediate- scale quantum ("nisq") era.
by using cqc's class- leading and platform- agnostic retargetable compiler t|ket> (tm), these programs were successfully executed on an ibm quantum computer, thus taking a notable step towards achieving "meaning- aware" and "grammatically informed" natural language processing - a dream of computer scientists since the earliest days of the computer age.
cqc looks forward to providing further details in the near future including ways to scale the programs so that meaningfully large numbers of sentences can be used on nisq machines as they themselves scale in quantum volume and using other types of quantum computers.
the full article with details and links to the appropriate github repository is noted below:
please click here
about cambridge quantum computing
cambridge quantum computing (cqc) is a world- leading quantum computing software company with over 60 scientists across offices in cambridge (uk), london, san francisco, washington, dc and tokyo.
cqc builds tools for the commercialisation of quantum technologies that will have a profound global impact.
cqc combines expertise in quantum software, specifically a quantum development platform (t|ket> (tm), enterprise applications in the area of quantum chemistry (eumen), quantum machine learning (qml), quantum natural language processing (qnlp) and quantum augmented cybersecurity (ironbridge (tm)).
quantum technology has the potential to revolutionize computation, cryptography, and simulation of quantum systems.
however, quantum physics places a new demand on information processing hardware: quantum states are fragile, and so must be controlled without being measured.
researchers at the niels bohr institute have now demonstrated a key property of majorana zero modes that protects them from decoherence.
the result lends positive support to the existence of majorana modes, and goes further by showing that they are protected, as predicted theoretically.
the results have been published in the prestigious scientific magazine, nature.
normal computers are limited in their ability to solve certain classes of problems.
the limitation lies in the fact that the operation of a conventional computers is based on classical states, or bits, the fundamental unit of information that is either 0 or 1.
in a quantum computer, data is stored in quantum bits, or qubits.
according to the laws of quantum mechanics, a qubit can be in a superposition of states -  a 0 and 1 at the same time.
by taking advantage of this and other properties of quantum physics, a quantum computer made of interconnected qubits should be able to tackle certain problems much more efficiently than would be possible on a classical computer.
there are many different physical systems that could in principle be used as quantum bits.
the problem is that most quantum systems lose coherence very quickly- the qubit becomes a regular bit once measured.
this is why researchers are still searching for the best implementation of quantum hardware.
enter the majorana zero mode, a delocalized state in a superconductor that resists decoherence by sharing quantum information between separated locations.
in a majorana mode, the information is stored in such a way that a disturbance of either location leaves the quantum information intact.
"we are investigating a new kind of particle, called a majorana zero mode, which can provide a basis for quantum information that is protected against measurement by a special and who knows, perhaps unique property of these particles.
majorana particles don't exist as particles on their own, but they can be created using a combination of materials involving superconductors and semiconductors.
what we find is that, first of all, the majorana modes are present, verifying previous experiments, but more importantly that they are protected, just as theory predicts," says villum kann rasmussen professor charles marcus, director of the center for quantum devices (qdev) and station q copenhagen, at the niels bohr institute, university of copenhagen.
nanowires for quantum technology
the center for quantum devices is a leading research center in quantum information technology - with activities in theory, experiment, and materials research.
semiconductor nanowires around 10 micrometers long and around 0.1 micrometers in diameter, coated with superconducting aluminum were used to form isolated islands of various lengths.
by applying a strong magnetic field along the axis of the wire, and cooling the wires to below a tenth of a kelvin, a new kind of superconducting state, called a topological superconductor, was formed.
quantum states are protected
in 2012, physicists at delft university in the netherlands found the first signatures of majorana zero modes in a similar system, with further evidence revealed in subsequent experiments around the world.
now, researchers at the center for quantum devices have demonstrated critical predictions regarding their behavior, namely that their quantum states are protected in a fundamentally different manner from conventional quantum states.
the experiments were carried out by phd candidate sven albrecht and postdoc andrew higginbotham, now at the university of colorado/nist, usa, using new superconductor- semiconductor hybrid nanowires developed by assistant professor peter krogstrup in collaboration with marcus and professor jesper nygard.
"the protection is related to the exotic property of the majorana mode that it simultaneously exists on both ends of the nanowire, but not in the middle.
to destroy its quantum state, you have to act on both ends at the same time, which is unlikely", says sven albrecht.
albrecht explains that it was a challenging effort to demonstrate the protection experimentally.
the researchers had to repeat their experiment many times with nanowires of different lengths in order to show that the protection improved with wire length.
"exponential protection is an important check as we continue our basic exploration, and ultimately application, of topological states of matter.
two things have pushed the field forward- from the first majorana sightings at delft to the present results- the first is strong interaction between theory and experiment.
the second is remarkable materials development in copenhagen, an effort that predates our center.
without these new materials, the field was rather stuck.
that's behind us now."
says charles marcus.
the majority (71%) of global organizations view quantum computers as a major security threat, a digicert report found.
with quantum computing being such a new concept, these threats haven't become widespread yet, but are expected to within the next three years.
digicert's 2019 post- quantum crypto survey report, conducted by rerez research, surveyed 400 enterprise organizations in the us, germany, and japan.
despite the impressive capabilities quantum computing promises, 95% of respondents said they are discussing at least one tactic for protecting themselves against the dangers of quantum computing.
quantum computers, which are still in the early stages of development, could potentially be able to process and solve massive computational problems that exceed the capabilities of current supercomputers.
mathematical problems that require days of calculation on current supercomputers could theoretically be solved instantaneously on quantum computers.
massive algorithms are digestible for quantum computers, opening doors in a variety of spaces including navigation, seismology, pharmaceuticals, physics, machine learning, decryption, encryption, and more, reported zdnet.
one area that is extremely exciting for those in the quantum computing realm is the prospect of quantum computers being able to crack rsa cryptography, which is commonly used for secure data transmission.
however, quantum computers' ability to crack encrypted data is precisely what makes them so dangerous, if in the wrong hands.
the dangers of quantum computing
more than half (55%) of the survey respondents said quantum computing is somewhat or extremely threatening to security today, and 71% of respondents said it would be a large threat in the future.
these fears resulted in many organizations turning to post- quantum cryptography (pqc), or crypto algorithms that secure systems against quantum computer attacks, the report found.
the median prediction for when post- quantum cryptography would be necessary for organizations to combat quantum computer threats was 2022, the report said, indicating organizations must prepare now.
some 83% of respondents emphasized the importance of it teams learning quantum- safe practices.
it organizations fear the capabilities of quantum attacks, which would make encrypted data vulnerable to hackers, but the high costs of post- quantum cryptography make it difficult to implement.
because of high costs, organizations must allocate resources responsibly toward post- quantum cryptography.
more than half (56%) of organizations are currently doing so, but other companies must follow if they want to stay protected.
the top tactics for preparing for quantum computing attacks included monitoring systems, understanding their organization's level of crypto- agility, understanding their organization's current risk level, and building knowledge about post- quantum cryptography, the report found.
how to plan for a quantum future
the report identified these best practices to help companies plan for a quantum future:
know your risk and establish a quantum crypto maturity model.
understand the importance of crypto- agility in your organization and establish it as a core practice.
work with leading vendors to establish digital certificate best practices and ensure they are tracking post- quantum cryptography industry progress to help you stay ahead of the curve, including updates to their products and solutions.
for more, check out why post- quantum encryption will be critical to protect current classical computers on techrepublic.
also see
rmit university researchers have trialled a quantum processor capable of routing quantum information from different locations in a critical breakthrough for quantum computing.
the work opens a pathway towards the "quantum data bus", a vital component of future quantum technologies.
the research team from the quantum photonics laboratory at rmit in melbourne, australia, the institute for photonics and nanotechnologies of the cnr in italy and the south university of science and technology of china, have demonstrated for the first time the perfect state transfer of an entangled qubit on an integrated photonic device.
quantum photonics laboratory director dr alberto peruzzo said after more than a decade of global research in the specialised area, the rmit results were highly anticipated.
"the perfect state transfer has emerged as a promising technique for data routing in large- scale quantum computers," peruzzo said.
"the last 10 years has seen a wealth of theoretical proposals but until now it has never been experimentally realised.
"our device uses highly optimised quantum tunnelling to relocate qubits between distant sites.
the difference between standard computing and quantum computing is comparable to solving problems over an eternity compared to a short time.
"quantum computers promise to solve vital tasks that are currently unmanageable on today's standard computers and the need to delve deeper in this area has motivated a worldwide scientific and engineering effort to develop quantum technologies," peruzzo said.
"it could make the critical difference for discovering new drugs, developing a perfectly secure quantum internet and even improving facial recognition.''
peruzzo said a key requirement for any information technology, along with processors and memories, is the ability to relocate data between locations.
full scale quantum computers will contain millions, if not billions, of quantum bits (qubits) all interconnected, to achieve computational power undreamed of today.
while today's microprocessors use data buses that route single bits of information, transferring quantum information is a far greater challenge due to the intrinsic fragility of quantum states.
"great progress has been made in the past decade, increasing the power and complexity of quantum processors," peruzzo said.
robert chapman, an rmit phd student working on the experiment, said the protocol they developed could be implemented in large scale quantum computing architectures, where interconnection between qubits will be essential.
"we experimentally relocate qubits, encoded in single particles of light, between distant locations," chapman said.
"during the protocol, the fragile quantum state is maintained and, critically, entanglement is preserved, which is key for quantum computing."
the research, experimental perfect state transfer of an entangled photonic qubit, will be published in nature communications.
engineers at caltech have shown that atoms in optical cavities - tiny boxes for light - could lead to the creation of a quantum internet.
their national science foundation- funded work was published in the journal nature physics.
quantum networks would connect quantum computers through a system that operates at a quantum, rather than classical, level.
in theory, quantum computers will one day be able to perform certain functions faster than classical computers by taking advantage of the special properties of quantum mechanics.
nsf- funded researchers are working to create the building blocks of a quantum network.
credit: wikimedia commons
as they can with classical computers, engineers would like to be able to connect multiple quantum computers to share data and work together in a quantum computer network.
"while important in their own right, quantum computer networks also represent an important step toward realizing the goal of a secure quantum internet," explained fil bartoli, director of nsf's division of electrical, communications and cyber systems.
networks would open the door to several applications, including solving computations that are too large to be handled by a single quantum computer and establishing unbreakably secure communications using quantum cryptography.
a quantum network needs to be able to transmit information between two points without altering the quantum properties of the information being transmitted.
one current model works like this: a single atom or ion acts as a quantum bit (or "qubit") storing information via one of its quantum properties, such as spin.
to read that information and transmit it elsewhere, the atom is excited with a pulse of light, causing it to emit a photon whose spin is entangled with the spin of the atom.
the photon can then transmit the information entangled with the atom over a long distance via fiber optic cable.
researchers led by caltech's andrei faraon, an applied physicist and electrical engineer, constructed a nanophotonic cavity, a beam that is about 10 microns long - a fraction of an inch - with periodic nano- patterning, sculpted from a piece of crystal.
in this cavity, scientists can excite a ytterbium ion and efficiently detect the resulting photon it emits, whose spin can be used to read the information stored in the ion's spin.
"advances like this fundamental research in quantum information science are important milestones to enable the long- term development of quantum technology," added alex cronin, a program officer in nsf's division of physics.
to commemorate the fourth anniversary of ibm making the first quantum computer available via the cloud, the company is spearheading a worldwide challenge designed to help anyone- regardless of their tech background- build quantum computing skills.
ibm launched the first quantum computer that could be programmed over the cloud in may 2016, the company said.
four years later, it has a community of 225,000 users, a fleet of 18 cloud- based quantum systems boasting a greater than 95% uptime, and more than 100 clients as part of the ibm q network, according to ibm.
the ibm quantum challenge, running may 4- 8, lets anyone tackle programming a quantum computer through the use of circuits, from writing their first "hello quantum" circuit to solving a complex optimization problem, ibm said.
"trying to explain quantum computing without resorting to incorrect analogies has always been a goal for our team," ibm said.
"as a result, we have continuously invested in education, starting with opening access to quantum computers, and continuing to create tools that enable anyone to program them."
ibm created the first interactive open source textbook, the company said.
see: quantum computing: myths v. realities (techrepublic)
"i'm proud of our team's decision four years ago to put our quantum systems online with the ibm quantum experience, aiming to accelerate innovation at a global scale by collaborating with now over 200,000 scientists, developers, and students around the world," said jay gambetta, an ibm fellow and vice president of quantum computing at ibm, in a statement.
"hands- on training such as this coding challenge, hackathons, and our quantum internship program is the best way to encourage new diverse talent to prepare for quantum computing careers."
as developers program quantum computers, what they are really doing is building and running quantum circuits, ibm said.
to support learning about quantum circuits the company is suggesting that those interested:
* read the qiskit textbook chapter where ibm defines quantum circuits as we understand them today.
dive in to explore quantum computing principles and learn how to implement quantum algorithms on your own.
* watch its newly launched live lectures called "circuit sessions," or get started programming a quantum computer by watching "coding with qiskit."
subscribe to the qiskit youtube channel to watch these two series and more.
quantum in use
quantum computing is already helping foster change.
for example, researchers at ibm and daimler ag, the parent company of mercedes- benz, wanted to help tackle the challenge of capacity and the speed at which car batteries can be charged.
the team "used a quantum computer to model the dipole moment of three lithium- containing molecules, which brings us one step closer to the next- generation lithium sulfur (li- s) batteries that would be more powerful, longer lasting and cheaper than today's widely used lithium ion batteries," ibm said.
in another example, ibm research and the university of notre dame scientists wanted to learn how to build cheaper and more efficient energy options.
the team used a cloud- based ibm quantum computer "to simulate how a chemical reaction outcome is controlled by the time evolution of the entangled state of the two reactants, and how this spin chemistry phenomenon is affected by the gradual loss of magnetization and dephasing caused by thermal fluctuations."
spin chemistry is a subfield of chemistry that deals with magnetic spin effects in chemical reactions.
notre dame researchers had for years used classical computers to study spin chemistry, ibm said.
"simulations created using those computers, however, required the introduction of artificial noise to try to realistically mimic chemical reactions, ibm said.
"working together, our team of scientists used a quantum computer to simulate how spin effects control the reaction yield."
the scientists used open pulse, a programming language within the qiskit open- source quantum computing framework, to specify pulse- level control on the quantum device, according to ibm.
the hope is that openpulse will become more of a tool to engineer noise and change quantum signals, ibm said.
"the greater control openpulse can offer, the better future experiments can simulate- and use- noise to better understand complex chemical phenomena such as artificial photosynthesis and solar energy conversion," ibm said.
in recognition of everyone's participation in the quantum challenge, ibm said it will award digital badges and provide additional sponsorship to the python software foundation.
"we believe the full potential of quantum computing reaches beyond our expectations," said gambetta.
"let's build a quantum future together."
also see
a common blue pigment used in the ps5 note could have an important role to play in the development of a quantum computer, according to a paper published today in the journal nature.
the pigment, copper phthalocyanine (cupc), which is similar to the light harvesting section of the chlorophyll molecule, is a low- cost organic semiconductor that is found in many household products.
crucially, it can be processed into a thin film that can be readily used for device fabrication, a significant advantage over similar materials that have been studied previously.
now, researchers from the london centre for nanotechnology at ucl and the university of british columbia have shown that the electrons in cupc can remain in 'superposition' - an intrinsically quantum effect where the electron exists in two states at once - for surprisingly long times, showing this simple dye molecule has potential as a medium for quantum technologies.
the development of quantum computing requires precise control of tiny individual "qubits", the quantum analogs of the classical binary bits, '0' and '1', which underpin all of our computation and communications technologies today.
what distinguishes the "qubits" from classical bits is their ability to exist in superposition states.
the decay time of such superpositions tells us how useful a candidate qubit could be in quantum technologies.
if this time is long, quantum data storage, manipulation and transmission become possible.
lead author marc warner from the london centre for nanotechnology, said: "in theory, a quantum computer can easily solve problems that a normal, classical, computer would not be able to answer in the lifetime of the universe.
we just don't know how to build one yet.
"our research shows that a common blue dye has more potential for quantum computing than many of the more exotic molecules that have been considered previously."
cupc possesses many other attributes that could exploit the spin of electrons, rather than their charge, to store and process information which are highly desirable in a more conventional quantum technology.
for example, the pigment strongly absorbs visible light and is easy to modify chemically and physically, so its magnetic and electrical properties can be controlled.
dr warner added: "the properties of copper phthalocyanine make it of interest for the emerging field of quantum engineering, which seeks to exploit the quantum properties of matter to perform tasks like information processing or sensing more effectively than has ever been possible."
in new quantum information technologies, fragile quantum states have to be transferred between distant quantum bits.
researchers at eth have now realized such a quantum transmission between two solid- state qubits at the push of a button.
data transmission is the backbone of the modern information society, on both the large and small scale.
on the internet, data are exchanged between computers all over the world, most often using fibre optic cables.
inside a computer, on the other hand, information has to be shuttled back and forth between different processors.
a reliable exchange of data is also of great importance for the new quantum information technologies that are currently being developed- but at the same time, it is also fiendishly difficult.
at the eth in zurich, a team of physicists led by andreas wallraff of the laboratory for solid state physics has now succeeded in transmitting quantum information, at the push of button and with high fidelity, between two quantum bits roughly a metre apart.
their results are published in the scientific journal nature this week.
flying quantum bits
the main peculiarity of quantum information technologies, such as quantum computers and quantum cryptography, is the use of quantum bits or "qubits" as the elementary unit of information.
unlike binary classical bits, qubits can exist in so- called superposition states.
this results in the possibility to build extremely powerful computers that make use of those superposition states to perform calculations much more efficiently and faster than classical computers.
however, those states are also very sensitive and cannot be transmitted simply using conventional techniques.
the problem is that the state of a stationary qubit first has to be transformed into a so- called "flying" qubit, for instance a photon, and then back into another stationary qubit.
a few years ago, researchers were able to transmit the quantum state of an atom in this way.
wallraff and his co- workers have now succeeded in realizing such a transmission from one superconducting solid- state qubit to another one some distance away.
to do so, the physicists connected two superconducting qubits using a coaxial cable.
the quantum state of the first qubit, which is defined by the number of superconducting electron pairs (also known as cooper pairs) contained in it, was first transferred to a microwave photon of a resonator using very precisely controlled microwave pulses.
from that resonator, the photon could then fly through the coaxial cable to a second resonator, inside of which its quantum state was transferred via microwave pulses onto the the second qubit.
similar experiments were recently carried out at yale university.
deterministic rather than probabilistic
"the important point of our method is that the transmission of the quantum state is deterministic, which means that it works at the push of a button," philipp kurpiers, a ph.d. student in wallraff's lab, emphasizes.
in some earlier experiments, a transfer of quantum states was realized, but that transmission was probabilistic: sometimes it worked, but most of the time, it didn't.
a successful transmission could, for instance, be signaled by a "heralding photon."
whenever the transmission didn't work, the researchers simply tried again.
in that way, the effective quantum transmission rate was much reduced.
for practical applications, therefore, deterministic methods such as the one now demonstrated at eth are clearly advantageous.
"our transmission rate for quantum states is among the highest ever realized, and at 80 percent, our transmission fidelity is very good in the first realization of the protocol," says andreas wallraff.
using their technique, the researchers were also able to create a quantum mechanical entanglement between the qubits as many as 50,000 times per second.
the transmission procedure itself took less than a millionth of a second, which means that there is quite a bit of room for improvement in the transmission rate.
quantum mechanical entanglement creates an intimate link between two quantum objects even across large distances, a feature that is used for cryptography or quantum teleportation.
quantum transfer for quantum computers
as a next step, the researchers want to try to use two qubits each as transmitter and receiver, which makes entanglement swapping between the qubit pairs possible.
such a process is useful for larger quantum computers, which could be built in the next few years.
so far, they only consist of a handful of qubits, but for a few hundred qubits, researchers will have to determine how to connect them most effectively in order to exploit the advantages of a quantum computer in the best possible way.
much like clusters of single computers used today, quantum computer modules could then be connected together using wallraff's technique.
the transmission distance, which is currently about a metre, could certainly be increased.
wallraff and his colleagues recently demonstrated that an extremely cold, and thus superconducting, cable could transmit photons over distances of several tens of metres with very little loss.
wiring together a quantum computing centre, therefore, seems to be quite feasible.
for now, full- fledged quantum computers are the stuff of science fiction - in last summer's blockbuster movie transformers, the bad guys use quantum computing to break into the u.s. army's secure files in just 10 seconds flat.
but prem kumar, the at&t professor of information technology in the department of electrical engineering and computer science and the director of the center for photonic communication and computing at northwestern university's mccormick school of engineering, and his research group are one step closer to realizing that technology - though for far better purposes.
the group recently demonstrated one of the basic building blocks for distributed quantum computing using entangled photons generated in optical fibers, and their research was published in the april 4 edition of physical review letters.
"because it is done with fiber and the technology that is already globally deployed, we think that it is a significant step in harnessing the power of quantum computers," kumar says.
quantum computing differs from classical computing in that a classical computer works by processing "bits" that exist in two states, either one or zero.
quantum computing uses quantum bits, or qubits, which, in addition to being one or zero can also be in a "superposition," which is both one and zero simultaneously.
this is possible because qubits are quantum units like atoms, ions, or photons that operate under the rules of quantum mechanics instead of classical mechanics.
the "superposition" state allows a quantum computer to process significantly more information than a classical computer and in a much shorter time.
the area of quantum computing took off about 14 years ago after mathematician/physicist peter shor created a quantum algorithm that could factor large integers much more efficiently than a classical computer.
such an algorithm put the computer world in a tizzy because many web sites secure information like credit card and bank account numbers over the internet through the public- key cryptography method known as rsa, after its inventors rivest, shamir, and adleman.
this method is based on the assumption that it is computationally infeasible to factor very large integers on classical computers.
though researchers are still many years away from creating a quantum computer capable of running the shor algorithm, progress has been made.
kumar's group, which uses photons as qubits, found that they can entangle two indistinguishable photons together in an optical fiber very efficiently by using the fiber's inherent nonlinear response.
they also found that no matter how far you separate the two photons in standard transmission fibers they remain entangled and are "mysteriously" connected to each other's quantum state.
for this paper, kumar and his team used the fiber- generated indistinguishable photons to implement the most basic quantum computer task - a controlled- not gate, which allows two photonic qubits to interact.
"this device that we demonstrated in the lab is a two- qubit device - nowhere near what's needed for a quantum computer - so what can you do with it"" kumar says.
"it's nice to demonstrate something useful to give a boost to the field, and there are some problems at hand that can be solved right now using what we have."
the defense advanced research projects agency has funded the group's next effort to study how to implement a quantum network for physically demonstrating efficient public goods strategies, which are similar to the mechanism design theory that nobel laureate roger myerson laid the foundation for while at northwestern.
kumar says such a network could help out with high stakes auctions, like if, for example, the department of defense wanted to build an expensive airplane and sends out a request for bids.
no one company can build the entire airplane, and there could be 15 companies that can build some part of the airplane, whether it's a navigation system or an engine.
but instead of just giving the project to the lowest bidder, the government could save public dollars by allowing these companies to bid in a complicated way that makes the process more efficient.
maybe the engine company has worked with the fuselage company before and, if they worked together again, could be more efficient and less expensive than another two companies working together.
they could then send in a conditional set of bids, along with regular bids if the two companies were to work with other companies as well.
"figuring out the best possible outcome is possible with quantum computers," kumar says.
"based on these fiber- type gates that we are building utilizing entanglement, the auctioneer has an efficient way of determining optimal outcomes when bidders make conditional bids.
when the computation is done, it reveals only the winning strategy, and all other bids disappear."
kumar says they hope to perform this experiment sometime in the next year.
wouldn't it be nice if we could just jump from using computers based on circuits to machines based on quantum bits (qubits)?
things would run ever so much faster.
alas, the problem is, scientists have to first figure out how to make it all work, and thus far, little real progress has been made.
one of the main problems is that in using light as the medium, there needs to be a way to have the photons interact in a measurable way, to see if the qubit is representing an on or off state.
thus far, researchers have used something called interferometers to do the job, which unfortunately because of their high sensitivity, tend to come out of alignment easily and often; not something that leads to good computing.
things are looking up however, as new research being done at the air force research laboratory in rome, new york by a team of computer scientists, suggests that interferometers could be embedded in a hologram, as they describe in their paper published on the preprint server arxiv; in effect, freezing them in place and preventing them from going out of alignment.
that's the good news.
the bad news is that using holograms to freeze the interferometers in place would mean that they couldn't reprogrammed; thus the resulting computing device would be but a one trick pony.
there's also the problem of scalability, because the use of interferometers means using the output of one as input to the next and because holograms by their nature take up a certain amount of space, it would mean stacking millions or even billions of them, which just wouldn't be practical.
currently the team is looking at an off the shelf product called the optigate to build their holographic interferometers, which would be easy and convenient, which is good because even if the final product can't be used as a true computer, it does seem possible that they could be used as a dedicated component on a larger system for such tasks as error- correction computations or in memory busses.
there's also the optimism factor at stake here.
building a computer that used quantum components would surely breed more enthusiasm for added research into finding a way to build a truly quantum computer, the holy grail of computer technology.
more information: quantum computing in a piece of glass, arxiv:1112.3489v1 [quant- ph] http://arxiv.org/abs/1112.3489
abstract
quantum gates and simple quantum algorithms can be designed utilizing the diffraction phenomena of a photon within a multiplexed holographic element.
the quantum eigenstates we use are the photon's linear momentum (lm) as measured by the number of waves of tilt across the aperture.
two properties of quantum computing within the circuit model make this approach attractive.
first, any conditional measurement can be commuted in time with any unitary quantum gate - the timeless nature of quantum computing.
second, photon entanglement can be encoded as a superposition state of a single photon in a higher- dimensional state space afforded by lm.
our theoretical and numerical results indicate that optigrate's photo- thermal refractive (ptr) glass is an enabling technology.
we will review our previous design of a quantum projection operator and give credence to this approach on a representative quantum gate grounded on coupled- mode theory and numerical simulations, all with parameters consistent with ptr glass.
we discuss the strengths (high efficiencies, robustness to environment) and limitations (scalability, crosstalk) of this technology.
while not scalable, the utility and robustness of such optical elements for broader quantum information processing applications can be substantial.
quantum computing remains mysterious and elusive to many, but usc viterbi school of engineering researchers might have taken us one step closer to bring such super- powered devices to practical reality.
the usc viterbi school of engineering and information sciences institute is home to the usc- lockheed martin quantum computing center (qcc), a super- cooled, magnetically shielded facility specially built to house the first commercially available quantum optimization processors - devices so advanced that there are currently only two in use outside the canadian company d- wave systems inc., where they were built: the first one went to usc and lockheed martin, and the second to nasa and google.
quantum computers encode data in quantum bits, or "qubits," which have the capability of representing the two digits of one and zero at the same time - as opposed to traditional bits, which can encode distinctly either a one or a zero.
this property, called superposition, along with the ability of quantum states to "interfere" (cancel or reinforce each other like waves in a pond) and "tunnel" through energy barriers, is what may one day allow quantum processors to ultimately perform optimization calculations much faster than is possible using traditional processors.
optimization problems can take many forms, and quantum processors have been theorized to be useful for a variety of machine learning and big data problems like stock portfolio optimization, image recognition and classification, and detecting anomalies.
yet, exactly because of the exotic way in which quantum computers process information, they are highly sensitive to errors of different kinds.
when such errors occur they can erase any quantum computational advantage.
therefore developing methods to overcome errors is of paramount importance in the quest to demonstrate "quantum supremacy".
in a new article, usc researchers walter vinci, tameem albash, and daniel lidar, put forth a scheme to minimize errors.
their solution, explained in the article "nested quantum annealing correction" published in the journal nature quantum information, is focused on reducing and correcting errors associated with heating, a type of errors that is common and particularly detrimental in quantum optimizers.
simply cooling the quantum processor further is not possible since the specialized dilution refrigerator that keeps it cool already operates at its limit, at a temperature approximately 1000 times colder than outer space.
vinci, albash and lidar have developed a new method to suppress heating errors, which they call "nested quantum annealing correction."
by coupling several qubits together on a d- wave two tm quantum optimizer, without changing the hardware of the device, these qubits act effectively as one qubit that experiences a lower temperature.
the more qubits are coupled, the lower is the temperature experienced, and in this manner the researchers can minimize the effect of heating as a source of noise or error.
furthermore, this nesting scheme is implementable not only on platforms such as the d- wave processor on which it was tested, but also on other future quantum optimization devices with different hardware architectures.
the researchers believe that this work is an important step in eliminating a bottleneck for scalable quantum optimization implementations.
"our work is part of a large scale effort by the research community aimed at realizing the potential of quantum information processing, which we all hope might one day surpass its classical counterparts," qcc scientific director, and usc viterbi professor, daniel lidar said.
more information: walter vinci et al, nested quantum annealing correction, npj quantum information (2016).
doi: 10.1038/npjqi.2016.17 walter vinci et al.
nested quantum annealing correction, npj quantum information (2016).
doi: 10.1038/npjqi.2016.17
ibm has a fleet of quantum computers.
that much is fairly well known since ibm has been actively promoting quantum computing for several years.
but ibm's quantum story will get all the more interesting next month, when a 53 qubit computer joins the line, making it the most powerful quantum computer available for use outside ibm.
"next month, ibm will make a 53- qubit quantum computer available to clients via its q network quantum cloud computing service," said bits&chips .
that network, said asian scientist magazine, and grew into an "ecosystem of fortune 500 companies, start- ups, universities and national research labs."
ibm's new machine will be part of the company's quantum computation center in poughkeepsie, new york state, marking an unveiling of its 14th quantum computer.
the center "is essentially a data center for ibm's quantum machines," said frederic lardinois in techcrunch.
the facility houses an array of other quantum computers.
"the involvement of poughkeepsie is no coincidence," said john dunn in naked security.
poughkeepsie is the heritage site where ibm built many of the mainframes that made its name synonymous with business computing.
building computers that operate under a totally different set of rules- that is how an ibm staffer starts off to explain quantum computers to a teen in an educational video.
"quantum mechanics is a branch of science...and we are using it to totally reimagine how computing works."
she spins a penny.
never just head nor just tails but a combo of heads and tails.
as cnet described the behavior of quantum computing, it is simultaneously evaluating multiple possibilities.
"quantum physics promises to change computing by ditching the traditional zeroes and ones of computing states in favour of quantum phenomena like superposition and entanglement- where separate particles influence each other and wave interference," wrote john oates in the register.
"in research terms, it promises to allow fundamentally different approaches to research in fields from chemistry and physics to financial analysis."
lucian armasu clarified what ibm means in talking about "quantum volume" and why it matters- just chasing higher qubit counts has not excited ibm.
"a high qubit number doesn't mean too much unless the error rate is sufficiently small, too," he said in tom's hardware.
this is why ibm uses a "quantum volume" formula that takes into account both the number of qubits and the error rate.
earlier this year, rebecca tan in asian scientist magazine discussed how quantum computing had its share of challenges.
there's something called coherence time of qubits, "the length of time that researchers can maintain a qubit's quantum state.
to protect them from random interference such as mechanical vibration, electromagnetic waves and temperature fluctuations, a quantum processor's qubits are kept in a dilution refrigerator that is cooled to extremely low temperatures of 10- 15 millikelvin, about a hundred times colder than outer space."
fabienne lang in interesting engineering noted we're still at an early phase.
she added, "quantum computing is limited by tricky physics, and the fact that quantum computers need to be stored at very cold and specific temperatures means that it limits the development of these systems."
stephen shankland made a similar point in cnet: "quantum computing remains a highly experimental field, limited by the difficult physics of the ultra- small and by the need to keep the machines refrigerated to within a hair's breadth of absolute zero to keep outside disturbances from ruining any calculations."
early phase indeed.
interest among potential users mounts but don't expect to see these devices turning up in dorm rooms.
ibm q's doug mcclure said "our goal is to help the global ibm q community get "quantum ready"- to prepare to take full advantage of the quantum computing era as it arrives."
ibm's vision of quantum computing adoptions does not take the shape of expectations they will replace classical computers any time soon.
ibm's talia gershon said this was just the beginning of a "many- decade adventure."
researchers have studied how a 'drumstick' made of light could make a microscopic 'drum' vibrate and stand still at the same time.
a team of researchers from the uk and australia have made a key step towards understanding the boundary between the quantum world and our everyday classical world.
quantum mechanics is truly weird.
objects can behave like both particles and waves, and can be both here and there at the same time, defying our common sense.
such counterintuitive behaviour is typically confined to the microscopic realm and the question "why don't we see such behaviour in everyday objects?"
challenges many scientists today.
now, a team of researchers have developed a new technique to generate this type of quantum behaviour in the motion of a tiny drum just visible to the naked eye.
the details of their research are published today in new journal of physics.
project principal investigator, dr. michael vanner from the quantum measurement lab at imperial college london, said: "such systems offer significant potential for the development of powerful new quantum- enhanced technologies, such as ultra- precise sensors, and new types of transducers.
"excitingly, this research direction will also enable us to test the fundamental limits of quantum mechanics by observing how quantum superpositions behave at a large scale."
mechanical vibrations, such as those that create the sound from a drum, are an important part of our everyday experience.
hitting a drum with a drumstick causes it to rapidly move up and down, producing the sound we hear.
in the quantum world, a drum can vibrate and stand still at the same time.
however, generating such quantum motion is very challenging.
lead author of the project dr. martin ringbauer from the university of queensland node of the australian research council centre for engineered quantum systems, said: "you need a special kind of drumstick to make such a quantum vibration with our tiny drum."
in recent years, the emerging field of quantum optomechanics has made great progress towards the goal of a quantum drum using laser light as a type of drumstick.
however, many challenges remain, so the authors' present study takes an unconventional approach.
dr. ringbauer continues: "we adapted a trick from optical quantum computing to help us play the quantum drum.
we used a measurement with single particles of light- photons- to tailor the properties of the drumstick.
"this provides a promising route to making a mechanical version of schrodinger's cat, where the drum vibrates and stands still at the same time."
these experiments have made the first observation of mechanical interferences fringes, which is a crucial step forward for the field.
in the experiment, the fringes were at a classical level due to thermal noise, but motivated by this success, the team are now working hard to improve their technique and operate the experiments at temperatures close to absolute zero where quantum mechanics is expected to dominate.
these future experiments may reveal new intricacies of quantum mechanics and may even help light the path to a theory that links the quantum world and the physics of gravity.
scientists with the institute for molecular engineering at the university of chicago have made two breakthroughs in the quest to develop quantum technology.
in one study, they entangled two quantum bits using sound for the first time; in another, they built the highest- quality long- range link between two qubits to date.
the work brings us closer to harnessing quantum technology to make more powerful computers, ultra- sensitive sensors and secure transmissions.
"both of these are transformative steps forward to quantum communications," said co- author andrew cleland, the john a. maclean sr.
professor of molecular engineering at the ime and uchicago- affiliated argonne national laboratory.
a leader in the development of superconducting quantum technology, he led the team that built the first "quantum machine," demonstrating quantum performance in a mechanical resonator.
"one of these experiments shows the precision and accuracy we can now achieve, and the other demonstrates a fundamental new ability for these qubits."
scientists and engineers see enormous potential in quantum technology, a field that uses the strange properties of the tiniest particles in nature to manipulate and transmit information.
for example, under certain conditions, two particles can be "entangled"- their fates linked even when they're not physically connected.
entangling particles allows you to do all kinds of cool things, like transmit information instantly to space or make unhackable networks.
but the technology has a long way to go- literally: a huge challenge is sending quantum information any substantial amount of distance, along cables or fibers.
in a study published april 22 in nature physics, cleland's lab was able to build a system out of superconducting qubits that exchanged quantum information along a track nearly a meter long with extremely strong fidelity- with far higher performance has been previously demonstrated.
"the coupling was so strong that we can demonstrate a quantum phenomenon called 'quantum ping- pong'- sending and then catching individual photons as they bounce back," said youpeng zhong, a graduate student in cleland's group and the first author of the paper.
one of scientists' breakthroughs was building the right device to send the signal.
the key was shaping the pulses correctly- in an arc shape, like opening and closing a valve slowly, at just the right rate.
this method of 'throttling' the quantum information helped them achieve such clarity that the system could pass a gold standard measurement of quantum entanglement, called a bell test.
this is a first for superconducting qubits, and it could be useful for building quantum computers as well as for quantum communications.
the other study, published april 26 in science, shows a way to entangle two superconducting qubits using sound.
a challenge for scientists and engineers as they advance quantum technology is to be able to translate quantum signals from one medium to the other.
for example, microwave light is perfect for carrying quantum signals around inside chips.
"but you can't send quantum information through the air in microwaves; the signal just gets swamped," cleland said.
the team built a system that could translate the qubits' microwave language into acoustic sound and have it travel across the chip- using a receiver at the other end that could do the reverse translation.
it required some creative engineering: "microwaves and acoustics are not friends, so we had to separate them onto two different materials and stack those on top of each other," said audrey bienfait, a postdoctoral researcher and first author on the study.
"but now that we've shown it is possible, it opens some interesting new possibilities for quantum sensors."
more information: y. p. zhong et al.
violating bell's inequality with remotely connected superconducting qubits, nature physics (2019).
doi: 10.1038/s41567- 019- 0507- 7
a. bienfait et al.
phonon- mediated quantum state transfer and remote qubit entanglement, science (2019).
doi: 10.1126/science.aaw8415
for years, physicists have been heralding the revolutionary potential of using quantum mechanics to build a new generation of supercomputers, unbreakable codes, and ultra- fast and secure communication networks.
the brave new world of quantum technology may be a big step closer to reality thanks to a team of university of calgary researchers that has come up with a unique new way of testing quantum devices to determine their function and accuracy.
their breakthrough is reported in today's edition of science express, the advanced online publication of the prestigious journal science.
"building quantum machines is difficult because they are very complex, therefore the testing you need to do is also very complex," said barry sanders, director of the u of c's institute for quantum information science and a co- author of the paper.
"we broke a bunch of taboos with this work because we have come up with an entirely new way of testing that is relatively simple and doesn't require a lot of large and expensive diagnostic equipment."
similar to any electronic or mechanical device, building a quantum machine requires a thorough understanding of how each part operates and interacts with other parts if the finished product is going to work properly.
in the quantum realm, scientists have been struggling to find ways to accurately determine the properties of individual components as they work towards creating useful quantum systems.
the u of c team has come up with a highly- accurate method for analyzing quantum optical processes using standard optical techniques involving lasers and lenses.
"it is a completely different approach to quantum characterization than we have seen before," said post- doctoral researcher mirko lobino, the paper's lead author.
"this process will be able to tell us if something is working correctly and will hopefully lead the way towards a quantum certification process as we move from quantum science to making quantum technology."
the development of quantum computers is considered the next major advancement in computer processing and memory power but is still in its infancy.
unlike regular silicon- based computers that transmit information in binary units (bits) using 1 and 0, quantum computers use the subatomic physical processes of quantum mechanics to transmit information in quantum bits (qubits) that can exist in more than two states.
computers based on quantum physics are predicted to be far more powerful than computers based on classical physics and could break many of the most advanced codes currently used to secure digital information.
quantum physics is also being used to try and create new, unbreakable encryption systems.
the same research group at the u of c, led by physics professor alexander lvovsky, made headlines earlier this year when they were one of two teams to independently prove it's possible to store a special kind of light, called a "squeezed vacuum."
that work is considered the initial step towards creating memory systems for quantum computing.
quantum physics sets the laws that dominate the universe at a small scale.
the ability to harness quantum phenomena could lead to machines like quantum computers, which are predicted to perform certain calculations much faster than conventional computers.
one major problem with building quantum processors is that the tracking and controlling quantum systems in real time is a difficult task because quantum systems are overwhelmingly fragile: manipulating these systems carelessly introduces significant errors in the final result.
new work by a team at aalto could lead to precise quantum computers.
the researchers report controlling quantum phenomena in a custom- designed electrical circuit called a transmon.
chilling a transmon chip to within a few thousandths of a degree above absolute zero induces a quantum state, and the chip starts to behave like an artificial atom.
one of the quantum features that interests researchers is that the energy of the transmon can only take specific values, called energy levels.
the energy levels are like steps on a ladder: a person climbing the ladder must occupy a step, and can't hover somewhere between two steps.
likewise, the transmon energy can only occupy the set values of the energy levels.
shining microwaves on the circuit induces the transmon to absorb the energy and climb up the rungs of the ladder.
in work published 8 february in the journal science advances, the group from aalto university led by docent sorin paraoanu, senior university lecturer in the department of applied physics, has made the transmon jump more than one energy level in a single go.
previously, this has been possible only by very gentle and slow adjustments of the microwave signals that control the device.
in the new work, an additional microwave control signal shaped in a very specific way allows a fast, precise change of the energy level.
dr. antti vepsalainen, the lead author, says, "we have a saying in finland: 'hiljaa hyvaa tulee' (slowly does it).
but we managed to show that by continuously correcting the state of the system, we can drive this process more rapidly and at high fidelity."
dr. sergey danilin, one of the co- authors, describes quantum control- the process of using chips like transmons to build quantum computers- by extending the "climbing a ladder" analogy.
"to get a useful quantum system, you need to imagine climbing a ladder while holding a glass of water- it works if one does it smoothly, but if you do it too fast, the water spills.
certainly, this requires a special skill."
the researchers found that in the quantum world, the trick for climbing the ladder quickly without spilling any water is by carefully jumping two rungs at a time.
this short- cut up the energy ladder was achieved by making the transmon absorb two microwave photons at the same time.
the laws of nature put a restriction on how fast any quantum energy switch can occur, even with short- cuts, a restriction called the "quantum speed limit."
to their delight, the aalto scientists found that their new method resulted in changes to the energy level that took place at speeds that were close to this theoretically calculated limit.
the wider impact of controlling high- speed energy transfers in quantum systems is also exciting to the team.
of potentially high importance are quantum computing and quantum simulation applications, which requires fast and highly robust operations such as state preparation and the creation of quantum gates.
dr. paraoanu sees other opportunities, as well: "we would like to understand more deeply the processes related to energy transfer, which are ubiquitous in the natural world and in the technology that surround us.
for example, are there any fundamental limits to how fast we can charge the battery of an electric car?"
in the rapidly developing field of quantum technologies, it is possible that this new control method will find multiple applications.
while quantum computer researchers have strung together a few atoms to demonstrate that logic circuits using quantum operations could actually work, the question of how to scale such circuits to build useful computers has gone largely unanswered so far.
now, a physicist at the national institute of standards suggests, a new type of quantum computer architecture could hold the solution.
a fundamental barrier to quantum computing has been the delicate balance required at the atomic level to maintain the quantum states that represent digital logic.
the difficulty of maintaining quantum states creates a high error rate that in turn requires complicated error correction systems, and those schemes only get more complicated as the number of gates increases.
the right architecture would allow large- scale quantum computers to tolerate the noise levels that might be found in actual working quantum circuits, said nist's emanuel knill, who works at the boulder, colo., facility and who published a detailed description of the proposal in a recent issue of nature.
"there is a tremendous gap between theory and experiment in quantum computing," knill said.
"this work reduces the gap, showing that building quantum computers may be easier than we thought."
knill is part of a new movement to create fault- tolerant quantum computer architectures that have a reasonable chance of being built using what is now known about quantum logic gate construction.
until now, all proposed error- correcting schemes would produce quantum circuits in which the error correction functions would consume most of the computing resources.
knill proposes a nested hierarchy of qubits that allows errors to be corrected as part of the basic operation of logic gates, rather than requiring complicated error correction operations as an add- on.
qubits are the logical equivalent of bits in standard cmos circuits, but they have unusual computational properties owing to quantum- mechanical effects.
the lure of qubit- based computing is a dramatic increase in computational power compared with conventional, transistor- based boolean logic.
the important parameter for designing reliable quantum computers is the error probability per gate (epg).
current proposals for quantum computers require the epg to be lower than 10- 6.
experimental physicists ex- pect to be able to achieve only a hundredfold higher epg, of 10- 4.
and current experiments are not even close to that higher figure: the epg for the ion- trap technology being developed at nist, for example, is around 3 x 10- 2, knill said.
knill's architecture proposes to bridge that gap.
the proposed architecture could achieve reliable computing with nist's current ion- trap rate, but the amount of hardware would be prohibitively high.
an important aspect of the design approach, however, is the ability to make trade- offs between a specified epg rating and the amount of hardware required to achieve reliable results.
computer simulations of the proposed architecture show that an efficient computer could be created for an epg in the 10- 4 to 10 to the 10- 2 range.
one goal of the research is to predict the amount of hardware needed based on a given epg and the desired complexity of the computation.
the error- correcting starts with a base level of qubits that would perform the simplest possible error correction on the base array.
quantum teleportation, in which the quantum state of one group of atoms transfers to another group with no physical link, would then shift the corrected states to successive levels in the hierarchy.
groups of adjacent physical qubits would represent virtual qubits with successively lower error rates.
the higher the number of qubits in the base level, the higher the ultimate level of error correction.
for example, a string of 36 physical qubits could be grouped in nine strings of four adjacent qubits, each group of four representing a pair of qubits on the next level in the hierarchy.
the mathematics of quantum operations on the groups of four would be used to derive the states of the next level in the hierarchy, eliminating possible errors in the base level.
error correction thus would occur as part of the teleportation of quantum states to higher levels in the hierarchy.
in this case, at level 3, the original 36 quantum states would produce a single qubit pair with a highly reduced probability of error.
since teleportation allows the quantum state to be transferred directly, no knowledge of its parameters is required, and thus no read/write operations are involved.
unique to quantum mechanics, this type of operation is effective in controlling errors because the transfer of information has no physical effect on nearby qubits.
qubit 'bus'
another project at nist has proposed quantum teleportation as a means of creating a quantum- computing "bus" for sending data between qubit- based gates, in similar fashion to the buses found in conventional ics and computers.
the proposed architecture, created by nist researchers carl williams, gavin brennan and daegene song, uses teleportation to emulate the standard von neumann computer achitecuture, but there is no actual physical bus involved.
the scheme has a positive effect on error reduction, but whether that reduction would be sufficient to enable its practical use with current quantum logic gates is an open question.
knill cited another quantum computer architecture, proposed by andrew steane at oxford univesity's centre for quantum computation, that uses architectural features to reduce errors.
steane's approach is to make error correction a part of the process of reading the state of qubits and to use a set of quantum- computing codes that are optimized to enable teleportation between ancilla blocks.
knill expects steane's method to be efficient at epgs of 10- 3, reducing the amount of hardware required by a factor between 10 and 100 over other approaches to error correction.
but the actual trade- off between epg and hardware complexity will be fixed by the architecture.
more- accurate theory and computer simulations will be necessary to make definitive judgments on just how much complexity the various fault- tolerant quantum computer architectures will actually require.
the lure of quantum computing is the dramatic increase in computational power that can be extracted from quantum gates.
a 10- bit operation in a conventional circuit would equate to a parallel operation on 1,024 bits in a quantum computer.
but that assumes that no errors are made during the computation.
a new technique to study the properties of molecules and materials on a quantum simulator has been discovered.
the ground- breaking new technique, by physicist oleksandr kyriienko from the university of exeter, could pioneer a new pathway towards the next generation of quantum computing.
current quantum computing methods for studying the properties of molecules and materials on such a minute scale rely upon an ideal fault- tolerant quantum computer or variational techniques.
this new proposed approach, instead relies on the implementation of quantum evolution that would be readily available in many systems.
the approach is favourable for modern state- of- the- art quantum setups, notably including cold atom lattices, and can serve as a software for future applications in material science.
the study could pave the way to studying the properties of strongly correlated systems, including coveted fermi- hubbard model, which can potentially offer the explanation of high- temperature superconductivity.
the research is published in the new nature journal npj quantum information.
dr. kyriienko, part of the physics department at the university of exeter and lead author said: "so far i have seen that the ability to run quantum dynamics can be used for finding the ground state properties.
"the question, however, remains- can we use it for studying excited states?
can we devise other powerful algorithm based on the principles?
the experience tells this is possible, and will be a subject of future efforts."
the idea of quantum simulation was proposed by nobel prize winner richard feynman in 1982, where he suggested that quantum models can be most naturally simulated if we use a well- controlled and inherently quantum system.
developing on this idea, a separate branch of quantum information science has emerged, based on the notion of quantum computer- a universal quantum device where digital sequences of operations (quantum gates) allow to solve certain problems with superior scaling of required operation as compared to conventional classical computers.
however, the original feynman's intention, which was later named analog quantum simulation, so far was mostly used for observing dynamical properties of quantum systems, while precluding finding the ground state associated to various computation tasks.
in the new study, oleksandr kyriienko has shown that it is possible to exploit sequential evolution of the system with wavefunction overlap measurements, such that effective study of ground state properties becomes possible with analog quantum simulators.
the main technique which allows to reach ground state is effective representation of non- unitary operator which "distils" the ground state by running the sum of unitary evolution operators for different evolution times.
importantly, the study suggests that dynamics of the quantum system is a valuable resource for computation, as the ability to propagate the system paired with overlap measurements can give access to the low- temperature spectrum of a quantum system which define its behaviour.
the findings establish the framework with dynamics- based quantum simulation using programmable quantum simulators, and serve as a quantum software to many well- controlled quantum lattice systems where large number of atoms (~100) precludes classical simulation.
this in turn can revolutionize our understanding of complex condensed matter systems and chemistry.
more information: oleksandr kyriienko, quantum inverse iteration algorithm for programmable quantum simulators, npj quantum information (2020).
doi: 10.1038/s41534- 019- 0239- 7
when scientists develop a full quantum computer, the world of computing will undergo a revolution of sophistication, speed and energy efficiency that will make even our beefiest conventional machines seem like stone age clunkers by comparison.
but, before that happens, quantum physicists like the ones in uc santa barbara's physics professor john martinis' lab will have to create circuitry that takes advantage of the marvelous computing prowess promised by the quantum bit ("qubit"), while compensating for its high vulnerability to environmentally- induced error.
in what they are calling a major milestone, the researchers in the martinis lab have developed quantum circuitry that self- checks for errors and suppresses them, preserving the qubits' state(s) and imbuing the system with the highly sought- after reliability that will prove foundational for the building of large- scale superconducting quantum computers.
it turns out keeping qubits error- free, or stable enough to reproduce the same result time and time again, is one of the major hurdles scientists on the forefront of quantum computing face.
"one of the biggest challenges in quantum computing is that qubits are inherently faulty," said julian kelly, graduate student researcher and co- lead author of a research paper that was published in the journal nature.
"so if you store some information in them, they'll forget it."
unlike classical computing, in which the computer bits exist on one of two binary ("yes/no", or "true/false") positions, qubits can exist at any and all positions simultaneously, in various dimensions.
it is this property, called "superpositioning," that gives quantum computers their phenomenal computational power, but it is also this characteristic which makes qubits prone to "flipping," especially when in unstable environments, and thus difficult to work with.
"it's hard to process information if it disappears," said kelly.
however, that obstacle may just have been cleared by kelly, postdoctoral researcher rami barends, staff scientist austin fowler and others in the martinis group.
the error process involves creating a scheme in which several qubits work together to preserve the information, said kelly.
to do this, information is stored across several qubits.
"and the idea is that we build this system of nine qubits, which can then look for errors," he said.
qubits in the grid are responsible for safeguarding the information contained in their neighbors, he explained, in a repetitive error detection and correction system that can protect the appropriate information and store it longer than any individual qubit can.
"this is the first time a quantum device has been built that is capable of correcting its own errors," said fowler.
for the kind of complex calculations the researchers envision for an actual quantum computer, something up to a hundred million qubits would be needed, but before that a robust self- check and error prevention system is necessary.
key to this quantum error detection and correction system is a scheme developed by fowler, called the surface code.
it uses parity information -  the measurement of change from the original data (if any) -  as opposed to the duplication of the original information that is part of the process of error detection in classical computing.
that way, the actual original information that is being preserved in the qubits remains unobserved.
why?
because quantum physics.
"you can't measure a quantum state, and expect it to still be quantum," explained barends.
the very act of measurement locks the qubit into a single state and it then loses its superpositioning power, he said.
therefore, in something akin to a sudoku puzzle, the parity values of data qubits in a qubit array are taken by adjacent measurement qubits, which essentially assess the information in the data qubits by measuring around them.
"so you pull out just enough information to detect errors, but not enough to peek under the hood and destroy the quantum- ness," said kelly.
this development represents a meeting of the best in the science behind the physical and the theoretical in quantum computing -  the latest in qubit stabilization and advances in the algorithms behind the logic of quantum computing.
"it's a major milestone," said barends.
"because it means that the ideas people have had for decades are actually doable in a real system."
quantum computers of the future hold promise for solving complex problems more quickly than ordinary computers.
for example, they can factor large numbers exponentially faster than classical computers, which would allow them to break codes in the most commonly used cryptography system.
there are other potential applications for quantum computers, too, such as solving complicated chemistry problems involving the mechanics of molecules.
but exactly what types of applications will be best for quantum computers, which still may be a decade or more away from becoming a reality, is still an open question.
in a new caltech study, accepted by the institute of electrical and electronics engineers (ieee) 2017 symposium on foundations of computer science, researchers have demonstrated that quantum computing could be useful for speeding up the solutions to "semidefinite programs," a widely used class of optimization problems.
these programs include so- called linear programs, which are used, for example, when a company wants to minimize the risk of its investment portfolio or when an airline wants to efficiently assign crews to its flights.
the study presents a new quantum algorithm that could speed up solutions to semidefinite problems, sometimes exponentially.
quantum algorithms are sets of instructions that tell quantum computers what to do to solve problems.
"one of the goals of quantum computing is to speed up computations to levels that far exceed what classical computers can do," says fernando brandao, the bren professor of theoretical physics at caltech.
brandao's co- author is krysta svore of microsoft, which partially funded the study.
the new quantum algorithm would, in particular, greatly speed up semidefinite programs that are used to learn unknown quantum states.
brandao says that this type of "quantum learning" problem is faced by researchers who study large quantum systems in a variety of different systems such as superconducting qubits, which are quantum information units similar to computer bits that would operate based on superconducting technology.
the semidefinite programs are used to give a description of how the quantum matter is behaving, and this, in turn, allows the researchers to better understand the bizarre states of the subatomic world.
"this type of application is a good candidate for use in quantum computing," says brandao.
"we are still far from knowing all the applications of quantum computing, and that's part of the excitement- there are possibilities we haven't even dreamed of yet."
the study, titled, "quantum speed- ups for semidefinite programming," was funded by microsoft, the national science foundation, and caltech.
at&t's foundry is teaming up with the california institute of technology to help accelerate the development of quantum networking technologies, the carrier announced wednesday.
the work will be completed through the newly formed alliance for quantum technologies (aqt), which will focus on uniting industry, government, and academic players to develop technology behind and practical applications of quantum networking.
the collaboration will also include a research and development program called inqnet (intelligent quantum networks and technologies), which will focus on the need for capacity and security in communications through future quantum networking technologies.
"quantum computing and networking holds the potential to radically transform how we connect as a society.
it will make the impossible possible, as the internet once did," at&t's vp of ecosystem and innovation igal elbaz commented.
"the at&t foundry was founded to advance new products and services through innovation and collaboration.
it's the ideal place for this work as quantum technologies become a rapidly developing field in industrial research."
according to at&t, quantum networking is the process of connecting quantum computers and devices together to create superfast and secure networks.
at the core, the idea is to apply the laws of quantum mechanics to processing and information distribution.
but getting there requires knowledge from across a range of disciplines, including physics, engineering, computer science, and applied mathematics, the carrier said.
the carrier said quantum computers of the future won't include traditional pc elements like a keyboard or monitor.
instead, they will include advanced technologies like cryogenics for cooling, lasers, and other solid- state, electronic, optical, and atomic devices.
at&t indicated moving that concept from the lab to reality is a heavy lift, but one that could help speed scientific discoveries, boost machine intelligence, and create networks with capabilities beyond what we can imagine today.
at&t said quantum technology is still in its early stages, but one of the first demonstrations of intelligent and quantum network technologies will be in quantum entanglement distribution and relevant benchmarking and validation studies using commercial fiber provided by the carrier.
quantum cryptography is the safest way to encrypt data.
it utilizes the fact that transmitted information can only be measured with a strictly limited degree of precision.
scientists at ludwig- maximilians- university in munich and eth zurich have now discovered how the use of a quantum memory affects this uncertainty.
a quantum particle is hard to grasp, because one cannot determine all its properties precisely at the same time.
measurements of certain parameter pairs such as position and momentum remain inaccurate to a degree given by heisenberg's uncertainty principle.
this is important for the security of quantum cryptography, where information is transmitted in the form of quantum states such as the polarization of particles of light.
a group of scientists from lmu and the eth in zurich, including professor matthias christandl, has now shown that position and momentum can be predicted more precisely than heisenberg's uncertainty principle would lead one to expect, if the recipient makes use of a quantum memory that employs ions or atoms.
the results show that the magnitude of the uncertainty depends on the degree of correlation ("entanglement") between the quantum memory and the quantum particle.
"the result not only enhances our understanding of quantum memories, it also provides us with a method for determining the degree of correlation between two quantum particles", says christandl.
"moreover, the effect we have observed could yield a means of testing the security of quantum cryptographic systems."
(nature physics online, july 25, 2010)
unlike classical computers, quantum computers operate not with bits, but with quantum bits or qubits, quantum mechanical states of particles.
the crucial feature of qubits is that they can exist in different states at once, not just 0 or 1, but also as a superposition of 0 and 1.
the ability to exploit superposition states is what makes quantum computers potentially so powerful.
"the goal of our research is to work out how quantum memories, i.e.
memory systems for qubits, might be utilized in the future and how they affect the transmission of quantum bits", explains christandl, who left lmu munich in june 2010 to take up a position in the institute of theoretical physics at the eth in zurich.
heisenberg's uncertainty principle plays a central role in quantum computing, because it sets a fundamental limit to the accuracy with which a quantum state can be determined.
quantum mechanics also tells us that the measurement of a parameter can itself perturb the state of a particle.
if, for example, one were to measure the position of a particle with infinite precision, the particle's momentum would become completely uncertain.
quantum cryptography uses this effect to encrypt data, for instance by entangling two quantum particles in a way that the probability with which the measurement of one particle yields a certain value depends on the state of the other particle.
eavesdropping can thus easily be uncovered, because any measurement will change the state of the particle measured.
the teams at lmu and the eth zurich have now shown that the result of a measurement on a quantum particle can be predicted with greater accuracy if information about the particle is available in a quantum memory.
atoms or ions can form the basis for such a quantum memory.
the researchers have, for the first time, derived a formula for heisenberg's principle, which takes account of the effect of a quantum memory.
in the case of so- called entangled particles, whose states are very highly correlated (i.e.
to a degree that is greater than that allowed by the laws of classical physics), the uncertainty can disappear.
according to christandl, this can be roughly understood as follows "one might say that the disorder or uncertainty in the state of a particle depends on the information stored in the quantum memory.
imagine having a pile of papers on a table.
often these will appear to be completely disordered -  except to the person who put them there in the first place."
"our results not only improve our understanding of quantum memories, they also give us a way of measuring entanglement", says christandl.
"the effect could also help us to test the security of quantum cryptographic systems."
one can picture the method as a game in which player b transmits a particle to player a.
a then performs a measurement on the particle, introducing an uncertainty.
a subsequent measurement by b will only yield the value determined by a with an uncertainty given by heisenberg's principle.
"but if b uses a quantum memory", says christandl, "he can determine the correct value and win the game."
more information: "the uncertainty principle in the presence of quantum memory", m. berta, m. christandl, r. colbeck, j.m.
renes, r. renner
nature physics, 25 july 2010.
doi:10.1038/nphys1734
netherlands - the prospect of a quantum internet has excited physicists for two decades.
a quantum internet will allow the transmission of information around the world with perfect security and make cloud- based quantum computing a reality.
but first, physicists must perfect the technology of quantum routing- the ability to receive and transmit quantum information without destroying it.
that's a significant challenge.
the key is a technique called quantum teleportation, which transmits information from one point to another without it passing through the space in between.
this is a routine operation in any decent quantum optics lab but quantum routing- which concatenates the process- is another challenge altogether.
wolfgang pfaff at the kavli institute of nanoscience delft in the netherlands and a few pals say they've take a significant step toward this goal with the first demonstration of diamond teleporters that can act as nodes in a quantum network.
"these results establish diamond spin qubits as a prime candidate for the realization of quantum networks for quantum communication and network- based quantum computing," they say.
the fundamental difficulty in quantum routing is that quantum information is fragile stuff.
so quantum teleportation has always involved creating a qubit, teleporting it and then immediately measuring it to check whether teleportation has been successful.
however, the process of measurement destroys quantum information.
so an important goal is to create routers that can read and write quantum information without destroying it.
the process of quantum routing is to first read the stored quantum information, to then teleport it and finally to store it again ready to be teleported along the next leg of the network.
one of the most promising approaches is to store a qubit in the spin of atomic nuclei.
that's because nuclei are relatively immune from the electric and magnetic fields that can destroy, or decohere, quantum information when it is stored in electronic spins.
nuclear spins are also easy to store in crystals, unlike photons which are famously slippery and difficult to store.
so a number of groups have been working on quantum memories consisting of single atoms of things like nitrogen or phosphorous.
these store information in the spin of their nucleus but can transfer it to electronic spin where it can then be passed on to a photon, for example, for transmission.
but how best to store single atomic memories in a way that allows them to be robustly controlled?
one approach is to bury phosphorous atoms in silicon and then address them with an electrode on the surface.
the trouble here is that these atoms tend to migrate through the silicon lattice and so get lost.
another option is to embed nitrogen atoms in a diamond crystal and control them with microwaves.
this has turned out to be more promising because the atoms are trapped more securely in the lattice and are in any case easy to spot because they fluoresce.
this is exactly the approach that pfaff and pals have perfected at delft.
they created two quantum memories out of a pair of diamond crystals with single nitrogen atoms embedded in each.
they then stored a qubit in the nuclear spin of the first crystal using microwave pulses.
this is later transferred to an electron spin orbiting the nucleus.
in the process of teleportation, they create a pair of entangled photons and send one to each crystal.
the interaction between the entangled photon and the stored qubit in the first crystal allows the quantum information it carries to be teleported to the second crystal where it ends up stored in the nuclear spin of the nitrogen atom there.
"the source state is successfully teleported in each of the experimental runs," say pfaff and co.
the key thing here is that the qubit ends up stored in the second crystal, ready for transmission to yet another crystal.
in other words, pfaff and co have demonstrated the techniques necessary for quantum routing.
there are challenges ahead, of course.
in these experiments, the crystals are only three meters apart but they will ultimately have to be separated by telecommunication optical fiber.
so a key goal will be to get this technique working at wavelengths suited to standard optical fiber so that it is compatible with modern communications infrastructure.
that's easier said than done.
and obviously, an important step will be routing quantum information from one node to another via a third, something that nobody has yet achieved.
in the meantime, these guys have taken a small but an important step towards robust, scalable quantum networking.
"the ability to generate remote entanglement and to control and read out multiple qubits per node as shown in the present teleportation experiment makes nv centers a leading candidate for realizing a quantum network," they say.
so having twiddle their thumbs for two decades waiting for quantum routers, perhaps physicists will not have too much longer to wait until they finally get them.
an international research group led by scientists from the university of bristol, uk, and the university of queensland, australia, has demonstrated a quantum algorithm that performs a true calculation for the first time.
quantum algorithms could one day enable the design of new materials, pharmaceuticals or clean energy devices.
the team implemented the 'phase estimation algorithm'- a central quantum algorithm which achieves an exponential speedup over all classical algorithms.
it lies at the heart of quantum computing and is a key sub- routine of many other important quantum algorithms, such as shor's factoring algorithm and quantum simulations.
dr xiao- qi zhou, who led the project, said: "before our experiment, there had been several demonstrations of quantum algorithms, however, none of them implemented the quantum algorithm without knowing the answer in advance.
this is because in the previous demonstrations the quantum circuits were simplified to make it more experimentally feasible.
however, this simplification of circuits required knowledge of the answer in advance.
unlike previous demonstrations, we built a full quantum circuit to implement the phase estimation algorithm without any simplification.
we don't need to know the answer in advance and it is the first time the answer is truly calculated by a quantum circuit with a quantum algorithm."
professor jeremy o'brien, director of the centre for quantum photonics at the university of bristol said: "implementing a full quantum algorithm without knowing the answer in advance is an important step towards practical quantum computing.
it paves the way for important applications, including quantum simulations and quantum metrology in the near term, and factoring in the long term."
amazon is now offering some of its enterprise customers the ability to explore quantum computing over the cloud.
enterprise customers such as boeing are collaborating with amazon web services (aws) to explore how developing and testing quantum algorithms in simulations could lead to breakthroughs in advanced material research.
ibm has been offering quantum cloud computing services to select customers since 2016.
to be clear, there are no quantum computing applications in use at this time.
and no quantum computer does anything that remotely resembles practical work.
but big tech is anticipating a quantum computing age, and ibm, microsoft, and now amazon have been pestered long enough by their big enterprise customers to get started exploring this quantum future.
braket
the aws product is called braket, a name that comes from the bra- ket notation used to describe quantum states.
bra- ket notation was created by the nobel prize winning physicist paul dirac in 1939.
according to amazon, braket is currently "in preview" but will launch to all aws customers in 2020.
amazon is joining microsoft, ibm and google in the fight to commercialize quantum computing for enterprise customers.
since the computing hardware that powers quantum computers is notoriously fickle, quantum cloud computing is attractive for enterprise customers that seek to benefit from the technology without having to be responsible for quantum computing hardware maintenance and repair.
quantum computers are also incredibly expensive to build, maintain and repair.
the select amazon clients that will use braket will have access to experiment with multiple quantum computers from companies like rigetti computing, iono and d- wave systems.
microsoft, ibm and amazon are currently allowing their enterprise customers to experiment with quantum cloud computing and explore new applications to advance their specific industry interests.
but ibm has been offering enterprise customers access to its quantum computers since 2016, and microsoft announced last month that it would be offering its customers the use of multiple quantum computers, so what's the difference between them?
the differences in ibm, microsoft and amazon's quantum cloud computing services
the first difference is perhaps the most significant: ibm has its own quantum computers, while microsoft and amazon do not.
ibm developers can join their q network and use the open- source programming framework qiskit to interact with their quantum computers.
ibm's q system one, touted as the world's first integrated universal quantum computing system for commercial applications, is only accessible through the cloud.
pictured here is ibm's q system one.
jpmorgan chase is the main financial services partner with ibm to develop and pioneer quantum computing applications for the financial industry that will improve trading, risk analysis and portfolio optimization, among other things.
daimler ag will be working with ibm to create new use cases for quantum computing in the automotive and transportation sectors, including optimizing manufacturing processes and fleet logistics, as well as exploring quantum chemistry to discover new materials.
samsung is working with the q system one over the cloud to understand how quantum computing could help improve its material science processes and explore the impact it may have on the company's semiconductor and electronics interests.
(image courtesy of ibm.)
microsoft announced its quantum computing cloud service azure quantum in early november.
azure quantum offers both free and paid access to prototype quantum computers, as does ibm's q network.
azure quantum is not available at this time.
microsoft's quantum hardware partners are honeywell, ionq and qci.
honeywell and ionq use quantum processors that write data using single ions that cannot escape electromagnetic fields.
qci uses the other approach to quantum computing hardware.
it uses superconductive circuitry, which is also the technique favored by google, ibm and amazon.
microsoft has at least one partner involved in azure quantum: dow chemical, which is exploring quantum computing to make headway on extremely difficult chemistry problems.
its service will be open to the public soon.
amazon's braket is using ionq (like microsoft) and two others: the d- wave 2000q and the rigetti 16q aspen- 4.
as an addendum to the unveiling of braket, amazon announced that it is opening a new facility: aws center for quantum computing.
it will be located near the caltech campus in pasadena, calif., which will give some of the brightest minds on the planet access to a few things: amazon's quantum computing experts, a world- class facility, and other researchers who will join them in their experiments and work.
amazon is hoping the center will yield new technologies and innovations that will propel the company toward its goal of figuring out how to mass produce quantum computers and recognize which applications and problems are best solved on quantum computers.
quantum computers are based on quantum bits, or qubits.
qubits can exist in a state of 1 or 0 like classical bits, but also in superpositions of 1 and 0.
this opens up a huge new range of data attributes because the superposition can be any combination of the range of values of 1 and 0, rather than just 1 or just 0.
this means that an infinite number of states are possible on a qubit.
the potential benefits of quantum computing are staggering.
by exploiting the bizarre properties of quantum mechanics, quantum computers could vastly outperform classical computers in certain types of difficult calculations.
bottom line
a huge amount of work is needed before quantum computing will have any real practical or commercial uses.
innovations are happening, and real business value added via quantum computing is still nonexistent.
but by this time next year, amazon, ibm, microsoft and google will all have quantum cloud computing services available for enterprise customers.
"if moore's law holds for another 10- 15 years," says dr. raymond laflamme, "we'll have transistors the size of atoms."
laflamme is a physicist at the university of waterloo in ontario, canada.
he is part of a team of physicists working on making quantum computing a reality.
in the may 1 issue of physical review letters, laflamme's team, which is composed of scientists from the institute of quantum computing at the university of waterloo, the perimeter institute for theoretical physics in waterloo, and mit's department of nuclear engineering, proposed a benchmark for determining the effectiveness of future quantum computers.
right now, the definition of moore's law is that data density doubles every 18 months.
at that rate, classical computing will be unable to handle the information in less than two decades.
while this presents challenges, laflamme can see the possibilities as well.
"computers never really do what we want them to do," he says.
"it's more of an approximation."
but he believes that when we take computers to the quantum level, we just might be able to get those pesky machines to do exactly what we want them to do.
the problem now, says laflamme, is that in classical physics, and in classical computing, a bit, a piece of information, can only occupy one position at a time.
"but," he explains, "the laws of physics change at the quantum level.
in quantum mechanics, they can exist in two places at once."
so, he says, the question becomes whether or not we can harness this property.
laflamme says we can.
"ten years ago we saw this was possible, and this allows us to solve problems that were intractable before."
over the past seven or eight years, explains laflamme, physicists and mathematicians have come up with blueprints for quantum computers.
they can be implemented in small systems that can actually be controlled in a lab.
they demonstrate how to control a small number of qubits (bits of quantum information).
but how can one compare these blueprints and find the most promising model?
this is where laflamme's team comes in.
their prl paper describes a benchmark that can be used to determine how well a quantum computer works.
the algorithm they demonstrate in the paper effectively demonstrates a benchmark for a 12- qubit system.
while this amount of information is not particularly impressive (since it can be done on a classical computer), laflamme points out its usefulness:
"right now we need a classical computer to see how it works.
it's kind of like a crutch.
but when we get up to 30 or 40 qubits, we won't be able to do it.
what we do today is to find ways to control the system so that we can go deeper into the quantum world where classical computers will not be of help to understand what is going on here."
today, physicists are working on ways to understand how quantum systems work.
"back when the wright brothers were building airplanes," laflamme explains, "some physicists said that we couldn't build such a thing.
but now we have huge boeing 747s.
it would have been heresy to claim that a huge metal contraption could carry people through the sky."
the key, says laflamme, is to understand how it works.
once we understand how quantum mechanics works, and how to control it, quantum computers with amazing capability can be built and used.
the difference will be as profound as the changes in flight.
these changes will come about as a result of establishing benchmarks for quantum computers and developing the systems with the most likely success.
"right now," says laflamme, "we show two methods [in the paper].
one takes many resources and is incredibly precise.
the other takes fewer resources and is not as precise."
unfortunately, the more precise method, while stronger and better, is not scalable.
it cannot be made into a practical pattern to be copied and made into several models of a quantum computer.
"what we are working toward," says laflamme, "and what you will probably see next year, is a way to bring the best of both methods together."
even though there are a few scientists that still pooh- pooh the idea of building quantum computers, laflamme is confident.
"we will learn the systems, and as we go deeper we will find the best way to control this force of nature.
quantum computing is not a figment of imagination."
by miranda marquit, copyright 2006 physorg.com
researchers from the university of sussex have created a new method for producing and controlling qubits- and it's led to the first truly accessible plan for a real- world quantum computer made from available tech.
quantum computers will play a vital role in society as they will provide significantly reduced computational times, efficient algorithm solving, and improved security.
but if quantum computers are so great and highly sought after, why are they not available?
the truth is that billions upon billions of dollars (as well as some of the greatest minds) have been thrown at the quantum world but the quantum computer is still confined to the laboratory setup.
unlike classical computers which rely on bits storing '1' and '0', qubits store quantum information.
so far, qubits have generally been created in labs using various methods and various materials.
qubit creation and the study of concepts like quantum entanglement have thus far only been possible in the controlled spaces of laboratories.
but researchers at the university of sussex are trying to take quantum computer technology out of the realm of academia and into the hands of researchers everywhere.
qubit creation: voltage instead of lasers
while there are multiple methods of creating qubits and performing quantum calculations, one method that shows significant promise is the use of trapped ions.
typically, two lasers are directed towards an individual ion which creates a trapped ion (i.e., a charged atom) which can be used as a qubit.
however, two lasers are needed per qubit which means a quantum computer with any real power- billions of qubits- would need billions of lasers which is clearly not practical.
however, this is where the university of sussex comes in as researchers there have found a new production method that could really be the spark to set off the quantum computer race.
researchers at the university of sussex have successfully created qubits without the need of laser pairs to focus on ions.
instead, the ions, which sit on top of the 'quantum chip', are exposed to both a magnetic field gradient as well as a microwave field.
then, individual ions can be moved towards and away from other ions on the chip using voltages.
this movement of ions is crucial in the design as the movement of the ions allows qubits to interact with each other or to be separated.
this work has been led by professor winfried hensinger and dr. sebastian weidt at the ion quantum technology research group at the sussex centre for quantum technologies.
the voltage control has three main levels of voltage which can make the ions perform three different functions.
the lowest voltage causes the ion to perform no logical function, the second voltage causes the ion to become a one- input quantum gate, and the third voltage causes the ion to interact with a neighbor ion and become a two- input quantum gate.
according to the team, the quantum chip behaves more like an fpga than a cpu in having programmable gates.
what makes this design very important is the fact that all the gates use the same magnetic and microwave fields in addition to being voltage- controlled.
this makes the production of a quantum chip trivial in comparison to current methods of having to use highly precise lasers which need a target accuracy of 5um.
the fact that the ions can be controlled by voltages means that interfacing a quantum chip with a classical electronic device could be as easy as connecting two standard devices (say, a microcontroller and serial memory device).
producing the needed magnetic field can be done using current carrying wires- but what about the microwave field?
does the field need to be perfect and precise?
according to the team, neither phase uniformity or intensity need to be equal everywhere.
the only major requirement of the field is that a reasonable amplitude is applied throughout the device.
this further strengthens the argument that this quantum computer production technique could provide a foundation for all future quantum devices.
blueprints for the next steps
this quantum chip creation method is already proving to be invaluable in the development of quantum computers.
when it was announced, professor hensinger was quoted as saying "we will construct a large- scale quantum computer at sussex making full use of this exciting new technology."
now, mere months later, it's obvious that he's been busy.
yesterday, hensinger and weidt (working with an international team and the british government) published a paper in science advances detailing a plan to use quantum computer modules to network together into a scalable quantum computer.
this "blueprint" is the first of its kind as a clear and feasible path towards a working quantum computer.
the paper emphasizes that the research into replacing lasers in quantum chip creation is vital.
the simple application of voltage to create the chips, paired with the use of an architecture that utilizes modern silicon fabrication techniques, means that companies and governments around the world would likely be able to create such module- based quantum computers with relative ease.
the result could be accessible- and possibly remarkably powerful- quantum computers.
read the new paper in science advances here.
read more
will quantum computers be the end of bitcoin?
the quantum bridge - new quantum emitters
quantum computers and photon emission
rigetti computing, a pioneer in hybrid quantum- classical computing systems, has been awarded up to $8.6 million from the defense advanced research projects agency (darpa), as part of a larger collaboration with the nasa quantum artificial intelligence laboratory (quail) and universities space research association (usra), to develop a full- stack system with proven quantum advantage for solving real world problems.
in particular, the work will address complex scheduling problems that remain hard or impossible for classical computers to solve.
using quantum computers to find new solutions could have important implications for national security, such as real- time strategic asset deployment, as well as commercial applications including global supply chain management, network optimization, or vehicle routing.
"we're honored to be chosen by darpa and believe we are uniquely positioned to demonstrate quantum advantage for this class of problem," said mandy birch, senior vice president, engineering strategy at rigetti.
"we believe strongly in an integrated hardware and software approach, which is why we're bringing together the scalable rigetti chip architecture with the algorithm design and optimization techniques pioneered by the nasa- usra team."
the collaboration will focus on developing a superconducting quantum processor, hardware- aware software, and custom algorithms based on real- world scenarios.
the work will leverage rigetti's fab- 1- the only dedicated quantum integrated circuit foundry in the u.s.- to manufacture chips that scale beyond 100 qubits.
in addition, the nasa- usra team will design methods for benchmarking the hardware against classical computers to determine quantum advantage.
the grant is part of the darpa onisq (optimization with noisy intermediate- scale quantum) program.
the goal of the program is to establish that quantum information processing using nisq devices has a quantitative advantage for solving real- world combinatorial optimization problems as compared with the best known classical methods.
about rigetti computing
rigetti computing is an integrated quantum systems company.
through our quantum cloud services platform, rigetti machines can be integrated into any public, private or hybrid cloud.
rigetti serves customers in finance, insurance, government, defense, and energy with custom software and full- stack quantum computing solutions focused on simulation, optimization, and machine learning applications.
the company is based in berkeley, ca with offices in fremont, ca, washington, d.c., london, and australia.
learn more at rigetti.com.
lauren rugani
the rules of quantum mechanics describe how atoms and molecules act very differently from the world around us.
scientists have made progress toward teasing out these rules- essential for finding ways to make new molecules and better technology- but some are so complex that they evade experimental verification.
with the advent of open- access quantum computers, scientists at the university of chicago saw an opportunity to do a very unusual experiment to test some of these quantum principles.
their study, which appeared jan. 31 in nature communications physics, essentially hijacks a quantum computer to discover fundamental truths about the quantum behavior of electrons in molecules.
"quantum computing is a really exciting realm to explore fundamental questions.
it allows us to observe aspects of quantum theory that are absolutely untouchable with classical computers," said prof. david mazziotti, professor of chemistry and author on the paper.
one particular rule of quantum mechanics, called the pauli exclusion principle, is that two electrons cannot occupy the same position in space at the same time.
in many cases, a molecule's electrons experience additional restrictions on their locations; these are known as the generalized pauli constraints.
"these rules inform the way that all molecules and matter form," said mazziotti.
in this study, mazziotti, prof. david shuster and graduate student scott smart created a set of algorithms that would ask ibm's q experience computer to randomly generate quantum states in three- electron systems, and then measure where the electrons are most probably located.
"suppose that the generalized pauli constraints were not true: in that scenario, about half of the quantum states would exhibit a violation," said smart, the first author on the paper.
instead, in the many quantum states formed, they found that violations of generalized pauli constraints occurred very rarely in a pattern consistent with noise in the quantum circuit.
the results provide strong experimental verification, the scientists said.
"the simplest generalized pauli constraints were discovered theoretically on a classical computer at ibm in the early 1970s, so it is fitting that for the first time they would be experimentally verified on an ibm quantum computer," mazziotti said.
the discovery is another breakthrough at the frontier of quantum efforts at the university; recent efforts have included a three- laboratory quantum "teleporter," steps toward more powerful quantum sensors, and a collaboration to develop algorithms for emerging quantum computers.
an open question is how the generalized pauli constraints may be useful for improving quantum technology.
"they will potentially contribute to achieving more efficient quantum calculations as well as better error correction schemes- critical for quantum computers to reach their full potential," mazziotti said.
intel labs today unveiled what is believed to be a first- of- its- kind cryogenic control chip - code- named "horse ridge" - that will speed up development of full- stack quantum computing systems.
horse ridge will enable control of multiple quantum bits (qubits) and set a clear path toward scaling larger systems - a major milestone on the path to quantum practicality.
pictured above, stefano pellerano, principal engineer at intel labs, holds horse ridge.
the new cryogenic control chip will speed development of full- stack quantum computing systems, marking a milestone in the development of a commercially viable quantum computer.
(credit: walden kirsch/intel corporation).
developed together with intel's research collaborators at qutech, a partnership between tu delft and the netherlands organization for applied scientific research (tno), horse ridge is fabricated using intel's 22nm finfet technology.
in- house fabrication of these control chips at intel will dramatically accelerate the company's ability to design, test and optimize a commercially viable quantum computer.
"while there has been a lot of emphasis on the qubits themselves, the ability to control many qubits at the same time had been a challenge for the industry.
intel recognized that quantum controls were an essential piece of the puzzle we needed to solve in order to develop a large- scale commercial quantum system.
that's why we are investing in quantum error correction and controls.
with horse ridge, intel has developed a scalable control system that will allow us to significantly speed up testing and realize the potential of quantum computing," stated jim clarke, intel's director of quantum hardware.
why it matters:
in the race to realize the power and potential of quantum computers, researchers have focused extensively on qubit fabrication, building test chips that demonstrate the exponential power of a small number of qubits operating in superposition.
however, in early quantum hardware developments - including design, testing and characterization of intel's silicon spin qubit and superconducting qubit systems - intel identified a major bottleneck toward realizing commercial- scale quantum computing: interconnects and control electronics.
with horse ridge, intel introduces an elegant solution that will enable the company to control multiple qubits and set a clear path toward scaling future systems to larger qubit counts - a major milestone on the path to quantum practicality.
what quantum practicality is:
quantum computers promise the potential to tackle problems that conventional computers can't handle by leveraging a phenomena of quantum physics that allows qubits to exist in multiple states simultaneously.
as a result, qubits can conduct a large number of calculations at the same time - dramatically speeding up complex problem- solving.
the quantum research community is still at mile one of a marathon toward demonstrating quantum practicality, a benchmark against which the quantum research community can determine whether a quantum system can deliver game- changing performance to solve real- world problems.
intel's investment in quantum computing covers the full hardware and software stack in pursuit of the development and commercialization of a practical, commercially viable quantum system.
why horse ridge is important:
to date, researchers have been focused on building small- scale quantum systems to demonstrate the potential of quantum devices.
in these efforts, researchers have relied on existing electronic tools and high- performance computing rack- scale instruments to connect the quantum system inside the cryogenic refrigerator to the traditional computational devices regulating qubit performance and programming the system.
these devices are often custom- designed to control individual qubits, requiring hundreds of connective wires into and out of the refrigerator in order to control the quantum processor.
this extensive control cabling for each qubit will hinder the ability to scale the quantum system to the hundreds or thousands of qubits required to demonstrate quantum practicality, not to mention the millions of qubits required for a commercially viable quantum solution.
with horse ridge, intel radically simplifies the control electronics required to operate a quantum system.
replacing these bulky instruments with a highly- integrated system- on- chip (soc) will simplify system design and allow for sophisticated signal processing techniques to accelerate set- up time, improve qubit performance and enable the system to efficiently scale to larger qubit counts.
more about horse ridge:
horse ridge is a highly- integrated, mixed- signal soc that brings the qubit controls into the quantum refrigerator - as close as possible to the qubits themselves.
it effectively reduces the complexity of quantum control engineering from hundreds of cables running into and out of a refrigerator to a single, unified package operating near the quantum device.
designed to act as a radio frequency (rf) processor to control the qubits operating in the refrigerator, horse ridge is programmed with instructions that correspond to basic qubit operations.
it translates those instructions into electromagnetic microwave pulses that can manipulate the state of the qubits.
named for one of the coldest regions in oregon, the horse ridge control chip was designed to operate at cryogenic temperatures - approximately 4 kelvin.
to put this in context, 4 kelvin is only warmer than absolute zero - a temperature so cold that atoms nearly stop moving.
this feat is particularly exciting as intel progresses its research into silicon spin qubits, which have the potential to operate at slightly higher temperatures than current quantum systems require.
today, a quantum computer operates at in the millikelvin range - just a fraction of a degree above absolute zero.
but silicon spin qubits have properties that could allow them to operate at 1 kelvin or higher temperatures, which would dramatically reduce the challenges of refrigerating the quantum system.
as research progresses, intel aims to have cryogenic controls and silicon spin qubits operate at the same temperature level.
this will enable the company to leverage its expertise in advanced packaging and interconnect technologies to create a solution with the qubits and controls in one streamlined package.
quantum corp. (nasdaq: qmco) today named integrated media technologies inc. (imt) global partner of the year in media and entertainment for achieving the highest revenue in annual sales among all of quantum's channel partners in fy2020.
the company brings expertise in media asset management and workflow consulting, pre- sales, sales, as well as providing quantum certified professional services and managed services.
they saw notable success selling quantum's f- series nvme servers for editing, rendering and processing video content and other large unstructured data sets.
imt will be honored at the virtualq | nab partners 2020 event, a virtual event quantum will host for partners on april 23, 2020.
"imt is a trusted extension of quantum's sales force, and for the second year in a row they have achieved top sales figures by demonstrating a command of the unique challenges media and entertainment companies face when managing video data," said jamie lerner, president and ceo, quantum.
"imt shares our relentless commitment to our customers' success and an understanding of the technologies that are essential for maintaining business continuity through these challenging times."
"quantum's commitment to innovation with their f- series nvme storage has been a key factor to imt's success this past year," said jason kranitz, president, systems integration, imt.
"imt shares in quantum's vision to focus on video, and together we have been able to win significant market share with customers in media, broadcast, post production, sports production, and corporate video."
join quantum at virtualq | nab partners 2020
due to the current covid- 19 pandemic, in place of quantum's traditional pre- nab partner event held in las vegas, the company will host a video event for partners on april 23 at 9:00 am pdt / 12:00 pm edt.
during this session quantum executives and key team members will share information on the upcoming virtualq | nab end user event, including tips on how to schedule appointments with customers; learning sessions; quantum's vision for "changed workflows forever," and important updates to quantum's product roadmap and marketing initiatives.
about quantum
quantum technology and services help customers capture, create and share digital content - and preserve and protect it for decades.
with solutions built for every stage of the data lifecycle, quantum's platforms provide the fastest performance for high- resolution video, images, and industrial iot.
that's why the world's leading entertainment companies, sports franchises, researchers, government agencies, enterprises, and cloud providers are making the world happier, safer, and smarter on quantum.
see how at www.quantum.com.
quantum, and the quantum logo are registered trademarks of quantum corporation and its affiliates in the united states and/or other countries.
all other trademarks are the property of their respective owners.
"safe harbor" statement: this press release contains "forward- looking" statements.
all statements other than statements of historical fact are statements that could be deemed forward- looking statements.
specifically, but without limitation, statements regarding the collaboration between quantum and its partner imt to win market share with customers in media, broadcast, post production, sports production and corporate video.
are forward- looking statements within the meaning of the safe harbor.
all forward- looking statements in this press release are based on information available to quantum on the date hereof.
these statements involve known and unknown risks and uncertainties, including the ongoing impact of the covid- 19 pandemic on the economy, which is outside of quantum's control, , the company's ability to develop and deliver the solutions needed to meet the increasing demand for managing video and unstructured data, and other factors that may cause quantum's actual results to differ materially from those implied by the forward- looking statements.
more detailed information about these risk factors are set forth in quantum's periodic filings with the securities and exchange commission, including, but not limited to, those risks and uncertainties listed in the section entitled "risk factors," in quantum's annual report on form 10- k filed with the securities and exchange commission on august 6, 2019.
quantum expressly disclaims any obligation to update or alter its forward- looking statements, whether as a result of new information, future events or otherwise, except as required by applicable law.
physicists developing a prototype quantum hard drive have improved storage time by a factor of more than 100.
the team's record storage time of six hours is a major step towards a secure worldwide data encryption network based on quantum information, which could be used for banking transactions and personal emails.
"we believe it will soon be possible to distribute quantum information between any two points on the globe," said lead author manjin zhong, from the research school of physics and engineering (rspe).
"quantum states are very fragile and normally collapse in milliseconds.
our long storage times have the potential to revolutionise the transmission of quantum information."
quantum information promises unbreakable encryption because quantum particles such as photons of light can be created in a way that intrinsically links them.
interactions with either of these entangled particles affect the other, no matter how far they are separated.
the team of physicists at anu and the university of otago stored quantum information in atoms of the rare earth element europium embedded in a crystal.
their solid- state technique is a promising alternative to using laser beams in optical fibres, an approach which is currently used to create quantum networks around 100 kilometres long.
"our storage times are now so long that it means people need to rethink what is the best way to distribute quantum data," ms zhong said.
"even transporting our crystals at pedestrian speeds we have less loss than laser systems for a given distance."
"we can now imagine storing entangled light in separate crystals and then transporting them to different parts of the network thousands of kilometres apart.
so, we are thinking of our crystals as portable optical hard drives for quantum entanglement."
after writing a quantum state onto the nuclear spin of the europium using laser light, the team subjected the crystal to a combination of a fixed and oscillating magnetic fields to preserve the fragile quantum information.
"the two fields isolate the europium spins and prevent the quantum information leaking away," said dr jevon longdell of the university of otago.
the anu group is also excited about the fundamental tests of quantum mechanics that a quantum optical hard drive will enable.
"we have never before had the possibility to explore quantum entanglement over such long distances," said associate professor matthew sellars, leader of the research team.
"we should always be looking to test whether our theories match up with reality.
maybe in this new regime our theory of quantum mechanics breaks."
their research is published in nature.
more information: "optically addressable nuclear spins in a solid with a six- hour coherence time."
nature 517, 177- 180 (08 january 2015) doi: 10.1038/nature14025
"quantum information: spin memories in for the long haul."
nature 517, 153- 154 (08 january 2015) doi: 10.1038/517153a
quantum computing
us semiconductor giant intel has unveiled a control chip which it says is a milestone towards commercial quantum computing.
the horse ridge cryogenic control chip is anticipated to speed up the development of quantum computers thanks to its ability to control multiple qubits (the quantum equivalent of a bit) at a time.
jim clarke, intel's director of quantum hardware, said in a press release: "while there has been a lot of emphasis on the qubits themselves, the ability to control many qubits at the same time had been a challenge for the industry.
intel recognized that quantum controls were an essential piece of the puzzle we needed to solve in order to develop a large- scale commercial quantum system.
that's why we are investing in quantum error correction and controls.
with horse ridge, intel has developed a scalable control system that will allow us to significantly speed up testing and realize the potential of quantum computing."
see also:
the chip was developed in collaboration with qutech, a partnership between delft university and the netherlands organisation for applied scientific research and fabricated with existing in- house intel processes.
the potential of the chip comes from the fact that it bypasses existing, custom designed methods of controlling individual qubits, which lack the same potential for scale owing to their complexity.
the new control chip is able to operate in proximity to the qubits at cryogenic temperatures, hence being significantly more simple.
indeed, the chip takes its name from one of the coldest places in the state of oregon.
quantum computing is experiencing a burst of activity of late, with google confirming the achievement of quantum supremacy (the point at which a quantum computer performs a task beyond the means of conventional computers) back in october.
the advent of the technology poses questions owing to quantum computers' anticipated capabilities in fields such as cryptography, where, due to their highly parallel nature, they can crack encryption methods that would take conventional computer aeons.
(image: intel)
a new protocol for estimating unknown optical processes, called unitary operations, with precision enhanced by the unique properties of quantum mechanics has been demonstrated by scientists and engineers from the university of bristol, uk, and the centre for quantum technologies in singapore.
the work, published in the june issue of optica, could lead to both dramatically better sensors for medical research and new approaches to benchmark the performance of ultra- powerful quantum computers.
history tells us the ability to measure parameters and sense phenomena with increasing precision leads to dramatic advances in identifying new phenomena in science and improving the performance of technology: famous examples include x- ray imaging, magnetic resonance imaging (mri), interferometry and the scanning- tunnelling microscope.
scientists are understanding how to engineer and control quantum systems to vastly expand the limits of measurement and sensing is growing rapidly.
this area, known as quantum metrology, promises to open up radically alternative methods to the current state- of- the- art in sensing.
in this new study, the researchers re- directed the sensing power of quantum mechanics back on itself to characterise, with increased precision, unknown quantum processes that can include individual components used to build quantum computers.
this ability is becoming more and more important as quantum technologies move closer to real applications.
dr xiao- qi zhou of bristol's school of physics said: "a really exciting problem is characterizing unknown quantum processes using a technique called quantum process tomography.
you can think of this as a problem where a quantum object, maybe a photonic circuit of optics or an atomic system, is locked in a box.
we can send quantum states in and we can measure the quantum states that come out.
our challenge is to correctly identify what is in the box.
this is a difficult problem in quantum mechanics and it is a highly active area of research because its solution is needed to enable us to test quantum computers as they grow in size and complexity."
one major shortcoming of quantum process tomography is that precision using standard techniques is limited by a type of noise known as 'shot noise'.
by borrowing techniques from quantum metrology, the researchers were able to demonstrate precision beyond the shot noise limit.
they expect their protocol can also be applied to build more sophisticated sensors that identify molecules and chemicals more precisely by observing how they interact with quantum states of light.
co- author rebecca whittaker, a phd student in bristol's centre for quantum photonics said: "the optical process we measured here can be used to manipulate quantum bits of information in a quantum computer but they can also occur in nature.
for example, our setup could be used to measure how the polarisation of light is rotated by a sample.
we could then infer properties of that sample with better precision.
"increasing measurement precision is particularly important for probing light- sensitive samples where we want to get as much information as we can before our probe light damages or causes alterations to the sample.
we feel this will have a big impact on the tools used in medical research."
the researchers' protocol relies on generating multiple photons in an entangled state and this study demonstrates that they can reconstruct rotations which act on the polarisation of light.
a form of quantum weirdness is a key ingredient for building quantum computers according to new research from a team at the university of waterloo's institute for quantum computing (iqc).
in a new study published in the journal nature, researchers have shown that a weird aspect of quantum theory called contextuality is a necessary resource to achieve the so- called magic required for universal quantum computation.
one major hurdle in harnessing the power of a universal quantum computer is finding practical ways to control fragile quantum states.
working towards this goal, iqc researchers joseph emerson, mark howard and joel wallman have confirmed theoretically that contextuality is a necessary resource required for achieving the advantages of quantum computation.
"before these results, we didn't necessarily know what resources were needed for a physical device to achieve the advantage of quantum information.
now we know one," said mark howard, a postdoctoral fellow at iqc and the lead author of the paper.
"as researchers work to build a universal quantum computer, understanding the minimum physical resources required is an important step to finding ways to harness the power of the quantum world."
quantum devices are extremely difficult to build because they must operate in an environment that is noise- resistant.
the term magic refers to a particular approach to building noise- resistant quantum computers known as magic- state distillation.
so- called magic states act as a crucial, but difficult to achieve and maintain, extra ingredient that boosts the power of a quantum device to achieve the improved processing power of a universal quantum computer.
by identifying these magic states as contextual, researchers will be able to clarify the trade- offs involved in different approaches to building quantum devices.
the results of the study may also help design new algorithms that exploit the special properties of these magic states more fully.
"these new results give us a deeper understanding of the nature of quantum computation.
they also clarify the practical requirements for designing a realistic quantum computer," said joseph emerson, professor of applied mathematics and canadian institute for advanced research fellow.
"i expect the results will help both theorists and experimentalists find more efficient methods to overcome the limitations imposed by unavoidable sources of noise and other errors."
contextuality was first recognized as a feature of quantum theory almost 50 years ago.
the theory showed that it was impossible to explain measurements on quantum systems in the same way as classical systems.
in the classical world, measurements simply reveal properties that the system had, such as colour, prior to the measurement.
in the quantum world, the property that you discover through measurement is not the property that the system actually had prior to the measurement process.
what you observe necessarily depends on how you carried out the observation.
imagine turning over a playing card.
it will be either a red suit or a black suit - a two- outcome measurement.
now imagine nine playing cards laid out in a grid with three rows and three columns.
quantum mechanics predicts something that seems contradictory - there must be an even number of red cards in every row and an odd number of red cards in every column.
try to draw a grid that obeys these rules and you will find it impossible.
it's because quantum measurements cannot be interpreted as merely revealing a pre- existing property in the same way that flipping a card reveals a red or black suit.
measurement outcomes depend on all the other measurements that are performed - the full context of the experiment.
contextuality means that quantum measurements can not be thought of as simply revealing some pre- existing properties of the system under study.
that's part of the weirdness of quantum mechanics.
quantum entanglement is a key feature of quantum computing.
yet, how can researchers verify that a quantum computer actually incorporates large- scale entanglement?
conventional methods require a large number of repeated measurements, presenting research difficulties.
aleksandra dimic from the university of belgrade and borivoje dakic from the austrian academy of sciences and the university of vienna have developed a novel method for which even a single experimental run suffices to prove the presence of entanglement.
their results are published in the online open access journal npj quantum information.
the ultimate goal of quantum information science is to develop quantum computers, fully- fledged controllable devices that make use of the quantum states of subatomic particles to store information.
as with all quantum technologies, quantum computing is based on a peculiar feature of quantum mechanics known as quantum entanglement.
the basic units of quantum information, qubits, need to correlate in this particular way in order for the quantum computer to achieve its full potential.
one of the main challenges is to make sure that a fully functional quantum computer is working as anticipated.
in particular, scientists need to show that the large number of qubits are reliably entangled.
conventional methods require a large number of repeated measurements on the qubits for reliable verification.
the more often a measurement is repeated, the more certain researchers can be about the presence of entanglement.
therefore, benchmarking entanglement in large quantum systems requires a lot of resources and time, which is practically difficult or simply impossible.
can we prove entanglement with only a low number of measurement trials?
in the current study, the researchers have developed a novel verification method requiring significantly fewer resources, and in many cases, even only a single measurement to prove large- scale entanglement with a high confidence.
aleksandra dimic from the university of belgrade suggests this analogy: "consider a machine that simultaneously tosses 10 coins.
we manufactured the machine such that it should produce correlated coins.
we now want to validate whether the machine produces the anticipated result.
imagine a single trial revealing all coins landing on tails.
this is a clear signature of correlations, as 10 independent coins have 0.01 percent chance of landing on the same side simultaneously.
from such an event, we certify the presence of correlations with more than 99.9 percent confidence.
this situation is very similar to quantum correlations captured by entanglement."
borivoje dakic says, "in contrast to classical coins, qubits can be measured in many, many different ways.
the measurement result is still a sequence of zeros and ones, but its structure heavily depends on how we choose to measure individual qubits.
we realized that, if we pick these measurements in a peculiar way, entanglement will leave unique fingerprints in the measured pattern."
the method promises a dramatic reduction in time and resources needed for reliable benchmark of future quantum devices.
more information: aleksandra dimic et al, single- copy entanglement detection, npj quantum information (2018).
doi: 10.1038/s41534- 017- 0055- x
researchers at delft university of technology have created a quantum circuit that enables them to listen to the weakest radio signal allowed by quantum mechanics.
this new quantum circuit opens the door to possible future applications in areas such as radio astronomy and medicine (mri).
it also enables researchers to do experiments that can shed light on the interplay between quantum mechanics and gravity.
we have all been annoyed by weak radio signals at some point in our lives: our favourite song in the car turning to noise, being too far away from our wifi router to check our email.
our usual solution is to make the signal bigger, for instance by picking a different radio station or by moving to the other side of the living room.
what if, however, we could just listen more carefully?
weak radio signals are not just a challenge for people trying to find their favourite radio station, but also for magnetic resonance imaging (mri) scanners at hospitals, as well as for the telescopes scientists use to peer into space.
in a quantum 'leap' in radio frequency detection, researchers in the group of prof. gary steele in delft demonstrated the detection of photons or quanta of energy, the weakest signals allowed by the theory of quantum mechanics.
quantum chunks
one of the strange predictions of quantum mechanics is that energy comes in tiny little chunks called 'quanta'.
what does this mean?
"say i am pushing a kid on a swing," lead researcher mario gely said.
"in the classical theory of physics, if i want the kid to go a little bit faster i can give them a small push, giving them more speed and more energy.
quantum mechanics says something different: i can only increase the kid's energy one 'quantum step' at a time.
pushing by half of that amount is not possible."
for a kid on a swing these 'quantum steps' are so tiny that they are too small to notice.
until recently, the same was true for radio waves.
however, the research team in delft developed a circuit that can actually detect these chunks of energy in radio frequency signals, opening up the potential for sensing radio waves at the quantum level.
from quantum radio to quantum gravity?
beyond applications in quantum sensing, the group in delft is interested in taking quantum mechanics to the next level: mass.
while the theory of quantum electromagnetism was developed nearly 100 years ago, physicists are still puzzled today on how to fit gravity into quantum mechanics.
"using our quantum radio, we want to try to listen to and control the quantum vibrations of heavy objects, and explore experimentally what happens when you mix quantum mechanics and gravity," gely said.
"such experiments are hard, but if successful we would be able to test if we can make a quantum superposition of space- time itself, a new concept that would test our understanding of both quantum mechanics and general relativity."
argonne, uchicago scientists perform initial measurements through a 52- mile fiber- optic testbed.
scientists from the u.s. department of energy's (doe) argonne national laboratory and the university of chicago launched a new testbed for quantum communication experiments from argonne last week.
argonne scientists, (left to right) sean sullivan, gary wolfowicz, joseph heremans and alan dibos, worked on the quantum loop project and demonstrated the operation of the testbed by generating, transmitting, and detecting optical pulses through one and then both fiber loops.
(image by argonne national laboratory.)
the argonne quantum loop consists of a pair of connected 26- mile fiber- optic cables that wind circuitously between argonne to the illinois tollway near bolingbrook, il, and back.
at 52 total miles, it is currently among the longest ground- based quantum communication channels in the country.
the loop will serve as a testbed for researchers interested in leveraging the principles of quantum physics to send unhackable information across long distances.
researchers at argonne and uchicago plan to use the testbed to explore science underlying quantum engineering systems and to harness the properties of quantum entanglement, a phenomenon albert einstein famously characterized as  "spooky action at a distance."
quantum entanglement links two (or more) particles so that they are in a shared state - such that whatever happens to one immediately affects the other, no matter how far they have travelled apart.
"inaugurating this quantum loop is a significant step for chicago and the nation in building a large- scale quantum network that can enable secure data transmissions over long distances," said principal investigator david awschalom, senior scientist in the materials science division at argonne, liew family professor in molecular engineering at the university of chicago, and director of the chicago quantum exchange.
"the loop will enable us to identify and address challenges in operating a quantum network and can be scaled to test and demonstrate communication across even greater distances to help lay the foundation for a quantum internet."
argonne scientist alan dibos aligns optics to bulk ion ensembles in a three- kelvin cryostat.
(image by argonne national laboratory.)
argonne scientists joe heremans, alan dibos and gary wolfowicz, who worked on the quantum loop project, demonstrated the operation of the testbed by generating and transmitting optical pulses through one and then both fiber loops.
they witnessed a delay of 200 microseconds for the transit time of the laser pulse along one fiber loop, which is consistent with the speed of light in the glass fiber.
they also began to use the loop for a series of experiments, including transmitting signals from photons emitted from ensembles of ions.
these ions can be used as a quantum memory for the network.
a functional quantum memory, which entails the storage and retrieval of quantum states, is a key technological advance needed for quantum communication and a quantum internet.
"we will need many of these quantum memories spaced out over about 100 km to relay the quantum signal through a network.
the quantum loop enables us to test and refine this quantum memory technology before deploying it in large scale," said tian zhong, scientist in the nanoscience and technology division at argonne, and assistant professor of molecular engineering at the university of chicago.
"research leading to science infrastructure such as the quantum loop will ensure that america remains a world leader in this pivotal, rapidly evolving field which will open up important new avenues of investigation in areas like quantum data transfer and secure communications," said doe under secretary of science paul dabbar.
"we look forward to continued increased support and accomplishment for this and other areas of quantum information science."
"this quantum loop is a significant capability for the scientific communities in quantum physics, communications and computing," said paul kearns, argonne national laboratory director.
"these experiments demonstrate how argonne's world- leading scientists and engineers help ensure u.s. leadership in essential quantum information science."
argonne's unique suite of world- class facilities, including the center for nanoscale materials and the advanced photon source - both doe office of science user facilities - as well as the quantum factory, enable researchers to build and characterize materials and devices for quantum communication.
in addition to the quantum loop, argonne plans to develop a two- way quantum link network with fermi national accelerator laboratory.
when the two projects are connected, the quantum link, also supported by doe, is expected to be among the longest links in the world to send secure information using quantum physics.
argonne is a member of the chicago quantum exchange, a catalyst for advancing academic and industrial efforts in the science and engineering of quantum information.
the chicago quantum exchange, directed by awschalom, is headquartered at the pritzker school of molecular engineering at the university of chicago and includes argonne, fermilab and the university of illinois at urbana- champaign as core members.
for more information, visit chicago quan tum .org.
the quantum loop, and the argonne- fermi quantum link network, are supported by the doe office of science basic energy sciences program.
rigetti computing, a leading provider of integrated quantum computing systems, today announced that its quantum computers will be available to users through amazon braket, a new, fully managed amazon web services (aws) solution that allows scientists, researchers, and developers to begin experimenting with computers from quantum hardware providers in a single place.
rigetti has also joined the aws partner network (apn) as a solutions provider working with enterprise customers to develop quantum computing applications.
"by collaborating with aws, we will be able to deliver access to our systems to a much broader market and help accelerate the growth of this emerging industry," said chad rigetti, founder and ceo of rigetti computing.
users of amazon braket service will have access to rigetti's latest quantum processors based on 32- qubit superconducting chip technology.
this creates the potential for organizations running scientific or industrial applications on aws to augment those workflows by integrating quantum capabilities into their existing application architectures.
"we believe that opening up access to current stage quantum computers is a crucial step in accelerating the development of useful applications.
that's why, in designing amazon braket, we chose to collaborate with providers like rigetti, who have built promising technologies that are of interest to our customers," said simone severini, director, quantum computing, amazon web services, inc. "together, we can make it easier for researchers to innovate, with the shared goal of realizing the true long- term potential of quantum computing."
the new collaboration significantly expands customer access to rigetti quantum systems, which have been available since 2017.
to date, users have leveraged rigetti's increasingly powerful systems to run more than 120 million programs to advance near- term applications in simulation, optimization and complex systems.
as part of the apn, rigetti's application development team works across quantum hardware platforms to develop custom software solutions focused on simulation, optimization and machine learning for industry- leading organizations in finance, insurance, pharmaceuticals, defense, and energy.
about rigetti
rigetti computing builds and delivers integrated quantum systems over the cloud and develops software solutions optimized for hybrid quantum- classical computing.
founded in 2013, the company is headquartered in berkeley, ca with offices in fremont, ca; washington, d.c.; adelaide, australia; and london, uk.
indiana university's quantum science and engineering center will investigate possibilities created by the strange properties of quantum theory, particularly the phenomenon known as quantum entanglement.
within the past two decades or so, scientists have realized that entanglement can revolutionize technologies in fields such as cryptography, computing, and the creation of new materials and sensors.
these recent developments hold such promise that some refer to them as the start of the "second quantum revolution."
the center reflects the important contributions iu is making to this second quantum revolution.
the first revolution was the development of quantum mechanics and the 20th- century technologies it made possible, such as lasers and semiconductors.
the u.s. government has endorsed the vast potential of quantum science research with passage of the national quantum initiative act to "accelerate quantum research and development for the economic and national security of the united states."
"there is significant quantum- related research going at iu in physics, mathematics, chemistry and informatics," said david baxter, co- leader of the quantum science and engineering center, and professor of physics in the college of arts and sciences at iu bloomington.
"a number of these efforts are world- class, and the collaborations made possible by the new center will only increase their impact."
support for the center comes from the iu bloomington emerging areas of research program, which contributed $3 million toward the center's creation.
the emerging areas of research program and the center are overseen by the office of the vice provost for research at iu bloomington.
since receiving the funding in early 2019, the center's researchers have received more than $3 million in additional funding from the department of energy and national science foundation.
much of the work funded is focused on quantum simulation, an area of research that sets iu apart in the quantum science field.
by definition, quantum problems are hugely complex and difficult, leading scientists to develop a way to simulate quantum problems to enable more controlled and accurate studies.
a quantum simulator uses trapped ions (ions are atoms or molecules with a particular charge).
until now, ions have had to be trapped in a one- dimensional chain.
researchers involved with the center have succeeded in trapping ions in a two- dimensional plane.
think of one dimension as a line, and two dimensions as a sheet of paper.
the added dimension can give scientists exponentially greater power to understand quantum problems and would provide a brand- new experimental system in quantum science.
"a 2d ion trap quantum simulator would be the first of its kind and would offer unprecedented opportunities to address open questions in quantum physics, for which we currently have no reliable approaches," said gerardo ortiz, professor of physics in the college of arts and sciences, who co- leads the new center with david baxter.
the quantum science and engineering center will also spur educational opportunities for iu students, including new undergraduate courses and potentially one of the country's first programs in quantum engineering.
the new field "will be vital to full realization of the second quantum revolution," baxter said.
"quantum science and engineering is the future of communications, computation and cryptography," said jeff zaleski, iu bloomington's interim vice provost for research.
"indiana university's intellectual expertise in quantum science truly puts the university at the forefront of this developing field, and we are eager to drive the leading edge of research in this area."
indiana university
fashioning themselves "latter- day edisons," researchers at the university of nebraska contend that their architecture for quantum- dot development is 500 percent better than its nearest competition.
quantum- dot devices, which use the quantum nature of electrons to switch between binary states, could be a solution to problems encountered by ever- shrinking conventional transistors.
"we set a world record by demonstrating the largest nonlinear coefficient for a semiconductor quantum dot," said supriyo bandyopadhyay, the lead researcher.
"previous architectures have been highly praised for achieving a tiny percent increase, but we got a 500 percent increase with our design."
bandyopadhyay's work has led to a university of nebraska patent issued on the 500 percent- better quantum dot.
besides bandyopadhyay, five other researchers assisted in the work.
they are rod dillon, ned ianno, latika menon, paul snyder and frazer williams.
the patent covers an inexpensive construction method for vast arrays of quantum dots, involving an easy- to- perform electrochemical process on an aluminum substrate.
quantum dots are described as "spontaneously" forming atop the aluminum substrate in a regular array suitable for processing information.
"we've been working on this process for five or six years now, and it's far from perfect the chemical solution has to be just right, the power level of the electrical current has to be just right and used for exactly the right length of time," said bandyopadhyay.
without the precise controls of their patented process, bandyopadhyay explained, the quantum dots won't arrange themselves in the regular arrays needed to create computing machinery.
bandyopadhyay estimated that it will take his team five more years to perfect his quantum- dot manufacturing process.
binary replacements
quantum dots, in theory, can produce tiny computing architectures that fit as many as 10,000 devices in a space with the thickness of a human hair.
according to bandyopadhyay, quantum computers later in the decade will start to replace binary computers, and will begin to make military satellite circuitry immune from electronic- warfare attacks even sooner.
"you could build a quantum computer that has 2 to the 1,000th bits of data, which you could never do with a binary computer, because the number 21,000 is larger than the number of atoms in the known universe.
but a quantum computer could store that many bits with just 1,000 atoms," said bandyopadhyay.
the trick that quantum dots perform to encode so much information is called "superposition," which allows a quantum bit, or "qbit," to keep its state nebulous until data is read out at the end of a computation.
in this manner, a qbit can simultaneously represent all the possible states a normal bit could be in, which is imagined as having all states superimposed on top of each other.
this superposition of states enables, for instance, an 8- qbit addition to simultaneously represent all possible 8- bit values added to all possible 8- bit values simultaneously.
bandyopadhyay calls the superposition phenomenon in qbits its "quantum parallelism."
in his view, the hardest part of building a quantum computer comes from arranging atomic states of qbits in such a way that quantum mechanical manipulations will result in the desired calculations.
"we have been working on quantum computer research for about three years and have succeeded in demonstrating computer memories," he said.
"but we are at least five years away from having a small- scale quantum computer in the lab, and commercial versions won't be available for at least 20 years."
much sooner, however, the nebraska team hopes to demonstrate specific improved optical devices based on quantum dots that should shield military satellites from laser attacks.
such devices could also improve infrared imaging, night vision, surveillance and other electronic- warfare attributes.
since quantum dots operate on principles immune from most external tampering techniques, highly secure communications systems can be built, according to bandyopadhyay.
it is a truth universally acknowledged that quantum computing must have entanglement.
"entanglement," andrew white tells physorg.com, "is normally considered a non- negotiable part of quantum information processing.
in fact, if you told me a couple of years ago that you could do quantum computing without entanglement, i would have been pretty skeptical - to say the least!"
white says that he first heard the idea of non- entanglement quantum computing from carl caves.
"i was intrigued when professor caves, on sabbatical here in australia from new mexico, mentioned that there were sober predictions that entanglement wasn't always necessary."
white leads a team of young experimental scientists at the university of queensland in brisbane, australia.
ben lanyon, marco barbieri, marcelo almeida and white have been studying deterministic quantum computing with only one pure qubit (dqc1).
"entanglement is not the final story on what makes quantum information processing powerful," white insists.
the australian team's results can be found in physical review letters: "experimental quantum computing without entanglement."
"normally, in order for quantum computing to work," white explains, "we need to encode the information into quantum bits- qubits- which are in a noise- free pure state.
it's known that the entanglement between these is what makes standard quantum computing powerful."
he continues, "with a dqc1 scheme, you only have to have one pure qubit, and the rest can be noisy or mixed."
the idea behind quantum information processing using entanglement is that noiselessness has to be applied in order to provide a substantial advantage over classical computing.
dqc1, though, could potentially offer a more efficient and less resource- intensive method of quantum computing, since entanglement would no longer be a necessity.
"for this demonstration," white says, "we used the smallest possible example: a circuit with just two qubits, one pure and one mixed.
we ran a phase- estimation algorithm as a small example, and found in every setting there was zero entanglement, but that most of the states couldn't be described efficiently in a classical manner."
white points out that this is suggestive that there are other possibilities, beyond entanglement, that contribute to the power provided by quantum information processing.
"we're still chewing through the implications," he says.
"this is not a universal panacea," white admits.
"for some problems and algorithms you just need pure qubits and entanglement, problems such as shor's algorithm.
however, there are applications and problems where the dqc1 method will work quite well, and will be more efficient than trying to get qubits that are all pure."
with so many different architectures and schemes for quantum computing - all of them trying to create a system in which all the qubits are pure - it is rare to see a group looking to find applications for a quantum information system that makes allowances for impurity and the introduction of noise - insisting that entanglement is not necessary.
"the fact is that certain classes of problems don't need entanglement, and they don't need all of the purity.
in some cases, all that is needed is one pure qubit and the rest could be mixed.
really, with dqc1, you don't have to work as hard as you think you do."
we are starting to build more complicated algorithms to get an idea of where this could go.
regardless, the idea that entanglement may not be necessary for some types of quantum computing is big news."
more information: b. p. lanyon, m. barbieri, m. p. almeida, and a. g. white.
"experimental quantum computing without entanglement."
physical review letters (2008).
available online: http://link.aps.org/abstract/prl/v101/e200501 .
recent developments bode well for comprehensive quantum information systems.
work at labs in colorado and austria has increased the ability to store quantum states on groups of atoms, and researchers in georgia have found a way to transfer quantum states over networks.
two groups- at the national institute of standards and technology (nist; boulder, colo.) and at the university of innsbruck's institute of theoretical physics- have separately pushed up the number of ions that can exist in a simultaneous superposition of states.
just as nist researchers were reporting a successful experiment observing six rubidium ions in a synchronized state of superposition (see nov. 28, page 12), the innsbruck group announced the observation of eight calcium ions in a magnetic ion trap.
previously, quantum computing research had established quantum entanglement in five photons.
meanwhile, a team at the georgia institute of technology has found a way to build quantum state "repeaters"- systems that regenerate a quantum state being transmitted over a network- which would enable larger quantum networks to be built.
the first application of the development will likely be in emerging quantum encryption systems that operate over optical networks.
in theory, quantum repeaters could exchange secure encryption keys.
by demonstrating the temporary storage and retrieval of quantum information from a cloud of rubidium atoms, the georgia tech researchers have verified the possibility of building such systems.
"we have demonstrated that we can store quantum information on clouds of rubidium atoms for up to 10 microseconds, then read it back out," said postdoctoral researcher thierry chaneliere, working in the lab of professors alex kuzmich and brian kennedy at georgia tech.
"this could someday enable a quantum repeater- one more building block for quantum computers and networks."
qubits are the basic unit of quantum information, just as a bit, represented by a device with two possible states, is the basic unit of today's information processors.
the essential difference between the two building blocks is the ability of qubits to represent two quantum states that can simultaneously exist on a single photon or ion.
reading the value of a quantum state, however, destroys the dual- state representation, so quantum computers and networks must be able to process states internally without interacting with the environment.
the disturbance of quantum states following a data read is what makes quantum encryption the ultimate secure protocol.
any attempt to eavesdrop on a stream of quantum states will alter them, making it possible for sender and receiver to detect the interception of information.
"we have made an important step forward, but it's still a building block.
there will be a lot of steps and several more years before these things mature in a practical way," said kuzmich.
also contributing to the work were doctoral candidates dzmitry matsukevich, stewart jenkins and shau- yu lan.
last year, kuzmich's group reported transferring atomic- state information from rubidium atoms to photons, saying it was the first time quantum information had been transferred from matter to light.
now, in the new demonstration, the quantum information changed from photons to stationary atoms, stored from 500 nanoseconds to 10 microseconds, with the photon then regenerated with its original quantum information intact.
the experiment used two clouds of rubidium atoms at opposite ends of a 100- meter- long optical fiber that had been cooled to near absolute zero, thereby limiting the available quantum states.
both clouds were held captive by strong magneto- optical traps; then the first rubidium cloud was stimulated to emit a photon into the optical fiber, sending it to the second rubidium cloud.
the photon contained the quantum information describing the resonance state of the rubidium atoms, and when it hit the second rubidium cloud the quantum information was transferred, under the direction of a control laser.
the second cloud stored the photon for 500 nanoseconds to 10 microseconds before being induced to give it up, directly re- encoding the quantum information onto it.
the researchers characterized the mechanism by which the quantum information was transferred from the photon's spin to the atom's vibration as a light field excitation called a dark- state polariton, which can be later recovered from the atoms by inducing them to emit a photon.
"we store the information from the photon in the state of excitation of many atoms in the second ensemble," said chaneliere.
"it's really information about spin, but we store it in each atom in the ensemble, all of which are slightly flipped."
by storing quantum information- transferring the spin of photons to the vibrations of an atom and back again- kuzmich's group has demonstrated the feasibility of a quantum repeater, which would enable future quantum communications to move beyond their current direct- connection limitation.
such systems would have virtually no distance limitation, since another repeater could always be added.
quantum registers and other computer memory components might also be enabled someday by the georgia tech store- and- retrieve approach.
"now that we can store and retrieve quantum information, we want to work toward quantum networks where each node is a quantum computer," said chaneliere.
the lab recently demonstrated entanglement of two atomic qubits that were separated by a distance of 5.5 meters.
entanglement- a special kind of quantum synchronicity whereby reading out the state of one of an entangled pair determines the value of the other- had previously been demonstrated only over a distance measured in millimeters.
"we generated entanglement of atomic qubits and showed that we can take this entanglement and map it from atoms to photons," said kuzmich.
as other groups, such as the nist and university of innsbruck researchers, push up the number of ions that can store information, the georgia tech demo of networked quantum states becomes a powerful method for linking qubits stored in different locations.
only eight ions in a superposition of states can potentially represent more than 65,000 quantum states, so modest increases in the physical size of quantum computing components can have an exponential impact on data throughput.
but maintaining the state of entanglement that allows a group of particles to represent large amounts of data simultaneously is difficult.
in the case of particles like ions, near- absolute- zero temperatures are required to damp thermal vibrations, and complicated laser and magnetic field configurations are needed to maintain the quantum states.
"it is very difficult to control six ions precisely for a long enough time to do an experiment like this," said nist researcher dietrich leibfried.
the experimental conditions robust, however, with the results being repeated thousands of times.
in addition, the nist system was in an extreme configuration in which all six atoms are in a spin- up and spin- down state simultaneously.
in such a configuration, measuring the state of one ion would cause all six to collapse into specific states, loosing coherence.
for the innsbruck experiment, the ions were in a more- independent configuration in which some of the information could be extracted without destroying the quantum coherence of the system.
- chappell brown contributed to this report.
r. colin johnson has been a technology editor at ee times since 1986, covering next- generation electronics technologies.
he's the author of the book, cognizers - neural networks and machines that think, is acontributing editor on slashdot.org, and is a kyoto prize journalism fellow for his coverage of advanced technologies and international issues.
a team from the department of energy's oak ridge national laboratory has conducted a series of experiments to gain a better understanding of quantum mechanics and pursue advances in quantum networking and quantum computing, which could lead to practical applications in cybersecurity and other areas.
ornl quantum researchers joseph lukens, pavel lougovski, brian williams, and nicholas peters- along with collaborators from purdue university and the technological university of pereira in colombia- summarized results from several of their recent academic papers in a special issue of the optical society's optics & photonics news, which showcased some of the most significant results from optics- related research in 2019.
their entry was one of 30 selected for publication from a pool of 91.
conventional computer "bits" have a value of either 0 or 1, but quantum bits, called "qubits," can exist in a superposition of quantum states labeled 0 and 1.
this ability makes quantum systems promising for transmitting, processing, storing, and encrypting vast amounts of information at unprecedented speeds.
to study photons- single particles of light that can act as qubits- the researchers employed light sources called quantum optical frequency combs that contain many precisely defined wavelengths.
because they travel at the speed of light and do not interact with their environment, photons are a natural platform for carrying quantum information over long distances.
interactions between photons are notoriously difficult to induce and control, but these capabilities are necessary for effective quantum computers and quantum gates, which are quantum circuits that operate on qubits.
nonexistent or unpredictable photonic interactions make two- photon quantum gates much more difficult to develop than standard one- photon gates, but the researchers reached several major milestones in recent studies that addressed these challenges.
for example, they made adjustments to existing telecommunications equipment used in optics research to optimize them for quantum photonics.
their results revealed new ways to use these resources for both traditional and quantum communication.
"using this equipment to manipulate quantum states is the technological underpinning of all these experiments, but we did not expect to be able to move in the other direction and improve classical communication by working on quantum communication," lukens said.
"these interesting and unanticipated findings have appeared as we delve deeper into this research area."
one such tool, a frequency beam splitter, divides a single beam of light into two frequencies, or colors, of light.
"imagine you have a beam of light going down an optical fiber that has a particular frequency, say, red," lukens said.
"then, after going through the frequency beam splitter, the photon will leave as two frequencies, so it will be both red and blue."
the members of this team were the first researchers to successfully design a quantum frequency beam splitter with standard lightwave communications technology.
this device takes in red and blue photons simultaneously, then produces energy in either the red or the blue frequency.
by using this method to deliberately change the frequencies of photons, the team tricked the stubborn particles into beneficial interactions based on quantum interference, the phenomenon of photons interfering with their own trajectories.
"it turned out that off- the- shelf devices can deliver impressive control at the single- photon level, which people didn't know was possible," lougovski said.
additionally, the researchers completed the first demonstration of a frequency tritter, which splits a beam of light into three different frequencies instead of two.
their results indicated that multiple quantum information processing operations can run at the same time without introducing errors or damaging the data.
another key accomplishment was the team's design and demonstration of a coincidence- basis controlled- not gate, which enables one photon to control a frequency shift in another photon.
this device completed a universal quantum gate set, meaning any quantum algorithm can be expressed as a sequence within those gates.
"quantum computing applications require much more impressive control levels than any sort of classical computing," lougovski said.
the team also encoded quantum information in multiple independent values known as degrees of freedom within a single photon, which allowed them to observe quantum entanglement- like effects without needing two separate particles.
entanglement usually involves two linked particles in which changes made to the state of one particle also apply to the other.
finally, the researchers have completed quantum simulations of real- world physics problems.
in collaboration with scientists at the air force research laboratory, they are now developing tiny, specialized silicon chips similar to those common in microelectronics in pursuit of even better photonic performance.
"in theory, we can get all these operations onto a single photonic chip, and we see a lot of potential for doing similar quantum experiments on this new platform," lukens said.
"that's the next step to really move this technology forward."
future quantum computers will allow scientists to simulate incredibly complex scientific problems that would be impossible to study on current systems, even supercomputers.
in the meantime, the team's findings could help researchers embed photonic systems into current high- performance computing resources.
"we have a very diverse and talented team," lougovski said.
"the most important thing is we're getting results."
quantum computers promise speedy solutions to some difficult problems, but building large- scale, general- purpose quantum devices is a problem fraught with technical challenges.
to date, many research groups have created small but functional quantum computers.
by combining a handful of atoms, electrons or superconducting junctions, researchers now regularly demonstrate quantum effects and run simple quantum algorithms- small programs dedicated to solving particular problems.
but these laboratory devices are often hard- wired to run one program or limited to fixed patterns of interactions between the quantum constituents.
making a quantum computer that can run arbitrary algorithms requires the right kind of physical system and a suite of programming tools.
atomic ions, confined by fields from nearby electrodes, are among the most promising platforms for meeting these needs.
in a paper published as the cover story in nature on august 4, researchers working with christopher monroe, a fellow of the joint quantum institute and the joint center for quantum information and computer science at the university of maryland, introduced the first fully programmable and reconfigurable quantum computer module.
the new device, dubbed a module because of its potential to connect with copies of itself, takes advantage of the unique properties offered by trapped ions to run any algorithm on five quantum bits, or qubits- the fundamental unit of information in a quantum computer.
"for any computer to be useful, the user should not be required to know what's inside," monroe says.
"very few people care what their iphone is actually doing at the physical level.
our experiment brings high- quality quantum bits up to a higher level of functionality by allowing them to be programmed and reconfigured in software."
the new module builds on decades of research into trapping and controlling ions.
it uses standard techniques but also introduces novel methods for control and measurement.
this includes manipulating many ions at once using an array of tightly- focused laser beams, as well as dedicated detection channels that watch for the glow of each ion.
"these are the kinds of discoveries that the nsf physics frontiers centers program is intended to enable," says jean cottam allen, a program director in the national science foundation's physics division.
"this work is at the frontier of quantum computing, and it's helping to lay a foundation and bring practical quantum computing closer to being a reality."
the team tested their module on small instances of three problems that quantum computers are known to solve quickly.
having the flexibility to test the module on a variety of problems is a major step forward, says shantanu debnath, a graduate student at jqi and the paper's lead author.
"by directly connecting any pair of qubits, we can reconfigure the system to implement any algorithm," debnath says.
"while it's just five qubits, we know how to apply the same technique to much larger collections."
at the module's heart, though, is something that's not even quantum: a database stores the best shapes for the laser pulses that drive quantum logic gates, the building blocks of quantum algorithms.
those shapes are calculated ahead of time using a regular computer, and the module uses software to translate an algorithm into the pulses in the database.
putting the pieces together
every quantum algorithm consists of three basic ingredients.
first, the qubits are prepared in a particular state; second, they undergo a sequence of quantum logic gates; and last, a quantum measurement extracts the algorithm's output.
the module performs these tasks using different colors of laser light.
one color prepares the ions using a technique called optical pumping, in which each qubit is illuminated until it sits in the proper quantum energy state.
the same laser helps read out the quantum state of each atomic ion at the end of the process.
in between, a separate laser strikes the ions to drive quantum logic gates.
these gates are like the switches and transistors that power ordinary computers.
here, lasers push on the ions and couple their internal qubit information to their motion, allowing any two ions in the module to interact via their strong electrical repulsion.
two ions from across the chain notice each other through this electrical interaction, just as raising and releasing one ball in a newton's cradle transfers energy to the other side.
the re- configurability of the laser beams is a key advantage, debnath says.
"by reducing an algorithm into a series of laser pulses that push on the appropriate ions, we can reconfigure the wiring between these qubits from the outside," he says.
"it becomes a software problem, and no other quantum computing architecture has this flexibility."
to test the module, the team ran three different quantum algorithms, including a demonstration of a quantum fourier transform (qft), which finds how often a given mathematical function repeats.
it is a key piece in shor's quantum factoring algorithm, which would break some of the most widely- used security standards on the internet if run on a big enough quantum computer.
two of the algorithms ran successfully more than 90% of the time, while the qft topped out at a 70% success rate.
the team says that this is due to residual errors in the pulse- shaped gates as well as systematic errors that accumulate over the course of the computation, neither of which appear fundamentally insurmountable.
they note that the qft algorithm requires all possible two- qubit gates and should be among the most complicated quantum calculations.
the team believes that eventually more qubits- perhaps as many as 100- could be added to their quantum computer module.
it is also possible to link separate modules together, either by physically moving the ions or by using photons to carry information between them.
although the module has only five qubits, its flexibility allows for programming quantum algorithms that have never been run before, debnath says.
the researchers are now looking to run algorithms on a module with more qubits, including the demonstration of quantum error correction routines as part of a project funded by the intelligence advanced research projects activity.
ibm has created a 17- qubit quantum computer and is making plans to timeshare the machine with other companies via cloud computing.
while this is an important step, it isn't quite enough to make quantum computers truly competitive compared to supercomputers.
what will it take to bring quantum computing into the commercial realm- and how long until we get there?
classical computing has been around for many years and has completely transformed the human race.
near instant communication between any two individuals used to be a dream.
the idea of large calculations being done faster than you can blink was unimaginable.
the concept of free information and education was too much for any university to handle.
but it comes as no surprise that, now that these concepts are a reality, we've become dependent on them.
this dependence places pressure on the industry to produce more powerful devices with every passing year.
this was not an issue in the past since silicon devices were easy to scale down.
but, with transistor gates as small as one- atom thick, shrinking may no longer be possible.
silicon, the building block of modern semiconductors, is already being phased out by intel and future devices using feature sizes of 7nm and smaller will instead be made from materials such as indium- gallium- arsenide (ingaas).
increases in computational power may no longer come from feature size reduction.
image courtesy of richard wheeler [cc by- sa 3.0]
one solution for increasing computational power is the use of quantum computers (though their creation isn't likely to allow faster consumer devices).
a common application is reliant on control flow, discrete mathematics, and io handling.
a quantum computer, however, is designed to solve statistical problems and scenarios which involve large amounts of data.
the best way to understand it is to compare a classical processor (such as an i7) to an imaginary quantum processor (iq7 for example).
the i7 could add 1000 numbers together much faster than the q7, but the q7 could solve a game (such as checkers) much faster than the i7 due to the possible number of moves that the game possesses.
so why are quantum computers so good at parallel data crunching?
a classical computer is made up of transistors which handle two possible states: on (1) and off (0).
for each additional bit, the amount of information that can be represented is equal to 2n where n is the number of bits.
for example, four bits can represent one of 16 possible states and eight bits can represent one of 256 possible states.
by comparison, a quantum bit- or qubit- can hold three states: on (1), off (0), and a superposition state.
while the on and off states behave in an identical manner to classical bits, the superposition is what drives quantum computation.
this superposition is a linear probability that lies between 0 and 1, allowing four qubits to represent all 16 different states at the same time where each one of those 16 states has a complex amplitude reflecting its probability of being observed.
read more
ibm's 17- qubit computer
so it's pretty obvious that quantum computing provides many advantages over classical computers for complex, parallel data processing.
while such tasks are not commonly found in the everyday device, they are almost too common in many different industries, including financial data processing, insurance, scientific models, oil reserves, and research.
currently, supercomputers are used for such parallel data processing but, if a quantum option were available, it's a safe bet that each of these sectors would do anything to get one.
this has been one of the major drives in quantum computer technology with many companies trying to produce such a machine.
for example, d- wave systems have their series of specialized quantum annealing processors, while many other researchers and companies are trying to find methods of producing universal quantum gates.
ibm's q quantum computer.
image courtesy of ibm
however, ibm has just taken the lead with their 17- qubit quantum computer.
what makes the ibm quantum computer a game changer is that it is a universal quantum computer as opposed to being a highly specialized device.
many other quantum systems currently available are usually of the annealing persuasion, which is good for optimization problems but not for other quantum problems such as database searches.
the ibm machine, however, can be configured to execute just about any quantum problem.
ibm has decided to sell time on the computers to business and researchers alike through their ibm q program accessed via the internet (i.e., over the cloud).
this will allow developers and researchers to create a quantum program anywhere around the world and then have it executed with the press of a button.
50 is the magic number
ibm's made strides with its previous 5- qubit quantum computer.
this 17- qubit machine is obviously yet another milestone.
however, many say that even a 17- qubit computer is not "good enough" because classical computers can still process the same information in a smaller time frame.
in fact, it has been stated that classical computers can model quantum computers up to 50 qubits in size.
this means that, for a quantum computer to become "better" at solving quantum related problems than a classical computer, it has to contain at least 50 qubits.
of course, this assumes that such quantum computer simulations on classical computers do not improve.
so google is ambitiously planning to release a 49- qubit quantum computer by the end of this year.
considering the size difference between the ibm machine and the proposed google machine, however, it's likely safe to assume that googles machine may not be entirely universal.
phys.org
an attainable future
it's safe to say that quantum computers, despite becoming increasingly more powerful, are still very far away from being commercially available.
ibm's cloud- based scheme, however, does technically place quantum computing into the commercial realm.
supercomputers are still very powerful compared to quantum computers and their cost- to- performance ratio makes them highly economical.
but, unlike fusion power (which is always 20 years away), quantum computers really could make their debut when either ibm or google release the world's first 50- qubit computer.
"icon - international cooperation and networking" is an internal funding program launched by the fraunhofer- gesellschaft to bring top international researchers together and to facilitate cooperation with excellent foreign research institutions on a project basis.
fraunhofer ilt and qutech, the quantum institute of the delft university of technology and the netherlands organization for applied scientific research tno, are now starting to collaborate within the icon project "low- noise frequency converters for the first quantum internet demonstrator - qfc- 4- 1qid".
qutech is one of the world's leading research centers in the fields of quantum computing and quantum internet.
the first project phase will last three years and comprises joint research activities with a total volume of approx.
2.5 million euros.
tailor- made photons connect qubits
quantum computers will soon make it possible to perform highly complex calculations and algorithms in the shortest possible time and, thus, will revolutionize information technology.
in the future, several quantum computers will be connected to a quantum internet in an absolutely secure manner creating many new possibilities such as for instance distributed quantum computing.
to accomplish this, photonics is a key technology: individual photons and quantum states can be specifically generated, manipulated and controlled using laser technology.
in the qfc- 4- 1qid project, the partners are developing technologies with which the wavelength or frequency of individual photons can be specifically converted without impairing quantum information.
they aim to then transmit the photons through glass fibers with low loss and to couple qubits - the smallest computing units of a quantum computer - over long distances.
designing the corresponding quantum frequency converters poses a great challenge - they must exhibit high overall efficiency and low noise in the output signal.
it is a matter of converting photons emitting at a wavelength of 637 nm from nitrogen- vacancy centers in diamond, which serve as qubits at the qutech in delft.
"for long- distance connections with the lowest possible transmission losses, these photons must be modified so that their wavelengths are in the telecommunications bands between 1500 nm and 1600 nm," explains florian elsen, project manager and coordinator for quantum technology at fraunhofer ilt.
so far, only the basic principle of quantum frequency converters has been demonstrated.
the frequency converters with specifications relevant to the application will be implemented in the qfc- 4- 1qid project in the first step using laboratory setups.
this will be followed later by the development of prototypes and integrated components - for example in funded follow- up projects and r&d collaborations with industry partners.
the world's first quantum internet demonstrator of the qutech collaboration (1qid) will connect four cities in the netherlands in 2022, each with access to a common quantum system.
in 2014, the tu delft and the dutch organization tno founded the research center qutech, which serves both the scientific and engineering sectors.
by participating in the new icon project, the fraunhofer- gesellschaft is helping to create the vital technological prerequisites for the first quantum internet and positioning itself as a sought- after international research partner in the field of new quantum technologies.
interested visitors can gain further insight into current research at an expert forum on current quantum technologies at "akl'20 - international laser technology congress" on may 6, 2020 in aachen, germany.
honeywell announced tuesday that it is on track to have a quantum computer with a quantum volume of at least 64 qubits within the next three months.
tony uttley, president of honeywell quantum solutions, said that at any scale quantum computing is a controls problem.
"any quantum computer requires a set of vacuum systems, cooling, and vibration control, and these are things that honeywell has been working on for decades," he said.
quantum volume measures computational ability, indicating the relative complexity of a problem that can be solved by a quantum computer.
when released, honeywell's quantum computer will have twice that of the next alternative in the industry.
"our trajectory is to increase the volume by an order of magnitude every year for the next five years," he said.
when honeywell releases its quantum computer in the next three months, it will have a quantum volume of at least 64, twice that of the next alternative in the industry.
see: cloud providers 2019: a buyer's guide (free pdf)
james sanders, a cloud transformation analyst at 451 research, said that honeywell's announcement shows the variety of ways there are to build quantum computers.
honeywell's quantum computer uses trapped- ion technology, which leverages numerous, individual, charged atoms (ions) to hold quantum information.
honeywell's system applies electromagnetic fields to hold (trap) each ion so it can be manipulated and encoded using laser pulses.
honeywell's trapped- ion qubits can be uniformly generated with errors more well understood compared with alternative qubit technologies that do not directly use atoms.
uttley also said that another element of honeywell's quantum work is the ability to do mid- circuit measures, which is the quantum equivalent of putting an if statement into an algorithm.
"we will be able to pause mid- calculation, interrogate a qubit- it is a one or a zero- and then depending on the answer, you do something different with the rest of the equation," he said.
the key to doing that is long coherence times.
selling quantum as a service
the company also announced it has made strategic investments in two leading quantum computing software providers and signed a partnership to develop quantum computing algorithms with jpmorgan chase.
honeywell's quantum computer will be a full stack cloud computer via api.
uttley said that honeywell will offer access to its quantum computer through microsoft azure.
the company has been developing quantum use cases with aerospace, chemicals, and oil- and- gas applications, including creating new catalysts for use in oil- and- gas processing.
also see
the georgia institute of technology announced its agreement to join the ibm q hub at the oak ridge national laboratory (ornl) to help advance the fundamental research and use of quantum computing in building software infrastructure and developing specialized error mitigation techniques.
georgia tech will have cloud access, via the oak ridge hub, to the world's largest fleet of universal quantum computing systems for commercial use case exploration and fundamental research.
"access to ibm machines will allow georgia tech to build software infrastructure to make it easier to operate quantum machines, create specialized error mitigation techniques in software - thereby mitigating some of the hardware errors - and develop algorithms and applications for the emerging noisy intermediate- scale quantum (nisq) computing paradigm," said moinuddin qureshi, a professor in georgia tech's school of electrical and computer engineering, in a press release.
"access will also allow georgia tech researchers to better understand the error patterns in existing quantum computers, which can help with developing the architecture for future machines."
as part of the ornl hub, georgia tech will join a community of companies, startups, academic institutions and research labs working to advance quantum computing and explore practical applications.
georgia tech will leverage ibm's quantum expertise and resources, qiskit software and developer tools, and will have cloud- based access to ibm's quantum computation center.
ibm makes available through the cloud 15 of the most- advanced universal quantum computing systems available, including a 53- qubit system - the most qubits of a universal quantum computer commercially available in the industry.
research is being conducted worldwide to develop a new type of computational device known as a quantum computer, based on the principles of quantum physics.
quantum computers could tackle specialized computational problems such as integer factorization, understanding materials properties or optimization challenges much faster than conventional digital computers.
quantum computers will use one of a number of possible approaches to create quantum bits - units known as qubits - to compute and store data, giving them unique advantages over computers based on silicon transistors.
the agreement will give georgia tech access to ibm's premium systems, including the 53- qubit quantum computer.
"in the regime between 50 and 60 qubits is where quantum machines can potentially do computations that are beyond the capabilities of existing conventional computers," qureshi said.
- edited by chris vavra, associate editor, control engineering, cfe media and technology, cvavra@cfemedia.com.
researchers appear to be quickly zeroing in on practical physical devices that could use quantum states to process information.
called quantum computers, the machines could reach a new level of computational efficiency.
however, realizing even a single quantum gate in an actual physical system has proved to be a daunting task.
now, two recent demonstrations seem to hold out more promise for practical quantum computing.
nec corp. has developed a solid- state device that could function as a quantum bit, a basic component for quantum computing, while two researchers in the united states, one at lucent technologies bell laboratories (murray hill, n.j.) and the other at michigan state university (east lansing, mich.), have reported a new approach to building arrays of quantum gates.
"thus far, no quantum computer using a solid- state electronic device has been reported.
we have made a big step by showing the possibility of integrating quantum gates using solid- state devices," said jun'ichi sone, assistant general manager of nec fundamental research laboratories.
because it can be fabricated into large- scale circuitry like current electronic devices, the development has opened the possibility of applying today's semiconductor technology to build a quantum computer.
the nec team was able to verify two important points: the researchers directly observed the electronic device's wave behavior and they observed that two waves corresponding to two quantum states could be superposed, which is the fundamental operation that distinguishes quantum logic from conventional boolean gates.
the research is a part of the core research for evolutional science and technology (crest) project funded by the japan science and technology corp. (jst).
the idea of a quantum computer began as a theoretical exercise.
physicists wanted to examine what would change in basic information theory if quantum states, rather than macroscopic properties such as voltage levels, were used to encode and process information.
an important difference arose when registers of n bits were processed by alu (arithmetic- logic unit)- type operations.
the state of a quantum bit, or "qubit," is only determined when its value is observed.
thus during a computation, a string of qubits has potentially either a logical 0 or 1 value.
physically, they exist in an ambiguous "superposition of states" so that a string of alu operations is potentially processing all possible strings of bits.
only on readout of the result an "observation" in quantum mechanics does the string of bits become a specific value.
thus, in contrast to a conventional computer, quantum registers can perform single operations on combinatorial sets of data, making them far more powerful.
the quantum computer was an interesting theoretical experiment that helped both physicists and information theorists adopt a new view of their subjects.
one problem that remained was how to design useful algorithms using quantum states.
a turning point came when peter shor, a scientist at bell laboratories, published a quantum algorithm showing that a fairly modest quantum computer could factor very large numbers in seconds.
since computer security systems depend on the difficulty of factoring large numbers, this result implied that a small quantum computer could potentially crack any security system.
"it takes one trillion years to factorize a 200- digit number with present supercomputers, but it would take only one hour or less with a quantum computer," said sone.
the surprising result galvanized experimental scientists into the search for practical quantum- computing devices.
the principle of quantum computing operation has been verified using a few gates, each of them consisting of an atom or molecule.
demonstrations using large and expensive nuclear magnetic- resonance- imaging equipment have verified the principle of computation on a superposition of states, but it is not easy to design a quantum computer using those elements.
"to realize a quantum computer, we need a solid- state integrated circuitry in which gate allocation can be freely designed."
said sone.
to build a qubit processor with solid- state devices, nec researchers developed a superconductive circuit fabricated on a silicon substrate.
the circuit consists of an aluminum "box" called a cooper- pair box confining electrons, a reservoir electrode, a pulse gate and a prove electrode.
the cooper pair box is extremely small 0.05- micron x 0.7- micron x 15- nm thick.
it is connected to the reservoir electrode, which supplies electrons through a thin oxidized layer on the surface of the box.
the connection forms a josepheson junction of al/al2 o3 /al.
the prove electrode is also connected to the box through a tunnel junction.
the aluminum box contains about 1 billion electrons.
when no voltage is applied by the pulse gate, the metal box is electrically neutral.
when a voltage is applied by the gate, a pair of electrons moves from the reservoir electrode to the box.
the number of electrons that moves is always one pair through the tunnel junction under 1k temperature.
with the additional two electrons, the box takes a charge.
though billions of electrons in the aluminum box are involved, the box generates only two quantum states: a wave corresponding to a neutral state and a wave corresponding to a charged state with two electrons.
by controlling the pulse width of the gate, the two waves can be superposed, which is called quantum coherent superposition.
the operation of the box the coherence of the waves is the exact operation required for a quantum gate when working as a computing element.
the coherence superposition was observed by the prove electrode measuring a tunnel current without causing a large impact on the coherence.
nec researchers reported that they could maintain the coherence for about 2 nanoseconds, and within that period, the gate operated more than 30 times.
that number is not sufficient for large- scale quantum computing, said nec, but its researchers are optimistic that they can extend the number of operations.
"we have made one quantum bit gate this time.
the next step will be two quantum bits, which will be another big challenge.
then we have the challenge to integrate, say, about 10,000 gates, maintaining quantum coherence.
it is too early to say when quantum computing will be realized," said jaw- shen tsai, one of the researchers in charge of this project.
in contrast to the nec solid- state device, the bell labs- michigan state university proposal uses electrons floating on a liquid- helium surface inside a vacuum chamber.
the researchers reported their results in a recent issue of the journal science.
the system has some promising characteristics.
a large number of qubits can be represented, up to a billion in one chamber.
in addition, the entire set of electrons can be maintained in a coherent superposition of states for up to a tenth of a millisecond.
this is a long time compared with the nanosecond switching time of the electron states themselves.
the physics of superfluid liquid helium systems has been thoroughly studied experimentally, and the researchers were able to build on established techniques to show that decoherence effects, which would nullify the quantum operations, can be controlled to an acceptable level.
the demonstration also included a system for reading out the results of a computation.
a "program" for the electron system would be a physical description in the form of a "hamiltonian" representaion a set of quantum equations that would govern how the entire collection of electrons evolve in time.
the researchers admit that constructing hamiltonian representations to solve useful real- world problems is still a very difficult, and largely unexplored problem.
but with this demonstration, a physical means of running such a program appears to be in the offing.
qc ware france, a wholly owned subsidiary of the quantum computing- as- a- service company qc ware, today announced that it has been selected as one of the thirty- two winners of the bpifrance concours d'innovation i- nov award.
this grant program is highly competitive and hosted by bpifrance, the esteemed french public investment bank.
the grant supports innovative projects carried out by startups and smes and provides funding to companies developing technology that have high potential to impact the french economy and beyond.
qc ware won its award within the program's "deep tech" category for research that will push the envelope on quantum machine learning, one of the most promising applications of quantum computing.
qc ware is the only quantum computing company to have received this award.
"qc ware is experiencing dynamic growth across several industries, proving that there is a practical path for quantum computing applied to solving complex enterprise challenges worldwide.
bpifrance recognizes the tremendous potential of quantum computing," said matt johnson, qc ware ceo.
"by granting qc ware the concour d'innovation i- nov award bpifrance acknowledges that qc ware will strategically help enterprises in the future.
we are honored to receive such a prestigious award."
qc ware is a quantum computing software company that develops among others computational finance applications that run on quantum computers.
by harnessing the properties of quantum physics, quantum computers have the potential to navigate through a vast number of possibilities and come up with a probable solution faster.
qc ware has worked extensively with institutions in the financial sector, helping them to explore how quantum computing could be used to speed up financial calculations and ai- based decision making.
qc ware's work in this space includes collaborating with goldman sachs to gain in- depth knowledge on the near term impact of quantum computers and on the development of new algorithms that will enable quantum computers to outperform concurrent classical computers for computational finance applications.
qc ware france's bpifrance grant project will focus on quantum machine learning, involving creating software tools for classical data scientists, with capabilities for classification, clustering and reinforcement learning.
this research will go toward adding features to forge, qc ware's cloud service.
qc ware developed forge to enable large enterprises and public- sector organizations to start building quantum skills and prepare for the potential disruption that quantum computing will bring to the market in the near future.
iordanis kerenidis, head of quantum algorithms international at qc ware, led qc ware france's submission to bpifrance.
at qc ware, kerenidis oversees the design and development of quantum algorithms for machine learning.
he is also director of the paris centre for quantum computing (pcqc) and one of the world leaders in the field of quantum machine learning.
his work has been highly influential in the quantum machine learning community.
about bpifrance
since 2013, bpifrance has become the one- stop shop for entrepreneurs with a vastly comprehensive toolbox offering in the field to customers through 50 branches.
bpifrance believes in serving the future, by being entrepreneur- centric and heavily decentralized.
bpifrance supports micro- businesses, smes and mid- caps but also accompanies large caps that are considered important to the interests of france in context of france's national economy, territories, or employment.
bpifrance offers solutions to every key step in a company's growth, including business creation, financing, guarantees or equity investment.
about qc ware
qc ware is a quantum computing- as- a- service company building enterprise solutions that run on quantum computing hardware.
qc ware's mission is to be the first company to offer a practical application providing quantum advantage over classical computers.
qc ware is working towards that goal with one of the world's strongest teams of quantum algorithms scientists and through its multiple industry collaborations in finance, aerospace, material design, automotive, and oil and gas.
the company is headquartered in palo alto, ca and recently opened an office in paris, with plans to open an office in tokyo in 2020.
one of the essential features required for the realization of a quantum computer is quantum entanglement.
a team of physicists from the university of vienna and the austrian academy of sciences (oaw) introduces a novel technique to detect entanglement even in large- scale quantum systems with unprecedented efficiency.
this brings scientists one step closer to the implementation of reliable quantum computation.
the new results are of direct relevance for future generations of quantum devices and are published in the current issue of the journal nature physics.
quantum computation has been drawing the attention of many scientists because of its potential to outperform the capabilities of standard computers for certain tasks.
for the realization of a quantum computer, one of the most essential features is quantum entanglement.
this describes an effect in which several quantum particles are interconnected in a complex way.
if one of the entangled particles is influenced by an external measurement, the state of the other entangled particle changes as well, no matter how far apart they may be from one another.
many scientists are developing new techniques to verify the presence of this essential quantum feature in quantum systems.
efficient methods have been tested for systems containing only a few qubits, the basic units of quantum information.
however, the physical implementation of a quantum computer would involve much larger quantum systems.
yet, with conventional methods, verifying entanglement in large systems becomes challenging and time- consuming, since many repeated experimental runs are required.
building on a recent theoretical scheme, a team of experimental and theoretical physicists from the university of vienna and the oaw led by philip walther and borivoje dakic, together with colleagues from the university of belgrade, successfully demonstrated that entanglement verification can be undertaken in a surprisingly efficient way and in a very short time, thus making this task applicable also to large- scale quantum systems.
to test their new method, they experimentally produced a quantum system composed of six entangled photons.
the results show that only a few experimental runs suffice to confirm the presence of entanglement with extremely high confidence, up to 99.99 percent.
the verified method can be understood in a rather simple way.
after a quantum system has been generated in the laboratory, the scientists carefully choose specific quantum measurements which are then applied to the system.
the results of these measurements lead to either confirming or denying the presence of entanglement.
"it is somehow similar to asking certain yes- no questions to the quantum system and noting down the given answers.
the more positive answers are given, the higher the probability that the system exhibits entanglement," says valeria saggio, first author of the publication in nature physics.
surprisingly, the amount of needed questions and answers is extremely low.
the new technique proves to be orders of magnitude more efficient compared to conventional methods.
moreover, in certain cases the number of questions needed is even independent of the size of the system, thus confirming the power of the new method for future quantum experiments.
while the physical implementation of a quantum computer is still facing various challenges, new advances like efficient entanglement verification could move the field a step forward, thus contributing to the progress of quantum technologies.
when the quantum computer was imagined 30 years ago, it was revered for its potential to quickly and accurately complete practical tasks often considered impossible for mere humans and for conventional computers.
but, there was one big catch: tiny- scale quantum effects fall apart too easily to be practical for reliably powering computers.
now, a team of scientists in japan may have overcome this obstacle.
using laser light, they have developed a precise, continuous control technology giving 60 times more success than previous efforts in sustaining the lifetime of "qubits," the unit that quantum computers encode.
in particular, the researchers have shown that they can continue to create a quantum behavior known as the entangled state- entangling more than one million different physical systems, a world record that was only limited in their investigation by data storage space.
this feat is important because entangled quantum particles, such as atoms, electrons and photons, are a resource of quantum information processing created by the behaviors that emerge at the tiny quantum scale.
harnessing them ushers in a new era of information technology.
from such behaviors as superposition and entanglement, quantum particles can perform enormous calculations simultaneously.
the report of their investigation appears this week in the journal apl photonics.
"there is a problem of the lifetime of qubits for quantum information processing.
we have solved the problem, and we can continue to do quantum information processing for any time period we want," explained akira furusawa, of the department of applied physics, school of engineering at the university of tokyo and lead researcher on the study.
"the most difficult aspect of this achievement was continuous phase locking between squeezed light beams, but we have solved the problem."
quantum computers are considered a next generation of computing after the integrated circuit, silicon- chip based computers that now dominate information processing technology.
current computers use long strings of zeros and ones- called bits- to process information.
by contrast, quantum computers process information by harnessing the remarkable power of quantum mechanics that encodes 0s and 1s in quantum states called qubits.
qubits configure in two unusual ways: "superposition" and "entanglement."
brace yourself- quantum behaviors are unusual.
einstein himself characterized entanglement as "spooky action at a distance."
start with the fact that quantum systems can be in several states simultaneously- the up and down of superposition, for example.
particles also exhibit the quantum behavior of entanglement.
it is a deeply intimate property between quantum particles that unites them perfectly in a shared existence, even at immense distance.
in other words, spooky.
and it is this spooky action- entanglement- that the university of tokyo team discovered how to manage so it can be applied to run quantum computers.
for the next steps on this promising path toward making quantum computing practical, furusawa envisions creating 2- d and 3- d lattices of the entangled state.
"this will enable us to make topological quantum computing, which is very robust quantum computing," he said.
more information: "generation of one- million- mode continuous- variable cluster state by unlimited time- domain multiplexing," by jun- ici yoshikawa, shota yokoyama, tishiyuki kaji, chanond sornphiphatphong, yu shiozawa, kenzo makino and akira furusawa, apl photonics, september 27, 2016, http://scitation.aip.org/conte ... 6/10.1063/1.4962732.
intel has unveiled a new hardware solution focused on quantum computing: horse ridge is the first cryogenically- controlled processor designed to accelerate the development of full- stack quantum computing systems.
quantum computers promise to address problems that conventional computing solutions cannot handle.
the underlying technology is quantum physics; since a quantum bit (or qubit) can exist simultaneously in multiple states, it can be used to conduct a large number of calculations at the same time, significantly speeding up the resolution of complex problems.
at the beginning of the development of quantum computing, scientists focused on the realization of qubits.
if we imagine the bit like a coin that, once launched, can reveal its face with the value 1 or 0, a qubit (quantum bit) is a coin that spins like a spinning top - allowing it to have more values simultaneously - until it is stopped to read the measured value.
those organizations pursuing silicon quantum processors (there are several other than intel) that measure the property of electron spin.
quantum computers can perform a large number of calculations at the same time, but in doing so, they generate excessive amounts of heat.
consequently, to be effective, they must operate at temperatures close to absolute zero (i.e., very close to - 273.15 degc).
these devices, which are often custom- designed, tend to require hundreds of cables in and out of the cryogenic fridge to control the quantum processor, making many quantum computing systems look like weird steampunk machines, with cables popping out from all sides (fig.
1).
researchers are in a furious race to build quantum computers.
in the early years of quantum hardware development with testing and characterization, intel identified a significant bottleneck toward the realization of quantum processing on a commercial scale: interconnections and control electronics.
horse ridge, named after one of the coldest places in oregon (where intel has some of its most extensive operation), was developed together with qutech researchers, a partnership between tu delft and tno (netherlands organization for applied scientific research).
in the race to create quantum computers, many are focusing on implementations in silicon, in large part because silicon quantum computers can be made using common cmos silicon processing.
horse ridge was manufactured using intel's 22- nanometer finfet process.
this should dramatically accelerate intel's ability to design, test, and optimize a commercial quantum computer.
jim clarke, director of quantum hardware at intel, explained that, until now, much emphasis has been placed on the qubit itself, but one of the main challenges is the simultaneous control of multiple qubits.
intel's new chip simplifies the design and size of quantum computers.
horse ridge is a highly integrated mixed- signal system on chip (soc) that reduces the complexity of quantum control engineering.
it is programmed with instructions that correspond to basic qubit operations.
the soc translates these instructions into electromagnetic microwave pulses that can manipulate the state of the qubits.
a quantum computer works in the millikelvin range, which is only a fraction of a degree above absolute zero.
silicon spin qubits have properties that could allow them to operate at 1 kelvin or higher temperatures, which would drastically reduce the cooling challenges of quantum technology.
intel aims to ensure that cryogenic controls and qubit spindles operate at the same temperature level to create more advanced and compact solutions.
by making it easier to control multiple qubits, horse ridge is helping to define a path toward scaling more complex systems.
intel expects the result to be more easily manipulated quantum computers that will also allow the company to leverage its expertise in advanced packaging and interconnected technologies.
intel's investment in research and development ensures a complete hardware and software base in the development of an efficient and commercially viable quantum system.
it is necessary to stress the importance of small- scale quantum systems to demonstrate the absolute potential of qubits.
in horse ridge, intel has developed a scalable control system that should accelerate the testing and the potential of quantum computing.
maurizio holds a ph.d. in physics and is a telecommunication engineer and journalist.
he has worked on various international projects in the field of gravitational wave research.
he collaborates with research institutions to design data acquisition and control systems for space applications.
he is the author of several books published by springer, as well as numerous scientific and technical publications on electronics design.
ibm has announced the development of a quantum computing platform that will allow users to access and program its 5 qubit quantum computer over the internet.
called the ibm quantum experience, it is, the company claims, the first ever system to allow outside users to work with a quantum computer via the cloud.
the cloud and computer, in this instance is an ibm facility in new york with a team that has been focused on developing a true quantum computer.
the current machine is housed in an extremely cold storage facility and those who wish to use it must learn to use the interface that has been developed and the programming language that ibm has created to go along with it- it looks, team members note, like writing music; commands are entered by adding symbols to a staff.
quantum computer technology is still, of course, in its infancy- no one has yet built such a machine that can compete with the current digital models, though when they do, it will be called quantum supremacy- most in the field agree that there is still a long way to go before that happens.
in the meantime, researches are building computers that are both analog and quantum, which reduces much of the error correction that is inherent in quantum computing.
most in the field do not expect quantum supremacy to be reached until engineers figure out a way to dramatically increase the number of qubits being used, to perhaps 50, as opposed to the five, seven, eight or nine that have been developed thus far.
ibm is not alone in trying to figure out how to build a true quantum computer, google has made clear its ambitions (building a quantum computer that uses 100 qubits in just a few years time) as have several college groups.
most agree that as the technology improves, so too will the number of groups working on creating something truly useful- machines able to take advantage of the unique special properties of quantum physics, such as superposition, are expected to be able to do things that modern computers cannot, such as very accurately forecast the weather.
in its announcement, ibm explains its reasoning for brining quantum computer programming to people who do not have access to such systems- the more people that are exposed to such machines and the skill sets that are required to program them, they believe, the sooner we will all be able to benefit from the development of such machines.
research has demonstrated laser control of quantum states in an ordinary silicon wafer and observation of these states via a conventional electrical measurement.
the findings- published in the journal nature communications by a uk- dutch- swiss team from the university of surrey, university college london, heriot- watt university in edinburgh, the radboud university in nijmegen, and eth zurich/epf lausanne/paul scherrer institute in switzerland- mark a crucial step towards future quantum technologies, which promise to deliver secure communications and superfast computing applications.
the team demonstrated a quantum on/off switching time of about a millionth of a millionth of a second - the fastest- ever quantum switch to be achieved with silicon and over a thousand times faster than previous attempts.
"quantum computing exploits the fact that, according to quantum mechanics atoms can exist in two states at once, being both excited and unexcited at the same time.
this is known as a superposition state, and is most famously illustrated by schrodinger's quantum cat which is simultaneously dead and alive" said dr. ellis bowyer, one of the surrey researchers who made the laser measurements, he added "this superposition of orbital states is very delicate, but we discovered that silicon provides an amazingly clean environment for the phosphorus atoms trapped inside where our quantum information is being stored.
we put the atoms into a superposition state with a very short (a few trillionths of seconds) laser pulse from the felix laser facility, and then, we showed we can create a new superposition which depends on the exact time at which a second laser pulse arrives.
we found that the superposition state even survives when electrons are flying around the trapped atom while current was flowing through the chip, and even more strangely, the current itself depends on the superposition state".
the team has recently been awarded further funding from the uk epsrc (engineering and physical sciences research council) to investigate how to connect many of these quantum objects to each other, creating the bigger building blocks needed for quantum computers.
this next phase of research could enable the creation of fast quantum silicon chips, and other kinds of devices such as super- accurate clocks and ultra- sensitive bio- medical sensors.
"quantum superpositions and the resulting quantum technologies are only just beginning to make an impact, but we believe that with new advances in silicon, it is only a matter of time before it becomes more part of the everyday.
this work brings that time closer by showing that exotic quantum features, more usually demonstrated with unimaginably tiny things in university physics labs can also be seen using an ordinary voltmeter," said dr thornton greenland of ucl.
"what is exciting is that we can see these exotic quantum phenomena in that most common material, silicon, using a measurement as simple as that of the electrical resistance" thus the time is drawing nearer when we'll be able to take advantage of make a computer that does a tremendous number of calculations simultaneously, and that provides unprecedentedly secure computing, impenetrable to hackers."
researchers have achieved a breakthrough in quantum communications and computing using a teleporter and a paradoxical cat.
the breakthrough is the first- ever transfer, or teleportation, of a particular complex set of quantum information from one point to another, opening the way for high- speed, high- fidelity transmission of large volumes of information, such as quantum encryption keys, via quantum communications networks.
the research was published in the april edition of the journal science.
teleportation - the transfer of quantum information from one location to another using normal, "classical" communications - is one of the fundamental quantum communication techniques.
the cat in the equation was not a living, breathing feline but rather "wave packets" of light representing the famous "thought experiment" known as schrodinger's cat.
schrodinger's cat was a paradox proposed by early 20th century physicist erwin schrodinger to describe the situation in which normal, "classical" objects can exist in a quantum "superposition" - having two states at once.
professor elanor huntington, in the school of engineering and information technology at unsw's canberra campus at the australian defence force academy (adfa), was part of a team led by university of tokyo researchers.
she said the team's achievement was another step towards building a super- powerful quantum computer and transmitting quantum information.
"one of the limitations of high- speed quantum communication at present is that some detail is lost during the teleportation process.
it's the star trek equivalent of beaming the crew down to a planet and having their organs disappear or materialise in the wrong place.
we're talking about information but the principle is the same - it allows us to guarantee the integrity of transmission.
"just about any quantum technology relies on quantum teleportation.
the value of this discovery is that it allows us, for the first time, to quickly and reliably move quantum information around.
this information can be carried by light, and it's a powerful way to represent and process information.
previous attempts to transmit were either very slow or the information might be changed.
this process means we will be able to move blocks of quantum information around within a computer or across a network, just as we do now with existing computer technologies.
"if we can do this, we can do just about any form of communication needed for any quantum technology."
the experiments were conducted on a machine known as "the teleporter" in the laboratory of professor akira furusawa in the department of applied physics in the university of tokyo.
professor huntington, who leads a research program for the centre for quantum computation and communication, developed the high- speed communication part of the teleporter at unsw's canberra campus with phd student james webb.
even on the scale of everyday life, nature is governed by the laws of quantum physics.
these laws explain common phenomena like light, sound, heat, or even the trajectories of balls on a pool table.
but when applied to a large number of interacting particles, the laws of quantum physics actually predict a variety of phenomena that defy intuition.
in order to study quantum systems made of many particles, physicists must first be able to simulate them.
this can be done by solving the equations describing their inner workings on supercomputers.
but while moore's law predicts that the processing power of computers doubles every couple of years, this is a far cry from the power needed to tackle the challenges of quantum physics.
the reason is that predicting the properties of a quantum system is enormously complex, demanding a computational power that grows exponentially with the size of the quantum system -  an "intrinsically complex" task, according to professor vincenzo savona, who directs the laboratory of theoretical physics of nanosystems at epfl.
"things become even more complicated when the quantum system is open, meaning that it is subject to the disturbances of its surrounding environment," savona adds.
and yet, tools to efficiently simulate open quantum systems are much needed, as most modern experimental platforms for quantum science and technology are open systems, and physicists are constantly in search of new ways to simulate and benchmark them.
but significant progress has been made thanks to a new computational method that simulates quantum systems with neural networks.
the method was developed by savona and his phd student alexandra nagy at epfl -  and independently by scientists at universite paris diderot, the heriot- watt university in edinburgh, and the flatiron institute in new york.
the total body of work is being published across three papers in physical review letters.
"we basically combined advances in neural networks and machine- learning with quantum monte carlo tools," says savona, referring to a large toolkit of computational methods that physicists use to study complex quantum systems.
the scientists trained a neural network to represent simultaneously the many quantum states in which a quantum system can be cast by the influence of its environment.
the neural- network approach allowed the physicists to predict the properties of quantum systems of considerable size and arbitrary geometry.
"this is a novel computational approach that addresses the problem of open quantum systems with versatility and a lot of potential for scaling up," says savona.
the method is set to become a tool of choice for the study of complex quantum systems, and, looking a bit more into the future, for assessing the effects of noise on quantum hardware.
story source:
materials provided by ecole polytechnique federale de lausanne.
note: content may be edited for style and length.
quantum computers promise to perform certain types of operations much more quickly than conventional digital computers.
but many challenges must be addressed before these ultra- fast machines become available, among them, the loss of order in the systems - a problem known as quantum decoherence - which worsens as the number of bits in a quantum computer increases.
one proposed solution is to divide the computing among multiple small quantum computers that would work together much as today's multi- core supercomputers team up to tackle big digital operations.
the individual computers in such a system could communicate quantum information using bose- einstein condensates (becs) - clouds of ultra- cold atoms that all exist in exactly the same quantum state.
the approach could address the decoherence problem by reducing the number of bits necessary for a single computer.
now, a team of physicists at the georgia institute of technology has examined how this bose- einstein communication might work.
the researchers determined the amount of time needed for quantum information to propagate across their bec, essentially establishing the top speed at which such quantum computers could communicate.
"what we did in this study was look at how this kind of quantum information would propagate," said chandra raman, an associate professor in georgia tech's school of physics.
"we are interested in the dynamics of this quantum information flow not just for quantum information systems, but also more generally for fundamental problems in physics."
the research is scheduled to be published in the april 19 online edition of the journal physical review letters.
the research was funded by the u.s. department of energy (doe) and the national science foundation (nsf).
the work involved both an experimental physics group headed by raman and a theoretical physics group headed by associate professor carlos sa de melo, also in the georgia tech school of physics.
the researchers first assembled a gaseous bose- einstein condensate that consisted of as many as three million sodium atoms cooled to nearly absolute zero.
to begin the experiment, they switched on a magnetic field applied to the bec that instantly placed the system out of equilibrium.
that triggered spin- exchange collisions as the atoms attempted to transition from one ground state to a new one.
atoms near one another became entangled, pairing up with one atom's spin pointing up, and the other's pointing down.
this pairing of opposite spins created a correlation between pairs of atoms that moved through the entire bec as it established a new equilibrium.
the researchers, who included graduate student anshuman vinit and former postdoctoral fellow eva bookjans, measured the correlations as they spread through the cloud of cold atoms.
at first, the quantum entanglement was concentrated in space, but over time, it spread outward like drop of dye diffuses through water.
"you can imagine having a drop of dye that is concentrated at one point in space," raman said.
"through diffusion, the dye molecules move throughout the water, slowly spreading throughout the entire system."
the research could help scientists anticipate the operating speed for a quantum computing system composed of many cores communicating through a bec.
"this propagation takes place on the time scale of ten to a hundred milliseconds," raman said.
"this is the speed at which quantum information naturally flows through this kind of system.
if you were to use this medium for quantum communication, that would be its natural time scale, and that would set the timing for other processes."
though relevant to communication of quantum information, the process also showed how a large system undergoing a phase transition does so in localized patches that expand to attempt to incorporate the entire system.
"an extended system doesn't move from one phase to another in a uniform way," said raman.
"it does this locally.
things happen locally that are not connected to one another initially, so you see this inhomogeneity."
beyond quantum computing, the results may also have implications for quantum sensing - and for the study of other physical systems that undergo phase transitions.
"phase transitions have universal properties," raman noted.
"you can take the phase transitions that happen in a variety of systems and find that they are described by the same physics.
it is a unifying principle."
raman hopes the work will lead to new ways of thinking about quantum computing, regardless of its immediate practical use.
"one paradigm of quantum computing is to build a linear chain of as many trapped ions as possible and to simultaneously engineer away as many challenges as possible," he said.
"but perhaps what may be successful is to build these smaller quantum systems that can communicate with one another.
it's important to try as many things as possible and to keep an open mind.
we need to try to understand these systems as well as we can."
einstein infamously dismissed quantum entanglement as spooky action at a distance and quantum uncertainty with his quip that god does not play dice with the universe.
aside from revealing his conceptual prejudices, einstein's rejection of these now- established hallmarks of quantum mechanics point to the field's elusive nature: coherent quantum mechanical phenomena, such as entanglement and superposition, are not apparent at macroscopic levels of scale.
in fact, a common view is that on these scales quantum behavior is masked by decoherence, or even that quantum mechanics itself needs revision.
encouragingly, however, researchers at the vienna center for quantum science and technology (vcq), university of vienna, have recently proposed an experimental design that would use a macroscopic mechanical resonator, short optical pulses and optical microcavities to realize quantum state tomography, squeezing, and state purification that could shed light on this elusive boundary between the quantum and classical worlds.
led by michael r. vanner in prof. markus aspelmeyer's aspelmeyer group for quantum f ... quantum information at the nano- and microscale, the team - which also included i. pikovski, g. d. cole, m. s. kim, c. brukner, k. hammerer, and g. j. milburn - faced a number of challenges in devising their optomechanical scheme to fully reconstruct quantum states of mechanical motion.
one of the most fundamental is the attempt to observe quantum mechanical behavior of a macroscopic mechanical object, since any potential quantum features would exhibit themselves only on truly miniscule scales.
"for the mechanical structures that we consider," vanner explains, "one needs to resolve position displacements of about a femtometer," or one- trillionth of a millimeter.
"this is a mind- bogglingly small distance that is, in fact, smaller than even the diameter of a hydrogen nucleus."
this then leads to additional challenges: in the attempt to measure an object's position, the object moves and causes positional smearing by injecting uncertainty into the resulting position information, which is referred to as the standard quantum limit (sql).
"the first challenge that we had to overcome was to find a method which circumvents the sql," vanner continues.
"the second was that making measurements of the position alone is insufficient to reconstruct a quantum state.
this is because the quantum state contains all that is, at least in principle, knowable about the object.
and so, one needs to also measure all the complementary properties of the state, such as its momentum, and to do so also in an equally precise manner."
since no existing microscopy technology is capable of resolving quantum- scale features, the team addressed these challenges with optical interferometry.
"perhaps where we benefited most," vanner reflects, "was from the work of v. b. braginsky, who made several seminal contributions to the field of quantum measurement1.
in particular he introduced a scheme using short pulses of light that can overcome the sql."
a short pulsed interaction can achieve this because the mechanical object has very little time to move during the interaction, and thus smearing can be dramatically reduced.
"braginsky developed this technique to make sensitive force detectors with the goal of detecting gravitational waves," notes vanner.
"we've utilized this technique to allow for very sensitive position measurements.
what we introduce in our proposal is a protocol using these pulsed measurements to perform quantum state reconstruction, which was our primary interest, and also a protocol to prepare low entropy squeezed states."
the state reconstruction scheme works in much the same way as many modern medical imaging techniques - that is, by taking images from many angles, as in x- ray computed tomography, it is possible to determine the three- dimensional internal structure within the body.
"applying this analogy to our case," vanner continues, "the internal structure is the quantum state and the angles are its various properties: position, momentum, and their combinations.
our state reconstruction protocol uses appropriately timed pulses of light to access all these properties, thus providing a means to determine all the information in the quantum state."
an important point is that the team has analyzed the experimental feasibility and demonstrated that the scheme is realizable with current state- of- the- art technology.
vanner is optimistic about the development of additional innovations and extensions in pulse sequences and measurements based on their pulsed design.
"as an example," vanner notes, "we're currently trying to compliment our work reported in pnas by developing pulsed approaches to quantum state preparation.
combining such results with our state reconstruction results provides a complete experimental framework."
in terms of how their findings might enhance the future exploration of quantum mechanical phenomena on a macroscopic scale, vanner points out that one important quantum mechanical phenomenon that is little explored in the laboratory is decoherence - the term given to the processes by which the environment surrounding a quantum object gains information about its state, often leading to the undesirable consequence of loss of quantum coherence between superposition components.
"decoherence is often regarded as one of the primary hindrances in efforts to construct a quantum computer.
the quantum state tomography scheme that we have introduced can be used to observe and characterize decoherence, thus providing vital experimental data for the development of quantum mechanics based technology."
moreover, adds vanner, "it is a fascinating prospect that quantum information can be encoded into the motion of a mechanical object.
this may lead to a number of interesting possibilities, such as transduction between flying qubits - i.e., photons - and qubits in a solid state device or superconductor.
a pulsed approach may indeed be a feasible route to achieving this goal."
in addition to decoherence as discussed above, adds vanner, "an attractive feature of the quantum state reconstruction scheme is that it can reconstruct and analyze any quantum state of motion.
thus, a large number of state- dependent quantum effects can be studied.
for example, one could utilize the fragility of a quantum superposition state as an extremely sensitive detector."
for vanner, one of the key prospects is to see their design actually realized.
"we're currently building an experiment to implement our quantum state reconstruction protocol," he concludes.
"i'm finding it very exciting to be able to physically implement our ideas and begin to experimentally see behavior that is predicted in our theoretical model."
more information: pulsed quantum optomechanics, pnas, published online before print september 7, 2011, doi: 10.1073/pnas.1105098108
1related: quantum nondemolition measurements: the route from toys to tools, v. b. braginsky and f. ya.
khalili, reviews of modern physics 68, 1- 11 (1996), doi: 10.1103/revmodphys.68.1
the race toward the first practical quantum computer is in full stride.
companies, countries, collaborators, and competitors worldwide are vying for quantum supremacy.
google says it's already there.
but what does that mean?
how will the world know when it's been achieved?
using classical computers, computational scientists at pnnl have set a mark that a quantum system would need to surpass to establish quantum supremacy in the realm of chemistry.
that's because the fastest classical computers available today are getting better and better at simulating what a quantum computer will eventually be expected to do.
to prove itself in the real world, a quantum computer will need to be able to outdo what a fast supercomputer can do.
and that's where the pnnl- led team have set a benchmark for quantum computers to beat.
"classical simulation of quantum chemistry problems serves as a goalpost for quantum computers," said karol kowalski, a computational chemist at pnnl.
"when a quantum computer can beat what our best parallel computing systems can do, quantum computing developers will know they are where they need to be.
this is a benchmark to inspire innovation."
at 113 electrons, the recent benchmark simulation is the largest quantum system ever simulated at this precise level of accuracy using a classical computer.
working with collaborators in hungary and the czech republic, the pnnl team set the benchmark by simulating the structure of an important chemical structure in nitrogenase, an enzyme that converts nitrogen in the atmosphere into usable fertilizer for plants.
the enzyme is the subject of intense study because it may hold to key to producing enough food to feed an ever- growing global population.
understanding how this enzyme is able to break the strong nitrogen triple bond, while expending very little energy, could be key to new catalyst design, eventually providing abundant fertilizer currently produced using a chemical process requiring large energy inputs.
shrinking the quantum chemistry problem
"complex quantum chemistry is exactly the kind of problem where having a quantum computer available could really make a difference," said sriram krishnamoorthy, a high- performance computing expert and quantum computing lead scientist at pnnl.
"we are working on creating the programs that will run on quantum computers.
"when quantum computers arrive, we will be ready for them," said krishnamoorthy.
krishnamoorthy, kowalski, and their pnnl colleagues are working collaboratively with partners at microsoft, through the northwest quantum nexus, to both simulate how a quantum computer will work and write programs that will work on any quantum computer that emerges from the intense global competition.
"conventional computers, including today's fastest supercomputers, are inadequate for simulating quantum systems required to describe challenging and important molecular systems and processes," said kowalski.
"better computational tools are needed to understand chemical systems and design new materials."
until a full- scale quantum computer is available, the pnnl team worked with microsoft experts to develop a bridge between current digital computers and what comes next.
the workflow takes advantage of what classical computers do well now, while using the current capabilities of quantum computing to describe chemical transformations relevant to industrial processes such as energy generation and energy storage.
the key, according to the research team, was to take the output of a classical computer and be able to convert that information into an input that can be interpreted by a quantum computer.
the researchers published that quantum computing method in mid- 2019.
since then, the pnnl team has taken another huge step in bridging classical and quantum computers.
they developed a computer algorithm that takes advantage of a mathematical trick called "downfolding."
essentially, downfolding makes difficult and time- consuming calculations possible on current test- bed quantum computers.
"this is like shrinking a large box into a much smaller box," said kowalski.
"in this case, the box represents a huge numerical space.
we use a more compact description in a quantum computer, and what comes out accurately represents the energy of the much larger system.
it's a bridge between classical computing and what will be quantum computing in the coming years."
it may seem like a mathematical magic trick, but kowalski adds that the method uses properties of quantum mechanics and a series of rigorous mathematical theories that are reliable and reproducible.
opening new doors
the downfolding method not only opens up avenues to quantum computing, it also makes possible new, much more efficient and accurate ways of analyzing and validating the reams of data generated every day from the u.s. investment in u.s. department of energy (doe)- supported light sources used to study our world in subatomic detail.
"we have shown how the quantum behavior of excited electronic states can be analyzed with hamiltonian downfolding," said kowalski.
"this provides a way to use theory to validate data interpretation."
these interim steps in the path to quantum computing are essential because they provide essential benchmarks that help show how close the world is to achieving quantum supremacy.
"we will be able to test the output of quantum computers against these calculations," said krishnamoorthy.
"if quantum computers can produce results close to these results, we will know they work."
more information: jiri brabec, et al.
massively parallel quantum chemical density matrix renormalization group method.
arxiv:2001.04890v1 [physics.chem- ph]: https://arxiv.org/abs/2001.04890
nicholas p. bauman et al.
downfolding of many- body hamiltonians using active- space models: extension of the sub- system embedding sub- algebras approach to unitary coupled cluster formalisms, the journal of chemical physics (2019).
doi: 10.1063/1.5094643
nicholas p. bauman et al.
quantum simulations of excited states with active- space downfolded hamiltonians, the journal of chemical physics (2019).
doi: 10.1063/1.5128103
quantum bounds are numbers (such as 4, 6, and 2[?
]2) that naturally appear in quantum experiments, similar to how the number p emerges in circles.
but just as how p pops up in a wide variety of areas beyond circles, in a new study physicists have found that quantum bounds are not exclusive to quantum theory but also emerge in purely classical experiments.
the results suggest that attempts to define quantumness should not be concerned with quantum bounds, since there is nothing inherently quantum about them.
the physicists, diego frustaglia et al., at the university of sevilla in spain, have published a paper on the emergence of quantum bounds in classical experiments in a recent issue of physical review letters.
different experiments, same bounds
in their study, the researchers performed three classical experiments that correspond to three famous quantum experiments involving quantum bounds.
these quantum experiments are a sequential version of the bell inequality and two other related quantum inequalities, all of which are used to distinguish between quantum and classical phenomena.
in order to show that a system exhibits quantum effects, these experiments traditionally attempt to show that a system can violate a quantum inequality.
the greater the violation, the more quantum the system.
the maximum violation of a quantum inequality is the quantum bound.
the quantum bounds arise from probability distributions in the experiments and are specific numbers- for instance, the bell inequality has a quantum bound of 2[?
]2 (approximately 2.82), which is known as tsirelson's bound.
the other two inequalities addressed here have quantum bounds of 4 and 6.
both theoretically and experimentally, no violation of a quantum inequality has ever surpassed these bounds.
in the new study, the researchers showed that these same quantum bounds emerge in experiments in which classical waves travel along an ordinary transmission line.
the researchers found that the probabilities originating from the detection of wave intensities at the end of the transmission line follow the same distribution as the probabilities of detecting violations of the quantum inequalities.
specifically, the classical experiments yield bounds of 2.78, 3.93, and 5.93 for the three analogous experiments.
in all three cases, these values are actually slightly closer to their theoretical values mentioned above than the values obtained in quantum experiments are, providing strong evidence that both quantum and classical experiments produce the same bounds.
interpreting the results
one of the many implications of the study is that it offers new insight into what it means to be quantum.
by showing that quantum bounds are not unique to quantum theory, but are universal bounds, the findings show that ongoing attempts to define quantum theory should not focus on these bounds.
instead, the results provide a clue for finding a true quantum feature by revealing an important difference between the way in which the classical and quantum systems produce the same bounds.
while the classical systems require some kind of extra resource, such as memory, the quantum systems do not.
so a complete description of quantum theory should explain how quantum systems can violate the same bounds that classical systems do, but without using extra resources.
as the researchers explain, this approach of investigating classical systems to better understand quantum mechanics tends to be the opposite of most research.
"we somehow reverted the strategy followed by the founders of quantum theory," frustaglia told phys.org.
"in the early times of quantum mechanics, microscopic systems were subject to an intense questioning naturally biased towards classical physics.
the result was a set of oddities interpreted as the paradigmatic features of the quantum realm: the particle- wave duality (is it a particle or a wave?
), the schrodinger's cat (is it dead or alive?
), and the heisenberg's uncertainty principle (where and how fast is it?).
"as a consequence, it was soon understood that quantum systems should be interrogated in their own specific language, eventually provided by modern quantum theory.
it is then pertinent to address the possibility of interrogating classical systems with questions inspired by quantum physics.
this is what we did, indeed, finding that classical systems with an underlying wave mechanism answer these questions in the same way truly quantum systems do.
but one has to choose your system carefully: one would not be able to make it by using plain balls, for instance."
in the future, the physicists plan to investigate how the universal bounds might emerge in the first place.
"our results show that the 'quantum' bounds are common to many physical theories," said coauthor adan cabello at the university of sevilla.
"this suggests that the reason for these bounds is something very simple and arguably inherent to the kind of theories we are interested in: theories in which 'measurements' produce repeatable results which are not affected by some other measurements.
"surprisingly, this simple idea singles out many 'quantum' bounds.
when we adopt this perspective, what is really significant is the fact that these bounds are actually reachable in nature.
this shows that no hypothetical physical principle is acting and leads us to the conjecture that one of the physical principles that singles out quantum theory is precisely that one: there is no principle determining the probabilities of the outcomes of these 'measurements.'
"one plan is to prove that this simple idea is responsible for all quantum bounds.
another plan is to test whether it is really true that these bounds can be reached with quantum systems.
so far, and only very recently, h. s. poh et al.
have confirmed the so- called tsirelson bound, 2[?
]2, with four significant digits, but there is absolutely no experimental evidence of whether we can 'touch' these bounds in other scenarios.
also, it would be great to derive quantum theory from the assumption that there are no laws of nature determining or limiting the probabilities of measurement outcomes, and that the whole machinery of the theory follows from the aesthetic preference in the way we define 'measurements.'"
finally, the physicists also plan to investigate potential applications, such as building quantum technologies with the help of classical systems.
"although inefficient in the sense that they require more memory or space, classical systems are sometimes better to produce 'quantum' numbers than quantum systems themselves," frustaglia said.
"in contrast to quantum systems, which are very sensitive to the environment, the wires in our experiment can be bent, moved, heated, etc., and the results are the same.
this suggests a future in which quantum technologies are actually built using quantum systems plus classical systems imitating quantum systems.
it also raises the question as to whether similar 'quantum' features with potential functionalities can emerge in other supports as complex networks of artificial or biological nature.
an appropriate answer to this questions requires multidisciplinary efforts that we are presently considering."
the california institute of technology has demonstrated that quantum entanglement can simultaneously transfer whole blocks of quantum information, providing a proof- of- concept for future quantum hds.
the caltech team claimed that its device is the harbinger of quantum hds that someday may challenge traditional storage technologies with optical memories that use entanglement for access.
in its demonstration, the team transfered the state of four quantum memories to an optical signal and back again, claiming that the principle could be extended to any number of parallel transfers into and out of future quantum hds.
"we have shown four quantum memories talking to four quantum channels that can be coherently absorbed by virtue of electromagnetic transparency, which slows down the light to zero for storage," said kyung soo cho, a doctoral candidate at caltech
electromagnetically- induced transparency is a coherent optical nonlinearity which renders a medium transparent, enabling light encoded with quantum states to be stopped within a quantum memory device.
caltech's entanglement technique used lasers to cool the four quantum memories- each a collection of 1 million cesium atoms magnetically separated by 1 millimeter.
the magnetic spin of each atom in the quantum memory spins either up or down, collectively describing a spin- wave that represents the whole ensemble.
by simultaneously irradiating the quantum memories with laser beam encoding, the spin waves of the four quantum memories were identically entangled.
this technique, called "measurement induced entanglement," was first achieved at caltech five years ago, but only for two ensembles.
caltech has now demonstrated theoretically that the technique can be extended to any number of nodes.
the caltech group also characterized the decay of the entangled quantum states among the separate nodes, showing how the system decays from a complex quantum state into classical memory values in what they claims is a predictable, repeatable manner.
next, the researchers want to study the dynamics of entanglement decay as applied to entangled spin waves in quantum magnetic memories.
they also want to expand quantum "metrology" whereby quantum states can be generated, stored and transferred using measurements of the ordinarily fragile states of quantum memories.
research funding was provided by the national science foundation, the defense department and northrop grumman corp., among others.
as we saw during the 2016 us election, protecting traditional computer systems, which use zeros and ones, from hackers is not a perfect science.
now consider the complex world of quantum computing, where bits of information can simultaneously hold multiple states beyond zero and one, and the potential threats become even trickier to tackle.
even so, researchers at the university of ottawa have uncovered clues that could help administrators protect quantum computing networks from external attacks.
"our team has built the first high- dimensional quantum cloning machine capable of performing quantum hacking to intercept a secure quantum message," said university of ottawa department of physics professor ebrahim karimi, who holds the canada research chair in structured light.
"once we were able to analyze the results, we discovered some very important clues to help protect quantum computing networks against potential hacking threats."
quantum systems were believed to provide perfectly secure data transmission because until now, attempts to copy the transmitted information resulted in an altered or deteriorated version of the original information, thereby defeating the purpose of the initial hack.
traditional computing allows a hacker to simply copy and paste information and replicate it exactly, but this doesn't hold true in the quantum computing world, where attempts to copy quantum information- or qudits- result in what karimi refers to as "bad" copies.
until now.
for the first time, professor karimi's team was able to clone the photons that transmit information, namely the single carriers of light known as qubits, as well as quantum theory allows, meaning that the clones were almost exact replicas of the original information.
however, in addition to undermining what was previously thought to be a perfect way of securely transmitting information, the researchers' analyses revealed promising clues into how to protect against such hacking.
"what we found was that when larger amounts of quantum information are encoded on a single photon, the copies will get worse and hacking even simpler to detect," said frederic bouchard, a university of ottawa doctoral student and lead author of an open access publication that appeared this month in the renowned journal science advances.
"we were also able to show that cloning attacks introduce specific, observable noises in a secure quantum communication channel.
ensuring photons contain the largest amount of information possible and monitoring these noises in a secure channel should help strengthen quantum computing networks against potential hacking threats."
karimi and his team hope that their quantum hacking efforts could be used to study quantum communication systems, or more generally to study how quantum information travels across quantum computer networks.
to read their paper, visit the science advances website.
as multiple research groups around the world race to build a scalable quantum computer, questions remain about how the achievement of quantum supremacy will be verified.
quantum supremacy is the term that describes a quantum computer's ability to solve a computational task that would be prohibitively difficult for any classical algorithm.
it is considered a critical milestone in quantum computing, but because the very nature of quantum activity defies traditional corroboration, there have been parallel efforts to find a way to prove that quantum supremacy has been achieved.
researchers at the university of california, berkeley, have just weighed in by giving a leading practical proposal known as random circuit sampling (rcs) a qualified seal of approval with the weight of complexity theoretic evidence behind it.
random circuit sampling is the technique google has put forward to prove whether or not it has achieved quantum supremacy with a 72- qubit computer chip called bristlecone, unveiled earlier this year.
the uc berkeley computer theorists published their proof of rcs as a verification method in a paper published monday, oct. 29, in the journal nature physics.
"the need for strong evidence for quantum supremacy is under- appreciated, but it's important to pin this down," said study principal investigator umesh vazirani, roger a. strauch professor of electrical engineering and computer science at uc berkeley.
"besides being a milestone on the way to useful quantum computers, quantum supremacy is a new kind of physics experiment to test quantum mechanics in a new regime.
the basic question that must be answered for any such experiment is how confident can we be that the observed behavior is truly quantum and could not have been replicated by classical means.
that is what our results address."
the other investigators on this paper are adam bouland and bill fefferman, both postdoctoral research fellows, and chinmay nirkhe, a ph.d. student, all in vazirani's theoretical computing research group.
investment in quantum is heating up
the paper comes amid accelerated activity in government, academia and industry in quantum informational science.
congress is considering the national quantum initiative act, and last month, the u.s. department of energy and the national science foundation announced nearly $250 million in grants to support research in quantum science and technologies.
at the same time, the lawrence berkeley national laboratory and uc berkeley announced the formation of berkeley quantum, a partnership designed to accelerate and expand innovation in quantum information science.
the stakes are high as international competition in quantum research heats up and the need for increasingly complex computations grows.
with true quantum computing, problems that are impractical for even the fastest supercomputers to date could be relatively efficient to solve.
it would be a game- changer in cryptography, simulations of molecular and chemical interactions and machine learning.
quantum computers are not confined by the traditional 0s and 1s of a traditional computer's bits.
instead, quantum bits, or qubits, can encode 0s, 1s and any quantum superposition of the two to create multiple states simultaneously.
when google unveiled bristlecone, it said the empirical proof of its quantum supremacy would come through random circuit sampling, a technique in which the device would use random settings to behave like a random quantum circuit.
to be convincing, there would also need to be strong evidence that there is no classical algorithm running on a classical computer that could simulate a random quantum circuit, at least in a reasonable amount of time.
detecting quantum accents
vazirani's team referred to an analogy between the output of the random quantum circuit and a string of random syllables in english: even if the syllables don't form coherent sentences or words, they will still possess an english "accent" and will be recognizably different from greek or sanskrit.
they showed that producing a random output with a "quantum accent" is indeed hard for a classical computer through a technical complexity theoretic construct called "worst- to- average- case reduction."
the next step was to verify that a quantum device was actually speaking with a quantum accent.
this relies on the goldilocks principle- a 50- qubit machine is large enough to be powerful, but small enough to be simulated by a classical supercomputer.
if it's possible to verify that a 50- qubit machine speaks with a quantum accent, then that would provide strong evidence that a 100- qubit machine, which would be prohibitively hard to simulate classically, would do so, as well.
but even if a classical supercomputer were programmed to speak with a quantum accent, would it be able to recognize a native speaker?
the only way to verify the output of the speaker is by a statistical test, said the berkeley researchers.
google researchers are proposing to measure the degree of matching by a metric called "cross- entropy difference."
a cross- entropy score of 1 would be an ideal match.
the alleged quantum device may be regarded as behaving like an ideal quantum circuit with random noise added.
fefferman and bouland say the cross- entropy score will certify the authenticity of the quantum accent provided the noise always adds entropy to the output.
this is not always the case - for example if the noise process preferentially erases 0s over 1s, it can actually reduce the entropy.
"if google's random circuits are generated by a process that allows such erasures, then the cross- entropy would not be a valid measure of quantum supremacy," said bouland.
"that's partly why it will be very important for google to pin down how its device deviates from a real random quantum circuit."
these results are an echo of work that vazirani did in 1993 with his student ethan bernstein, opening the door to quantum algorithms by presenting speedups by quantum computers violating a foundational principle of computer science called the extended church- turing thesis.
peter shor of bell labs took their work one step further by showing that a very important practical problem, integer factorization, could be exponentially sped up by a quantum computer.
"this sequence provides a template for the race to build working quantum computers," said vazirani.
"quantum supremacy is an experimental violation of the extended church- turing thesis.
once that is achieved, the next challenge will be to design quantum computers that can solve practically useful problems."
in a quantum superposition, a quantum object can be in two incompatible states at the same time, which is famously illustrated by schrodinger's dead- and- alive cat.
recent research has shown that it's possible to have a superposition not only of incompatible states, but also of incompatible orders of events.
we often think of events occurring in a definite chronological order, with event a happening (and causing) event b, or vice versa.
but in certain quantum processes, events don't happen in a single definite order, but instead both orders (a before b, and b before a) occur at the same time.
this counterintuitive superposition- like phenomenon is called "causal nonseparability."
"in everyday life, we are used to experiencing one thing always happening after another, effects following their causes," mateus araujo at the university of vienna and the institute for quantum optics and quantum information in vienna, austria, told phys.org.
"so it is a bit unsettling to realize that deep down nature doesn't work like this, that things can happen without a definite causal order, where we cannot say what is the cause and what is the effect."
until now, causal nonseparability in quantum mechanics has been conceived only in a very abstract way, with no clear physical interpretation.
but in a new paper published in the new journal of physics, araujo and coauthors have described an example of a physical quantum process that demonstrates causal nonseparability.
"the theory of relativity has already shaken the idea that there is an absolute, global time, and that everyone experiences the flow of time and time relations in the same way: two different observers in different reference frames may, for instance, disagree on which event happens before the other," said coauthor cyril branciard at cnrs and the universite grenoble alpes in grenoble, france.
"quantum theory, on the other hand, has shaken our understanding of reality by telling us that physical systems may not have well- defined properties, and may be in a 'superposition' of incompatible states.
for example, a poor cat could be both alive and dead at the same time.
now we find that not just physical properties, but also causal relations (or causal orders) themselves can be undefined, and can be put in some kind of superposition- a phenomenon that had not been observed experimentally until very recently."
quantum switch
the causally nonseparable quantum process that the physicists explore here is called a quantum switch, which was recently proposed as a way to improve the efficiency of quantum computers.
in the new study, the physicists introduced a test for causal nonseparability, which they explain is similar to tests for quantum entanglement.
both types of tests produce a certain range of values if all operations are performed in the "classical" way (that is, using only causally separable resources or non- entangled states, respectively) but produce a different range of values if these conditions are not met.
the physicists showed that the causal nonseparability of not only the quantum switch, but of any causally nonseparable process, can be detected by their new test.
this may make the test useful for identifying causal nonseparability in other systems that may be experimentally implemented in the future.
as the researchers explain, just because the quantum switch is causally nonseparable (meaning the operations do not follow a definite order), this does not mean that it violates any causal inequality (which would happen if a future event were to cause a past event).
this is because there is no definite past or future in the quantum switch; neither event definitely comes before or after the other.
although the quantum switch does not violate any causal inequality, the question remains open as to whether any practical, physical process that can be experimentally realized may do so.
causally nonseparable computing
previous research has shown that the quantum switch has computational advantages over standard causally separable protocols, which suggests that causal nonseparability may have applications in quantum computing.
"the way quantum computers (or any device that performs some quantum information processing task) are studied typically assumes that the operations they perform are done in a definite order," branciard said.
"this is, for instance, a basic assumption of the standard 'circuit model' of quantum computation, which is commonly used to describe the functioning of quantum computers.
hence, the vast majority of results we know about the power of quantum computers (for example, what kinds of problems they can solve, with what efficiency, and the complexity of the algorithms they can run) apply only for cases with a definite causal order between all operations- in other words, for causally separable resources.
"the fact that quantum theory also allows for causally nonseparable resources (for example, the quantum switch) thus opens up new possibilities, and it is thus natural to expect that causally nonseparable resources can outperform causally separable ones for certain tasks."
an example of such a task, previously proposed by physicist giulio chiribella, involves determining whether the order of operations of a certain process matters: do you get the same result when you perform "a then b" as you do when performing "b then a"?
if the result is the same, then the operations are said to commute; if not, they anticommute.
to answer this question, a causally separable process must perform both "a then b" and "b then a" orders and compare the results.
on the other hand, a causally nonseparable process such as the quantum switch performs both orders simultaneously, in a quantum superposition, which solves the problem in one step.
not only is the nonseparable process more efficient, but in some cases it may be the only way to solve the problem- for instance, the "black boxes" used to perform the quantum operations may be destroyed after a single use, so that the procedure can only be performed once.
since the quantum switch is the simplest example of causal nonseparability, the physicists hope that different kinds of causally nonseparable processes might allow for even stronger advantages over causally separable ones.
"more generally, i expect causally nonseparable processes to find applications in various other kinds of situations- just like entanglement proved to be useful for various applications in quantum information processing," branciard said.
"the full power of causal nonseparability is still to be discovered, and this makes this line of research particularly exciting!"
ludwig maximilian university of munich researchers have uncovered a novel effect that, in principle, offers a means of stabilizing quantum systems against decoherence.
the discovery could represent a major step forward for quantum information processing.
the laws of classical physics provide an adequate description of how our universe behaves on the macroscopic scales that are accessible to our everyday experience.
in the world of classical mechanics, the state of a physical system and its future evolution is fully determined by the instantaneous locations and velocities of its constituent particles.
at the microscopic level, however, where the dynamics involves minute changes in energy - as in the case of atoms or electrons in a solid - things are very different.
here quantum mechanics reigns supreme, and the mathematical form of its laws allows even single particles to occupy states that correspond to a combination, or superposition, of distinct classical states.
in this case, the position and velocity of a particle can only be described in terms of probabilities.
"this means that the system has a much greater range of possible states available to it.
it is therefore far more complex and much more difficult to describe, but the complexity also offers novel opportunities for technical applications," says lmu physicist dr. thomas barthel.
one potential application of quantum effects is in quantum computers, which are the subject of intensive research.
miniaturization of conventional electronic computers has been so rapid that it component sizes are fast approaching the limit at which quantum phenomena must be explicitly taken into account.
current efforts focus on minimizing the perturbations introduced by such effects, but the quantum computer turns this paradigm on its head.
it seeks to exploit quantum effects such as complex superpositions for information processing, and promises to vastly increase the efficiency of computing.
however, the controlled application of quantum effects is itself subject to one severe limitation: quantum mechanical states are extremely fragile.
if a quantum mechanical system is not effectively shielded from its surroundings, its interactions with the environment lead to rapid decay of its quantum properties.
thus, if one uses a probe to measure the position or velocity of a quantum particle - an atom, for example - the measurement itself forces the system to adopt a single defined state, and the superposition is irrevocably destroyed.
when a quantum system is coupled to its environment, something very similar occurs.
the interaction with the environment is, in effect, a kind of measurement, and the information stored in the quantum system is irrevocably lost.
"the system then behaves in accordance with the normal - i.e.
boring - laws of classical mechanics," says barthel.
many- body systems can resist decoherence
physicists refer to this phenomenon as decoherence, and it is the bane of every experimenter who wants to learn more about the quantum mechanical properties of a system or utilize them for technical applications.
until now, it was commonly accepted that the decay of quantum coherence always occurs exponentially with time.
however, in their new study, instead of using a simple system such as an isolated electron or ion, barthel and his colleague dr. zi cai consider a "many- body system", such as the electrons in a solid, which consists of very large numbers of particles.
"we found that, in this case, the time- dependence of the coherence decay can be qualitatively different," barthel explains.
if the system is made up of a very large collection of particles, the interactions between these particles can alter the coherence decay from the typical exponential behavior of simpler systems to a much slower power law decay.
interactions between the particles can therefore minimize the destructive influence of the environment.
the two scientists have in effect discovered a previously unsuspected fundamental effect, which is of potentially great significance tor future experiments on, and applications of, quantum states.
"with our study, we have uncovered a feature with which the decoherence of a quantum system can be tuned and substantially reduced - this represents an important advance, in particular for the field of quantum information processing," as barthel underlines.
in principle, the effect can be exploited to protect the integrity of quantum information.
its discovery thus brings practical quantum computing, and simulations of complex quantum systems with the help of experimentally tractable quantum systems, a step closer to reality.
mention the word 'teleportation' and for many people it conjures up "beam me up, scottie" images of captain james t kirk.
but in the last two decades quantum teleportation - transferring the quantum structure of an object from one place to another without physical transmission- has moved from the realms of star trek fantasy to tangible reality.
quantum teleportation is an important building block for quantum computing, quantum communication and quantum network and, eventually, a quantum internet.
while theoretical proposals for a quantum internet already exist, the problem for scientists is that there is still debate over which of various technologies provides the most efficient and reliable teleportation system.
this is the dilemma which an international team of researchers, led by dr stefano pirandola of the department of computer science at the university of york, set out to resolve.
in a paper published in nature photonics, the team, which included scientists from the freie universitat berlin and the universities of tokyo and toronto, reviewed the theoretical ideas around quantum teleportation focusing on the main experimental approaches and their attendant advantages and disadvantages.
none of the technologies alone provide a perfect solution, so the scientists concluded that a hybridisation of the various protocols and underlying structures would offer the most fruitful approach.
for instance, systems using photonic qubits work over distances up to 143 kilometres, but they are probabilistic in that only 50 per cent of the information can be transported.
to resolve this, such photon systems may be used in conjunction with continuous variable systems, which are 100 per cent effective but currently limited to short distances.
most importantly, teleportation- based optical communication needs an interface with suitable matter- based quantum memories where quantum information can be stored and further processed.
dr pirandola, who is also a member of the york centre for quantum technologies, said: "we don't have an ideal or universal technology for quantum teleportation.
the field has developed a lot but we seem to need to rely on a hybrid approach to get the best from each available technology.
"the use of quantum teleportation as a building block for a quantum network depends on its integration with quantum memories.
the development of good quantum memories would allow us to build quantum repeaters, therefore extending the range of teleportation.
they would also give us the ability to store and process the transmitted quantum information at local quantum computers.
"this could ultimately form the backbone of a quantum internet.
the revised hybrid architecture will likely rely on teleportation- based long- distance quantum optical communication, interfaced with solid state devices for quantum information processing."
we're now one step closer to quantum computing becoming a reality thanks to research led by a team of university of sydney physicists, who have found a new way to detect changes in charges smaller than one electron.
the research is published in this week's edition of physical review letters.
"our new method for detecting charge in quantum systems is exciting and has implications for a range of nanotechnologies," said associate professor david reilly, from the arc centre for engineered quantum systems in the school of physics at the university of sydney.
"we've been successful in finding a new, more convenient way to detect changes in charge of a single electron on quantum dots.
quantum dots are nanoscale systems that can confine or trap single electrons," explained associate professor reilly.
"electrons confined to quantum dots are very nice systems for storing and manipulating quantum information, where data is encoded in the quantum mechanical aspects of the electron.
our goal is to scale- up a large number of quantum dots to ultimately create a machine to process quantum information - a quantum computer."
ever since nobel prize winner richard feynman highlighted the potential of quantum computing in the 1980s, scientists have been attempting to build quantum computers capable of solving some of the largest and most complex problems, with much greater efficiency than conventional computers.
"we've focused on quantum dots as their properties can be tuned in the laboratory - we can control their energy spectrum by turning a knob in the lab."
"being able to detect single electron charges on the quantum dots is absolutely essential, as it's the way information is retrieved from such quantum mechanical systems.
we call it 'read- out' and it's analogous to reading information from the memory or a hard drive in a regular classical computer," said associate professor reilly.
"without the ability to read- out quantum information, we have no way of getting the answer to a computation!"
the team, including school of physics phd students james colless, alice mahoney and john hornibrook, and associate professor andrew doherty and associate professor david reilly, with two scientists from the university of california, santa barbara, have found a new way of detecting charge on the quantum dots using the gate electrodes already in the system.
"previously, sensitive electrometers which measure minute charges were used to read- out the electron state on quantum dots.
these work well, but they are somewhat separate devices built onto the ends of the quantum dot system.
they are a bit like having microphones nearby that can pick up the sound of electrons," explained associate professor reilly.
"what we have shown is that the gates or electrodes that are already in place to create the quantum dot in the first place, can also act as read- out detectors.
this means you don't need separate devices and you don't need to worry about how to place those separate electrometer devices."
"whereas the old system was like having microphones nearby to detect sound, our new system could be likened to using the walls of a room as in- built microphones - you don't need separate microphones for every room of the house, just use the walls as microphones," said associate professor reilly.
"our new method makes the whole quantum system easier to build and use, as adding nanoscale electrometers for every quantum dot in a million- dot- array is a hard problem.
by using the electrodes already in the system, we've found an efficient new way to measure charge in the big quantum systems of the future."
the new method of detection allows for read- out in large dot arrays with no limitation on the size of the array for the read- out method to work.
james colless, whose phd research contributed greatly to the finding, said, "the technologies that we are developing are part of a global research effort to advance the prospect of quantum computing.
in a similar way to how billions of transistors can now be placed on a single silicon computer chip, in the future we would like to engineer semiconductor chips containing huge numbers of interacting quantum two- level systems - called qubits.
the work presented in this paper suggests a new method of reading out qubits that enables this goal."
one of the most exciting and diverse fields of science today involves quantum information processing.
there are many designs for quantum computers suggested, and a few that have been demonstrated.
among the demonstrated suggestions for a quantum computer is a one- way quantum computation process that makes use of a two- photon four- qubit cluster state.
kai chen, a scientist at the physikalisches institut in heidelberg, germany and the university of science and technology of china (ustc) in hefei, china, tells physorg.com, "one- way quantum computing model was proposed years ago, but our experiment is a brand new demonstration of the computing model."
chen and his team, lead by prof. jian- wei pan, which consists of colleagues from the physikalisches institut as well as from ustc and the national chiao tung university in hsinchu, taiwan, present their results in a physical review letters piece titled, "experimental realization of one- way quantum computing with two- photon four- qubit cluster states."
"our new model of quantum computing is different from the quantum circuit model, which has an input and an output."
chen says.
"we use two- photon cluster states, and information is written onto the cluster, processed, and read out from the cluster by one- particle measurements only."
he does point out that work is needed to produce this method of obtaining output: "we have designed a specific order and choices of measurements to get desired output."
cluster states in quantum computing are highly entangled states deemed necessary in one- way quantum computing.
in the quantum world, entanglement among quantum objects, such as qubits, is described with reference to the others, even though they may be spatially separated.
indeed, chen and his colleagues performed their experiment showing a two- photon four- qubit cluster state entangling photons in both spatial and polarization modes.
chen says that this demonstration of quantum computing is more efficient than other photonic schemes.
"developing and using two- photon cluster states allows us to be four magnitudes more efficient than the previous sources.
we are increasing the efficiency of quantum computing."
he also points out that the new design for photonic quantum computing developed by pan's team allows for high fidelity.
"with the previous source, there is a lot of intrinsic noise due to multi- photon generation," chen says.
"using two- photon, our system offers much lower noise with a very high fidelity quantum gate."
this means that more of the information is passed on, and less of it is lost in background noise.
chen explains that this type of quantum computing is an optical quantum computer, using light.
"we have designed a new scheme for producing the four- qubit cluster states, which are based on techniques that we have developed before for generating hyper- entangled states.
with our new designs, the scheme is expected to motivate further progress in quantum computing."
he continues: "we think this quantum computing technique with optics has a very bright future."
what kind of a future?
chen and his colleagues are already working on ideas for the future of quantum information processing.
"we are working on extending qubit numbers to perform more complicated tasks," he says.
in their experiment chen and his peers implemented a grover's search algorithm.
they hope that being able to increase their cluster states to eight qubits or more will "exponentially increase the ability to do quantum computing."
chen continues: "if we combine our technique of optics with quantum memory using atoms, we can extend our abilities of performing quantum computation and quantum communication.
one can think that in the future, we can get a true quantum computer, and have a global quantum network."
researchers gathered this week to extend the use of quantum effects in semiconductors.
quantum effects result from the confinement of electrons, or holes, by restricting their free movement (perpendicular to the direction of crystal growth for quantum dots), thereby enabling their quantum effects to dominate.
at the international conference on the physics of semiconductors in vienna, austria, university of new south wales (sydney, austrailia) claimed a world's first for quantum effects: successful fabrication of quantum wires from gallium arscenide.
dubbed "hole quantum wires," the researchers reported on different aspects of hole quantum wires.
other researchers discussed controlling spin in quantum dots, including those formed in graphene sheets and nanotube transport of holes with quantum spin to "q- bit" calculations of a quantum "hall effect."
ballistic transport in quantum wires, bound electron- holes (excitons) in semiconductor quantum dots and optical control of spin polarization were also hot topics.
new methods of handling nitrides, bose condensates and quantum- effect optical devices such as quantum- cascade lasers and single- photon lasers are also emerging, researchers said.
researchers have made a fundamental breakthrough in quantum computing by creating a machine that can operate at room temperature.
in a paper published in the november 15 issue of the journal science, an international research team has detailed how they designed a quantum computer that maintained superposition for 39 minutes at 25 degrees c.
superposition, the attribute that separates quantum computers from classical computers, allows quantum bits, also known as qubits, to exist as both a 1 and 0 simultaneously.
by leveraging the laws of quantum mechanics, quantum computers can harness vastly greater computing power than a classical machine.
according to stephanie simmons, an oxford university scientist and author of the paper, "[the] lifetimes [of our qubits] are at least ten times longer than those measured in previous experiments,' simmons continued, 'we've managed to identify a system that seems to have basically no noise.
they're high- performance qubits.'
while most of the excitement surrounding this breakthrough has concerned the computer's ability to operate well above absolute zero, another important aspect of the discovery was related to its materials.
to create their quantum computer researchers used a silicon base; suggesting that quantum computers could be produced more easily through the use of relatively common materials.
additionally, since most semiconductor manufacturing techniques are based on silicon designs, these already commonplace methods could be used to build quantum computers.
while researchers still have a long way to go before they create large, long- living quantum computers, this recent breakthrough could spur increased investment in the technology's development.
image and video courtesy of stef simmons, wikipedia & veritasi
usc scientists have demonstrated a theoretical method to enhance the performance of quantum computers, an important step to scale a technology with potential to solve some of society's biggest challenges.
the method addresses a weakness that bedevils performance of the next- generation computers by suppressing erroneous calculations while increasing fidelity of results, a critical step before the machines can outperform classic computers as intended.
called "dynamical decoupling," it worked on two quantum computers, proved easier and more reliable than other remedies and could be accessed via the cloud, which is a first for dynamical decoupling.
the technique administers staccato bursts of tiny, focused energy pulses to offset ambient disturbances that muck sensitive computations.
the researchers report they were able to sustain a quantum state up to three times longer than would otherwise occur in an uncontrolled state.
"this is a step forward," said daniel lidar, professor of electrical engineering, chemistry and physics at usc and director of the usc center for quantum information science and technology (cqist).
"without error suppression, there's no way quantum computing can overtake classical computing."
the results were published today in the journal physical review letters.
lidar is the viterbi professor of engineering at usc and corresponding author of the study; he led a team of researchers at cqist, which is a collaboration between the usc viterbi school of engineering and the usc dornsife school of letters, arts and sciences.
ibm and bay area startup rigetti computing provided cloud access to their quantum computers.
quantum computers are fast, but fragile
quantum computers have the potential to render obsolete today's super computers and propel breakthroughs in medicine, finance and defense capabilities.
they harness the speed and behavior of atoms, which function radically different than silicon computer chips, to perform seemingly impossible calculations.
quantum computing has the potential to optimize new drug therapies, models for climate change and designs for new machines.
they can achieve faster delivery of products, lower costs for manufactured goods and more efficient transportation.
they are powered by qubits, the subatomic workhorses and building blocks of quantum computing.
but qubits are as temperamental as high- performance race cars.
they are fast and hi- tech, but prone to error and need stability to sustain computations.
when they don't operate correctly, they produce poor results, which limits their capabilities relative to traditional computers.
scientists worldwide have yet to achieve a "quantum advantage" -  the point where a quantum computer outperforms a conventional computer on any task.
the problem is "noise," a catch- all descriptor for perturbations such as sound, temperature and vibration.
it can destabilize qubits, which creates "decoherence," an upset that disrupts the duration of the quantum state, which reduces time a quantum computer can perform a task while achieving accurate results.
"noise and decoherence have a large impact and ruin computations, and a quantum computer with too much noise is useless," lidar explained.
"but if you can knock down the problems associated with noise, then you start to approach the point where quantum computers become more useful than classic computers."
usc research spans multiple quantum computing platforms
usc is the only university in the world with a quantum computer; its 1098- qubit d- wave quantum annealer specializes in solving optimization problems.
part of the usc- lockheed martin center for quantum computing, it's located at usc's information sciences institute.
however, the latest research findings were achieved not on the d- wave machine, but on smaller scale, general- purpose quantum computers: ibm's 16- qubit qx5 and rigetti's 19- qubit acorn.
to achieve dynamical decoupling (dd), the researchers bathed the superconducting qubits with tightly focused, timed pulses of minute electromagnetic energy.
by manipulating the pulses, scientists were able to envelop the qubits in a microenvironment, sequestered -  or decoupled -  from surrounding ambient noise, thus perpetuating a quantum state.
"we tried a simple mechanism to reduce error in the machines that turned out to be effective," said bibek pokharel, an electrical engineering doctoral student at usc viterbi and first author of the study.
the time sequences for the experiments were exceedingly small with up to 200 pulses spanning up to 600 nanoseconds.
one- billionth of a second, or a nanosecond, is how long it takes for light to travel one foot.
for the ibm quantum computers, final fidelity improved threefold, from 28.9 percent to 88.4 percent.
for the rigetti quantum computer, final fidelity improvement was a more modest 17 percent, from 59.8 to 77.1, according to the study.
the scientists tested how long fidelity improvement could be sustained and found that more pulses always improved matters for the rigetti computer, while there was a limit of about 100 pulses for the ibm computer.
overall, the findings show the dd method works better than other quantum error correction methods that have been attempted so far, lidar said.
"to the best of our knowledge," the researchers wrote, "this amounts to the first unequivocal demonstration of successful decoherence mitigation in cloud- based superconducting qubit platforms ... we expect that the lessons drawn will have wide applicability."
high stakes in the race for quantum supremacy
the quest for quantum computing supremacy is a geopolitical priority for europe, china, canada, australia and the united states.
advantage gained by acquiring the first computer that renders all other computers obsolete would be enormous and bestow economic, military and public health advantages to the winner.
congress is considering two new bills to establish the united states as a leader in quantum computing.
in september, the house of representatives passed the national quantum initiative act to allocate $1.3 billion in five years to spur research and development.
it would create a national quantum coordination office in the white house to supervise research nationwide.
a separate bill, the quantum computing research act by sen. kamala harris, d- calif., directs the department of defense to lead a quantum computing effort.
"quantum computing is the next technological frontier that will change the world and we cannot afford to fall behind," harris said in prepared remarks.
"it could create jobs for the next generation, cure diseases and above all else make our nation stronger and safer.
...
without adequate research and coordination in quantum computing, we risk falling behind our global competition in the cyberspace race, which leaves us vulnerable to attacks from our adversaries," she said.
under the terms of the joint initiative, an ibm q system one quantum computer is to be installed at a german location.
it will be the first facility of its kind in europe.
the ibm q system one is designed to perform multi- qubit operations to an extremely high level of quality, stability, reliability and reproducibility.
these factors and the resultant large quantum volume - a measure of the power of a quantum computer - mean that the ibm q system one is the ideal platform for state- of- the- art research into concrete quantum computing applications in science and industry.
quantum computing promises to deliver the power required to analyze the complex systems of business and industry, to disentangle the convoluted interdependencies in molecular and chemical reactions, to master complex optimization problems and to significantly increase the performance of artificial intelligence.
such advances could open the door to new scientific discoveries and deliver enormous improvements in supply- chain management, logistics and the modeling of financial data and data for classic engineering problems.
the german federal government is to invest 650 million euros over the next two years to promote the advance of quantum technology from basic research to market- ready applications.
the establishment of the fraunhofer center for quantum computing is in accordance with the objectives of the federal government's framework program.
at the same time, it will provide the ibm q network with a major european hub for quantum computing.
the focus here will be on achieving a unique concentration of quantum skills in germany and building a community of researchers, developers, it professionals and industry experts in this field.
this joint initiative between the fraunhofer- gesellschaft and ibm will bring together prominent partners from research and industry under the common roof of a new fraunhofer competence center for quantum computing.
to be known as the fraunhofer center for quantum computing, this facility will be operated and managed by the fraunhofer- gesellschaft, which already conducts research throughout the field of quantum technology at 14 of its institutes.
"quantum technology is set to have a major impact on germany's future, in both the scientific and economic sphere," says germany's federal minister of education and research, anja karliczek.
"last year, the federal government therefore launched the program 'quantum technology: from basic principles to market applications', which provides a clearly defined framework for action.
federal funds of 650 million euros will be invested in research and the development of quantum technology over the period until 2022.
fraunhofer- gesellschaft's collaboration with ibm in the field of quantum computing can make an important contribution to the realization of this program.
it is vital that we now begin developing various fields of application for quantum computing, not least for small and medium- sized companies, which play a significant role in the german economy."
"bavaria will get a quantum computer - i.e., a computer that is very much faster than any of the current generation," adds the bavarian minister of economic affairs, regional development and energy, hubert aiwanger.
"researchers from the renowned fraunhofer- gesellschaft are going to be cooperating with ibm.
this state- of- the- art computer will provide a major boost for bavarian research and industry, placing us at the very forefront of this sector."
"ibm has been in germany for over 100 years and over that period has continuously invested in the country's digital future.
having its own quantum computer system will enable germany to advance research, development and business in europe and beyond," says matthias hartmann, general manager of ibm germany.
"it will bolster the country's position as a leading hub for technology and innovation."
"this partnership is a pioneering initiative in the field of applied quantum computing and marks a crucial advance for german research institutions as well as companies of all sizes in our country," explains professor reimund neugebauer, president of the fraunhofer- gesellschaft.
"the installation of an ibm q system in europe is unprecedented and will enable the development of new strategies for quantum computing, at the new fraunhofer center, under full data sovereignty according to european law."
in cooperation with the ibm q network, the new fraunhofer center will help harness the full potential of quantum computing.
participating companies will have access to ibm's advanced quantum systems via the ibm cloud.
experts from industry and research require new skills and know- how in order to capitalize on quantum computing.
as part of the ibm q network, companies will therefore receive support and training from industry- leading specialists at ibm.
researchers at the center for quantum nanoscience within the institute for basic science (ibs) have made a major breakthrough in controlling the quantum properties of single atoms.
in an international collaboration with ibm research in san jose, california, using advanced techniques, the scientists identified which mechanisms destroy the quantum properties of individual atoms by manipulating the magnetic state of a single iron atom on a thin insulator.
using a scanning tunneling microscope with an atomically sharp metal tip, they were able to image individual iron atoms and measure and control the time that they maintain their quantum behavior.
their findings, published in the journal science advances, show that the loss in quantum state superposition is mainly caused by nearby electrons that the researchers precisely injected into the iron atom.
"we found that almost every electron destroys the quantum state," explains dr. philip willke, first author of the study.
"in addition, we found that nearby fluctuating magnets had a similar negative impact.
while our experiments decreased the state of superposition on purpose, it also gave us valuable clues on how to improve the atoms' quantum states."
andreas heinrich, director of the ibs center for quantum nanoscience, said, "understanding these destructive interactions allows us to avoid them in future experiments and improve the performance of magnetic quantum sensors that, in this case, only consist of a single atom."
quantum nanoscience relies on harnessing the properties of atoms and molecules for potential advances in quantum sensing, potentially improving devices using such technology, including hospital mri machines.
quantum computers could also potentially benefit from this research.
while still in early development, quantum computation promises to outperform classical computers in tasks such as database management, search and optimization.
a quantum system can maintain two quantum states simultaneously, a condition known as the superposition of quantum states.
however, when such a quantum system interacts in particular environments- either through desired or undesired contact- this superposition of states is easily destroyed.
physicists seek to understand and control these processes.
researchers at the california institute of technology (caltech) have demonstrated quantum entanglement for a quantum state stored in four spatially distinct atomic memories.
their work, described in the november 18 issue of the journal nature, also demonstrated a quantum interface between the atomic memories- which represent something akin to a computer "hard drive" for entanglement- and four beams of light, thereby enabling the four- fold entanglement to be distributed by photons across quantum networks.
the research represents an important achievement in quantum information science by extending the coherent control of entanglement from two to multiple (four) spatially separated physical systems of matter and light.
the proof- of- principle experiment, led by william l. valentine professor and professor of physics h. jeff kimble, helps to pave the way toward quantum networks.
similar to the internet in our daily life, a quantum network is a quantum "web" composed of many interconnected quantum nodes, each of which is capable of rudimentary quantum logic operations (similar to the "and" and "or" gates in computers) utilizing "quantum transistors" and of storing the resulting quantum states in quantum memories.
the quantum nodes are "wired" together by quantum channels that carry, for example, beams of photons to deliver quantum information from node to node.
such an interconnected quantum system could function as a quantum computer, or, as proposed by the late caltech physicist richard feynman in the 1980s, as a "quantum simulator" for studying complex problems in physics.
quantum entanglement is a quintessential feature of the quantum realm and involves correlations among components of the overall physical system that cannot be described by classical physics.
strangely, for an entangled quantum system, there exists no objective physical reality for the system's properties.
instead, an entangled system contains simultaneously multiple possibilities for its properties.
such an entangled system has been created and stored by the caltech researchers.
previously, kimble's group entangled a pair of atomic quantum memories and coherently transferred the entangled photons into and out of the quantum memories (http://media.caltech.edu/press_releases/13115).
for such two- component- or bipartite- entanglement, the subsystems are either entangled or not.
but for multi- component entanglement with more than two subsystems- or multipartite entanglement- there are many possible ways to entangle the subsystems.
for example, with four subsystems, all of the possible pair combinations could be bipartite entangled but not be entangled over all four components; alternatively, they could share a "global" quadripartite (four- part) entanglement.
hence, multipartite entanglement is accompanied by increased complexity in the system.
while this makes the creation and characterization of these quantum states substantially more difficult, it also makes the entangled states more valuable for tasks in quantum information science.
to achieve multipartite entanglement, the caltech team used lasers to cool four collections (or ensembles) of about one million cesium atoms, separated by 1 millimeter and trapped in a magnetic field, to within a few hundred millionths of a degree above absolute zero.
each ensemble can have atoms with internal spins that are "up" or "down" (analogous to spinning tops) and that are collectively described by a "spin wave" for the respective ensemble.
it is these spin waves that the caltech researchers succeeded in entangling among the four atomic ensembles.
the technique employed by the caltech team for creating quadripartite entanglement is an extension of the theoretical work of luming duan, mikhail lukin, ignacio cirac, and peter zoller in 2001 for the generation of bipartite entanglement by the act of quantum measurement.
this kind of "measurement- induced" entanglement for two atomic ensembles was first achieved by the caltech group in 2005.
in the current experiment, entanglement was "stored" in the four atomic ensembles for a variable time, and then "read out"- essentially, transferred- to four beams of light.
to do this, the researchers shot four "read" lasers into the four, now- entangled, ensembles.
the coherent arrangement of excitation amplitudes for the atoms in the ensembles, described by spin waves, enhances the matter- light interaction through a phenomenon known as superradiant emission.
"the emitted light from each atom in an ensemble constructively interferes with the light from other atoms in the forward direction, allowing us to transfer the spin wave excitations of the ensembles to single photons," says akihisa goban, a caltech graduate student and coauthor of the paper.
the researchers were therefore able to coherently move the quantum information from the individual sets of multipartite entangled atoms to four entangled beams of light, forming the bridge between matter and light that is necessary for quantum networks.
the caltech team investigated the dynamics by which the multipartite entanglement decayed while stored in the atomic memories.
"in the zoology of entangled states, our experiment illustrates how multipartite entangled spin waves can evolve into various subsets of the entangled systems over time, and sheds light on the intricacy and fragility of quantum entanglement in open quantum systems," says caltech graduate student kyung soo choi, the lead author of the nature paper.
the researchers suggest that the theoretical tools developed for their studies of the dynamics of entanglement decay could be applied for studying the entangled spin waves in quantum magnets.
further possibilities of their experiment include the expansion of multipartite entanglement across quantum networks and quantum metrology.
"our work introduces new sets of experimental capabilities to generate, store, and transfer multipartite entanglement from matter to light in quantum networks," choi explains.
"it signifies the ever- increasing degree of exquisite quantum control to study and manipulate entangled states of matter and light."
more information: in addition to kimble, choi, and goban, the other authors of the paper, "entanglement of spin waves among four quantum memories," are scott papp, a former postdoctoral scholar in the caltech center for the physics of information now at the national institute of standards and technology in boulder, colorado, and steven van enk, a theoretical collaborator and professor of physics at the university of oregon, and an associate of the institute for quantum information at caltech.
in an effort to extend the computational ability of current- generation quantum computers, ibm announced wednesday a method called "zero- noise extrapolation" that improves the accuracy of computations by repeating a given program multiple times with varying levels of controlled noise.
together, these calculations with varied levels of noise can be used to estimate the result of a calculation in an ideal condition where no noise exists.
the effects of environmental noise on quantum computers can be quite dramatic, even in small quantities.
computations on current quantum hardware are limited by a short coherence time- the amount of useful operational time in a calculation before quantum information is lost- and circuit depth, which measures the number of sequential operations that can be performed.
see: managing ai and ml in the enterprise 2019: tech leaders expect more difficulty than previous it projects (tech pro research)
zero- noise extrapolation is, essentially, the idea that, "if you had some way of controllably amplifying the strength of your noise, you could then extrapolate back to what your quantum computer would have been able to compute in the absence of that noise," ibm research scientist abhinav kandala told techrepublic.
"you can think of this as measuring many wrongs to reconstruct the right answer.
by doing that, we essentially see that we were able to achieve accuracies that would have been otherwise inaccessible to our hardware."
the experiment, as presented in "extending the computational reach of a noisy superconducting quantum processor," was performed on four qubits of a five- qubit system, though kandala notes that "there's nothing preventing us from scaling this to larger systems."
though refinements in manufacturing quantum systems will reduce the amount of noise that occurs naturally, this is a near- term solution for currently available noisy intermediate- scale quantum (nisq) machines, providing 10 times improvement in accuracy in the experiment, though kandala warns that figure will be different depending on the type of problem.
the applicability of zero- noise extrapolation will continue, as "despite all the improvements that we will have in error rates and coherence times, there will still be noise, and this noise will still affect computations that we attempt on our quantum computers," kandala said.
"essentially all these improvements will compound the reach of this technique."
this method has produced observable benefits when using quantum computers, providing the ability to access longer circuit depths.
"in the context of a chemistry simulation, we could prepare states which better represent the states of the molecule you're trying to simulate, and that was giving it computation accuracy," kandala said.
"this is what the endeavor of quantum computing is: you want to prepare states which are not so accessible to classical computation."
ibm's advancement brings practical use of quantum computers for businesses one step closer to reality, as enterprises are increasingly turning to quantum computers for path optimization and other logistics tasks.
this news wraps up a month of high- profile announcements for ibm's quantum computing initiatives.
on march 4, the company announced the availability of a quantum computer with a quantum volume of 16, a new high for the company, which ibm claims puts it on track to reach quantum advantage in the next decade, and doubles the performance of the systems available last year.
on march 19, ibm detailed their efforts to bring machine learning to quantum computers, creating a support- vector network on a quantum system for the first time, using the zero- noise extrapolation method.
for more on ibm q, check out "ibm opens q network hub in tokyo to help businesses explore quantum computing," as well as techrepublic's cheat sheet for quantum computing.
also see
austrian and chinese scientists have succeeded in teleporting three- dimensional quantum states for the first time.
high- dimensional teleportation could play an important role in future quantum computers.
researchers from the austrian academy of sciences and the university of vienna have experimentally demonstrated what was previously only a theoretical possibility.
together with quantum physicists from the university of science and technology of china, they have succeeded in teleporting complex high- dimensional quantum states.
the research teams report this international first in the journal physical review letters.
in their study, the researchers teleported the quantum state of one photon (light particle) to another distant one.
previously, only two- level states ("qubits") had been transmitted, i.e., information with values "0" or "1".
however, the scientists succeeded in teleporting a three- level state, a so- called "qutrit".
in quantum physics, unlike in classical computer science, "0" and "1" are not an 'either/or' - both simultaneously, or anything in between, is also possible.
the austrian- chinese team has now demonstrated this in practice with a third possibility "2".
novel experimental method
it has been known since the 1990s that multidimensional quantum teleportation is theoretically possible.
however: "first, we had to design an experimental method for implementing high- dimensional teleportation, as well as to develop the necessary technology", says manuel erhard from the vienna institute for quantum optics and quantum information of the austrian academy of sciences.
the quantum state to be teleported is encoded in the possible paths a photon can take.
one can picture these paths as three optical fibers.
most interestingly, in quantum physics a single photon can also be located in all three optical fibers at the same time.
to teleport this three- dimensional quantum state, the researchers used a new experimental method.
the core of quantum teleportation is the so- called bell measurement.
it is based on a multiport beam splitter, which directs photons through several inputs and outputs and connects all optical fibers together.
in addition, the scientists used auxiliary photons- these are also sent into the multiple beam splitter and can interfere with the other photons.
through clever selection of certain interference patterns, the quantum information can be transferred to another photon far from the input photon, without the two ever physically interacting.
the experimental concept is not limited to three dimensions, but can in principle be extended to any number of dimensions, as erhard emphasizes.
higher information capacities for quantum computers
with this, the international research team has also made an important step towards practical applications such as a future quantum internet, since high- dimensional quantum systems can transport larger amounts of information than qubits.
"this result could help to connect quantum computers with information capacities beyond qubits", says anton zeilinger, quantum physicist at the austrian academy of sciences and the university of vienna, about the innovative potential of the new method.
the participating chinese researchers also see great opportunities in multidimensional quantum teleportation.
"the basics for the next- generation quantum network systems is built on our foundational research today", says jian- wei pan from the university of science and technology of china.
pan recently held a lecture in vienna at the invitation of the university of vienna and the academy.
in future work, the quantum physicists will focus on how to extend the newly gained knowledge to enable teleportation of the entire quantum state of a single photon or atom.
image credit: rost9/shutterstock.com
"quantum" has recently become the buzzword in popular science and applied industries for the latest technological and scientific breakthroughs.
nanotechnology, superfast computing projects from google, ibm and intel, and even james bond movies have all been trading on the rising popularity of the word, with a somewhat questionable appropriation of it.
"quantum materials" is a similarly contested yet popular term.
this is perhaps one of the primary issues facing an emergent quantum materials industry, and it will be discussed in this article.
however, industry investments, academic research focuses, and government and non- government organization sources of funding are all pointing towards a future quantum materials industry that will be robust and relevant.
"quantum"
"quantum" simply means the smallest possible interacting size.
therefore, "quantum mechanics" describes how the smallest possible interacting particles in the universe - the atoms, electrons, bosons, neutrinos and other particles that are the building blocks of energy and matter - behave very differently from matter at larger scales.
the various corresponding technologies and scientific methods that relate to these minuscule scales are given the adjective "quantum".
"quantum materials"
"quantum materials", then, may be considered something of a misnomer.
all materials are regarded as "quantum" when studied and manipulated at their smallest, atomic scales of size, and any material could undergo this kind of study and manipulation.
however, a growing movement in academia and research is arguing for the importance of a blanket term like "quantum materials".
a 2016 article in nature physics, "the rise of quantum materials", delves into this debate.
the article discusses the history of condensed- matter physics.
this is the area of study that researches the physical properties of matter at both macro- and microscopic scales, as well as physical laws that govern it, such as electromagnetism and both classical and quantum mechanics.
the authors acknowledge that "quantum materials" may be something of a misnomer.
"on a trivial level," they write, "all materials exist thanks to the laws of quantum mechanics."
however, the term has gained in usefulness in recent years, and the authors argue, "there are good reasons to embrace quantum materials."
connecting the (quantum) dots
the main "good reason" described is that "quantum materials" can provide "a common thread linking disparate communities of researchers".
this is the benefit of creating a quantum materials industry.
this industry attention and investment will work to draw together research areas that are often disconnected.
areas including new synthetic materials development, advanced microscopy techniques such as afm or spectrometry, quantum physics, nanotechnology and especially nanorobotics, quantum surface metrology, and more will be bolstered by increasing connections with one another through application in industry.
this "good reason" also reveals one of the major issues with creating a quantum materials industry.
disconnected research, development, and commercial applications.
an emergent quantum materials industry will rely on a continually advancing advanced microscopy field.
as ways of observing matter at the quantum scale improve then better means of fabricating and manipulating the behavior of matter at the quantum scale will emerge.
similarly, as our understanding of matter's incredibly peculiar behavior at the quantum scale increases, then our ability to make use of these strange interactions will increase.
for example, quantum computing relies on the quantum effect of superposition: a particle can be either on or off simultaneously.
(this is the behavior described in the famous schroedinger's cat thought experiment.)
which materials can best undergo an inducement of this state remains a topic of research for some of the biggest investors in quantum computing, including google and ibm.
summary
the primary issue facing an emergent quantum materials industry can be summarised as one of connection.
the connection of research interests, experiments (often requiring the use of expensive laboratory equipment and extensive iterations), funding criteria, and advances in discrete fields.
this issue can be resolved the more "quantum materials" becomes a standard term under which companies and research groups can operate.
references and further reading
the rise of quantum materials.
(2016).
nature physics, 12(2), pp.105- 105.
disclaimer: the views expressed here are those of the author expressed in their private capacity and do not necessarily represent the views of azom.com limited t/a azonetwork the owner and operator of this website.
this disclaimer forms part of the terms and conditions of use of this website.
written by
ben pilkington is a freelance writer, editor, and proofreader with a master's degree in english literature from the university of oxford.
he is committed to clear and engaging written communication and enjoys telling complex, technical stories in a relevant and understandable way.
learn about the internal components of a quantum processor.
introduction
first, let's start by analyzing the concept and components of classical computing.
classic computers will obey the principles of classical physics.
a classical computer will perform operations using information stored in the form of bits; whose value is either zero (0) or one (1).
now, when we program a classical computer; we will have a cpu which has an input, an output, and software which regulates the cpu.
this is called a turing machine, which also happens to be the substructure of your cell phones and laptops computing power.
in spite of the relative simplicity, a turing machine may be constituted to simulate any given computer algorithm's logic.
unfortunately, even as classical computers have become faster and more concise; they are unable to solve arithmetic like factoring massive integers effectively.
in quantum computing, instead of having information stored in the form of bits, we have a new unit called a qubit or quantum bit, which carries quantum information.
in a classical system a bit can only be in two positions; either up or down (commonly represented as a zero or one).
in quantum computing, the qubit can be in any superposition of both at the same time.
qubits can be in the in the given states |0} and |1} (note: 0 and 1 are not always the given values for a qubit, various others may be used but with the same result) as well as any addition of the two; which will yield another valid quantum state x|0} + y|1} where the two variables x and y represent complex numbers.
think of the qubit's state as being a vector, then the superposed state just becomes additive (see figure 2.)
each additional qubit will then yield two times as many values.
if we were to have 3 qubits we would obtain coefficients for all combinations of 1 and 0 in a 3 digit sequence.
(|111},|011}, |001} etc....)
with this basic knowledge, we can analyze the processor inside a quantum computer; specifically the d wave quantum computer.
the elementary units of quantum computing
in the introduction, we covered how we can represent qubits symbolically as a 0 or 1, as well as a superposition
of both of the states.
we will now cover how qubits are constructed as well as their appearance.
in conventional computing, we are using the cmos transistors to encode bits of information.
this is done by regulating the voltage to transistors that are fitted with a bus to determine whether the state is a 0 or 1.
quantum transistors are somewhat similar, yet vastly different than our current cmos transistors.
figure 3 shows a quantum transistor known as a superconducting quantum interference device or a squid.
interference refers to the actual electrons, and how they act as waves that create interference patterns to cause quantum effects to occur.
this is the basis of quantum computing (basically a quantum transistor.)
the electron behaves as a qubit due to the nature of the material called niobium; which is what the gold loop in figure 3 is made of.
when the niobium is cooled to reach its critical temperature; it will manifest the qualities of quantum mechanics.
our classic transistors will encode in two states by regulating voltages.
the squid will encode the two states into magnetic fields which are designated down or up.
the two states are given as - 1, +1 in which the qubit can be in superposition of both.
this is done by combining the josephson effect (or the phenomenon of supercurrent) and the quantization of flux.
bcs pairs are tunneled through a weak link (which in this case would be a weak insulating barrier) between the niobium.
for each current below a given critical value, a supercurrent will be established between the two superconductors and will yield no voltage across the josephson junction.
any time a current is larger than the critical value, a voltage will be read across the junction.
figure three represents a superconducting qubit which each arrow representing a state of spin.
these states can be put into
superposition of - 1, +1.
the programming
the qubits need to be linked together in a fashion that is capable of relaying information.
the qubits are attached together by couplers which are also made from superconducting material.
when we combine the qubits and couplers together we are capable of creating a programmable structure of quantum mechanics.
(see figure 4.)
the superconducting qubit from figure3 is formed into rectangles; with each of the dots representing a coupler.
these couplers would in a sense couple the data or variables in an equation making it more efficient to solve.
processor operation
unfortunately, there are more components needed to create a functional quantum processor.
much of the structure and circuitry that outlines the qubits are composed of switches that function by the josephson effect.
this circuitry directs the information from the qubits into various memory components which store the data into a magnetized medium.
each of the qubits is equipped with read- out apparatuses.
the read- out will take the vector from the coherent superposition state and project it into a pure zero or one state while losing the phase information.
the probability of projection into zero or one state is taken by repeating the procedure many times and averaging the result.
these apparatuses will be inoperative while calculations are being made for the qubits to prevent the quantum behavior from being changed.
once calculations have been completed, and after each qubit has established itself into a classical state, the recorded data is converted into a chain of classical bits which can then be read by the user.
the structure of the processor is different from the typical silicon processor in that each qubit has individual memory devices instead of large cache areas.
quantum processing has been speculated to be able to utilize computing power massive orders of magnitude more than our conventional computers.
if we take a coherent state qubit system with x qubits then we can superpose 2x different sequences of bits (remember that each additional qubit will yield twice as many values, which is where the 2x comes from.)
now to equate that to conventional computers we take the difference in energy levels of the qubit, in this case, it happens to be in the gigahertz region; which gives us 2x gigahertz.
this means with 20 qubits a quantum processor could process approximately 2^20 operations per second.
based on these figures we can conclude that quantum processors have a substantially greater potential than that of conventional computers.
recently the dwave 2x system was manufactured and is considered to be the most powerful quantum computer to date.
it happens to operate at 0.015deg above absolute zero, and its processor generates no heat.
the system is comprised of over 1000 qubits that operate near absolute zero to generate a massive amount of quantum effects.
to put this into perspective, the system can search through 2^1000 solutions at once; which is more than every particle in the universe.
the dwave 2x has a rumored list price north of $15,000,000, and has been released for general availability.
further reading
toward a superconducting quantum computer
benchmarking a quantum annealing processor with the time- to- target metric (pdf)
a university of sydney team has solved a common problem in quantum sensing devices, which are used in biomedical imaging and have defence applications.
industrial sensors are everywhere in our technology and in order to function successfully they must be able to identify tiny signals from a cluttered background.
for most humans this is simple.
walk into a crowded room and you can pick out a single voice while ignoring everyone else.
that trick isn't so easy for industrial sensors - and the challenge gets even harder for super- sensitive quantum devices.
now, a team led by professor michael j. biercuk from the university of sydney, in collaboration with dartmouth college and johns hopkins applied physics laboratory in the us, has developed quantum control techniques enabling a new generation of ultra- sensitive sensors that can identify tiny signals while rejecting background noise down to theoretical limits.
"by applying the right quantum controls to a qubit- based sensor, we can adjust its response in a way that guarantees the best possible exclusion of the background clutter - that is, the other voices in the room," said professor biercuk, a chief investigator at the arc centre of excellence for engineered quantum systems.
while devices themselves have improved, the measurement protocols used to capture and interpret the signals have lagged behind.
quantum sensors therefore often return fuzzy results, which complicates interpretation of the data through a phenomenon known as "spectral leakage" - a bit like being distracted by the wrong voices in the room.
the university of sydney research, published on tuesday in nature communications, demonstrates control protocols that will help take advantage of improved sensor hardware.
the experiments, using trapped atomic ions, have reduced spectral leakage by many orders of magnitude over conventional methods.
professor biercuk said in certain circumstances, the methods they have developed are up to 100 million times better at excluding this background.
quantum sensors take advantage of the very thing that makes building quantum computers so difficult.
quantum bits, or qubits, are the building blocks of quantum computers but they are highly prone to losing their quantum properties due to interference from the environment.
this challenge can be turned on its head and used to build sensors that are much more responsive to the environment than classical technologies.
professor biercuk said the new protocols could have applications in medicine, such as imaging inside living cells using nanodiamonds.
they could also be used in defence and security systems that use quantum- enhanced magnetometers, devices that measure changes in magnetic fields for target identification and tracking.
he said: "our approach is relevant to nearly any quantum sensing application and can also be applied to quantum computing as it provides a way help identify sources of hardware error.
this is a major advance in how we operate quantum sensors."
professor biercuk has recently launched a venture- ca ... ital- backed spin- off from the work being done at the university of sydney.
q- ctrl aims to be the trusted provider of quantum control solutions for all new quantum technologies.
in 2017, ibm introduced "quantum volume" as a simple benchmark for quantum computers.
some skeptics argued that those engaged in the quest to build the world's first quantum computer shouldn't set the benchmarks.
but the quantum volume benchmark from ibm seems to capture multi- dimensional performance characteristics of a quantum processor in a way that is valid and universal.
typically, researchers use the speed by which a quantum gate performs an operation (gate speed), or gate fidelity, which measures the reliability of a gate operation.
when we benchmark a cpu or gpu for reviews at engineering.com, we don't measure just one characteristic of the hardware, like cache size or clock speed.
many different individual features have to be taken into consideration and weighed against each other in a comprehensive comparison.
quantum volume is a fundamental performance metric designed by ibm to determine the overall power of a quantum computer.
it accounts for circuit compiler efficiency, device connectivity, measurement and gate errors and device cross talk.
why does quantum volume count?
in theory, the larger the quantum volume of a quantum computer, the closer it approaches something called quantum advantage.
this term indicates the point at which the quantum volume is high enough to give a quantum computer an advantage in running quantum applications that perform better than anything classical computers can do.
at the american physical society march meeting today, ibm announced that it had achieved a record quantum volume from its q systems one quantum computer, which was unveiled at ces 2019 in january.
quantum computing represents a frontier where extremely complicated, real- world problems could be dissected and solved.
quantum applications could theoretically include anything from simulating chemistry and space to optimizing massive supply chains and modeling financial risk.
for those who have grown impatient and more skeptical of quantum computing, ibm now believes quantum advantage is achievable in the next decade, if quantum volume could predictably double every year in a manner similar to moore's law.
is this possible?
well, ibm has been doubling the quantum volume every year since 2017.
not many years under its belt, but we'll see what happens every year from now.
classical computing meets quantum computing at the technological limits of moore's law
moore's law dictates that roughly double the amount of transistors can be added to computer chips every 18 months.
the entire personal computing industry was built from this notion.
as transistors shrink in size to the atomic level, methods of manufacturing based in classical physics cease to be effective.
by next year, moore's law shows that transistors need to be the size of an atom.
a classical computer's transistor represents data as bits.
each bit has two values, a zero or a one.
quantum computers represent data in the form of transistors as either a zero, a one, or both zero and one.
when qubits (quantum bits) are simultaneously a zero and a one, this is called a state of superposition.
this state of superposition is what makes quantum computers to achieve speeds that are millions of times faster than classical computers.
(image courtesy of autodesk.)
a classical computer's transistor represents data as bits.
each bit has two values, a zero or a one.
quantum computers represent data in the form of transistors as either a zero, a one, or both zero and one.
when qubits (quantum bits) are simultaneously a zero and a one, this is called a state of superposition.
this state of superposition is what makes quantum computers to achieve speeds that are millions of times faster than classical computers.
since moore's law shows that transistors have to shrink to the molecular level by next year to continue the scale of growth the computing industry has grown accustomed to, the laws that govern quantum mechanics have to take the reign from those that govern classical physics.
when the first computers like the the harwell computer (pictured above) were being built, electrical engineers designed and built circuits to transmit flowing electrons (electrical current), and then developed transistors that adjusted resistance the level of non- conductivity) on command.
these resistors allowed engineers to create any state for specific voltage levels, but only two states were needed for electro- mechanical classical computers at the time.
as large electro- mechanical computers gave way to electrical computers (which could do everything mechanical but much faster with electricity).
electronic classical computers didn't take into account the huge amount of positions electrons can inhabit at a static momentum, where a mechanical device could only have one state at that same static momentum.
electronic classical computers were designed to boil down the number of states to two, so that true or false values (via boolean logic) could be ascribed for a sequential classical computation.
(image courtesy of public domain.)
a quantum computation uses quantum- mechanical properties like superposition, entanglement and interference to crunch data.
by storing and altering values of subatomic properties of atoms.
as moore's law shrinks the transistor to the atomic level, quantum mechanics as applied to computing is basically the only way forward, and ibm is looking to make that first quantum leap over classical computers.
how would a quantum computer leave a classical computer in the dust?
say you have a database filled with different supply chain data about thousands of different products, each with a different sku number, unit cost, price and so on.
if you wanted to search for specific information, say the name of a product or it's sku number, a classical computer would work sequentially through each row and column of the database until it found the information you requested it and displayed it in the search bar.
a quantum computer would search the whole database instantly.
qubits give a quantum computer the ability to read every line of every row and column at the same time.
any system of information that involved optimizing logistically complex tasks, such as those found in supply chain optimization, would be performed thousands of times faster than a classical computer.
bottom line
it's important to understand how nascent quantum computing is compared to classical computing.
ibm first rolled out public access to quantum computers in may 2016 through its ibm q experience quantum cloud service.
ibm developed quantum volume as a metric to better understand when quantum advantage is achieved.
the ibm q system one quantum computer, with its fourth- generation 20- qubit processor has a quantum volume of 16, doubled from last year, and the year before that.
building a large- scale universal and fault- tolerant computer requires doubling quantum volume every year into the next decade while keeping coherence times long and error rates low.
the potential use cases offer only the possibility of technological breakthroughs in altruistic endeavors like simulating battery- cell chemistry for electric vehicles to make them an order of magnitude more efficient.
of course, this technology will also be used for weapons engineering.
right now, quantum computing has had basically zero practical impact on global industry, science or engineering, and it certainly has its detractors.
quantum computing has to advance in a very significant manner in order to compete with classical computers, which are still raising the bar to higher and higher levels.
ibm continues to make progress, and the notion that quantum volume has to double every year for at least ten years in order to achieve quantum advantage over classical computers is compelling food for thought.
two dartmouth researchers have found a way to develop more robust "quantum gates," which are the elementary building blocks of quantum circuits.
quantum circuits, someday, will be used to operate quantum computers, super powerful computers that have the potential to perform extremely complex algorithms quickly and efficiently.
associate professor of physics and astronomy lorenza viola and post- doctoral fellow kaveh khodjasteh report their findings in the feb 27, 2009 issue of physical review letters.
their study is titled "dynamically error- corrected gates for universal quantum computing."
the futuristic realm of quantum computing considers units of information called quantum bits, or qubits, which can be carried by quantum- mechanical objects such as electrons or atoms.
unlike today's computers, which use binary strings of 0s and 1s, a quantum computer uses qubits that can each be in a superposition of 0 and 1.
as a result, quantum computers could efficiently solve computational problems beyond the reach of today's computers.
"an outstanding challenge stems from the fact that quantum bits are incredibly more prone to errors than their traditional- sized counterparts," says viola, who is the director of dartmouth's quantum information science initiative.
"all quantum gates, the building blocks for implementing complex quantum- mechanical circuits, are plagued by errors originating from both the interaction with the surrounding quantum environment or operational imperfections."
viola's and khodjasteh's study showed how to construct new quantum gates that can be "dynamically corrected" out of sequences from the available faulty gates.
in this manner, the researchers say, the net total error is approximately canceled.
"the key idea is to carefully exploit known relationships between unknown errors," says viola.
"dynamically corrected gates allow for substantially higher fidelity to be reaching quantum circuits, and can thus bring the implementation of reliable quantum- computing devices closer to reality."
oxford scientists have come a step closer to quantum 'supercomputers' by creating a new technique called 'bang- bang' to hold quantum information.
the method, which the researchers report today in nature physics, costs ps7 million pounds per gram, but fortunately the nanoscale of the information- holding molecules they have created - just ten atoms across - keeps the cost down.
the idea behind quantum computing is based on quantum mechanics, which allow an entity, such as an atom, to exist in multiple states simultaneously.
quantum computing is seen as the holy grail of computing because each individual piece of information, or 'bit', would exist in more than one state at once, making processing billions of times faster and thus dramatically widening the scope of what computers can do.
there's just one problem: no- one knows how to build a quantum computer yet.
the biggest hurdle is that the quantum state is only maintained as long as the quantum entity does not interact with anything.
once it is detected, or interacts in any way with the environment around it, the quantum bit (qubit) collapses into one state or another and loses the vital quality of existing in more than one state at once.
the challenge is how to isolate quantum information from its surroundings.
the team, from the materials science department, had a plan to 'cage' the qubit in a buckyball (a buckminster fullerene particle), a molecule which has a cage structure reminiscent of a football.
this isolates the qubit to some extent, but not quite enough.
the next step the researchers took was to apply the so- called 'bang- bang' method: the qubit is repeatedly hit with a strong pulse of microwaves which reverses the way in which it interacts with the environment.
dr john morton, one of the authors on the paper, said: 'the loss of information is like a child at a party game running with a blindfold on.
we keep regularly turning the child around.
if we do this quickly enough, the information remains intact (i.e.
the child never gets very far).'
dr simon benjamin, another of the authors, said: 'the experiment was a complete success.
we were able to show a very high level of decoupling of the nuclear spin from its environment, freezing the information exactly as planned.
it's likely that strategies like this will form a quintessential element in any future quantum computer.'
in 2019, the majority of practical problems conventional computers can easily solve would leave quantum computers sputtering in the dust of their own decoherence.
quantum computers are far from useful when compared to classical computers.
it may be important to ground oneself in this reality before attempting to verify the validity of a startling recent claim by tech juggernaut google: that they've achieved quantum supremacy.
in a 63- page paper entitled "quantum supremacy using a programmable superconducting processor," the mountain view- based corporation performed a complicated physics experiment using a 54 (though technically 53 because one qubit isn't working) qubit quantum computer code- named "sycamore."
in the paper, google claims that its sycamore quantum system correctly verified results generated by the system's output distribution generated from a random quantum circuit of specific form.
the test is known as "random quantum circuit sampling."
google claims that sycamore performed this task in 200 seconds, while asserting that today's fastest supercomputer, ibm's summit, would take 10,000 years to perform the same task.
ibm was quick to refute google's claims of quantum supremacy with a paper published on monday called "leveraging secondary storage to simulate deep 54- qubit sycamore circuits."
ibm's main refutation is that google did not correctly factor in that summit could be configured in a way that allows it to keep up with google's sycamore.
in their paper, ibm wrote that summit could simulate the quantum system and perform the random quantum circuit sampling calculation in 2.5 days if correctly configured, which is a long way from google's 10,000- year estimation.
a pioneering experiment on the entanglement frontier
quantum information science is largely funded by the unproven assertion that classical computer systems lack the capability to emulate highly entangled quantum systems.
google has achieved something remarkable, but whether it's actually quantum supremacy depends on how you define it.
there are two terms to consider: quantum advantage and quantum supremacy.
quantum advantage means that a quantum computer system can perform computations many orders of magnitude faster than a classical computer.
but quantum supremacy is accomplished only when quantum systems have computing power so great that they can perform calculations that conventional supercomputers could not.
google's sycamore quantum computer solved a problem with quantum algorithms that is essentially a super- polynomial speedup relative to the same operation performed on conventional computers.
in their paper, google laid claim to the achievement of quantum supremacy by indicating that ibm's summit supercomputer would take 10,000 years to finish the problem their sycamore accomplished in 200 seconds.
given ibm's refutation that it would only take 2.5 days for their summit computer to simulate the quantum computing system designed by google, it seems more likely that google has achieved quantum advantage in this particular niche computing case, rather than quantum supremacy.
quantum advantage is still a remarkable achievement given the unstable nature and ultra- high degree of complex engineering needed to make quantum computers perform at all.
if ibm's assertions are correct that google did not account for an optimized configuration of ibm's summit supercomputer, then quantum supremacy was not achieved.
ibm research has made its quantum computing power available to members of the public to run experiments.
the system is currently an approach towards building a universal quantum computer.
the cloud- enabled quantum computing platform, called ibm quantum experience, will allow users to run algorithms and experiments on ibm's quantum processor, work with the individual quantum bits (qubits), and explore tutorials and simulations around what might be possible with quantum computing.
"quantum computers are very different from today's computers, not only in what they look like and are made of, but more importantly in what they can do.
quantum computing is becoming a reality and it will extend computation far beyond what is imaginable with today's computers," said arvind krishna, senior vice president and director, ibm research.
"this moment represents the birth of quantum cloud computing.
by giving hands- on access to ibm's experimental quantum systems, the ibm quantum experience will make it easier for researchers and the scientific community to accelerate innovations in the quantum field, and help discover new applications for this technology."
quantum materials has entered into a joint venture with guanghui technology group (gtg) whereas gtg will invest $20 million us for building out quantum materials qdx quantum dot production facilities and quantum dots application development in china.
embedding quantum dot production regionally allows quantum materials to work closely with clients to customize quantum dot characteristics to optimize manufacturing efficiencies as well as supply chain logistics.
the joint venture will be registered in hong kong and operated as quantum materials asia co., ltd. gtg's investment and quantum materials' patented mass- production quantum dot manufacturing technology will enable quantum materials asia to start supplying quantum dots to clients in the display, lighting and solar energy industries by the third quarter of 2016.
"we have already held serious discussions with a number of potential customers in our region seeking quantum dots for diversified applications, including display, lighting and solar.
we have established our reputation as a key financial partner with the governments and industries by raising multiple billions of dollars in funding for projects driven by our customers over past decade," said liu xiao, ceo of guanghui technology group.
"this joint venture represents a new role for gtg in that we are not only supporting this project financially to facilitate major market expansion for quantum materials, but we will also be deeply involved in driving market penetration among our industry clients seeking to bring their technology and products to the next level."
"this agreement represents our commitment to provide customers with best- in- class quantum dot materials, including cadmium- free quantum dots.
our patented mass production process enables us to rapidly establish locally based manufacturing to meet their volume production demands," added stephen b. squires, founder and ceo of quantum materials corp. "combining our resources and expertise in advanced quantum dot technologies with gtg's strong financial support and business relationships with top- tier manufacturers of displays, lighting products and solar panels will facilitate generating tremendous business opportunities in both the government and private sectors."
from laptops to cellphones, today's technology advances through the ever- increasing speed at which electric charges are directed through circuits.
similarly, speeding up control over quantum states in atomic and nanoscale systems could lead to leaps for the emerging field of quantum technology.
an international collaboration between physicists at the university of chicago, argonne national laboratory, mcgill university, and the university of konstanz recently demonstrated a new framework for faster control of a quantum bit.
first published online nov. 28, 2016, in nature physics, their experiments on a single electron in a diamond chip could create quantum devices that are less to prone to errors when operated at high speeds.
accelerating quantum dynamics
to understand their experiment, one can look to the ultimate setting for speed in classical dynamics: the oval racetracks at the indianapolis or daytona 500.
to enable the racecars to navigate the turns at awesome speeds, the racetrack's pavement is "banked" by up to 30 degrees.
a student in newtonian mechanics could explain that this inward slope of the pavement allows the normal force provided by the road to help cancel the car's centrifugal acceleration, or its tendency to slide outward from the turn.
the greater the speed, the greater the bank angle that is required.
"the dynamics of quantum particles behave analogously," said aashish clerk, professor of theoretical physics at mcgill university.
"although the equations of motion are different, to accurately change the state of a quantum particle at high speeds, you need to design the right track to impart the right forces."
clerk, together with mcgill postdoctoral fellows alexandre baksic and hugo ribeiro, formulated a new technique to enable faster quantum dynamics by deftly absorbing detrimental accelerations felt by the quantum particle.
these accelerations, unless compensated, would divert the particle from its intended trajectory in the space of quantum states, similar to how the centrifugal acceleration deflects the racecar from its intended racing line on the track.
through conversations with members of his own group and the clerk group, david awschalom, professor in spintronics and quantum information at the institute for molecular engineering in the university of chicago, realized that the new theory could be used to speed up the diamond- based quantum devices in his labs.
however, just as constructing the banked speedways presented challenges in civil engineering, experimentally executing the control sequences envisioned by clerk and co- workers presented ones in quantum engineering.
building the quantum fast track required shining intricately- shaped, synchronized laser pulses on single electrons trapped at defects inside their diamond chips.
this experimental feat was achieved by lead author brian zhou, working with christopher yale, f. joseph heremans, and paul jerger.
"we demonstrated that these new protocols could flip the state of a quantum bit, from 'off' to 'on,' 300% faster than conventional methods," said awschalom, also a senior scientist at argonne national laboratory.
"shaving every nanosecond from the operation time is essential to reduce the impact of quantum decoherence," he explained, referring to the process by which quantum information is lost to the environment
professor guido burkard and adrian auer from the university of konstanz joined the awschalom and clerk groups to examine the data from the experiments.
a leading expert in diamond- based quantum systems, burkard remarked, "what is promising for translating these techniques beyond the laboratory is that they are effective even when the system is not perfectly isolated."
the researchers anticipate that their methods can be further applied for fast and accurate control over the physical motion of atoms or the transfer of quantum states between different systems, and convey benefits to quantum applications, such as secure communications and simulation of complex systems.
quantum computing has been brought a step closer to mass production by a research team led by scientists from the university of bristol that has made a transition from using glass to silicon.
the bristol team has been demonstrating quantum photonic effects in glass waveguides for a number of years but the use of a silicon chip to demonstrate photonic quantum mechanical effects such as superposition and entanglement, has the advantage of being a match to contemporary high volume manufacturing methods, the team claimed.
this could allow the creation of hybrid circuits that mix conventional electronic and photonic circuitry with a quantum circuit for applications such as secure communications.
one result could be that quantum mechanical computing could be deployed much sooner that had previously predicted.
it has been estimated that it could take a decade or more to develop and deploy all the circuitry for an end- to- end quantum computer although such a machine would theoretically have much greater performance than current electronic computers for solving very complex problems.
the bristol researchers' latest work, carried out with collaborators from toshiba central r&d in japan, heriot- watt university in scotland and delft university in the netherlands, created a silicon- on- insulator chip with circuits to demonstrate two- photon quantum interference and photon entanglement in a mach- zehnder interferometer.
these new circuits are compatible with existing optical fiber infrastructure and are ready to be deployed directly.
the silicon waveguides have widths of 450- nm and depths of 220- nm.
"using silicon to manipulate light, we have made circuits over 1000 times smaller than current glass- based technologies.
it will be possible to mass- produce this kind of chip using standard microelectronic techniques, and the much smaller size means it can be incorporated in to technology and devices that would not previously have been compatible with glass chips," said mark thompson, deputy director of the centre for quantum photonics in bristol university's school of physics, in a statement.
thompson told ee times that quantum secure communications could deployed commercially within five years.
over the same sort of period the bristol research team expects to demonstrate quantum computing chips developed to solve specific problems.
many of these may be particular to molecular, atomic and even quantum simulation or to some fundamentally hard- to- solve problems in mathematics, thompson said.
the bristol research team is in discussions with nokia about the use of quantum computing for use within secure mobile communications.
news articles:
ibm reports progress in quantum computing
with a quantum coprocessor in the cloud, physicists from innsbruck, austria, open the door to the simulation of previously unsolvable problems in chemistry, materials research or high- energy physics.
the research groups led by rainer blatt and peter zoller report in the journal nature how they simulated particle physics phenomena on 20 quantum bits and how the quantum simulator self- verified the result for the first time.
many scientists are currently working on investigating how quantum advantage can be exploited on hardware already available today.
three years ago, physicists first simulated the spontaneous formation of a pair of elementary particles with a digital quantum computer at the university of innsbruck.
due to the error rate, however, more complex simulations would require a large number of quantum bits that are not yet available in today's quantum computers.
the analog simulation of quantum systems in a quantum computer also has narrow limits.
using a new method, researchers around christian kokail, christine maier und rick van bijnen at the institute of quantum optics and quantum information (iqoqi) of the austrian academy of sciences have now surpassed these limits.
they use a programmable ion trap quantum computer with 20 quantum bits as a quantum coprocessor, in which quantum mechanical calculations that reach the limits of classical computers are outsourced.
"we use the best features of both technologies," explains experimental physicist christine maier.
"the quantum simulator takes over the computationally complex quantum problems and the classical computer solves the remaining tasks."
toolbox for quantum modelers
the scientists use the variational method known from theoretical physics, but apply it on their quantum experiment.
"the advantage of this method lies in the fact that we can use the quantum simulator as a quantum resource that is independent of the problem under investigation," explains rick van bijnen.
"in this way we can simulate much more complex problems."
a simple comparison shows the difference: an analog quantum simulator is like a doll's house, it represents reality.
the programmable variational quantum simulator, on the other hand, offers individual building blocks with which many different houses can be built.
in quantum simulators, these building blocks are entanglement gates and single spin rotations.
with a classical computer, this set of knobs is tuned until the intended quantum state is reached.
for this the physicists have developed a sophisticated optimization algorithm that in about 100,000 requests of the quantum coprocessor by the classical computer leads to the result.
coupled with extremely fast measurement cycles of the quantum experiment, the simulator at iqoqi innsbruck becomes enormously powerful.
for the first time, the physicists have simulated the spontaneous creation and destruction of pairs of elementary particles in a vacuum on 20 quantum bits.
since the new method is very efficient, it can also be used on even larger quantum simulators.
the innsbruck researchers plan to build a quantum simulator with up to 50 ions in the near future.
this opens up interesting perspectives for further investigations of solid- state models and high- energy physics problems.
built- in self- check
a previously unsolved problem in complex quantum simulations is the verification of the simulation results.
"such calculations can hardly or not at all be checked using classical computers.
so how do we check whether the quantum system delivers the right result," asks the theoretical physicist christian kokail.
"we have solved this question for the first time by making additional measurements in the quantum system.
based on the results, the quantum machine assesses the quality of the simulation," explains kokail.
such a verification mechanism is the prerequisite for even more complex quantum simulations, because the necessary number of quantum bits increases sharply.
"we can still test the simulation on 20 quantum bits on a classical computer, but with more complex simulations this is simply no longer possible," says rick van bijnen.
"in our study, the quantum experiment was even faster than the control simulation on the pc.
in the end, we had to take it out of the race in order not to slow down the experiment," says the physicist.
innsbruck quantum cloud
this research achievement is based on the unique collaboration between experiment and theory at the innsbruck quantum research center.
the expertise from years of experimental quantum research meets innovative theoretical ideas in tyrol, austria.
together, this leads to results that are recognized worldwide and establishes an internationally leading position of innsbruck's quantum research.
"15 years of very hard work have gone into this experiment," emphasizes experimental physicist rainer blatt.
"it is very nice to see that this is now bearing such beautiful fruit."
the theoretical physicist peter zoller adds: "we in innsbruck are not only leaders in the number of available quantum bits, but have now also advanced into the field of programmable quantum simulation and were able to demonstrate for the first time the self- verification of a quantum processor.
with this new approach, we are bringing the simulation of everyday quantum problems within reach."
the work now published in nature was financially supported by the austrian science fund fwf and the european union, among others.
story source:
materials provided by university of innsbruck.
note: content may be edited for style and length.
as multiple research groups around the world race to build a scalable quantum computer, questions remain about how the achievement of quantum supremacy will be verified.
quantum supremacy is the term that describes a quantum computer's ability to solve a computational task that would be prohibitively difficult for any classical algorithm.
it is considered a critical milestone in quantum computing, but because the very nature of quantum activity defies traditional corroboration, there have been parallel efforts to find a way to prove that quantum supremacy has been achieved.
researchers at the university of california, berkeley, have just weighed in by giving a leading practical proposal known as random circuit sampling (rcs) a qualified seal of approval with the weight of complexity theoretic evidence behind it.
random circuit sampling is the technique google has put forward to prove whether or not it has achieved quantum supremacy with a 72- qubit computer chip called bristlecone, unveiled earlier this year.
the uc berkeley computer theorists published their proof of rcs as a verification method in a paper published monday, oct. 29, in the journal nature physics.
"the need for strong evidence for quantum supremacy is underappreciated, but it's important to pin this down," said study principal investigator umesh vazirani, roger a. strauch professor of electrical engineering and computer science at uc berkeley.
"besides being a milestone on the way to useful quantum computers, quantum supremacy is a new kind of physics experiment to test quantum mechanics in a new regime.
the basic question that must be answered for any such experiment is how confident can we be that the observed behavior is truly quantum and could not have been replicated by classical means.
that is what our results address."
the other investigators on this paper are adam bouland and bill fefferman, both postdoctoral research fellows, and chinmay nirkhe, a ph.d. student, all in vazirani's theoretical computing research group.
investment in quantum is heating up
the paper comes amid accelerated activity in government, academia and industry in quantum informational science.
congress is considering the national quantum initiative act, and last month, the u.s. department of energy and the national science foundation announced nearly $250 million in grants to support research in quantum science and technologies.
at the same time, the lawrence berkeley national laboratory and uc berkeley announced the formation of berkeley quantum, a partnership designed to accelerate and expand innovation in quantum information science.
the stakes are high as international competition in quantum research heats up and the need for increasingly complex computations grows.
with true quantum computing, problems that are impractical for even the fastest supercomputers to date could be relatively efficient to solve.
it would be a game- changer in cryptography, simulations of molecular and chemical interactions and machine learning.
quantum computers are not confined by the traditional 0s and 1s of a traditional computer's bits.
instead, quantum bits, or qubits, can encode 0s, 1s and any quantum superposition of the two to create multiple states simultaneously.
when google unveiled bristlecone, it said the empirical proof of its quantum supremacy would come through random circuit sampling, a technique in which the device would use random settings to behave like a random quantum circuit.
to be convincing, there would also need to be strong evidence that there is no classical algorithm running on a classical computer that could simulate a random quantum circuit, at least in a reasonable amount of time.
detecting quantum accents
vazirani's team referred to an analogy between the output of the random quantum circuit and a string of random syllables in english: even if the syllables don't form coherent sentences or words, they will still possess an english "accent" and will be recognizably different from greek or sanskrit.
they showed that producing a random output with a "quantum accent" is indeed hard for a classical computer through a technical complexity theoretic construct called "worst- to- average- case reduction."
the next step was to verify that a quantum device was actually speaking with a quantum accent.
this relies on the goldilocks principle -  a 50- qubit machine is large enough to be powerful, but small enough to be simulated by a classical supercomputer.
if it's possible to verify that a 50- qubit machine speaks with a quantum accent, then that would provide strong evidence that a 100- qubit machine, which would be prohibitively hard to simulate classically, would do so, as well.
but even if a classical supercomputer were programmed to speak with a quantum accent, would it be able to recognize a native speaker?
the only way to verify the output of the speaker is by a statistical test, said the berkeley researchers.
google researchers are proposing to measure the degree of matching by a metric called "cross- entropy difference."
a cross- entropy score of 1 would be an ideal match.
the alleged quantum device may be regarded as behaving like an ideal quantum circuit with random noise added.
fefferman and bouland say the cross- entropy score will certify the authenticity of the quantum accent provided the noise always adds entropy to the output.
this is not always the case -  for example if the noise process preferentially erases 0s over 1s, it can actually reduce the entropy.
"if google's random circuits are generated by a process that allows such erasures, then the cross- entropy would not be a valid measure of quantum supremacy," said bouland.
"that's partly why it will be very important for google to pin down how its device deviates from a real random quantum circuit."
these results are an echo of work that vazirani did in 1993 with his student ethan bernstein when they presented the first formal evidence that quantum computers would exponentially speed up certain computations.
this opened the door to quantum algorithms by showing that quantum computers violate the extended church- turing thesis, a foundational principle in computer science.
peter shor of bell labs took their work one step further by showing that a very important practical problem, integer factorization, could be exponentially sped up by a quantum computer.
"this sequence provides a template for the race to build working quantum computers," said vazirani.
"quantum supremacy is an experimental violation of the extended church- turing thesis.
once that is achieved, the next challenge will be to design quantum computers that can solve practically useful problems."
the army research office, national science foundation, vannevar bush faculty fellowship and the national institute of standards and technology helped support this research.
rice university physicists have created a tiny "electron superhighway" that could one day be useful for building a quantum computer - a type of computer that uses quantum particles in place of the digital transistors found in today's microchips.
quantum computers may offer the ability to complete intense computing tasks like code- breaking, climate modeling, and biomedical simulation thousands of times faster.
today's computers use binary bits of data that are either ones or zeros.
quantum computers would use quantum bits, or "qubits," which can be both ones and zeros at the same time, thanks to the quirks of quantum mechanics.
this quirk gives quantum computers a huge edge in performing particular types of calculations.
researchers are taking a number of approaches to creating qubits.
rice university physicists are following the approach of "topological quantum computing."
topological designs are expected to be more fault- tolerant than other types of quantum computers because each qubit in a topological quantum computer will be made from a pair of quantum particles that have a virtually immutable shared identity.
the catch is that physicists have yet to create or observe one of these stable pairs of particles, which are called "majorana fermions."
the race to create them in a chip has just begun.
in particular, physicists believe the particles can be made by marrying a two- dimensional topological insulator - like the one created at rice university - to a superconductor.
also: nasa's jet propulsion laboratory generated a scheme for entering binary data into a quantum computer.
physicists have shown that a quantum battery- basically, a quantum system such as a qubit that stores energy in its quantum states- can theoretically be charged at a faster rate than conventional batteries.
this "quantum speedup" arises from quantum entanglement among multiple qubits, which essentially provides a shortcut between the qubits' uncharged and charged states, allowing for faster charging.
the physicists, felix c. binder, et al., have published a paper on the quantum battery, which they call "quantacell," in a recent issue of the new journal of physics.
"there has been much interest in the question if quantum physics can provide any advantage in thermodynamic processes (thermodynamics being the study of work and heat and their interconversion)," binder, a physicist at the university of oxford, told phys.org.
"our paper demonstrates with an example that a significant advantage can indeed be achieved when a short process duration is desired- quantum correlations ('quantum entanglement') can lead to a significant speedup."
the scientists investigated a quantum battery made of qubits, which can take a variety of physical forms, such as ions, neutral atoms, photons, etc.
qubits can exist in either one of two states or a superposition of both states at once.
in a quantum battery, the two states represent different energy levels.
charging a quantum battery means changing a qubit's state from a lower energy level to a higher energy level, while using (discharging) the battery does the reverse.
the scientists call these particular qubits "work qubits" (or simply "wits") because they can store energy that can later be used to do work.
so far, this description is similar to how a conventional battery works.
however, the important difference is that, as quantum systems, qubits may be entangled, meaning the qubits are so strongly correlated that an entire qubit array can be described by the same quantum state.
here, the researchers have shown that one consequence of entangling the qubits during the charging process is that it means a shorter distance has to be traveled through state space- that is, between the low and high energy states- than would be required without entanglement.
the scientists showed that, the more qubits, and hence the more entanglement, the faster the charging process.
in the example protocol, the charging time is inversely proportional to the number of qubits.
so hypothetically, if one work qubit took an hour to charge, six work qubits could be charged in 10 minutes.
in reality, however, typical quantum systems cannot remain quantum for anywhere near this length of time due to decoherence- interactions with the surrounding environment that destroy the quantum effects.
besides decoherence, another roadblock to using quantum batteries for real- life applications is that the amount of energy that they can store is tiny compared to the energy needs of, for example, mobile phones or electric vehicles.
"the energies of quantum systems tend to be many orders of magnitude smaller than even the smallest energies used by day- to- day appliances," explained coauthor john goold, a physicist at the abdus salam international centre for theoretical physics in trieste, italy.
"'size' is here a question of energy scales.
our study is a theoretical proof- of- principle that quantum physics can provide a speedup in depositing energy into a system.
these speedup effects would be relevant in two cases: 1) mechanical devices become small enough that energy scales are comparable to current implementations of quantum systems.
2) quantum systems are scaled up and robustly controllable at energy scales that are of practical importance."
to better understand the potential applications of quantum batteries, the researchers plan to further investigate the quantum effects of thermodynamic processes.
the big question is, can a quantum battery produce work (directed energy) rather than simply heat (disordered energy)?
"what we call 'quantum batteries' in the paper exists in labs: any controllable quantum system with stable energy eigenstates can be understood as a battery," binder said.
"the question is rather if the thermodynamic viewpoint will be helpful for quantum experiments with systems like ions, cold neutral atoms (for instance, on an optical lattice), condensed matter systems (for example, superconducting qubits and circuit quantum electrodynamics), or light."
hurricanes, traffic jams, demographic development - to predict the effect of such events, computer simulations are required.
many processes in nature, however, are so complicated that conventional computers fail.
quantum simulators may solve this problem.
one of the basic phenomena in nature is the interaction between light and matter in photosynthesis.
physicists of karlsruhe institute of technology (kit) have now made a big step towards quantum mechanics understanding of plant metabolism.
this is reported in the nature communications journal.
"a quantum simulator is the preliminary stage of a quantum computer.
contrary to a quantum computer, however, it is not able to make any calculations, but is designed for the solution of a certain problem," says jochen braumuller of kit's physikalisches institut (institute of physics).
as the high efficiency of photosynthesis cannot be understood completely with classical physical theories, researchers like braumuller use a quantum model.
together with scientists of the institut fur theoretische festkorperphysik (tfp, institute for theoretical solid- state physics), he demonstrated for the first time in an experiment that quantum simulations of the interaction between light and matter work in principle.
the interaction between light and matter in photosynthesis can be described as an interaction of photons of light with the atoms of matter on the microscopic level.
the high efficiency of this mechanism of nearly 100 percent suggests that it is subject to rules of quantum physics, which is difficult to simulate with classical computers and simple bits.
in standard computing, information is represented by a switch that can store information as zero or one.
quantum bits, by contrast, are able to assume the states of zero and one at the same time according to quantum physics rules.
hence, quantum computers or the simpler quantum simulators can solve the problem more quickly and efficiently.
braumuller and his co- authors have now developed one of the first functioning components for a quantum simulator of light- matter interaction: superconducting circuits as quantum bits represent the atoms, while electromagnetic resonators represent the photons.
the physicists succeeded in producing an effect with the quantum bit and the resonator assuming two opposite states at the same time.
"qubit and resonator are coupled," says michael marthaler of kit's tfp.
"this is also the reason for the exponentially improved calculation capacity compared to classical computers."
fulfilling of this fundamental principle of quantum mechanics has demonstrated feasibility of analog quantum simulation with superconducting circuits, the researchers say.
as a next step, they plan to extend their system via many other building blocks.
"classical simulation of this extended system would take longer than the age of the universe," says martin weides, who has been heading a working group at kit's physikalisches institut since 2015.
if the planned quantum mechanics simulation is successful, this will be a "milestone on the way towards a universal quantum computer."
for the first time, scientists have successfully operated a quantum gate between two remote particles of matter, marking an important step toward the development of a quantum computer.
in previous experiments, researchers have used photons, which are difficult to store.
using matter qubits enables the researchers to store the obtained quantum information, opening up new possibilities for the generation of remote networks of entangled qubits.
physicists peter maunz and coauthors from the university of maryland department of physics and national institute of standards and technology in college park, maryland, and the university of michigan in ann arbor, michigan, published their study in a recent issue of physical review letters.
"our work demonstrates a probabilistic remote entangling quantum gate," maunz told physorg.com.
"remote entangling gates are an essential building block for quantum repeaters which facilitate quantum communication over long distances.
furthermore, the remote link established by the entangling gate could be used to interconnect remote quantum processors and thus could be an important additional possibility to scale a future quantum computer."
as the scientists explain, their quantum gate works by entangling two ytterbium ions, each confined in its own trap positioned one meter apart.
the scientists suspended the ions into either a one or a zero state using microwave radiation.
the use of ion traps prevents anything from interacting with the ytterbium.
this allows the ions to hold states of both zero and one simultaneously so that the ions function as qubits.
next, the scientists shined a laser on the ions, causing each ion to emit a single photon.
the scientists collected a small amount of the emitted light from each ion and coupled the light into fibers.
the outputs of the fibers were then sent to interfere at a beam splitter, after which single photons were detected.
a simultaneous detection of two photons indicated the successful operation of the gate.
finally, the atomic states were identified to verify the claim.
as the physicists explain, this quantum gate could lead to the development of a "one- way" quantum computer.
in this technique, a large collection of qubits is first prepared in a highly entangled state, and then single qubit rotations and measurements are performed on the qubits to realize quantum computation.
since the measurements destroy the state of the qubits, the technique is called "one- way."
because it has certain advantages, this technique may be easier to implement than the quantum circuit model, which is the conventional model of quantum computing.
"the quantum circuit model and the one- way quantum computer a different but equivalent approaches to use quantum interactions and quantum entanglement for computation," maunz said.
"the one- way quantum computer starts with a very sophisticated, highly entangled state of a collection of qubits.
the computation is realized by measurements on single quantum bits and a classical feedforward of the results on rotations and measurements on the remaining qubits."
in order to realize a useful one- way quantum computer, the physicists still face some challenges.
for instance, they need to generate large collections of entangled qubits, since the current study demonstrates the technique with just one pair of qubits.
the scientists also hope to improve the reliability and speed of their entanglement method.
"one focus of our research will be to increase the success probability of the remote gate," maunz said.
"this can be done by increasing the fraction of scattered light that is collected and used for the gate.
we hope to achieve an increase high enough to allow us to scale the entanglement to more than two ions.
the remote entangling gate can also be combined with local gates which have been realized with trapped ions before.
this should facilitate the development of deterministic remote quantum gates and the transmission of quantum information over larger distances."
more information: p. maunz, s. olmschenk, d. hayes, d.n.
matsukevich, l.- m. duan, and c. monroe.
"heralded quantum gate between remote quantum memories."
physical review letters 102, 250502 (2009).
scientists have taken the next major step toward quantum computing, which will use quantum mechanics to revolutionize the way information is processed.
quantum computers will capitalize on the mind- bending properties of quantum particles to perform complex calculations that are impossible for today's traditional computers.
using high magnetic fields, susumu takahashi, assistant professor in the usc dornsife college of letters, arts and sciences, and his colleagues managed to suppress decoherence, which is one of the key stumbling blocks in quantum computing.
"high magnetic fields reduce the level of the noises in the surroundings, so they can constrain the decoherence very efficiently," takahashi said.
decoherence has been described as a "quantum bug" that destroys fundamental properties that quantum computers would rely on.
this research will appear in the online version of nature magazine on june 20.
quantum computing uses quantum bits, or qubits, to encode information in the form of ones and zeros.
unlike a traditional computer that uses traditional bits, a quantum computer takes advantage of the fact seemingly impossible fact that qubits can exist in multiple states at the same time, which is called "superposition."
while can a bit can represent either a one or a zero, a qubit can represent a one and a zero at the same time due to superposition.
this allows for simultaneous processing of calculations in a truly parallel system, skyrocketing computing ability.
though the concepts underpinning quantum computing are not new, problems such as decoherence have hindered the construction of a fully functioning quantum computer.
think of decoherence as a form of noise or interference, knocking a quantum particle out of superposition - robbing it of that special property that makes it so useful.
if a quantum computer relies on a quantum particle's ability to be both here and there, then decoherence is the frustrating phenomenon that causes a quantum particle to be either here or there.
the researchers calculated all sources of decoherence in his experiment as a function of temperature, magnetic field, and by nuclear isotopic concentrations, and suggested the optimum condition to operate qubits, reducing decoherence by approximately 1,000 times.
qubits in his experiment lasted about 500 microseconds at the optimum condition - ages, relatively speaking.
decoherence in qubit systems falls into two general categories.
one is an intrinsic decoherence caused by constituents in the qubit system, and the other is an extrinsic decoherence caused by imperfections of the system, for example, impurities and defects.
in their study, takahashi and his colleagues investigated single crystals of molecular magnets.
because of their purity, molecular eliminate the extrinsic decoherence, allowing researchers to calculate intrinsic decoherence precisely.
"for the first time we've been able to predict and control all the environmental decoherence mechanisms in a very complex system - in this case a large magnetic molecule," said phil stamp, ubc professor of physics and astronomy and director of the pacific institute of theoretical physics.
using crystalline molecular magnets allowed researchers to build qubits out of multiple quantum particles, rather than a single quantum object - the way most proto- quantum computers are built at the moment.
"this will obviously increase signals from the qubit drastically, so the detection of the qubit in the molecular magnets is much easier," takahashi said.
takahashi conducted his research as a project scientist in the institute of terahertz science and technology and department of physics at the university of california santa barbara and analyzed the data while at ucsb and usc.
takahashi has been in the usc dornsife college since 2010.
research for the article was performed in collaboration with phil stamp and igor tupitsyn of the university of british columbia, johan van tol of florida state university, and david hendrickson of uc san diego.
quantum materials- a family that includes superfluids, superconductors, exotic magnets, ultracold atoms and recently- discovered 'topological insulators'- display on a large scale some of the remarkable quantum effects usually associated with microscopic and subatomic particles.
but, while quantum mechanics explains the behaviour of microscopic particles, applying quantum theory to larger systems is far more challenging.
"while the potential of quantum materials, such as superconductors, is undeniable, we need to fully grasp the underlying quantum physics at play in these systems to establish their true capabilities," says chris vale, an associate professor at the centre for quantum and optical science, who led the research.
"that's a big part of the motivation for what we do."
associate professor vale and his colleagues, including sascha hoinka and paul dyke, also at swinburne, developed a new way to explore the behaviour of this family of materials.
they detected when a 'fermi gas' of lithium atoms, a simple quantum material, entered a quantum 'superfluid' state.
new system checks theories against experiment
their system allows theories of superconductivity and related quantum effects to be precisely checked against experiment, to see whether the theories are accurate and how they could be refined.
the researchers' advance was based on the fact that quantum materials' special properties emerge when their constituent particles enter a synchronised state.
the zero- resistance flow of electrons through superconductors, for example, emerges when electrons can team up to form 'cooper pairs'.
the team's sophisticated experimental set- up allowed this co- ordinated quantum behaviour to be detected.
by fine- tuning the interaction of their lasers with the fermi gas, associate professor vale and his colleagues were for the first time able to detect the elusive, low energy goldstone mode, an excitation that only appears in systems that have entered a synchronised quantum state.
"because our experiment provides a well- controlled environment and the appearance of the goldstone mode is very clear, our measurements provide a benchmark that quantum theories can be tested against before they're applied to more complex systems like superconductors," associate professor vale says.
"by developing methods to understand large systems that behave quantum mechanically, we're building the knowledge base that will underpin future quantum- enabled technologies."
the team's research has been published in the online journal nature physics.
a sort of holy grail for physicists and information scientists is the quantum computer.
such a computer, operating on the highly complex principles of quantum mechanics, would be capable of performing specific calculations with capabilities far beyond even the most advanced modern supercomputers.
it could be used for breaking computer security codes as well as for incredibly detailed, data- heavy simulations of quantum systems.
it could be used for applying precise principles of physics to understanding the minute details of the interactions of molecules in biological systems.
it could also help physicists unravel some of the biggest mysteries of the workings of the universe by providing a way to possibly test quantum mechanics.
such a computer exists in theory, but it does not exist in practicality - yet - as it would need to operate with circuitry at the scale of single atoms, which is still a daunting challenge, even to state- of- the- art experimental quantum science.
to build a quantum computer, one needs to create and precisely control individual quantum memory units, called qubits, for information processing.
qubits are similar to the regular memory "bits" in current digital computers, but far more fragile, as they are microscopic constituents of matter and extremely difficult to separate from their environment while at the same time increasing their number to a practical- size quantum register.
in particular, qubits need to be created into sets with precise, nonlocal physical correlations, called entangled states.
olivier pfister, a professor of physics in the university of virginia's college of arts & sciences, has just published findings in the journal physical review letters demonstrating a breakthrough in the creation of massive numbers of entangled qubits, more precisely a multilevel variant thereof called qmodes.
entanglement dwells outside our day- to- day experience; imagine that two people, each tossing a coin on their own and keeping a record of the results, compared this data after a few coin tosses and found that they always had identical outcomes, even though each result, heads or tails, would still occur randomly from one toss to the next.
such correlations are now routinely observed between quantum systems in physics labs and form the operating core of a quantum computing processor.
pfister and researchers in his lab used sophisticated lasers to engineer 15 groups of four entangled qmodes each, for a total of 60 measurable qmodes, the most ever created.
they believe they may have created as many as 150 groups, or 600 qmodes, but could measure only 60 with the techniques they used.
each qmode is a sharply defined color of the electromagnetic field.
in lieu of a coin toss measurement, the qmode measurement outcomes are the number of quantum particles of light (photons) present in the field.
hundreds to thousands of qmodes would be needed to create a quantum computer, depending on the task.
"with this result, we hope to move from this multitude of small- size quantum processors to a single, massively entangled quantum processor, a prerequisite for any quantum computer," pfister said.
pfister's group used an exotic laser called an optical parametric oscillator, which emitted entangled quantum electromagnetic fields (the qmodes) over a rainbow of equally spaced colors called an "optical frequency comb."
ultrastable lasers emitting over an optical frequency comb have revolutionized the science of precision measurements, called metrology, and paved the way to multiple technological breakthroughs.
the inventors of the optical frequency comb, physicists john hall of the national institute of standards and technology and theodor hansch of the max- planck institute for quantum optics, were awarded half of the 2005 nobel prize in physics for their achievement.
(the other half went to roy glauber, one of the founding fathers of quantum optics.)
with their experiments, pfister's group completed a major step to confirm an earlier theoretical proof by pfister and his collaborators that the quantum version of the optical frequency comb could be used to create a quantum computer.
"some mathematical problems, such as factoring integers and solving the schrodinger equation to model quantum physical systems, can be extremely hard to solve," pfister said.
"in some cases the difficulty is exponential, meaning that computation time doubles for every finite increase of the size of the integer, or of the system."
however, he said, this only holds for classical computing.
quantum computing was discovered to hold the revolutionary promise of exponentially speeding up such tasks, thereby making them easy computations.
"this would have tremendous societal implications, such as making current data encryption methods obsolete, and also major scientific implications, by dramatically opening up the possibilities of first- principle calculations to extremely complex systems such as biological molecules," pfister said.
quantum computing can be summarized by qubit processing; computing with single elementary systems, such as atoms or monochromatic light waves, as memory units.
because qubits are inherently quantum systems, they obey the laws of quantum physics, which are more subtle than those of classical physics.
randomness plays a greater role in quantum evolution than in classical evolution, pfister said.
randomness is not an obstacle to deterministic predictions and control of quantum systems, but it does limit the way information can be encoded and read from qubits.
"as quantum information became better understood, these limits were circumvented by the use of entanglement, deterministic quantum correlations between systems that behave randomly, individually," he said.
"as far as we know, entanglement is actually the 'engine' of the exponential speed up in quantum computing."
physicists have experimentally demonstrated an optical system based on an unconventional class of quantum mechanical systems that could lead to the development of new quantum optical devices.
the system is called a "pt- symmetric quantum walk," since it consists of single photons that occupy a superposition of states, called quantum walks, that obey parity- time (pt) symmetry- the property in which a system's coordinates in space and time can have their signs reversed without inherently changing the system.
the physicists, led by peng xue at southeast university in nanjing, have published a paper on the pt- symmetric quantum walks in a recent issue of nature physics.
"we present an experimental work tying together three concepts- non- unitary quantum walks at a single- photon level, pt symmetry, and topological edge states originating from floquet topological phases," xue told phys.org.
"each of these three concepts has attracted much attention in the past years in the scientific community.
the interplay of these elements in our experimental system will no doubt give rise to rich physics."
the new results build on discoveries made over the past 20 years regarding a new class of quantum systems called non- hermitian hamiltonians that deviate from conventional quantum systems.
in general, the hamiltonian of a quantum system, which is a measure of its total energy, must have eigenvalues that are real rather than complex numbers, where the eigenvalues are associated with the physical properties of the quantum system.
for many decades, it was thought that hamiltonians must be mathematically described using hermitian operators, since hermitians always have real eigenvalues.
although being hermitian is sufficient for a hamiltonian to have real eigenvalues, in 1998 physicists discovered that hamiltonians can be non- hermitian and still have real eigenvalues, as long as they obey pt symmetry.
this discovery opened up a whole new class of quantum systems for physicists to explore.
currently, the study of pt- symmetric non- hermitian systems is an area of active research that holds promise for a variety of applications, particularly in the field of optics.
the new study contributes to this research by demonstrating single- photon pt- symmetric quantum walks.
previously, physicists have theoretically investigated these systems, but the new study marks the first experimental demonstration due to the challenges involved in amplifying single photons.
"a pt- symmetric quantum walk is a non- unitary extension of the unitary quantum walk, which is in turn a quantum mechanical version of the classical random walk," xue explained.
"just as pt- symmetric non- hermitian hamiltonians broaden the horizon of conventional quantum mechanics, a pt- symmetric quantum walk represents a new kind of quantum walk with unique features that are quite different from those of a unitary quantum walk."
this demonstration, in turn, led the researchers to experimentally demonstrate exotic properties called floquet topological properties in pt- symmetric quantum walks for the first time.
the scientists observed that floquet topological edge states arise between regions with different bulk topological properties, suggesting that these systems contain intriguing quantum phenomena awaiting further exploration.
floquet topological properties are characterized by a pair of topological numbers, and controlling these properties may lead to the development of new quantum optical devices.
"i think our work may lead to a new generation of synthetic pt- symmetric systems," xue said.
"in pt- symmetric classical systems, recent progress may lead to applications in optical switching, modulation, sensors, wireless power transfer, and so on.
while our experiment demonstrates floquet topological states (a special topological matter with time- periodic drives) driven by pt- symmetric quantum dynamics, it provides a new platform where the interplay of pt- symmetric quantum dynamics and topological properties not only offer a quantum mechanical version of pt- symmetric systems, but may also lead to potential applications in quantum information, quantum computation, and quantum sensing."
a quantum internet may very well be the first quantum information technology to become reality.
researchers at qutech in delft, the netherlands, today published a comprehensive guide toward this goal in science.
it describes six phases, starting with simple networks of qubits that could already enable secure quantum communications- a phase that could be reality in the near future.
the development ends with networks of fully quantum- connected quantum computers.
in each phase, new applications become available such as extremely accurate clock synchronization or integrating different telescopes on earth in one virtual super- telescope.
this work creates a common language that unites the highly interdisciplinary field of quantum networking toward achieving the dream of a world- wide quantum internet.
a quantum internet will revolutionize communication technology by exploiting phenomena from quantum physics such as entanglement.
researchers are working on technology that enables the transmission of quantum bits between any two points on earth.
such quantum bits can be zero and one at the same time, and can be entangled- their fates are merged in such a way that an operation on one of the qubits instantly affects the state of the other.
this brings two features which are provably out of reach for the internet that we know today.
the first is that entanglement allows improved coordination between distant sites.
this makes it extremely suitable for tasks such as clock synchronization or the linking of distant telescopes to obtain better images.
the second is that entanglement is inherently secure.
if two quantum bits are maximally entangled, then nothing else in the universe can have any share in that entanglement.
this feature makes entanglement uniquely suitable for applications that require security and privacy.
many other applications of a quantum internet are already known, and more are likely to be discovered as the first networks come online.
researchers at qutech, a collaboration between delft university of technology and the netherlands organisation for applied scientific research tno have now set forth stages of quantum internet development distinguished by technological capabilities and corresponding applications.
the lowest stage of a true quantum network- a prepare and measure network- allows the end- to- end delivery of quantum bits between any two network nodes, one quantum bit at a time.
this is already sufficient to support many cryptographic applications of a quantum network.
the highest stage is the long- term goal of connecting large quantum computers on which arbitrary quantum applications can be executed.
in addition to providing a guide to further development, the work sets challenges both to engineering efforts and to the development of applications.
"on the one hand, we would like to build ever more advanced stages of such at network", says stephanie wehner, lead author of the work, "on the other hand, quantum software developers are challenged to reduce the requirements of application protocols so they can be realized already with the more modest technological capabilities of a lower stage."
co- author ronald hanson adds: "this work establish a much- needed common language between the highly interdisciplinary field of quantum networking spanning physics, computer science and engineering."
the first true quantum networks, allowing the end- to- end transmission of quantum bits, are expected to be realized in the coming years, heralding the dawn of a large- scale quantum internet.
ibm scientists have achieved an important milestone toward creating sophisticated quantum devices that could become a key component of quantum computers.
as detailed in the peer- review journal nano letters, the scientists have shot an electron through a iii- v semiconductor nanowire integrated on silicon for the first time.
ibm scientists are driving multiple horizons in quantum computing, from the technology for the next decade based on superconducting qubits, towards novel quantum devices that could push the scaling limit of today's microwave technology down to the nanometer scale and that do not rely on superconducting components, opening a path towards room- temperature operation.
now, ibm scientists in zurich have made a crucial fundamental breakthrough in their paper ballistic one- dimensional inas nanowire cross- junction interconnects.
using their recently developed template- assisted- selective- epitaxy (tase) technique to build ballistic cross- directional quantum communication links, they pioneered devices which can coherently link multiple functional nanowires for the reliable transfer of quantum information across nanowire networks.
the nanowire acts as a perfect guide for the electrons, such that the full quantum information of the electron (energy, momentum, spin) can be transferred without losses.
by solving some major technical hurdles of controlling the size, shape, position and quality of iii- v semiconductors integrated on si, ballistic one- dimensional quantum transport has been demonstrated.
while the experiments are still on a very fundamental level, such nanowire devices may pave the way towards fault- tolerant, scalable electronic quantum computing in the future.
the paper's lead author, ibm scientist dr. johannes gooth, noted that the milestone has implications for the development of quantum computing.
by enabling fully ballistic connections where particles are in flight at the nanoscale, the quantum system offers exponentially larger computational space.
earlier this year, ibm launched an industry- first initiative to build commercially available universal quantum computing systems.
the planned "ibm q" quantum systems and services will be delivered via the ibm cloud platform and will deliver solutions to important problems where patterns cannot be seen by classical computers because the data doesn't exist and the possibilities needed to explore to get to the answer are too enormous to ever be processed by classical systems.
