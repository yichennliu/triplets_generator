{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative to this notebook\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "src = os.path.join(src_path,\"src\")\n",
    "if src not in sys.path:\n",
    "    sys.path.append(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from json file\n",
    "df = pd.read_csv('src_path/data/02_intermediate/Article20.csv')\n",
    "\n",
    "# join heading and text column to single column\n",
    "df.columns = ['date','heading','text','link','empty']\n",
    "df['all']= df['heading'] + df['text']\n",
    "\n",
    "df = df.drop(['date','heading','text','link','empty'], axis=1 )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed'] = df['all'].apply(lambda x: nlp(x))\n",
    "\n",
    "def is_token_allowed(token):\n",
    "     '''\n",
    "         Only allow valid tokens which are not stop words\n",
    "         and punctuation symbols.\n",
    "     '''\n",
    "     if (not token or not token.string.strip() or\n",
    "         token.is_stop or token.is_punct):\n",
    "         return False\n",
    "     return True\n",
    "\n",
    "def preprocess_token(token):\n",
    "     # Reduce token to its lowercase lemma form\n",
    "     return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "def cleaning(row):\n",
    "  #print(d)\n",
    "  \n",
    "  doc = df['processed'].values[0]\n",
    "  #doc = row['processed']         #  <-------------------------------- WHY\n",
    "\n",
    "  #for word in doc:#\n",
    "  filtered_tokens = [preprocess_token(token)\n",
    "       for token in doc if is_token_allowed(token)]\n",
    "  return filtered_tokens\n",
    "\n",
    "\n",
    "df['cleaned'] = df.applymap(lambda x: cleaning(x) )\n",
    "#df['cleaned'] = df['processed'].apply(cleaning), also try to use map for efficiency\n",
    "\n",
    "# Rejoining meaningful-stemmed to single snetence\n",
    "def rejoin_words(row):\n",
    "    my_list = row['cleaned']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "df['rejoined'] = df.apply(rejoin_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = []\n",
    "# Iterate through rows\n",
    "for index, row in df.iterrows():\n",
    "    text = row['cleaned']\n",
    "    word_freq = Counter(text)\n",
    "    freq_words.append(word_freq.most_common(5))\n",
    "\n",
    "print(freq_words[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article = []\n",
    "#for w in mydoc:\n",
    "  #if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "    #article.append(w.lemma_)\n",
    "\n",
    "#print(article)\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['all']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    # taken only words (not punctuation)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "df['words'] = df.apply(identify_tokens, axis=1)\n",
    "\n",
    "\n",
    "# WHOLE CLEAN UP\n",
    "def stem_list(row):\n",
    "    my_list = row['words']\n",
    "    stemmed_list = [stemming.stem(word) for word in my_list]\n",
    "    return (stemmed_list)\n",
    "\n",
    "df['stemmed_words'] = df.apply(stem_list, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['stemmed_words']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "df['stem_meaningful'] = df.apply(remove_stops, axis=1)\n",
    "\n",
    "\n",
    "# Rejoining meaningful-stemmed to single snetence\n",
    "def rejoin_words(row):\n",
    "    my_list = row['stem_meaningful']\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "df['processed'] = df.apply(rejoin_words, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_content = df.iloc[1]\n",
    "#text = example_content['stem_meaningful']\n",
    "\n",
    "#print(text)\n",
    "\n",
    "#word_freq = Counter(text)\n",
    "#common_words = word_freq.most_common(5)\n",
    "#print(common_words)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row['stem_meaningful'])\n",
    "\n",
    "list1 = []\n",
    "# Iterate through rows\n",
    "for index, row in df.iterrows():\n",
    "    text = row['stem_meaningful']\n",
    "    word_freq = Counter(text)\n",
    "    list1.append(word_freq.most_common(5))\n",
    "\n",
    "print(len(list1))\n",
    "print(list1[0])\n",
    "print(list1[0][0])\n",
    "print(list1[0][0][0])\n",
    "\n",
    "\n",
    "print(list1[1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "word, freq = [], []\n",
    "for item in list1:\n",
    "  print(item)\n",
    "  for itemm in item:\n",
    "    word.append(itemm[0])\n",
    "    freq.append(itemm[1])\n",
    "  \n",
    "plt.bar(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_text = ('Gus Proto is a Python developer currently'\n",
    "     'working for a London-based Fintech company. He is'\n",
    "    ' interested in learning Natural Language Processing.'\n",
    "     ' There is a developer conference happening on 21 July'\n",
    "     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "     ' Language Processing\". There is a helpline number '\n",
    "     ' available at +1-1234567891. Gus is helping organize it.'\n",
    "    ' He keeps organizing local Python meetups and several'\n",
    "     ' internal talks at his workplace. Gus is also presenting'\n",
    "     ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "     ' Apart from his work, he is very passionate about music.'\n",
    "    ' Gus is learning to play the Piano. He has enrolled '\n",
    "     ' himself in the weekend batch of Great Piano Academy.'\n",
    "    ' Great Piano Academy is situated in Mayfair or the City'\n",
    "    ' of London and has world-class piano instructors.')\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "if not token.is_stop and not token.is_punct] \n",
    "\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)\n",
    "\n",
    "# Unique words\n",
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitthesisvenv8cb23d135c564173b914a1a8389f3e64",
   "display_name": "Python 3.7.3 64-bit ('Thesis': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}